This paper studies self supervised learning for graph neural networks by proposing a framework called LaGraph. Both theoretical analysis and experimental evaluation are provided in the paper.

We acknowledge the merits of this paper, which include studying a relatively less explored topic, providing theoretical analysis and comparison with other methods, and requiring less memory than a strong baseline.

On the other hand, there are also outstanding concerns (even after the discussions) regarding the novelty and significance of the proposed method (despite the claims of the authors during the discussions), whether the performance improvement over strong baselines is significant across different datasets, and missing a more comprehensive ablation study (beyond the preliminary results provided during the discussion period), among others.

In its current form, this is certainly a borderline paper for a top conference such as ICLR. It would be a better paper if the outstanding concerns could also be addressed before publication.
This paper proposes several innovations for machine translation. The reviewers had several questions about the claims that were made and the authors addressed these and also acknowledged that some of their formulations (e.g.  better ) would need to be qualified. Overall, there are several interesting ideas that have been put together in a sensible way, but the story is not super consistent.

The detailed exchanges between the reviewers are authors are commendable!
This paper explores a classification approach based on labeling pairs of inputs concurrently using a single network, rather than singletons. The authors test the approach on adversarial robustness (towards norm bounded perturbations), OOD detection next to basic standard accuracy calculations.

While the key idea is potentially interesting and the paper has received positive comments from the majority of reviewers, there were also some concerns that need to be addressed in a final manuscript:

* The paper does not motivate or explain theoretically why the joint classification framework is superior, beyond verbose arguments. These
arguments need to be better clarified and linked with the experimental evaluation.

* While the empirical results are perceived as positive by the reviewers, one reviewer has raised the concern about the comparisons. The adversarial robustness and OOD comparisons are indeed basic. The adversarial attack used here is quite a weak PGD attack with a small radius and low iteration budget. Possibly include stronger attacks. The OOD comparisons are with standard baselines only. Please include further comparisons.  

In its current form, the paper seems to be acceptable, and I strongly encourage the authors to improve both the theoretical justification, and empirical exploration in the final version.
Summary: The paper discusses Markov games with general function approximation, and investigates in particular reinforcement learning algorithms that learn a Nash policy in a trial and error fashion. They consider two settings: the decoupled one where the player does not observe the opponent’s policy and the coordinated one where optimistic planning is performed for both. The main contribution is a new complexity measure called Minimax Eluder dimension which is used to control the regret of the proposed algorithms. 

Discussions: The reviewers raised many minor concerns regarding the writing (typos of missing notation) and the clarity (missing discussions and explanations), which were addressed during the discussion phase. In light of this revision, the committee and myself judge that the paper should be accepted to ICLR.

Decision: Accept
The paper proposes a general framework to reason about fine grained distribution shifts, evaluating a large set of different approaches in a variety of settings. All reviewers recommend acceptance. While concerns were raised, including questions about the generality of the framework, unsurprising “tips”, and unclear take home messages, all reviewers find the work strong, with an elegant formulation, and useful insights. The AC agrees with the reviewers that this work addresses a very important problem, proposes an interesting unified framework and benchmark for domain shift analysis, and should be a valuable tool for the community to pursue further research in this area.
The paper gives a method for generating contrastive explanations, in terms of user specified concepts, for an agent in a sequential decision making setting. 

The reviewers found the paper to be a strong contribution to explainable AI and RL. There were some concerns about the writing, but the revisions have addressed most of these. 

Overall, I am delighted to recommend acceptance. I urge the authors to incorporate the feedback in the reviews in the final version.
At this time, this work is not yet ready for publication. The core idea influence functions was poorly explained in the initial submission, and although major changes to the paper were made to rectify this, at least some reviewers of the remain unconvinced and it is unclear that the paper has been fully evaluated with this confusion resolved. There are a sufficient number of other concerns around the paper, that having rectified these more fully and outside the tight time constraints of the rebuttal period, I hope for an interesting resubmission in future.
A new method for dynamic token normalization in ViTs (both within and across tokens) is introduced in the paper. As noted by the reviewers, the proposed method is technically sound, with a clear and solid motivation. The main raised concerns included the lack of experiments using larger models, unclear reason for the accuracy gains, and lack of experiments on other tasks beyond classification, such as detection and segmentation. The authors’ response was strong, clarifying other questions and providing additional experiments, for example, showing the effectiveness of the method on object detection, and when applied to larger models or architectures that explicitly model local context. Two reviewers recommend borderline rejection, but they did not participate in the discussion nor updated their reviews after the author response. The AC considers that their concerns were adequately addressed by the rebuttal, and agrees with the other two reviewers that the paper passes the acceptance bar of ICLR. The authors should carefully proofread the paper for the final version.
This paper presents a self supervised learning method for the multi modal setting where each modality has its own feature extraction mapping, and i) the extracted features shall be close for paired data,  ii) in the feature space each view has close to diagonal covariance, while iii) the scale for each feature dimension is constrained away from zero to avoid trivial features. The presentation is clear and the reviewers do not have major confusion on the methodology. There have been some discussions between the authors and reviewers, and most questions on the empirical study have been addressed by the authors with additional experiments. The remaining concern is on the novelty (difference from prior SSL methods especially Barlow Twins) and significance.  I think that while it is relatively straightforward to extend methods like Barlow twins to the multi modal setting, I do see the value of empirically demonstrating the effectiveness of an alternative loss to the currently pervasive contrastive learning paradigm, and hence the paper is worth discussion in my opinion. In the end, the method resembles classical multi modal methods like canonical correlation analysis, in terms of the objective (matching paired data in latent space) and constraints (un correlated feature in each view, and unit scale constraint for each feature dimension); such connections shall be discussed.
This paper studies the problem of dynamically selecting samples to replay given that all previous data is stored. The paper shows that in this setting, selecting which samples to replay outperforms several baselines over a variety of datasets. 

I believe that the reviewers understood this work, but their initial opinions were quite mixed. 

Two of the reviewers did not "accept" this setting (all past data stored and accessible) as a reasonable one for continual learning. The discussion did not lead to a reconciliation. 

I found truth in both views. On one side, I can believe that the proposed setting has applications (recommender systems where historical data is kept seems like a reasonable one). I also find the approach reasonable since "compute" is often the bottleneck and not memory/storage. On the other, I also see that this is specializing the CL problem a bit and so, while immediately useful, may or may not help to improve more general continual learning approaches. This is highly speculative. Another argument against this setting is that it is not absolutely clear that in this setting CL approaches are necessarily required. This really depends on the specifics of the problems.

Several of the questions and weaknesses discussed by other reviewers were also discussed and addressed by the reviewers. 

Overall, the final score from the reviewers makes this a very borderline paper. Further, even amongst the positive reviewers, one provides an overall recommendation of a 6 (marginally above the acceptance threshold). In the end, the paper was in the category of papers that were examined closely for possible acceptance, but the broad view of the area chair and the reviewers was that the paper could benefit from additional work before publication.
This paper presents a reinforcement learning architecture that uses an auxiliary k step step loss in the context of continuous control from image based states.

While the topic is relevant and potentially impactful, several reviewers have major concerns about the manuscript. Among these, I highlight:
  Reviewers J6YX, 38iT and Qru8 have concerns about the novelty and contribution of the approach compared to existing literature.
  Reviewers J6YX, TKuY, 38iT and Qru8 have concerns about the experimental evaluation and the quality of comparisons to baselines.

Overall, it seems that the paper would benefit from further polishing.
To solve imbalance classification problem, this paper proposes a method to learn example weights together with the parameters of a neural network. The authors proposed a novel mechanism of learning with a constraint, which allows accurate training of the weights and model at the same time. Then they combined this new learning mechanism and the method by Hu et al. (2019), and demonstrated its usefulness in extensive experiments.

I would like to thank the authors for their detailed feedback to the initial reviews, which clarified most of the unclear points in the manuscript. Overall the paper is well written and the effectiveness was demonstrated in experiments. Since the contribution is valuable to ICLR2022, I suggest its acceptance.
The paper extends the FNP model to multimodal settings using the mixture of graphs. However, there are legitimate concerns about the quality of experiments, such as baselines, as the reviewers mention. For example, mRNP is supervised, and comparison to DeepIMV is not fair. I encourage the authors to address them appropriately in the next version of the paper. 

The authors can significantly improve the presentation of ideas. Please avoid making hyperbole and excessively bold statements, as the reviewers have pointed out. This way, there will be room for a better demonstration of the novel parts of the paper. For example, the authors misuse the term "generative" for the proposed mRNP. There are multiple hand waving statements about the role of uncertainty that are not well supported in the current draft. I believe this paper can be a good paper by addressing the reviewers  comments.
This submission has been evaluated by 5 reviewers with 3 leaning towards borderline accept and 2 leaning towards borderline reject. Reviewers have been consistently concerned about several aspects of this work, i.e. that *the method is only demonstrated on toy datasets*, that there is an issue with the scalability to larger substructures, that the proposed approach did not excel *in the simple task of triangle counting* or even that *the authors did not perform any other experiments even on a toy dataset*, and that comparisons on Deep LRP re. efficiency were not provided, and *more complex settings and sensitivity* were not investigated. Reviewers also noted that the general idea of recursion did already appear in GNNs in one or another setting.

In making this decision, AC agrees that there is some potential in the proposed analysis and reviewers also highlighted this as a positive side of the submission. Yet, it is really hard to overlook at the same time the rebuttal where authors had the chance to address all reviewers comments regarding the experiments, their various details, and their variations.

Failing to address these comments to the satisfaction of the majority of reviewers makes it impossible for AC to recommend the acceptance even tough there is every chance that the paper will ultimately make it to a high quality venue after a thorough revision (reviewers have really given a fair number of good suggestions that should assist authors).
The submission proposes "feature flow regularization" during training to enforce (approximately) sparse network weights which can then be post hoc pruned.  The form of the regularizer is reasonably well motivated, and the method seems interesting.  The reviewers were split on this, with two recommendations for "marginally above" and one for "marginally below" the threshold of acceptance.  I therefore read the paper in detail, in addition to reading the reviews, rebuttal, and private reviewer comments.  The appendix on the sparsity accuracy tradeoff and its relationship to the hyperparameters k1 and k2 is an interesting experiment, and overall the authors were very engaged in the reviewing process.

In an initial reading, the term trajectory is indeed vague, although it is presented as a definition.  This ambiguity is reflected in the reviewer discussion where in response to Reviewer wyu7, the authors indicate that there are two different meanings of the word in different papers that are being confused.  In a mature presentation, these definitions should probably be given mathematically early on in a formal definition box, but this would require significantly tightening up the mathematical notation early on.

The results table does not show that the proposed method Pareto dominates other methods (accuracy, sparsity, and latency), which themselves are necessarily limited due to the very high number of published papers on network pruning.  Furthermore, some of the selected comparisons appear to be optimizing for different metrics rather than network sparsity, e.g. DCP reports better accuracy for VGG 16 after pruning is applied.  This indicates that the proposed method is somehow in the crowd, but does not seem to show a clear consistent improvement over SOTA.

Analysis in Appendix A.3 does not really depend on which kind of norm is used   the same conclusion will be reached that ||X|| decreases, while structured sparsity, e.g. with expected sparsity rates, is dependent on the kind of norm.  As such, it s OK, but not a particularly specific result.

On the whole, this indicates that the paper is interesting, but borderline with room for concrete improvements that go beyond the scope of a simple refinement for a camera ready presentation.
The authors provide an investigation into tuning learning rate schedules. The problem is certainly of great practical importance. After discussion, the reviewers felt the main idea of the paper is worth pursuing, but could use significant refinement. One reviewer suggests: "
"...better treatment of the background material, clearer identification on when the weight norm behaviour happens beside norm (possibly looking also for counter examples!), rethinking section 6, and a more convincing set of experiments (for showing convincing evidence about e.g. 5.2). Regarding this last point, I want to clarify that in my review I mentioned [1] not for the grid search, but rather for the time controlled experiments. If you go with random search for selecting the hyperparameters of the learning rate adaptation methods. I personally think that a recipe to make the comparison fair enough is to choose a prior distribution (e.g. uniform/log uniform) that covers reasonable values (e.g. as used for different datasets) with mean equal/close to the known well performing ("optimal") value." Other reviewers were generally of a similar opinion. The authors are encouraged to continue with the work, taking reviewer comments into account for updated versions.
The paper describes a genetic algorithm for molecular optimization under constraints. The aim is to generate molecules with better properties while close to an initial lead molecule. The proposed approach is a two stage one. The first stage aims to satisfy constraints and searches for feasible molecules that are similar to the lead. The second stage optimizes the molecular property. The method is evaluated on logP optimization task, with minor improvement over previous work.

The reviewers point out the following strengths and weaknesses:

Strengths:

  Molecular optimization under structural constraints is an important research direction.
  Comprehensive related work section.

Weaknesses:

  Lack of novelty because it is a standard application of genetic algorithm.
  The results show that the proposed method did not outperform existing baselines.
  The main claim of the paper (benefit of two stage procedure) is not supported by ablation study.
  The authors only conduct experiments on improving LogP, which is a benchmark that is too easy and not challenging.
  The objective function and cross over operation are the same or very similar to previous work.
  The experimental evaluation is limited, and the overall setting is not very relevant to real world tasks.

Overall, all reviewers vote for rejection. It is clear that the paper needs more work before it can be published.
The SketchODE submission is a continuously valued model for chirographic drawing data such as handwritten digits or sketches. It relies on variational sequence to sequence model where the latent code z is a global encoding of the drawing dynamical, and contains a neural controlled differential equation encoder to encode a discrete 2D drawing sequence s, and an augmented neural ODE decoder (conditional on the latent code z) to model both the first order dynamics both of the drawing velocity and of the pen state (effectively modelling second order dynamics on the pen position). The model enables to sample sketches by sampling latent codes, as well as to interpolate between two latent codes, and is evaluated on VectorMNIST (a new task), QuickDraw sketches, and DiDi schematics, where it is compared to discrete RNN based Seq2Seq and two more recent baselines.

Reviewers praised the idea of using continuously valued Neural ODEs for drawing, compelling properties of the model for conditional generation or interpolation, the new VectorMNIST dataset, and the writing. Reviewers had some concerns: overstating the novelty and contribution to general continuous seq2seq given that the evaluation was done only on chirographic drawing tasks (Q3GY, zrrF), some experimental details such as missing ablations, examples from QuickDraw or Didi, or comparisons with transformers (Q3GY, zrrF), clarifications on computational complexity (zrrF, S7jh), situating the work with respect to applications of Neural ODEs to physics (zrrF); most of these concerns were addressed in the rebuttal. Reviewer KvGm had the most concerns about the experimental section, but has increased their score after the discussion with the authors.

There was no discussion among the reviewers, only between the authors and reviewers zrrF and KvGm. After the authors  rebuttal, the scores became 8. 8, 6 and 5, and thus I believe that the paper meets the conference acceptance bar.
The paper develops an unrolled version of the PALM algorithm for sparse blind (or semi blind) source separation. The unrolled version includes a soft thresholding update, in which the thresholding parameter and one of the weight matrices is learned from data, with a least squares dictionary update, in which the step size is learned from data. The paper provides experimental results showing that this LPALM algorithm is less sensitive to the choice of hyper parameters (since step sizes, etc. are learned from data), and to the choice of the initial dictionary (perhaps since the W matrices are learned from similar examples). It also improves over PALM on experimental data from an astronomy problem. 

Reviewers expressed appreciation for the paper’s experimental results, and detailed investigation of the parameterization of unrolled PALM. They also highlighted some issues in the initial submission s exposition   in particular, the setting of the problem (what kind of training data is available, what is the relationship between the mixing matrices A at training and at test time), and a clearer explanation of why it makes sense to learn fixed matrices W^{(k)} which do not depend on A (given that A may change at test time). The revision improved the clarity of the paper, addressing most of these concerns. The submission contributes to the discussion on how to unroll dictionary learning / blind source separation algorithms, how the unrolled algorithm should be parameterized, and demonstrates good results on multispectral data analysis.
This paper explores the memorization of tokens in prior context in LSTM and Transformer based language models. While all reviewers agree this is an interesting and important direction worth studying, they raise several concerns about the validity of the experimental setup and the conclusions drawn about LSTMs. Primarily the shallow depth of the LSTM architecture seems to confound the main conclusion about their inferiority (Reviewer WHFY). Further exploration about what makes transformers better (e.g. attention) is also important to provide a more complete picture (Reviewer ax86). Other concerns include the use of synthetic data (Reviewer tpb6), a limited number of noun lists (Reviewer r2TC) and the lack of discussion about the practical significance of verbatim recall (Reviewer M2A9). Overall, while the paper takes a step towards an important insight about pretained LMs, it needs to be polished further and hopefully can be published at a future conference.
This paper introduces a new method for fine tuning large language models, which is lightweight since it only adds a small amount of parameters, while keeping the original parameters frozen. The main idea is to add a low rank matrix which is learned during fine tuning to the original weight matrices of the model, which are frozen. The reviewers agreed that the method is simple, original and well motivated. Moreover, it compares well compared to other fine tuning baselines, such as adaptors or full fine tuning. For these reasons, I recommend to accept this work to the ICLR conference.
This paper presents a novel framing of what s at stake when selecting/segmenting text for use in language model pretraining. Four reviewers with experience working with these models agreed that the conceptual and theoretical work here is insightful and worth sharing. The empirical work is fairly small scale and does not yet support broad conclusions, but reviewers did not see such conclusions as necessary for the paper to be valuable.
This work proposes a so called self supervised approach for few shot learning. The self supervision doesn t refer to the lack of use of any labels as in regular self supervised embedding learning methods (here support sets are labelled), but refer to the fact that the query set s labels aren t used in their proposed objective. Instead the query labels are predicted by a primary network, which then uses these predicted labels to predict the ground truth labels on the support set. The support set label predictions can thus be used to derive a learning signal for the model. Some results are presented that suggest the method is competitive with respect to the state of the art.

Reviewers are quite split on this work. Even the reviewers who are technically leaning toward accepting this work (rating of 6) mention concerns that are worrying, e.g. reviewer Gmcb and wTnW both mention concerns related to the fairness of the evaluation. 

I too share similar concerns. First, the method in question is effectively a transductive method (as opposed to inductive, as are many of the baselines this work compares to), a distinction that the paper does not make explicit or address directly. This distinction is important, as it is well known that transductive methods have an advantage over inductive methods. I did try to look for some published transductive baselines. One is the method of Zhang et al. (2021a), which the authors do beat on mini ImageNet, but not on CUB (and in fact, the paper only reports the results from Zhang et al. on mini ImageNet, even though the original paper actually reports results on CUB, which I find odd). On CUB, for 1 shot, Zhang et al. (2021a) outperforms SPDN, while for 5 shot SPDN does only very slightly better. The paper is not clear as to whether the compared baselines are transductive or not in the cross domain experiments either. 

Second, by introducing a dual network that is separate from the primal network, the proposed model effectively is increasing the capacity of their model, relative to using only a primal network. This capacity is mostly used when performing the self supervised optimization of the query labels, which would explain why this aspect of the proposed method is what yields the largest improvements. Given that capacity has a large effect on the performance of methods on few shot learning benchmarks, I m quite concerned that this is the more likely explanation for the (sometimes surprisingly large) improved performance.

That said, I don t find the paper entirely without merit. The label optimization procedure is neat, and is probably the most interesting innovation of the paper. The use of a primal dual architecture on the other hand is more incremental, e.g. relative to architectures used in semi supervised few shot learning.

Overall, at this point, given the lukewarm evaluation by reviewers and the lingering concerns (or at a minimum, lack of clarity) on the fairness of the evaluation, I m afraid I m not comfortable to recommend accepting this work as it currently stands.
This paper considers transferability measures both in the supervised and unsupervised domain. It identifies instabilities in the way that H score is computed and proposes to correct the issue with a shrinkage based covariance estimations. The proposed fix results in 80% absolute gain over the original H score and makes it competitive with state of the art LogME metric. The new shrinkage based H score is much faster to compute.

Reviewers agree that the paper makes interesting and important contributions. In particular, the reviewers appreciate that the paper takes a deeper look at existing metrics and propose valuable fixes instead of proposing yet another new metric. The paper demonstrates depth of statistic knowledge and proposes shrinkage operators to estimate high dimensional covariance. 

There are a few shortcomings of the paper, however, that suggests that the paper can benefit of another round of improvement. In particular, the paper is very dense with little motivation. Some of the choices in the paper can be motivated better. For instance, the hypothesis of lack of robustness in estimating H score is not demonstrated empirically. The reviewers also felt that the paper should extend experiments to other domains beyond image.
The authors attempt to tackle the problem of compositional generalization, i.e., the problem of generalizing to 
novel combinations of familiar words or structures. The authors propose a transfer learning strategy based on
pretraining language models. The idea is to introduce a pre finetuning task where a model is first trained on compositional train test splits from other datasets, before transferring to fine tuning on the training data from the target dataset. Although the technique
brings some improvements, and the authors do their best the address the reviewers  questions, it is still unclear:

a) Why the method should work in principle, whether there is a theoretical backing and how it formally relates to meta learning
b) How the approach compares to data augmentation methods since pre finetuning requires more data, albeit from a different
dataset. See for example: https://openreview.net/forum?id PS3IMnScugk
c) The whole approach would be more convincing if the authors could articulate *how* their method renders a model
more robust to distribution shifts (e.g., based on GOGS results it does not help structural generalization, do the gains 
come from lexical generalization?)
d) it would also be interesting whether this method works on larger scale or more realistic datsets like CFQ, ATIS or machine translation
https://arxiv.org/pdf/1912.09713.pdf
https://arxiv.org/abs/2010.11818
The paper shows that deep convolutional neural networks in the kernel regime restructure the eigenspaces of the inducing kernels, which leads to some insights regarding the range of space frequency combinations learned by such networks.

The reviewers identified a number of problems with the current submission. For instance, they found that the paper is hard to follow, it lacks clarity and the theorem statements are hard to understand. The authors also use a somewhat non standard experimental setup.

Despite an extensive discussion with the authors which cleared out a few minor problems, the bulk of the concerns of the reviewers were not successfully adressed. I am therefore not able to recommend acceptance. The authors need to improve the clarity of the paper and provide more discussions of the theorems in a resubmission, as well as potentially reconsider their experimental setup.
This paper proposes a  variational video prediction model FitVid and attains a better fit to video prediction datasets.  The draft was reviewed by four experts in the field and received mixed scores (1 borderline accept, 3 reject). The reviewers raised their concerns on lack of novelty, unconvincing experiment, and the presentation of this paper. For a video prediction model, fitting a dataset is quite important. But AC agrees with the reviewer jPAY. It will be more exciting to build a causal model of the world and enable it to perform future and counterfactual prediction (e.g,  CLEVRER).  The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
This work proposes an approach to encourage within layer diversity in neuron activations, and derive a generalization bound meant to motivate their approach.

None of the reviewers support the acceptance of this work, despite the authors  detailed rebuttals, with the majority of reviewers confirming their preference for rejection following the author response. Many raised concerns regarding the value of the accompanying theory. The empirical results demonstrated by the proposed regularizer were also not judged to be sufficiently compelling to compensate for the shortfall on the theory side.

I unfortunately could not find a good reason to dissent from the reviewers majority opinion, and therefore also recommend rejection at this time.
There was a consensus among the reviewers to reject the paper. While they noted that the paper proposed a new interesting stochastic algorithm for deep learning, they think the paper needs to be substantially improved in both theory and empirical study. The paper was judged quite incremental in comparison to the work of Öztoprak et al 2018 (where most of the theory was developed), while not showing improved empirical performance on the benchmarks.
This paper proposes a framework for few shot font generation. Reviewers thought that the model was well suited to the task. They were split on clarity, with some saying that the paper was easy to follow and others saying that it lacked sufficient detail. Overall reviewers found the technical novelty limited, saying that the approach was a “transformer variant”, while it was the first to apply a Transformer like model on few shot font generation, there wasn’t sufficient novelty in this task to have broad appeal to the ICLR community. The reviewers also pointed out some deficiency in the evaluation, concerning the chosen metrics (multiple reviewers requesting fidelity metrics) and missing baselines. Some reviewers posed questions to the authors but the authors did not respond to the reviews. There is a clear consensus to reject the paper.
This paper develops a deep convolutional network with RNN layers and 
a new data augmentation method for EEG motor imagery classification.

Reviewers agreed that the paper was not very clearly written, and that
without comparisons to other related methods or at least demonstration 
of the importance of each of the components of the model (through for 
example ablation analyses), it was hard to understand the generality
of the approach.

The authors did not respond to the reviews, so I am recommending not  
accepting this paper.
This submission proposes a method for steganography, i.e. hiding "secret messages" in images. Specifically, the proposed approach implements a procedure similar to adversarial example generation, where a perturbation is found that a) is imperceptible and b) can be decoded by a fixed decoder. This approach results in the ability to hide a significant amount of information (up to 3 bpp) with essentially no decoding errors. While the resulting perturbations are sometimes detectable by existing methods, the authors nevertheless provide a compelling case that their approach is a significant step forward and demonstrate an interesting new application to face hiding. Reviewers generally found the paper easy to follow and clear and felt the proposed approach provided a new and effective way to perform steganography. However, there were many requests for clarification of the work s situation in the larger literature of steganography and adversarial ML. The authors have clarified their contribution in the rebuttal, and as long as these changes have made it back into the paper, I am happy to recommend acceptance.
Reviewer rRp9 expressed concerns regarding the theoretical results included in Appendix A. In the discussion (not visible to the authors), the AC and Reviewer zn4a agree that the exposition in the original manuscript was confusing and could lead readers to assume these results were valid for the proposed algorithm. Also, in the original manuscript the presentation of the theoretical results in the appendix was quite poor (e.g. Proposition A.1). Having said that, the contributions and main points of the work are not affected by these observations as it is mainly an empirical study.

Following from the previous point, Reviewers rRp9 and zn4a pointed out that the overall presentation of the method, particularly the mathematical presentation could be improved. 

Reviewer zn4a points out that the method is not particularly novel, this was also indicated as a weakness by Reviewer iyVU. The main contributions of the work are to simultaneously solve the tensor factorization and vector quantization problems usinga form of projected gradient descent (with hard thresholding). While the empirical results seem promising, are somewhat limited. The authors could make them stronger by studying other applications on top of image classification (e.g. semi supervised setting, object detection or segmentation).

In the discussion (not visible to the authors), Reviewer iyVU stated in light of the other reviews, he/she does not oppose rejecting the work.

Overall, the method is technically sound and produces promising results. In its current form, however, the paper is not yet ready for publication. The AC encourages the authors to incorporate the feedback and resubmit the work to a different venue.
The authors present a method to learn representations of 3D atomic structure. They consider two cases: "small" and "large" molecules based on a metric that takes the spatial extension and number of atoms in the molecule into account. Small molecules are represented by an interatomic distance map. Large molecules are represented by a "sinusoidal function based absolute position encoding method". Both settings make use of a transformer architecture on top of the initial representation. The authors also introduce a subsampling step to select a subset of points/atoms and aggregate information from these.  Experimental results are shown for datasets relating to small molecule property prediction, protein ligand binding and a dataset from material science on metalorganic compounds.

Strengths:

  interesting modification of transformer architecture dedicated to the chemical compound.

Weaknesses: 

  Poor presentation of methods with respect to prior work
  Limited technical novelty. The distance map representation for small molecules and the sinusoidal function based absolute position encoding method for larger molecules have previously been proposed. Many components are built upon the design of PointNet++(Qi et al., 2017b) without significant modifications. The proposed "3D Transformer" is very similar to an attention based PointNet++ that is specially designed for molecular data.
  Experiments are applied only on classification tasks.

All reviewers voted for rejection. I recommend the authors to address the limitations listed above by improving the presentation with respect to prior work, clarifying more the novelty of the methods and including a more diverse range of experiments.
This paper introduces the "reversible instance normalization" (RevIN), a method for addressing temporal distribution shift in time series forecasting. RevIN consists in normalizing (subtracting the mean and dividing by the standard deviation) each layer of of deep neural network in a given temporal window for a given instance, and de normalizing by introducing learnable shifting and scaling parameters. 

The paper initially received one weak accept and two weak reject recommendations. The main limitations pointed out by reviewers relate to the limited novelty of the approach, the positioning with window normalization methods and hybrid methods in times series, and clarifications on experiments. The authors  rebuttal did a good job in answering the main concerns: rV5fo increased its grade from weak reject to clear accept, and RuPmn maintained its weak acceptance recommendation.

The AC carefully read the submission. The AC considers that the idea is simple yet meaningful. The large set of experiments are well conducted and conclusive. The rebuttal successfully answers to relevant issues raised by reviewers, regarding ablation studies (for highlighting the importance of the learnable de normalization), the impact of the temporal window, the comparison to hybrid approaches and the difference with respect to Adaptive normalization. The AC thus acknowledge that this submission draws important take home messages for the community, and therefore recommends acceptance.
This paper proposes a new regularizer, based on entropy maximization of samples near the decision boundary, to improve the calibration of neural networks while maintaining their accuracy. 

The method seems simple, sufficiently novel, and has promising results. However, based on the review process (described below), I feel the paper needs to significantly improve its evaluation and presentation before it can be accepted.  

The review process summary:

* Two reviews were eventually weakly positive about the paper: without major concerns, but not enthusiastic.

* One review (L8Yz) was not sufficiently informative. 

* One review (ESue) raised many points. I disagreed with most of these points, following the authors  discussion. However, a few points seemed valid, such as the not so impressive performance for OOD detection, which the authors did not address.

* I therefore asked for an additional review (iva2). The review concluded the paper is interesting and potentially useful, but requires another round of revision before it can be accepted, mainly because of its clarity and missing comparisons. I agree with these conclusions.
Thanks for your submission to ICLR.

This paper presents an extension to prototypical networks based on using hyperspheres to represent the prototypes.  Strong empirical results are presented using this approach.

Overall, this is a very borderline paper and could go either way.  The idea itself it simple, though the results seem to be fairly strong.  I read through the paper myself and tend to think that it could use a bit more work before it s ready.  Some of the issues raised by the reviewers particularly with respect to experiments and literature review are worth nailing down.  Further, I think that the method could be explored in a more principled/theoretical way.  For instance, when reading this idea, the first thing that pops into my mind is that representing the prototype with a hypersphere is very similar to representing a distribution (e.g., a Gaussian) using a mean and covariance (in this case, a spherical covariance).  Indeed, if you take the KL divergence between two spherical Gaussians, you get something very similar to the expression used in the paper.  This is all to say that there may be other more general directions to take this idea, or other interpretations of what is going on.

Please do keep in mind the comments of the reviewers when preparing a future version of the manuscript.
This paper considers the problem of active learning (AL) with data drawn from multiple domains. This framing motivates integrating work on domain shift detection and adaptation into standard AL approaches. 

The reviewers agreed that the work reports a robust set of experiments, which is a clear strength. However, they also raised key concerns, namely: (i) The heterogeneous setting considered is not particularly well motivated; (ii) The technical contributions of this work are limited. The latter would not be a major issue if the empirical evaluation addressed a clear open question (since this would constitute a useful contribution in and of itself), but the empirical contribution is somewhat limited given the unique setting considered and the relevant prior work (some of which seems to have been overlooked by the authors).
The paper presents a quite rigorous analysis of approximate implicit differentiation with warm starts applied to strongly convex upper level/strongly convex lower level and nonconvex upper level/strongly convex lower level bilevel optimization algorithms in a very general yet also very practical framework. They allow for stochastic errors in the algorithms solving the upper and lower level problems, making their work practical and applicable to real problems in machine learning (hyperparameter optimization), while analyzing in a way that is agnostic towards which algorithms are specifically used for the lower and upper level problems.

Three out of four reviewers were rather positive of the paper (scores: 6, 6, 8). One reviewer was very negative (score: 3). To my knowledge, the authors have convincingly answered all the points raised by the reviewer. Unfortunately, the reviewer did not follow up.

Similarly to reviewers, I found sections 1 3 to be extremely well written and to give a nice overview of the field. Section 4 had slight clarity issues (dense notation) that were addressed in the revision. Reviewer 6zLQ partially proof read proofs.

Overall, I recommend acceptance as a poster, as this paper is advancing stochastic implicit differentiation and should be of interest to many at the ICLR conference.
The paper uses neural networks for system identification.  The novelty of its contributions seems to be marginal, and the demonstration of its usefulness is not experimentally validated well enough.
This paper is good but at a borderline. One reviewer increased the score during the discussions. However, no reviewer was in strong favor. So that this paper is still a borderline one, and it is up to the SAC to decide.
The paper addresses the problem of non convex non concave min max optimization under the perspective of application of smoothed algorithms between two opponents.
The paper examines a model where the max player applied a zero memory smooth (from differential perspective) algorithm and min player SGD/SNAG or proximal methods providing results similar with the state of art. Convergence guarantees proposed were sound and experimental results on generative adversarial networks and adversarial training demonstrate the efficiency of the proposed algorithms.
This paper presents a tensor diagram view of the multi headed self attention (MHSA) mechanism used in Transformer architectures, and by modifying the tensor diagram, introduces a strict generalization of MHSA called the Tucker head self attention (THSA) mechanism. While there is some concern regarding the incremental nature of the proposition, the identification of where to usefully add the additional parameter that converts from MHSA to THSA was nontrivial, and the experimental results on the performance benefits across multiple tasks is convincing.
The submission considers a new acquisition function for active learning. The method considers the sensitivity of the prediction for a given datapoint with respect to parameter perturbations. Points with the largest variance under these perturbations are selected for labelling.  The method is simple and the empirical results are reasonable. Some weaknesses are the clarity of writing, and somewhat limited experimental comparisons.

The discussion was useful and helped improve the clarity. Additional experiments also helped improve the paper, although some reviewers still felt the experimental comparisons were lacking, including using entropy as a baseline acquisition function. Despite these improved scores, the overall average score remains below threshold I m afraid.

I feel this is a useful paper, but perhaps needs a little more polishing in the writing and some additional experiments. As such, it just falls short of the acceptance threshold.
The paper proposes a simple method for uniform sampling from generative manifold using change of variables formula. The method works by first sampling a much larger number of samples (N) from uniform distribution in the latent space and then does sampling by replacement (using probability proportional to change in volume) to generate a smaller number of final samples (k << N) that are seen as approximately sampled from a uniform distribution from the generative manifold. 

Reviewers had some questions/concerns about the confusing language in the abstract and introduction around the use of the term "uniform" which the authors have addressed satisfactorily. Authors have also provided results on quality (FID metric) of the generated samples as asked by the reviewers. 

While the proposed method is rather simple, has high computational cost, and novelty is marginal (as noted by two of the reviewers), reviewers agree it is above the acceptance bar.
The paper studies  neural architecture search for hyper relational knowledge graphs (HKGs).  A  search space is put forth, and  it is  searched with a differentiable search algorithm. The paper is technically strong. However,  there are some concerns about the narrow scope of the problem/solution, given that other more general formulations have also been studied.
PAPER: This paper proposes a method to learn joint representations from potentially missing data when (1) cross generation may be difficult, and/or (2) with large number of modalities. This is achieved by minimizing the divergence between a surrogate joint posterior and inferences from arbitrary subsets.
DISCUSSION: The reviews and discussion brought many relevant issues and concerns. The authors submitted a revised version that improved the clarity of the paper and added an important experiment with PolyMNIST. In their responses, authors also addressed some misunderstanding about JMVAE KL. The comparison with a relatively similar work, from Sutter et al., 2020, was only mentioned in the related work, with no direct comparisons. Also, the authors did not directly address the issue of studying tradeoffs between quality of generated samples and their coherences. It should also be noted that the advantage of the proposed SMVAE is marginal when the number of modalities increases, for the latent representation experiments on PolyMNIST.
SUMMARY: Enthusiasm for this paper was not unanimous. The reviewers brought some concerns about its differentiation with priori work, such as Sutton et al., 2020, and about a more detailed analysis of the tradeoffs. While the clarity of the paper improved during the revision, a good number of issues remained. I am leaning towards rejection.
The paper proposes to improve (generalized) zero shot learning, by training a generator jointly with the classification task, such that it generates samples that reduce the classification loss.  To achieve this, they use a zero shot model that has a (differentiable) closed form solution (ESZSL), so the full model can be optimized end to end. The approach is evaluated on the standard benchmarks of GZSL. 

Reviewers had some concerns regarding novelty compared with previous work and quality of experiments and evaluations. The authors answered most of these concerns in their rebuttal including discussion with previous work and additional evaluations.  As a result, the paper would be interesting for the ICLR audience.
The reviewers unanimously appreciated the quality of the experiments. The main point raised was about the related work by Wang et al. but that was addressed by the authors in the rebuttal. I thus encourage the authors to make sure that discussion is reflected in the final version of their work.
The paper considers the problem of path integration in cognitive maps, where combining proprioception with visual inputs is required to estimate the displacement.  The paper proposes a small mechanism (a resetting path integrator) that extends a conventional LSTM for this purpose.  The resulting networks demonstrate better performance and interpretability than a conventional LSTM on tested problems.

The reviewers raised many issues with the paper.  One concern was whether the problem was to model biological, artificial, or robot problems (reviewer hpxs, PuPV), which the authors successfully addressed by stating that it is a minimal model. Many other minor concerns were also addressed. However, significant concerns remained.  One is the emphasis on the cognitive map (reviewer CUYu, hpxs) for which path integration is a small part.  Another major concern is the significance of the results, with reference to the baselines and $R^2$ (AjJt, CUYu).  A third is on the generalizability of the method beyond single small examples (AjJT, hpxs, CUYu).

All reviewers indicate reject due to concerns that the paper is not ready for publication.  The paper is therefore rejected.
The paper claims that one of the most common (and obvious) pruning methods in the literature today (global magnitude pruning) is "overlooked" and "seen as a mediocre baseline by the community." As an active member of the pruning research community myself, I can attest that this is simply not true. I am in strong agreement with reviewer MHY2 and   after reading the discussion around that review and the paper itself in detail   I confidently recommend rejection.

Magnitude pruning itself dates back decades, at least to the work of Janowski (Pruning vs. Clipping in Neural Networks, 1988). The paper is correct that *global* magnitude pruning (in which all weights are compared in a layer agnostic manner) was largely ignored in favor of layer wise magnitude pruning (i.e., pruning all layers by the same amount) in much of the work that popularized magnitude pruning (e.g., Han et al., 2015). However, global magnitude pruning has become much more popular since that time. In work establishing the lottery ticket hypothesis, Frankle and Carbin (The Lottery Ticket Hypothesis) use it in certain cases and   later   in all cases (Frankle et al., Linear Mode Connectivity and the Lottery Ticket Hypothesis). In the past several years, global pruning in general has become the de facto way to use all new pruning heuristics (e.g., SNIP: Single Shot Network Pruning based on Connection Sensitivity; Picking Winning Tickets Before Training by Preserving Gradient Flow; Pruning Neural Networks without Any Data by Iteratively Conserving Synaptic Flow). Moreover, other papers have specifically advocated that global magnitude pruning is state of the art within recent years at this very conference: Comparing Rewinding and Fine Tuning in Neural Network Pruning (Renda et al., ICLR 2020 oral): "We propose a pruning algorithm...that matches state of the art tradeoffs between Accuracy and Parameter Efficiency across networks and datasets:...globally prune the 20% of weights with the lowest magnitudes." (This paper does not cite Renda et al. despite the fact that it is a prominent paper that directly contradicts the purported problem that the paper relies on to support the significance of the findings.)

In short, in the pruning literature, the idea that global pruning, magnitude pruning, or global magnitude pruning is overlooked or is not recognized as a strong baseline is simply preposterous. The reason that global magnitude pruning has "largely been ignored in recent years, generally being relegated to the position of a baseline for comparison" is because it is a simple technique whose efficacy has long been known and established   exactly what a good "baseline for comparison" should be.

The paper has narrowed its claims somewhat during the discussion and revision period, advocating for a one shot global magnitude pruning strategy that "does not require any complex pruning frameworks like RL or sparsification schedules [or]...iterative procedure." To do so, however, the proposed method replaces each of these "complex" hyperparameters with another set: whether or not to use a minimum threshold (MT) and where to set it. Even if the approach isn t iterative, the hyperparameter search necessary to set it almost certainly is, and it is unclear whether searching for the MT value is any more efficient than the other approaches. The costs of this hyperparameter search need to be measured. And iterative pruning s costs can often be mitigated by making pruning gradual, something the paper considers superficially in the revisions.

Finally, as reviewer MHY2 observes, one of the primary reason papers *don t* use global magnitude pruning is that, although it leads to higher sparsities than layerwise magnitude pruning, it also often leads to higher FLOP counts. Although FLOP counts are a terrible indicator of real world speedup, they are a much higher fidelity indicator than parameter count, which neglects the fact that   in convolutional networks   a small number of parameters can lead to vastly more FLOPs if they operate on larger activation maps (i.e., before the activation maps have been downsampled). In the revisions, the paper gives a token nod (and a superficial dismissal) to this fact in Sections 4.3 and 6, but the paper needs to fully acknowledge this point by measuring and discussing its consequences. "Look[ing] at this in future work" is not enough.

Due to these many concerns, I strongly recommend rejection.
The authors propose a new method for deepfake detection (ENST) which relies on high frequency information, low level/shallow features, and optical flow. In particular, EfficientNet B5 is used to extract the high frequency info and shallow features, and a Swin Transformer to capture discrepancies between optical flows. Empirical validation on FaceForensics++ and Celeb DF shows some improvements over the baselines.

The reviewers found this to be a relevant and timely topic. The reviewers also found that integrating information from the frequency domain, the spatial domain, and optical flow is a promising approach. There were three reviewers suggesting rejection, and one suggesting acceptance. After the rebuttal and discussion phase, the following remaining issues were highlighted: 
  **Limited technical novelty** (nearly all components used in this work were already expired in other work).
  Underwhelming empirical improvements given the fact that the model uses EfficientNet B5 and the SwinTransformer. 
  Many claims are still not supported by empirical evidence. For instance, to claim generalisation, an extensive analysis, including more datasets as well as competing methods should be carried out.
Many problems in machine learning rely on multi task learning (MTL), in which the goal is to solve multiple related machine learning tasks simultaneously. In this work, authors formalize notions of task level privacy for MTL via joint differential privacy (JDP). They propose an algorithm for mean regularized MTL, an objective commonly used for applications in personalized federated learning, subject to JDP. Then analyze objective and solver, providing certifiable guarantees on both privacy and utility. The main results, namely the convergence rate results, are hard to parse and hard to interpret. For example, as one reviewer pointed out, it is bounded below by a constant which is not properly explained. Further, comparisons to the literature in user level privacy (which is equivalent as the task level privacy) is not provided enough. Significant improvement in the presentation of the main results, along with an interpretable explanation of the contribution, is necessary for this manuscript.
The submission proposes a new approach to deriving a policy gradient type algorithm for multi agent RL (MARL) where the agents are interested in a common objective but with potentially different action spaces. It extends the monotone improvement property for single agent trust region based methods like TRPO to a multi agent update setting where the updates are performed in sequence by the agents, and uses this idea to derive new multi agent analogues of TRPO and PPO. These algorithms are shown to be competitive with existing strategies for MARL on a Starcraft environment, and superior in the case of common Mujuco benchmarks. 

All reviewers are unanimous in their appreciation for the paper s contributions. The initial concerns about clarity of the technical results, especially the improvement guarantee of the key lemma, that some reviewers had were addressed adequately by the author responses. Hence, I gladly recommend acceptance.
This work proposes a method for automatic adaptation of the learning rate via a estimating quadratic approximation of the full batch during training. The method motivated by two observed properties of the loss landscape, first the full batch loss along the gradient direction is well approximated by a quadratic polynomial, and second the optimal full batch step size does not change quickly during training. Two primary criticisms raised by reviewers is the weak experimental evidence provided to validate the method and similarities with other approaches for adapting the learning rate. Ultimately reviewers remain unconvinced by the rebuttal and maintained their scores. The AC further stresses the difficulty of properly (and fairly) comparing optimization methods in deep learning. As is consistently shown in the literature, optimizer performance is typically dominated by hyperparameter tuning, this is particularly problematic when submissions tune their own baselines as authors naturally are incentivized to tweak their own methods until the method looks favorable relative to others. Comparing directly against prior published results tuned by other researchers would help alleviate reviewer concerns regarding hyperparameter tuning.
The paper got four accepts (after the reviewers changed their scores), all with high confidences. The theories are complete and the experiments are solid. The AC found no reason to overturn reviewers  recommendations. However, the AC deemed that all the pieces are just routine, thus only recommended poster.
The paper introduces unsupervised skill discovery using Lipschitz constrained skills. It is well written and demonstrates the advantages in a solid experimental section.
This is a borderline paper. The most enthusiastic reviewer does not have much confidence in the score. The other reviewers think the paper has some value after the rebuttal, but also feel there is little technical novelty. The proposed applications of the approach are interesting.

After reading the reviews, rebuttal, and the paper, I agree that there is little technical novelty. The idea of adding node label noise to a GNN to improve GNN expressiveness dates back to (Murphy et al., 2019) and has been also explored by (Dasoulas et al., 2019), (Vignac et al., 2020), (Loukas, 2020) among others [one of which is suggested by a reviewer] (this literature is entirely missing from the paper). The paper has some novelty in proposing a regularization method for tackling the node level noise by augmenting the loss function with a denoising term. The oversmoothing justification is not properly investigated (whether the proposed solution really solves the issue in practice).

If there is space in the borderline decision boundary, this paper could be a worthwhile inclusion.

Dasoulas, G., Santos, L.D., Scaman, K. and Virmaux, A., 2019. Coloring graph neural networks for node disambiguation. arXiv preprint arXiv:1912.06058.
Loukas, A., 2020. How hard is to distinguish graphs with graph neural networks?. arXiv preprint arXiv:2005.06649.
Vignac, C., Loukas, A. and Frossard, P., 2020. Building powerful and equivariant graph neural networks with structural message passing. arXiv preprint arXiv:2006.15107.
Murphy, R., Srinivasan, B., Rao, V. and Ribeiro, B., 2019, May. Relational pooling for graph representations. In International Conference on Machine Learning (pp. 4663 4673). PMLR.
This main focus of this paper is graph modeling. Specifically, this paper considers a setting in which data is generated under continuous time dynamics based on neural ODE. Theoretical results regarding parameter estimation are provided. The results are also supported by experiments.

The reviewers appreciate a thorough response to their questions and think that this paper would be of interest to ICLR and ML community. Please address reviewers comments in your final version.
The paper introduces a method for uncertainty quantification for medical applications, which quantifies both aleatoric and epistemic components.

The paper initially received three strong reject recommendations. The main limitations pointed out by reviewers relate to the limited contributions (either methodological or applicative and clinical), the lack of positioning with respect to related works, the presentation needing improvement and the lack of experimental comparison with respect to recent relevant baselines. 
No rebuttal was provided. \
The AC carefully read the submission and agrees that the paper is premature for publication in the current form. Therefore, the AC recommends rejection.
The paper analyzes convolutional kernels and their sample complexity as compared to different architectures, and in particular the effect of pooling. The analysis proceeds by characterizing the RKHS in this setting (for a distribution on the cube) and using results by Mei and others to obtain separation between different architectures. 
The reviewers appreciated the fact that this is an example worked out in detail, resulting in a clear message about sample complexity gaps between architectures.
However, there was also concerns that some of the conclusions do appear in previous works, so that there is no surprising insight here. 
In future versions, the authors are encouraged to more clearly explain the novel aspects of the paper (as well as where the main technical novelties and tools are).
The paper studies the problem of task specific model compression obtained from fine tuning large pre trained language models. The work follows the line of research in which model size is reduced by decomposing the matrices in the model into smaller factors. Two step approaches apply SVD and then fine tuned the model on task specific data. The present work makes the observation that after the first step (the SVD compression) the model can dramatically lose its performance, due to the mismatched optimization objectives between the low rank approximation and the target task. The work provides evidence backing this claim. The paper proposes to address this problem by weighting the importance of parameters for the factorization according to the Fisher information. Experimental evaluation shows that the proposed method can achieve better results than variants that use truncated SVD of the weight matrices.

The paper is well written and easy to read. The method is simple and effective and can be applied to in a wide range of settings. The authors provided a thorough response which clarified several points. This led Reviewer Kuwu to increase the score to 6.

All three reviewers agree that the main observation in the work is interesting and informative for researchers and practitioners working on the problem.

Reviewer jnTC points out that the paper would have been stronger if it included theoretical exploration of the reasons behind the "importance of low SVs" phenomenon.

Reviewer Kuwu and jnTC consider the results marginally novel. Reviewer Kuwu considered the significance of the reported results to be limited, and put the work marginally above the acceptance threshold. Reviewer jnTC disagrees with this view, considers and appreciates the generality of the method and the fact that it can work well even for compressed models, while improving in accuracy by a few percent over competing approaches which result in similar parameter counts. The AC agrees with Reviewer jnTC.

Overall all reviewers consider the paper borderline but recommend accepting the paper. The AC overall the topic important (reducing the footprint of language models), the method simple and well motivated. The empirical evaluation is very thorough and shows clear gains across a large number of settings.
In this paper, the authors provide a model based approach for combining experimental and observational data in reinforcement learning, specifically in POMDPs.

The paper was not received very favorably by reviewers, with the main concerns revolving around: (a) writing quality, (b) validation, (c) extent of contribution given existing work on causal RL.

In preparing your revision, in addition to clarifying writing, and adding better validation, I would urge the authors to consult existing causal inference literature on point and partial identification in settings related to RL, such as off line policy learning.  This will help address issues of novelty by extending their approach to settings with more types of confounding.  In addition to useful references suggested by reviewers, another useful draft may be:

"Path Dependent Structural Equation Models." Srinivasan, R., Lee, J., Bhattacharya, R., and Shpitser, I.. In Proceedings of the Thirty Seventh Conference on Uncertainty in Artificial Intelligence.
Strengths:
* Well written paper
*Theoretical analysis demonstrates that dual encoder models have similar capacity as CA models
*New distillation algorithm for learning DE students from CA teachers

Weaknesses:
* No reviewer seems particularly excited about this work 
* Theoretical analysis doesn’t provide actionable insight   it does not directly motivate the suggested distillation methods
* Empirical results are lacking   reviewers asked for qualitative examples of improvements from their distillation method
This paper proposes an algorithm called LightWaveS to improve the ROCKET (and mini ROCKET) algorithm for multivariate time series classification, by using wavelet scattering instead of the kernel function. More than the usual number of reviewers were invited to provide independent reviews on the paper.

A concern was raised regarding the lack of hyperparameter search in the paper. The authors responded that this was intentional to avoid overfitting the solution to the tested datasets. This response is not convincing. Note that other important reasons to vary the hyperparameter values (as commonly adopted by ML researchers) are to study the sensitivity of the proposed method to hyperparameter settings and to perform more holistic performance comparison with other methods.

Other concerns on both novelty and significance have also been raised.

Although 2 of the 7 reviews show a weak support for acceptance, other reviewers have pointed out legitimate concerns that make this paper not ready for publication in ICLR in its current form. We appreciate the authors for clarifying some points in their responses and discussions and even including further results, but addressing all the concerns raised really needs a more substantial revision of the paper. We hope the comments and suggestions made by us can help the authors prepare a revised version that will be more ready for publication.
The authors introduce a method for improving reinforcement learning in sparse reward settings. In particular, they propose to take advantage of a suboptimal behavior policy as a guidance policy that is incorporated in a TRPO like update. The reviewers agree that this is a novel and interesting idea and given the authors  rebuttal with additional experiments, clarifications and discussions, they agreed to accept the paper. However, they also point out several flaws (e.g. evaluation on a more challenging sparse reward task such as Adroid) that I encourage the authors to address in the final version of the paper.
The paper proposes a subspace regularization technique that encourages the new class weight vector to be in the subspace spanned by those of the base classes for few shot class incremental learning. Even though similar techniques exist in few shot learning literature, reviewers appreciate the simplicity of the method and thorough experiments. The authors have revised the paper to include missing references suggested by reviewers during the rebuttal. They were not able to add experiment comparisons to Tao et al. (2020) and Chen & Lee (2021) as requested by reviewer Vrap due to missing code release.  Please consider adding them in your draft later.
The authors first consider a mean field two player zero sum game and consider quasistatic Wasserstein gradient flow dynamics for solving the problem. The dynamics is proved to be convergent under some assumptions. Finally, the authors provide a discretization of the gradient flow and using this proposes an algorithm for solving min max optimization problems. They use this algorithm for GAN s as the main example. Experimental results claim that the algorithm outperforms langevin gradient descent especially in high dimensionas.

This paper sits right at the border. But subsequent to the author response, one of the reviewers has updated the score and seems more positive about the paper. In view of this, I am leaning towards  an accept.
This paper shows minimax lower bounds on transfer learning for binary classification, in terms of a notion of transfer distance, defined in this paper. Experimental results try to show the validity of the proved minimax lower bounds. 

All reviewers acknowledge that the lower bounds are worthy contributions; however, none of the reviewers felt strong enough to champion this work, due to that:
  the theoretical sharpness of the lower bounds are not discussed in detail (Reviewers TfQT, pmYF, and dGvD). Remark 6 only discusses the regime of a small amount of source data and a large transfer distance, which is fairly limited.
  it is unclear to what extent the experiments validates the theory (Reviewer WXNa). Note that for a minimax lower bound, for any algorithm, there is some corresponding "worst case" datasets such that the algorithm does not do well; it is unclear if the datasets considered here are worst case at all. 
  the lower bound techniques are fairly standard.
  the comparisons between the lower bounds in this work and prior lower bounds (e.g. those in [1,2]) need to be discussed more thoroughly.

We encourage the authors to take into account the reviewers  feedback and revise the paper. 

[1] Hanneke and Kpotufe. On the value of target data in transfer learning. NeurIPS 2019.
[2] ​​Mansour, Mohri, Ro, Suresh, and Wu. A theory of multiple source adaptation with limited target labeled data. AISTATS 2021.
This paper proposes a novel Federated Learning (FL) framework that leverages the Neural Tangent Kernel (NTK), to replace the gradient descent algorithm for optimization. Specifically, the workers upload the labels and the Jacobian matrices to the server, and the server uses the tools from the NTK to obtain a trained neural network. However since this could lead to increased communication cost and compromise of data privacy, the authors propose data sampling and random projection techniques to alleviate the problem. The authors provide a theoretical analysis that the proposed scheme has a faster convergence than FedAvg under specific assumptions, and experimentally validate that it significantly outperforms previous FL algorithms, achieving similar test accuracy to ideal centralized cases.

Pros
  The idea of using NTK for model optimization without gradient descent and use of it in the FL setting is both interesting and novel.
  The paper properly discusses and tackles the new challenges posed by the introduction of the new method.
  The paper is well organized and clearly written, with sufficient discussion of related works and backgrounds. 

Cons

  The proposed method puts heavy computational burdens on the server side.
  The method violates the privacy preserving feature of FL by its nature, and while the proposed compression shuffling alleviates the concern, more discussion is necessary.
  Missing comparison against popular baselines such as FedProx and SCAFFOLD. 
  The faster convergence of the proposed method in comparison to FedAvg depends on the learning rate and is not always true.
  There is a gap between the theory and practice, which makes the practicality of the algorithm still questionable. 

Although the reviewers found the idea as novel, the proposed techniques for alleviating communication cost and privacy concerns convincing, and considered both the theoretical analysis and experimental validation thorough, all reviewers leaned toward rejection due to critical concerns unanswered. During the discussion period, the authors alleviate many of the minor concerns from the reviewers, but there were still remaining concerns on the gap between the theory and practice on its convergence behavior, and insufficient discussion of the privacy preserving feature of the proposed method, as well as shifting of computation burdens to the server. Thus, the reviewers reached a consensus that the paper is not yet ready for publication. 

Despite the low average score, the novelty of the idea and the quality of the paper is much higher than those of the accepted papers in my batch, and I strongly believe that this will become a high impact paper, if remaining concerns from the reviewers are properly resolved.
The authors provide a cubic regularization approach to non convex concave minimax problems. The reviewers highlight that the paper in its current form is not ready for publication due to issues such as the gap between the theory and the implementable algorithm.
In this paper, the authors consider the offline RL with only realizability and partial coverage assumption, under which a model based pessimistic policy optimization algorithm has been proposed and rigorously justified. Moreover, variety of special MDP models, including kernelized nonlinear regulator and linear mixture MDP, have been plugged into the general framework, which leads to different specific algorithm and refined guarantees. 

In general, the reviewers are positive to the submission. However, there are still issues need to be further discussed, 

  *Computation feasibility*: most of the reviewers raise the same concern about the computation feasibility and efficiency. Specifically, the proposed algorithm is too complicated, and thus, may not be practical. 

  *Comparison with existing statistical results*: both reviewers and I appreciate the summary in the paper about the coverage assumptions in the existing methods. However, a similar table for summarizing the complexity of existing algorithms, as well as detailed discussion, is also necessary for a better position of the proposed method among the literature, including both model based and model free RL.
This paper proposes a deep RL framework for the traditional schedule problem. The proposed algorithm is shown to be effective and has zero shot generalization abilities. Reviewers are mostly satisfied with the response and the overall evaluation is slightly positive. However, there are some drawbacks of the current paper preventing it from getting a higher evaluation: (1) The reviewers believe that the contribution might be small   at least for the RL area; the experimental performance for the scheduling problem is also not significantly improved compared to other methods (e.g. the search based ones). Hence the reviewers believe the contribution of the paper is limited. (2) There is a number of typos and language issues in its present version. The paper may need several rounds of polishment before publication. (3) There is a lack of theoretical justification for the proposed method.  In sum, the AC recommends a borderline rejection.
This paper provides an investigation into the quality of generations made by multimodal VAEs. All reviewers were in favor of accepting the paper, and there was quite a bit of detailed discussion and clarifications in the revised version of the paper which led two reviewers to raise their ratings. Overall this is an interesting contribution to the area and is an excellent fit for ICLR.
The paper looks at subspace recovery in the presence of outliers, of which there have been many formulations. They study a recent formulation, DPCP, but relax the requirement that the dimension of the subspace is known   obviously very important in practice. The approach is quite clever: they exploit the fact that for this non convex problem, starting a simple algorithm at a randomly chosen starting point will converge to a local minimizer, and they can run an ensemble of these algorithms (each with different starting points) and be guaranteed the solutions will span an appropriate subspace. This idea alone is a nice contribution. The paper has theory and experiments.

Most reviewers were positive about the paper. The most critical review, by 1qf1, still acknowledged that this paper has a lot of potential, but in their opinion the paper was not in a state ready for acceptance, especially regarding the formulation of the main result, Theorem 7.  The other reviewers were OK with the state of the paper, and the authors made changes in the rebuttal. Hence, while acknowledging the paper could possibly still be improved (what paper couldn t be!), I think the paper is in a good enough state to accept it for ICLR. I don t think there would be enough benefit to the community (authors, readers and reviewers) to ask for this to go through one more round of submission/revision.
This paper presents an "attack"—TextExplanationFooler (TEF)—that adversarially (minimally) edits inputs such that the resultant attribution assigned by common explanation methods changes substantially, while the prediction does not. This is an extension of methods and results in vision to NLP. The authors find that all methods are vulnerable to this attack. 

Reviewers agreed that the demonstration that perturbation attacks used in vision are applicable in NLP is interesting and may lead to follow up work. The paper would benefit from technical clarifications in several places (see R2), which were largely resolved in discussion.
All reviewers agreed that this is a strong paper, that the methodological contributions are both relevant and significant, and that the experimental validation is convincing. I fully share this viewpoint!
The reviewers agree that the paper studies an important and interesting problem and presents a good solution which is theoretically sound. The paper can be further improved by looking into more applications such as cold start recommendations.
This paper proposes an algorithmic approach to estimating upper and lower bounds of the rate distortion (R D) function of a data source on the basis of samples drawn from it. The proposed upper bound is based on the variational objective employed in the Blahut Arimoto algorithm, whereas the proposed lower bound is based on the dual characterization of the R D function. In both bounds neural networks trained with samples are utilized. Experimental results on four sources (Gaussian, banana shaped, GAN generated images, and natural images) are provided.

The four review scores were initially two positives and two negatives. Some reviewers evaluated positively the argument on the lower bound of the R D function. On the other hand, one reviewer showed his/her concern about lack of the argument on statistical confidence of the obtained bounds. In response, the authors have addressed it in Section A.6 in the Supplementary Materials (SM) of the revised manuscript with the experiment using GAN generated images (high dimensional data with low intrinsic dimension), and the results are summarized in Tables 1 4 and Figure 10 in SM. The authors have in their revision also provided a specification for the range of the sources to which the proposal would be applicable. Description of the experiments on images, which was missing in the initial version as pointed out by some reviewers, has been added in the revised manuscript. Still, as some reviewers mentioned, the main weakness is that the proposal failed to demonstrate its usefulness to estimate the lower bound in settings where the data dimension is truly high, as in the experiment with natural images, where statistical confidence analysis was not conducted either. Three reviewers have revised their respective scores upward after the author response.

Despite some weaknesses I think that this paper provides a novel and interesting algorithmic approach to estimating the rate distortion function. I would therefore like to recommend acceptance of this paper, and would like to encourage the authors to perform confidence analysis also for the experiments on natural images.
This paper analyzes the data scaling laws in NMT tasks with different network architectures and data qualities. The main purpose of this paper is to investigate how such different experimental setup affects the scaling law. The authors found that those difference does not have strong impact on the scaling exponent, and a small difference of model architecture and data noise can be compensated by larger data size.

This paper gives nice justification of data scaling law from some different aspects which is instructive to some extent. On the other hand, the paper has some weakness as listed in the following: (1) The scaling law itself has been analyzed by many papers, and its novelty is rather limited. I acknowledge that this paper investigates different aspects of the data scaling law and the size of experiments are larger than existing work. However, the result is rather unsurprising. (2) The experiments are conducted mostly on one language pair (English to German), it is still unclear whether the findings are universal to other language pairs. As the authors responded, exhaustive experiments over all language pairs are unrealistic but some more investigation to more general data sets could be conducted to strengthen the paper.  

This paper is around the borderline. Some reviewers were rather positive to this paper. However, they also pointed out the concerns I listed above and they do not show strong support on the paper.  
In summary, although this paper shows some instructive findings, it is still a bit below the threshold of acceptance.
The paper presents a neural architecture based on neural memory modules to model the spatiotemporal traffic data. The reviewers think this is an important application of deep learning and thus fits the topic of ICLR. The writing and the novelty of the proposed method need improvement.
This paper suggests a novel defense against adversarial perturbations where during training a loss term is added which enforces similar feature representations.
At test time: i) noise is added, ii) the feature loss is minimized

The authors report excellent results against AutoAttack but the problem is that AutoAttack expects a static, non randomized defense. Both is not the case for the defense proposed in the present paper. Therefore,  the evaluation with AutoAttack could significantly overestimate the actual robustness and the evaluation of the paper is therefore not valid. Thus adaptive attacks are needed, which are tailored to the defense mechanism, see e.g. Carlini et al, On Evaluating Adversarial Robustness, https://arxiv.org/abs/1902.0670. 

As two reviewers noticed, the suggested "adaptive attack" in the paper is not properly attacking the whole defense mechanism by unrolling the test time optimization and using additionally EOT. Thus it is unclear at the moment if the method is really robust. Moreover, the inference time is significantly increased so that it is questionable if this approach is practically relevant. Therefore this paper is not ready for publication yet.
The authors present a new memory augmented neural network that is related to the Kanerva machine of Wu et. al.  The reviewers considered the ideas in the paper novel and interesting, but were concerned about presentation issues and literature review.  The authors have improved both... however  authors: please even under limited space constraints, make more room for related work!  Clarifying your contribution in the context of the literature is critical for reader understanding, and neglecting this almost had your paper rejected out of hand.  

I am voting to accept
This paper proposes “Continual Federated Learning (CFL)” to study time evolving heterogeneous data. To do this the authors introduce time drift to capture data heterogeneity across time. The authors also present some preliminary convergence results. Finally, the authors carryout numerical experiments in time varying and heterogeneous settings. The reviewers identified the following strengths: (1) combining FL and CL is interesting, (2) the development of a new algorithm and providing some initial analysis is a good step. They also identified weaknesses as follows: (1) limited technical novelty as the use of replay buffer is quite standard, (2) cumbersome and not easy to interpret results, (3) lack of time evolving patterns with a common component (4) lack of different metrics that demonstrate how the algorithm is able to maintain accuracy as time shifts occur, (5) lack of questionable assumptions. The reviewers had a very bimodal view advocating acceptance with a score of 8 and 2 advocating a rejection and neither group changed their opinion. Although the authors thorough responses did alleviate the concerns IMO. My own reading of the paper is that this is an interesting paper working on an emerging area. However, I must agree with some of the reviewers that the final conclusions are not easy to interpret, and the assumptions are not fully motivated. After this is carried out, I think the novelty of the paper can also become much clear. Therefore, I cannot strongly advocate acceptance of the paper in its currently state given the scores. However, I very strongly encourage the authors to submit to a future ML venue after addressing the remaining comments of the reviewers. I would also like to commend the authors for a very strong rebuttal sorry the final decision couldn’t be more favorable given the borderline ratings and the aforementioned issues.
The paper addresses unsupervised domain adaptation under covariate shift and missing source and target features. Three approaches are proposed for tackling respectively covariate shift, missing data and simultaneous covariate shift and missing data. The proposed method relies on the minimization of the maximum mean discrepancy between the source and target representations in the different settings. Experiments are performed on a synthetic dataset and on two other datasets.

All the reviewers highlighted several weaknesses: lack of formal definitions and of formal analyses, lack of connection with existing approaches for handling missing data, weak reproducibility. The authors did not provide responses. Reject.
The paper studies how neural combinatorial solvers can be susceptible to adversarial examples and what implications does this susceptibility have on the evaluation of neural solvers. Besides proposing some successful adversarial attacks, the authors provide a method for adversarial training and show its effectiveness on improving robustness and generalization. All the reviewers agreed that this paper provides a set of very interesting and novel results.
A method for efficient exact computation of the generalized Gauss Newton matrix is given. Using this method the authors provide several empirical observations of first and second order statistics of neural networks during training. Additionally the authors use to tool to propose a new damping technique that some reviewers found particularly interesting. Reviewers noted that the low rank decomposition the authors provide is not new, and has been used in prior work, although the trick may not be widely known within the deep learning community. As such novelty is not a strength of the work, and reviewers suggested the authors could strengthen the work with a convincing demonstration that the method can be made to work at scale, as well as providing more detailed run time and memory comparisons with other approaches to calculating the GGN matrix. Although the authors agreed with reviewer suggestions, the paper was not updated during the rebuttal period. As such I recommend the authors resubmit with the proposed revisions.
The paper sheds light on issues with BN in continual learning and proposes a quite simple, which is a strength, solution to fix it. 

The Authors first draw attention to the fact that using recalculated moments boosts performance and reduces forgetting, which serves as an argument that at least partially BN contributes to catastrophic forgetting in continual learning. Given that BN remains quite important in certain application areas such as vision, it is a strong motivation for the paper.

The experiments are thorough and clearly show that CN is a practically relevant alternative to BN in continual learning.

One weakness of the paper is that the method is poorly motivated, and relatedly, it has quite limited novelty. CN combines the strengths and weaknesses of BN and GN. Hence, it is not clear why it outperforms both, given that it still has the issue of BN that normalization statistics might become outdated. This is one of the weaknesses pointed out by 9jXz who recommended rejecting the paper. It would be also nice to compare to Mode Normalization https://openreview.net/forum?id HyN M2Rctm. 

Other papers have suggested changing normalization for sequential learning. Changing batch normalization (to batch renormalization) was investigated in [1] in the context of continual learning. Relatedly, [2] proposes TaskNorm for meta learning.

Despite these issues, it is a solid contribution and it is my pleasure to recommend acceptance. In the camera ready, please describe more clearly the design principles behind CN.

[1] Rehearsal Free Continual Learning over Small Non I.I.D. Batches, https://openaccess.thecvf.com/content_CVPRW_2020/papers/w15/Lomonaco_Rehearsal Free_Continual_Learning_Over_Small_Non I.I.D._Batches_CVPRW_2020_paper.pdf

[2] TaskNorm: Rethinking Batch Normalization for Meta Learning, https://arxiv.org/abs/2003.03284
The work investigates the decision boundary of neural networks by quantifying in various ways the shape and curvature of the error set local to correctly classified inputs, dubbed the "adversarial subspace". First, a method is introduced which seeks to find the largest set of orthogonal directions starting at in input x which will all intersect the error set local to an image. This is motivated as a certain geometric measure of the error set, large sprawling error sets may have many orthogonal directions which intersect it local to the given input while small narrow error sets may have relatively few. Using this geometric measure, the authors compare the shape of adversarial subspaces of various image models both with and without adversarial training. After the rebuttal period, reviewers all felt that the work was borderline, with no one strongly advocating for the work. As noted by some reviewers, while some experiments may be interesting it was unclear what new insights the work contributes. For example, the authors argue that the change in the geometry of the error set explains why adversarial training works. It is unclear how this is an explanation more than it is simply an observation that the error set geometry has changed. An analogy would be trying to explain why Resnet 50 performs better than AlexNet by showing that it has higher test accuracy this only shows that it is better, but doesn t explain why.

During the discussion period the AC raised additional concerns regarding a sanity check that the author s main algorithm should pass. In particular, consider an error set x_1 >  K(x_2^2 + ... + x_n^2) + C, parameterized by constants K and C > 0. For all choices of K and C and starting point x   (0, ..., 0), the authors main algorithm will always return 1 as the dimensionality of this error set. It will find the vector (1, 0, ..., 0) and then terminate. However, this is problematic because we can choose K and C to make this error set either very narrow (e.g. K C 100) or very wide (K   .000001, C   .00001) the proposed algorithm will be unable to distinguish between this two extremes. Given this, it seems that greedily selecting the set of orthogonal directions starting at x can be very suboptimal if the intent is to find a maximum sized set of orthogonal directions. 

To conclude, the work would be substantially improved if it addresses two major weaknesses. First, there needs to be a clearer motivation for studying this notion of geometry of the error set, what new insights can the authors provide other than adversarial training changes the shape of the error set? Second, the method doesn t seem to be principled given it is unable to distinguish between the two extreme cases discussed above.
The paper presents a masking strategy to introduce the locality bias into the vision transformers. The experiments show the effectiveness of considering such inductive bias. The reviewers agreed on the importance of the research question and the simplicity of the algorithm. MaiT also has a straight forward sparse attention extension that performs on the complexity of $O(n)$ rather than $O(n^2)$.

The reviewers also listed some common concerns of the paper:

(1) The novelty of such a masking approach is relatively low. I don t think the ALS or the soft masking adding too much contribution to that. Similar ideas have been explored in a number of papers.

(2) Reviewers also raise concerns about the experiments. Inductive biases often help more in small settings (fewer parameters and FGLOPs) and gain less in the large settings. When comparing with the STOA models, I think this is basically the trend shown in the paper as well. While I appreciate the authors’ efforts in including more comparisons, I have to say I really don’t think the performance gain is significant enough especially in the large settings. Needless to say that there are many other ways of encoding the same locality bias into the model.

Based on the reviewers  judgements and my own opinion, I therefore recommend rejection of this paper.
While the reviewers appreciated the clarity of the work, there is a concern about the meaning of the proposed result and method. It is known that adding knowledge about an additional variable, in this case the environment, leads to a lower variance estimate. What is not known is the practical impact of using this new baseline or perhaps some other intuition stemming from that use of the baseline (for instance the origin of the variance). However, the results shown are not that compelling, a point which was raised by the reviewers, making the work below the bar for publication.
The paper proposes a new method to learn OT maps, and reframes it in the GAN literature. The initial method works when computing maps between equal dimensions, through duality and an identity (10   11, amply discussed in the reviewing process). Lemma 4.1 provides the main result. While the discussion right below on the fact that several functions (non OT maps) might maximize that criterion is not completely satisfactory, the result provides an interesting characterization. The second contribution adds a method to compute OT maps between spaces of unequal dimensions. Overall the contribution sounds a bit ad hoc, and one wonders whether this does really work (comments such as "we add small gradient penalty (Gulrajani et al., 2017) on potential ψω for better stability. The penalty in not included in Algorithm 1 to keep it simple." are strange and point to instability) but the overall creativity and new ideas in the paper seem to have garnered enough support from reviewers to push for an accept.
The paper proposes a novel method, PI3NN, for estimating prediction intervals (PIs) for quantifying the uncertainty of neural network predictions. The method is based on independently training three neural networks with different loss functions which are then combined via a linear combination where the coefficients for a given confidence level can be found by the root finding algorithm. A specific initialization scheme allows to employ the method to OOD detection. 

Reviewers agreed on the importance of the problem of producing reliable confidence estimates.  The proposed method addressed some of the limitations of the existing approaches, and reviewers valued that a theoretical as well as an empirical analysis is provided. 

On of the main criticisms was that the theoretical derivation of the method is based on the assumption of the noise being homoscedastic. This however is a common issue with other methods in this area, which are nevertheless all applied (and seem to work) on heteroscedastic data as well and are outperformed by the proposed method. Another main point that was criticized was that the empirical analysis was limited. In turn the authors added another experiments on another dataset and with another network architecture (a LSTM) to their analysis. Moreover, the authors adequately addressed a lot of the concerns and questions of the reviewers in their answers and the revised manuscript.  The final mean scores are exactly borderline (5.5) but with a higher confidence of reviewers voting for acceptance.  Based on the listed points, the paper should be accepted. 

I would encourage to  improve the discussion around the dependence on x in Section 3.2, which could still be made clearer, in the final version of the manuscript, and to add the discussion about the limitations of the theoretical analyses (i.e. the applicability  only to the homoscedastic settings) to the conclusion.
This paper proposes a meta structural causal model framework, to increase the representation capability of structural equation models. It also considers how to connect data to mechanisms. The paper is conceptually interesting. However, on the technical side, reviewers feel that without supporting proofs or empirical experiments, it is hard to justify the correctness of the proposal and judge its applicability to real world problems.

As authors claimed in their response, "it is our future work of interest to code our proposed framework into a working system and validate it in a proper setting given its early stage status on research in modeling causal cycles." I think some future version of the paper might be a great contribution to the field if a working system were included.
This works considers limitations of rehearsal based methods in the context of continual learning (classification and object detection). Rehearsal based methods provide a strong baseline, but a loss in predictive performance arises when the memory is limited in size. The authors propose to leverage compression (JPEG) to increase the number of data (images) stored in the memory. The approach is evaluated in the context of an autonomous driving application.

The additional experiments conducted by the authors were highly appreciated and helped clarify open questions (e.g., class incremental learning set up, DPP objective to determine size of the memory, quantity vs quality of compressed data, etc.). The authors addressed the issues raised by three out of four reviewers, who did not have further comments. The remaining reviewer found that the methodological contributions of this paper, namely of using compression in the context of CL, was pretty straightforward. However, the authors addressed the concerns raised by the reviewer regarding the selection of the compression quality q as far as I am concerned and conducted additional experiments to further demonstrate the usefulness of the approach. I would encourage the authors to include this discussion in the final version of the paper. I would also encourage them to include the additional experiments they conducted with fixed memory size and amount of memory that can be saved.
The paper proposed to learn a disentangled representation of spatiotemporal mobility data using a VAE based architecture, in order to separate spatial and temporal dependencies. This is an interesting and relevant problem, but the reviewers found the paper to be weak in motivation and empirical evaluations.
There was some discussion on this paper, both with the authors and between reviewers. On the one hand, there is a general agreement that the empirical results suggesting that spectral clustering based method can be competitive with SOTA methods on node classification benchmark is an interesting result. One the other hand, reviewers did not find a significantly novel contribution in the methodology proposed, and found that the empirical evaluation lacks depth and details to be really informative (eg, to understand why some methods work or not on some benchmarks). There is therefore a consensus that the paper is not ready for ICLR in its current form, but we hope that the reviews and discussion will help the authors prepare a revised version in the future.
This work is on stochastic convex optimization (SCO) in shuffle differential privacy (DP) models. In SCO, a learner receives a convex loss function L: Theta x X  > Reals, where Theta is a d dimensional vector of parameters and X is a set of data points. The objective is to use samples x1, x2, …, xn to find a parameter theta that minimizes the loss E_{x ~ D}[L(theta,x)], where the distribution D on X is unknown. The shuffle models considered are a ``sequential" model where the analyzer operates in rounds (and where a new set of users participate in a local DP protocol in every round), and a new, stronger "full" model in which the analyzer can request a specific subset of users to participate in a round, which in particular allows users  data to be queried more than once. This work shows that in the full model, one can develop excess population loss bounds matching the known best possible bounds in centralized DP; it is also shown that even the weaker sequential model offers improved excess population loss bounds over the best possible bound of sqrt(d/n) in the local setting.

The reviewers appreciated the novelty and technical depth of this work (despite concerns about part of the work being taking “off the shelf” results).
This is a borderline paper with some reviewers voted for acceptance and some think it is not still ready. What is clear is more efforts by the authors is needed to make the paper appealing to reviewers with different interests. Changes such as better writing, more in depth literature review, more convincing experiments can definitely improve the quality of the paper. I personally do not think regret analysis is needed for this work, but it was mentioned by a reviewer. I would suggest the authors to use the reviewers  comments, revise their work, and prepare it for future conferences.
All reviewers agreed this was a very strong submission: it was clearly written, was theoretically and experimentally interesting, and had excellent motivation. A clear accept. Authors: you ve already indicated that you ve updated the submission to respond to reviewer changes, if you could double check their comments for any recommendation you may have missed on accident that would be great! The paper will make a great contribution to the conference!!
The paper compared different architectures of deep neural nets for learning full 3D turbulence simulations.  On coarse grids, the proposed method predicts more accurately than the classical solvers, especially on preserving the high frequency information.  The reviews think the paper is clearly written with strong experiments. Pls include the suggested references in the final version.
This paper proposes a data imputation method for MCAR and MAR data by combining EM and normalizing flows.  The paper is clearly written.  The idea is interesting and they show better performance compared to MCFlow and competing methods on ten multivariate UCI data, MNIST and CFAR10 image data.

Issues regarding limited novelty compared to MCFlow was raised.
Issues regarding the validity of Assumption 2 on the dependencies in the latent space and observation space was also raised.
The reviewers agree that addressing long horizon tasks with off line learning and fine tuning afterwards from demonstrations is an interesting and relevant topic. The technical ideas about learning a relevance metric to select relevant off line data, and to learn an inverse skill dynamics models. The experimental results are convincing, even if success rates are sometimes lower than expected. All reviewers recommend acceptance of the paper.
This paper proposes a distributed containerized multi agent reinforcement learning(CMARL) framework that addresses three challenges in MARL: 1) Demanding data transfer. 2) Inter process communication. 3) Effective Exploration. Using a container that collects environment experiences from parallel actors into buffers and learns local policies, CMARL demonstrates notable performance improvements with respect to time as compared to state of the art benchmarks.

Although the reviewers acknowledge that the paper addresses a relevant topic, proposes an effective method, and is well written, after reading the authors  feedback and discussing their concerns, the reviewers reached a consensus about rejecting this paper in its current form. They feel that the contribution is too incremental and that the experimental comparisons are somehow unfair.

I suggest the authors take into consideration the reviewers  suggestions while preparing an updated version of their paper for one of the forthcoming machine learning conferences.
This paper studies the generalization error of semi supervised learning, where the algorithm gradually pseudo labels the data throughout the learning process. Theoretically, an upper bound on the generalization error is shown to decompose into a term that vanishes with successive labeling and another that does not, leading to a plateau in performance. This is studied analytically for a mixture of two Gaussians. Experimentally, similar behavior is also observed to occur in more realistic scenarios. What reviewers struggled with is to understand what part of the results are, to some extent, obvious, and what offer deeper insight. What is obvious: even if a Bayes classifier were available for pseudo labeling, feature overlap means that there is a plateau of noise beyond which labeling cannot improve. What is not obvious: is it even worth pseudo labeling, or could we make things worse? The merit of the paper is in elucidating the latter. There are several concerns that remain, however, even after discussions. First, there is whether the insight is substantial or not. Here, some comparison and contrast with existing literature suggests otherwise. Second, there is whether the experimentally observed behavior is an instance of the phenomenon described by theory. Here, better structured experiments are needed to tie in with the theory. Overall, although the paper presents compelling insight, it is not yet ready to disseminate. It needs a stronger argument for its added theoretical contribution and clearer experiments to support that the presented theory is indeed behind the empirical behavior of these iterative algorithms.
This paper propose a novel framework to increase cooperation in second order social dilemmas. This is based on encouraging homophilic incentives. Reviewers agree that the paper does not meet the standards of publication yet. In particular, they worry that the assumptions made are so restrictive as to make model inapplicable to interesting problems. There is also a concern that the work is simply not novel enough.
The authors propose a novel hypersolver framework for solving numerical optimal control problems, learning a low order ODE and a neural network based residual dynamics. They compare their framework with traditional optimal control solvers on a number of control tasks and demonstrate superior performance.

The reviewers are in consensus that the paper makes significant contributions that are validated by the experimental results. The only concern was that the experiments are largely on low dimensional systems, but the reviewers agreed that the results are still worthy of acceptance.
This paper presents a novel neural network architecture to predict interacting residues among two interacting proteins, and evaluates its performance on benchmarks. While the reviews were initially mixed, there has been a productive discussion and significant improvements in the paper during the discussion, including in particular much needed clarifications about the proposed methods, and more experimental results with an ablation study to better assess the benefits of various design choices. While no reviewer is willing to champion this paper as a "strong accept", due to the relatively modest novelty compared to existing methods, there is a consensus towards "weak accept" given the final quality of the work presented and potential usefulness of the method for the problem tackled.
The paper addresses vision based and proprioception based policies for learning quadrupedal locomotion, using simulation and real robot experiments with the A1 robot dog. The reviewers agree on the significance of the algorithmic, simulation, and real world results. Given that there are also real robot evaluations, and an interesting sim to real transfer, the paper appears to be an important acceptance to ICLR.
The reviewers all raise critical issues with regard to both description and equations, and indicate that figures are not helpful. This is even after the revision. In response to KMX2, the authors suggested they will post additional experiments, but did not return to indicate that. This seems critical to answer empirical concerns about generality of the approach. We recommend to address these issues, if the authors decide to resubmit. 

The meta review and recommendation discount the review of RC7c. Unfortunately, the AC and other reviewers, weren t able to engage RC7c in the discussion and the review was extremely short.
After carefully reading the reviews and the rebuttal, unfortunately I feel this work is not yet ready for acceptance. 

I want to acknowledge the effort put in the rebuttal for this work and I think all the changes greatly increased the value of the work. However, I feel that the work could greatly benefit from running on a different domain where the gain is more considerable. My worry is that the complexity of the method compared to the relatively small improvement (at least as perceived from the current results) will reduce considerably the attention the work will receive from the community (unfairly so). 
Or some of the analysis and ablation done (e.g. the flow visualization) which are now in the appendix could be brought in the main manuscript to be able to drive the message home. An understanding of the impact on the accuracy of the flow model on the overall performance (which as pointed out by reviewer aHc1 is a really hard task in a more natural context). Or maybe a 3D visually complex environment is exactly where this method will shine as flows are more complex and hence more informative. 

Overall I think this is solid work, but I feel it does not manage to convince the reader of the significance of the proposed approach. And hence, if published in this form, I feel it will do a disservice to the work, as it will not receive the attention it merits from the community.
The submission considers a method involving adversarial training to speed up the fine tuning of large pre trained transformer language models. Reviewers consider it to be a borderline paper.  Many suggestions are made by the reviewers which will help improve the presentation and substance and make it more useful for the community.
In the context of recurrent neural networks, the motivation of the paper is to explore the "space" between fully trained models and almost not trained models, e.g. echo state networks, using a formal approach. In fact, a modular approach has proven to be very successful in many practical applications, and in addition brain seems to adopt this strategy as well. The addressed theoretical issue is stability of the network (i.e., the network implements a contraction map.) Specifically, it is assumed that a network is composed of a set of subnetworks that meet by construction some stability condition, and the problem is to design a mixing weight matrix, interconnecting the latent spaces of the subnetworks, able to give stability guarantees during and after training. Some novel stability conditions are proposed as well as two different approaches to design a successful mixing weight matrix. The original submitted paper was not easy to read, and after revision major problems with presentation have been resolved, although the current version looks more like an ordered collection of results/statements than a smooth and integrated flow of discourse. The revision has also addressed some concerns by reviewers on the role of size and sparsity of the modules, as well as the sensitivity of the stabilization condition on the mixing weight matrix has been experimentally assessed, obtaining interesting results. Overall the paper reports interesting results, however the novelty of the contribution seems to be a bit weak, e.g. stability conditions on recurrent networks (although different from the reported ones) were already presented in literature. Also the idea of exploiting, in one of the proposed models,  the fact that the matrix exponential of a skew symmetric matrix is orthogonal to maintain the convergence condition during training, is not novel. Moreover, the experimental assessment does not provide a direct comparison, under the same architectural/learning setting, of the novel stability results versus the ones already presented in literature. Empirical results are obtained on simple tasks (using datasets with sequences of identical length), and relatively small networks, which limits a bit the scope of the assessment, as well as it is not clear if the observed improvements (where obtained) are statistically significant (especially when compared with results obtained by networks with the same order of parameters.) The quality of the assessment would increase significantly by considering datasets with sequences of different lengths, and involving more challenging tasks that do require larger networks.
This paper proposes a method to improve the sample efficiency of the HER algorithm by sampling goals from a distribution that is learned from human demonstrations. Empirical results on a simulated robotic insertion task show that the proposed method enjoys a better sample efficiency compared to HER.

The reviewers find the paper well written overall and the proposed idea reasonable. However, there are concerns regarding the limited novelty of the proposed method, which seems incremental. Also, the empirical evaluation suffers from a lack of diversity. The considered tasks are virtually all equivalent to an insertion task. The paper would benefit from further empirical evaluations that include tasks such as those considered in the original HER paper.
This paper is close to the borderline, but I think it is good enough that I recommend its acceptance. Although there were some problems raised by the reviewers, the authors managed to successfully address a majority of them. Having said that, I still recommend that the authors carefully analyze the reviews again and make sure that they incorporated reviewers  comments in the final version of the paper. A lot of them were constructive and might improve the quality of the paper.
The paper introduces a framework for enforcing constraints into deep NNs used for modeling spatio temporal dynamics characterizing physical systems. The authors consider different types of constraints (pointwise, differential and integral). They start from a formulation approximating PDEs as set of ODEs (method of lines). Their main idea is to approximate the solution of the equations using an interpolant between observations and imposing the constraints on this approximation function. The interpolant is built using basis functions located at observation points. The formalism considers irregular spatial grids and both soft and hard constraints. The main claim is then the introduction of a general formalism for considering different types of constraints on irregular grids. Experiments illustrate the behavior of the proposed method on different types of evolution equations and constraints.

The reviewers agree that the proposed approach is interesting and that some of the ideas are original. However, they also consider that the paper is not convincing enough to demonstrate the interest and novelty of the approach, compared to alternative methods. The experimental section mainly considers (except for one application) regular grids and constraints that could be handled by other methods as well. The authors should present cases where their method provides a clear advantage, distinct from existing solutions. The authors provided a well argued rebuttal, clarifying several points. However, all reviewers retained their original scores and encourage the authors to further develop the experimental analysis to present a stronger paper. In addition, the presentation could be improved, and some technical aspects better explained (e.g., description of interpolation methods, and some advice on which interpolant to choose for a given problem).
The authors present a method called "AdaRL" that learns a structured latent representation that characterizes relationships between different variables in an RL system. The method is evaluated on modified Pong and Cart Pole domains and it is shown to outperform other transfer learning baselines. The reviewers agree that the method makes sense and addresses an important problem of transfer in RL. The authors did a good job in the rebuttal to empirically validate their claims and provided extra experiments. The reviewers also point out that the evaluated domains are rather simple and the paper would benefit from evaluations in a more complex environment as well as better writing. Please focus on improving these aspects in the final version of the paper.
The authors develop a novel framework for certifying the robustness of RL agents against data poisoning attacks. They obtain lower bounds on the cumulative reward for several benchmark tasks.

Reviewers had concerns about certain organizational and technical aspects of the paper, but these were addressed well in the discussion phase and author responses. Hence, I recommend acceptance. However, I would urge the authors to incorporate points from the discussion phase into the revised version, in particular the discussion with reviewers xuEG and RQX2.
The aim of this paper is to propose a novel "GW" like discrepancy function between probability measures living in different spaces (here restricted to be Euclidean, with a squared euclidean distance as the base metric). While interesting (notably the idea of learning distinct maps mapping a random direction in a latent space onto two spaces) there are a few issues with presentation, incremental nature of work and importantly a few shortcomings in the empirical evaluation as detailed by reviewers. Hopefully these can be used to improve the draft for a future version.
This paper extends the Neural Relational Inference framework for probabilistic inference of interaction relations between entities, to a scenario where entities may have private features, which requires modifications of the standard graph encoders and decoders in NRI.

Reviewers appreciated both the model and the overall execution of the paper: the building blocks are clear, the evaluation does its job well. The main doubts are about the applicability of the setting, for which the authors don t provide too many examples. However, the construction is somewhat intuitive, and even in cases where private attributes aren t explicit, it may be valuable to disentangle the shareable attributes this way. We encourage the reviewers to discuss the applicability a bit further.

Typos: (not exhaustive, please doublecheck with a spell checker)
   multiple occurrences of Gumble instead of Gumbel
   bottom of pg 4, factorzied  > factorized
This paper tackles a really interesting and realistic problem: how does contradictory (potentially) fake information affect QA systems? The authors try to approach this problem by building a new dataset, starting with the widely used SQuAD and adding contradictory information. This is quite interesting, but the rest of the paper does not follow through. Reviewers ask a critical question: how would you distinguish the information that is fake, as opposed to valid, truthful information? Without this distinction, how would you train a language model to detect the fakeness and answer the question using the valid information? Unfortunately, the authors did not reply to this critical question, so it is difficult to judge the validity and contributions of this paper. There are also serious ethical implications which are discussed in the ethics review.
The paper modifies DPMs by replacing the denoising L2 losses with GANs to learn the iterative denoising process. This leads to excellent results using a small number of refinement steps. In some sense, this also takes away one of the key advantages of DPMs over GANs, which is DPMs minimize a well defined objective function. Nevertheless, the results are convincing, but not spectacular. I am not convinced that we should continue to report training FID on CIFAR 10. I would have like to see class conditional ImageNet results. Also, it is not clear whether the proposed technique provides additional gains on top of SoTA GANs. Overall, I recommend acceptance as a spotlight.
This paper tackles the challenging problem of learning against an opponent that may or may not be simultaneously learning as well. The key contribution of this paper is a learning algorithm that accounts for how the opponents may update their policies from past interactions. The proposed algorithm, MBOM, relies on the environment model to model a hierarchy of opponents using different depths of recursive reasoning (from non learning agents to deep recursive agents). It is agreed that this papers studies an important problem and shows promise. However, the current results aren t convincing enough. In particular, since there is no theoretical analysis, more empirical validation of the method is expected. The current experiments only considers a single opponent, and it is unclear how well the method works given accumulated errors through the recursion. Future submissions would benefit from additional empirical analysis (e.g., ablations) to help understand when and why MBOM works.
This paper gives a framework for using learning in combinatorial optimization problems.  In particular, active search is used to learn hueristics. The reviewers thought the paper had nice conceptual contributions for this approach and that the results would be very interesting to the community.
This paper proposes integrating three existing approaches to give a simple algorithm called TAIG for generating transferable adversarial examples under blackbox attacks.

In the original reviews, some strengths and weaknesses of the papers were highlighted although some of them have not reached general agreement after the discussion period.

Regarding the merits, it is generally felt that the experimental results are good and the idea of updating along the integrated gradients is new (despite a simple idea) and has some theoretical justification.

Nevertheless, even after the discussion period, some concerns still remain, including the technical novelty of the proposed method and the high computational requirements of the proposed method, among others.

We appreciate the authors for responding to the reviews by clarifying some points and providing further experimental results. The paper would be more ready for publication if all the comments and suggestions are taken into consideration to improve the paper more thoroughly.
Finally, all reviewers leaned towards rejection. The main concerns were missing methodological depth and questions regarding the experimental evaluation (unclear link between experimental outcomes and methodological details). The rebuttal was not perceived as being fully convincing, and finally nobody wanted to champion this paper. I think that this work has some potential, but in its present form, it does not seem to be ready for publication.
This paper considers the valuation problem for a cooperative game, and shows that some classical metrics (e.g. Shapley value), can be considered as approximations to the maximum entropy.

Reviewers were generally very positive. They especially praised the novelty and writing quality, while having some concerns about the quality of the empirical results. The authors did an excellent job responding to the reviewers, and resolved their main concerns. A few quibbles remain, however, and while the manuscript is very good as is, please consider the reviewer criticisms in creating an updated version.
Summary: Authors present an approach for transformer based object detection that “fully pretrains” the encoder structure of the transformer, and drops the pretrained convolutional backbone used in other works. 

Pros:
  Eliminates need of extra visual backbone
  Fewer parameters than other works
  Achieves competitive performance, especially controlling for model size

Cons:
  Multiple reviewers raised concerns about authors only evaluating their approach with pretraining from ImageNet 1K, which is not considered large scale. Authors replied with new experimental data including pretraining from ImageNet 22k, which improved results. 
  Multiple reviewers raised concerns about the need for more ablation experiments, which the authors addressed.

Reviewer scores lean toward accept. Those that lean toward reject raised issues that the authors appear to have addressed sufficiently. Not all reviewers have replied to authors, though understood at least some reviewers on this paper are on end of year time off.

Overall recommendation: accept.
This paper proposes an architecture for learned surface parameterization, with application to image unwarping, which can be coupled with differentiable rendering, multi view data, and other modern objective terms.  The shape of the document is parameterized using an SDF technique, coupled with neural rendering and objective terms inspired by classical geometry processing.  This machinery is quite "heavy," leading to slow training times.

As pointed out by reviewer QH85, there were some experimental discrepancies rightfully acknowledged by the authors which make comparisons to DewarpNet less favorable for the new method, at least from a quantitative perspective.  Visual inspection makes the comparison more favorable, although it would be preferable for the quantitative quality metrics and qualitative examples to align.  

Runtime measurements here are also not favorable and severely limit applicability of this technique in real world scenarios, as pointed out by reviewers hfPz and QH85.

While the mistaken quantitative results are forgivable, the AC agrees that the scope of this work is quite narrow; it is not clear where this architecture would be applied relative to the motivating application.
In this paper the authors consider a contextual batched bandit setting where they rely on  imputationin order to estimated the non executed actions in each batch. Even though the idea is quite ineteretsing, and can lead to new methods, there is still a lof of issues raised by the reviwers. In particular, part of the proof was incorrect (and the authors tried to fix it) but given the short time, the reviwers felt that this part should be rewritten and scrutanized further. Also, there are many suggestions by reviewers that the authors need to apply in order to make this work publishable.
After carefully reading the reviews and the rebuttal I feel the paper fails slightly short. 

Unfortunately some of the issues that I have are aligned with the feedback from reviewer 6YwU and pULY. 
A significant part of the paper is the formalism and theory introduced by this work, followed then by the empirical evaluation. The theory I feel is not sufficiently well formulated. I understand this is a complex topic, and one can only make minimal statements about a system (particularly when learning is involved). And I understand that the authors are looking at a slightly different phenomena, and not the traditional vanishing/exploding gradient problem, where they consider a per unit scenario. And I believe one can make a case that this alternative definition holds value and should be investigated. 

However, I believe being more explicit of this alternate view, and make sure that one does not go into the theory with the wrong preconception of what these results are about is important. And secondly making sure the claims are adequate is important and not overly strong (or over claiming). I think this is important particularly in such works, dealing with systems that do not allow a full mathematical analysis.  In particular, just to give some examples:
 1. Thm 3, pointed out by the reviewers as well. I don t understand the point of this thm. It basically says that around initialization things are well behaved. The same can be said or proven for many other methods. You argue that this is different, as in other models beside initialization forgetting is not controlled, while you could potentially control it by a forgetting gate. However this is not a theoretical, precise argument. The forgetting gate is learned as well. If we go back to the LSTM scenario, LSTM suffer for vanishing gradients. Also Gers et al. paper does not prove that trying to preserve error has to harm learning (it provides some empirical evidence that is the case, but there have been many other things that affected this results). The point here is not that forget gates are not useful, nor that the gating mechanism proposed by LSTM are not extremely useful. They are. Is that the Thm 3 can not prove or show that using mmRNN is a better way of mitigating (and trading of) vanishing gradient than another model. You do that through your empirical evidence, and I think that is how most of ML works. But is not clear what the point of the theorem is. 
 2. I do not understand how one reasons theoretically about epsilon in Def 1. I don t see how an empirical observation by Gers et al resolves this. It justifies maybe why vanishing gradients are not always problematic, but that should not affect the definition of what vanishing now means. In the current form, if T goes to infinite, even if technically the network does not suffer from vanishing gradients, the gradients go to 0. Or at least T and epsilon should somehow be tied together to make the definition work. 
 The issue of defining the vanishing / exploding gradient per unit is also that now is not clear what is problematic or not. Probably having exploding gradient for any given unit is bad, as it might affect the overall gradient. But having a few units suffering from vanishing gradient, is that problematic?  This things need to be quantified better.

I think overall to me the problem is that some of this mathematical statements do not seem to be strong enough or contextualized enough to be properly understood by the reader. I would have understood the formalism if it was trying to correct some misconception in the community, case in which it is important to formalize just to be precise. But I don t think this is what is happening here. As in stands it just feels sloppy.

And I think this retracts considerably from the empirical side of the work and reduces the space you had to give it enough attention. Which should have played main stage. I think the empirical work would have benefited from more analysis (showcasing some of the arguments you were making using the theory), which would have made for a much stronger and convincing paper. The current framing of the paper is unfortunately not the right one.
This paper proposes an elegant approach to object detection where an encoder network reads in an image and a decoder network outputs coordinate and category information via a sequence of textual tokens. This method does away with several object detection specific details and tricks such as region proposals and ROI pooling. The paper received positive reviews from all reviewers who agreed that this formulation of object detection was novel and provided a new perspective that may transfer to other computer vision tasks. One common concern amongst reviewers was the slow inference time due to the sequential nature of the decoder   and this concern was a central point of discussion between the authors and reviewers. My takeaway from this discussion is that this model is certainly slower than traditional computer vision models that can generate boxes in parallel. The slowdown however, is image dependent. Less cluttered environments require shorter output sequences. Moreover, such a model can easily be applied to concept localization, e.g. "Locate the horses", in which cases one can expect fewer objects of the desired category, and hence acceptable inference speeds. Importantly, the contributions of this paper are noteworthy in spite of the proposed architecture having the drawback of being slow. Given this, I recommend accepting this paper for its merits.
The authors propose a well presented approach to likelihood free inference. The reviewers are all in alignment in recommending this paper for acceptance. There was a healthy discussion between authors and reviewers, where the authors have already incorporated many of their recommendations. The potential for this methodology to be applied to situations with expensive simulators should be intriguing to a broad audience. As a result, I recommend for this paper to be accepted as a spotlight.
This manuscript presents a method to allow RNNs to chain together sequences of behaviors. Reviewers had numerous concerns but the most important is that the problem posed here is solved by a simple method: resetting the state of the RNN before processing a motif.

Overall, reviewers noted a few key topics, although this list is not exhaustive:
1. Experiments are in a very simple but confusing setting.
2. Even though alternatives exist to solving this problem, they are not considered.
3. The networks considered are very simple.
4. The manuscript is difficult to understand.
5. The task admits a trivial solution.

In more detail:

1. The setting of learning to memorize time series and outputting them on command is very simple compared to what most modern work considers. Moreover, there is much confusion in the manuscript about what the setting is precisely.  For example, the setting is described as "independently learn motor motifs in order to build a continuously expandable motif library". But there is no continuously expandable motif library, the motif library is fixed at test time. The authors focus heavily on calling this setting "motor motifs", but these are RNNs that output an arbitrary time series. They are in no sense motor programs and this work is not connected to the extensive literature on motor control in machine learning. More broadly, there is no clear mathematical definition of what the problem being solved is anywhere in the manuscript.

2. It is unusual for manuscripts to not present other baseline models. But more importantly, many other approaches exist to this problem. As one reviewer pointed out, the manuscript essentially sets out to solve a problem that is completely solved in machine learning today. It rejects the solutions that exist for arbitrary reasons, and then adopts its own new solution.

3. The models used are very simple, but this is a consequence of 1, the problem domain being very simple.

4. Reviewers had difficulty understanding the details of the task. In particular, the task description section begins with minutia about the implementation rather than succinct definition of the task.

5. Most critically, reviewers identified that the model could be hard reset and would have the same behavior as the model presented in the manuscript. The proposed solution is essentially hard resetting the state to zero as it stands. There is no reason why a hard reset cannot be followed by a smoothing operation   this seems to be the main objection of the authors.

Overall, the manuscript needs significant improvements. The task considered is too simple by modern ML standards and the fact that it admits a simple solution cannot be overlooked. Demonstrating the idea of the preparatory module on an existing ML task and dataset, while comparing with existing baseline models, carrying out ablations, and producing an extensive quantitative evaluation is what will get the community excited about preparatory modules.
The paper proposes a framework for learning the physical parameters of a physical system’s dynamics from a video. The model combines a differentiable neural ODE solver (NODE) with neural implicit representations through a local coordinate based network which reconstruct the frames based on the ODE solution. Both the static background and the moving objects are modeled via implicit representations. The system being differentiable, it can be trained to recover the physical parameters and the initial conditions of the ODE. Experiments are performed on two toy problems (pendulum and masses that are connected by a spring).

The reviewers agree on the originality of the approach. They however all consider that the paper falls short to demonstrate the potential of the proposed approach because of limited experiments, limited ablation analyses and comparison with baselines. The authors added a new experiment during the rebuttal, but this was not found sufficient to change the reviewers’ opinion.
This paper considers a variant of adversarial weight perturbations / sharpness aware minimization for graph (convolutional) neural networks for node and graph classification. In particular, they make two adjustments: “truncating”, i.e., limiting the weight perturbation to specific layers, and weighting the sharpness aware loss with the regular loss during training. The reviewers found that the theoretical justifications (characterization of vanishing gradient and understanding of non iid setting which was added during rebuttal) are interesting, but several reviewers also found the solution/empirical results not convincing enough. I recommend the authors to either shift the focus to the theoretical results or to strengthen the empirical results (and their connections with theory) following the comments of the reviewers.
This paper proposes a kernel diffusion method to improve upon density based clustering methods. The reviewers found the empirical results quite promising and there is consensus that there are some good ideas in this work. However, their criticisms are strikingly consistent that the technical details are lacking and some of the claims are not fully supported, and these criticisms were not found to be fully addressed in the author responses. I agree with the assessment that this is promising in a major revision toward a future submission but it is currently not complete, especially in the theoretical and technical details.
The paper studies the lottery ticket hypothesis in the context of deep image priors. Deep image priors are convolutional neural networks that are imposed as a prior for image reconstruction problems. A deep image prior can be an un trained convolutional network, or it can be a trained generator, and the paper considers both types of priors. Deep image priors are often highly over parameterized and thus the paper under review asks the question on whether the networks really have to be heavily parameterized or whether a small subnetwork will also do. The paper performs experiments on image restoration with an entirely un trained DNN (this constitutes the larges part of the paper) and on image restoration with a pre trained network. 

The paper received four reviews out of which three recommend weak acceptance and one strong acceptance.
  Reviewer wMsj finds it interesting that the paper shows that some networks are more suitable as deep image priors than others, but finds that the paper lacks evidence on why some structures are better than others.
  Reviewer Kjf3 strongly recommends acceptance (8) in a relatively generic review. The reviewer finds the paper is interesting as it provides a novel application of the lottery ticket hypothesis. However, the review also criticizes that the experimental evaluating lacks rigor.
  Reviewer jFko appreciates that the paper studies the lottery ticket hypothesis for the first time for un trained and pre trained image prior, and that the results are interesting as they suggest that the models can be made smaller and that this can even improve performance. The reviewer criticizes that the paper s presentation is confusing, and points out a few weaknesses in the empirical evaluation. The authors responded and after a brief discussion, the reviewer raised their score. 
  Reviewer vf8k provides a relatively brief review and criticizes that to find sparse networks, one requires the ground truth image, which is not accessible. The authors clarify that the method is transferable in that the network identified can be used for other images and is thus transferable. The reviewer was satisfied with this response. 

The score of this paper 6.5 after the discussion period. Three of the reviewers are on the fence about the paper, one reviewer is not, and that reviewer significantly impacted the score. This reviewer, however, did not provide convincing arguments about the merits of the paper. All reviewers find that `3: Some of the paper’s claims have minor issues. , and I agree with that statement.

I do not recommend acceptance of the paper in its current form, because of insufficiently rigorous experiments to justify the claims:
  Specifically, the paper s goal is to address the research question  do they [neural network based priors for image reconstruction] really have to be heavily parameterized?  The literature already answers this question since as the paper under review reads on page 3, the literature found that an under parameterized non convolutional model can function as an un trained image prior. Those underparameterized networks perform well and have fewer parameters than the best performing networks found in the paper under review.
  The paper argues that a sparse network can give better performance. This claim is based on an at most 0.1dB difference, which can only be achieved when choosing the optimal sparsity level, which is not clear to do without knowing the ground truth image.
This paper proposes X Mixup, a model that considers the source languages and target languages together for cross lingual transfer. The designed model takes a pair of sentences (or the translated sentences) in a source language and a target language as the input and computes the cross attention between them.  

The empirical results are convincing. Reviewers think this paper is well written and the idea is interesting.
*Summary:* Study generalization in kernel regression discussing the NTK case and experiments on finite width nets. 

*Strengths:* 
  Mix of theoretical and empirical results in an important topic. 
  Advances a promising recent line of work. 

*Weaknesses:* 
  Concerns about novelty and lack of comparison with existing works. 
  Concerns about insufficient contextualization of new notion of learnability. 
  Concerns about scope of results in relation to claims. 

*Discussion:* 

Reviewer gb7t (3) found their concerns about lack of novelty and comparison with prior works not sufficiently addressed in the authors’ responses. 7tiq (6) found the line of investigation promising, but also issues with presentation and found the theoretical results incremental. Mosm (5) finds that the theoretical part pertaining kernels does not offer much novelty and that the paper should have focused on the empirical study that links the NTK spectrum to generalization. q2g8 (8) confidently considers this a good paper. In their view it provides a nice theoretical analysis of generalization in the setting of kernel regression and the metric of learnability intuitive. However, they also found that the detests of the experiments are very artificial and problematic the desire of the article to extend the regime of the results to make claims about deep learning. 

A the end of the discussion period, the official reviewer ratings are mixed 3,5,6,8, indicating various strengths and weaknesses (also in case of the most favorable reviews). From the reviews and discussion, I infer that the topic is worthwhile and relevant, but at the same time that the paper might not be sufficiently convincing in its current form. Therefore I lean to reject the paper. To arrive at a clear conclusion, I consulted two additional researchers. 

*Additional assessment 1:* 

The first additional assessment found the work  underwhelming  but admitted there is a chance they might not have fully understood the work. 

*Additional assessment 2:*  

The second additional assessment provided following comments: The paper s first contribution (conservation law), I didn t see it elsewhere but I think it s quite expected. The testing performance of low frequency target functions and high frequency target functions are averaged. Thus the average performance is constant which is independent on the kernel. However, in practice, kernel learning performs well because real target functions have low frequency. And the very high frequency functions are unrealistic. 

About the paper s second contribution, I think the paper needs to explain how the result is different from Bordelon et al. (2020). I noticed that the method they use is different but the result seems quite similar. The paper also consider noiseless case and gives an approximation for MSE. 

Also the paper should explain more about the approximation being used. For example, how much error the approximation introduce and how the approximation is different from Bordelon s approximation. I see that in the proof Φ is approximated by a matrix where each element is standard Gaussian. For me I can t understand why the approximation is reasonable. 

I read through the reviews and rebuttals. I didn t see the discussion of the issue of approximation. But I think it s a major issue and the approximation is a very strong assumption. The appendix states: "we have made an approximation using the central limit theorem assuming that Φ is random with entries sampled i.i.d. from N (0, 1)". Here Φ is the matrix of eigenfunctions. Hence it is not clear how to apply the central limit theorem. 

*Conclusion:* 

I conclude that although the paper presents some interesting ideas on a relevant subject, it still has much room for improvement. Hence I recommend to reject this article. I encourage the authors to revise taking the above comments into consideration.
The authors propose a new continual learning setting with a few distinguishing features: 1) the task boundaries are blurry (in other words, past task samples can reappear); 2) training is online; and 3) evaluation using online accuracy (instead of average accuracy). The authors also propose a useful method for this scenario and benchmark it using four different datasets.

The first round of review pointed to two main limitations of the manuscript. 
+ The authors only provided small scale experiments. The reviewers argued that for the setup and method to have an impact having good results using larger scale data would go a long way. 
+ Whether “task free” and “class incremental” were compatible. 

For the former, the authors were very reactive and provided results using a standard "ImageNet for CL" dataset. 

For the latter, I must thank the authors and also the reviewers for discussing this thoroughly. In the end, my understanding is that there was a reconciliation that both were in fact compatible, but the reviewer suggested that this be discussed very clearly by the authors. I second this suggestion. The CL field given its many slightly different settings might be partly to blame here (reviewer Vfw2 made a similar comment, and I also thank them for playing a role in resolving the issue).


A few additional thoughts: 
+ I believe that more general setups in CL are worthwhile even in the absence of any immediate applications. This is especially true since some of the standard CL assumptions do not seem to be well motivated. However, I find that claiming that something is more realistic requires grounding (e.g. a set of examples from the "real world" or a specific domain/setting). I know the authors backed some of their claims with references, but different real world problems will come with different limitations and I would be hesitant to use phrases such as "most real world" settings without thorough justification.
+ While different from the core of your work, I believe the framework proposed in this other recent paper has similar goals (although the setup allows pre training and is not online). Might be worth knowing about it in case you do not: 
Online Fast Adaptation and Knowledge Accumulation (OSAKA): a New Approach to Continual Learning, NeurIPS 2020
https://papers.nips.cc/paper/2020/file/c0a271bc0ecb776a094786474322cb82 Paper.pdf

All in all, this is a good contribution that proposes an interesting and rich setting along with a good baseline method for it. I strongly encourage the authors to follow through on their promise to provide the community with code, dataset splits, Kaggle leaderboard, etc., as a way to maximize the impact of their work.
The reviewers have agreed that the paper is in borderline. Although the reviewers are not really convinced about the authors’ responses, they still acknowledge that the paper is interesting and developed some new techniques for the analysis of distributed optimization. 

The following concerns are raised by the reviewers from their discussions: 

1) The paper is heavily based on existing work. 
2) The theoretical advantages are based on the regime Hessian variance is 0 or small, but it is not clear if and when the Hessian variance is small for more complicated models, which means we will not know if the theory will be helpful in practice. Although the authors provide some experimental results in the rebuttal showing that $(L_+)^2 / (L_{\pm})^2$ can be large at initial iterations, it is still not clear how long will this advantage keep during training and how much the advantage is.
3) Reviewer wjjy increased the score from 5 to 6, considering that the additional result truly suggests that the implicit setting can hold in some case at the beginning of iteration, which makes the submission a complete story for him/her now, to some extent. But if we are more strict on the evaluation, the experiment result also suggests that the implicit assumption will not hold anymore over iterations, because the ratio is approaching 1 quickly, i.e., $ L_\pm$ is about the same order as $ L_+$, so there is a mismatch between theory and practice, which even brings out the risk that the paper will fail from the beginning because Sec 4.1 will not make sense anymore.
4) On the theory side, two main contributions of this paper is relaxation on the compressors used in MARINA and a new assumption to refine the analysis. These two contributions seem rather limited if only used to analyze this specific algorithm   it s unclear what the authors mean in practice or how they correlated to the MARINA. For example, it is still hard for us to compare or understand MARINA with another algorithm as we wouldn t know if the improvement from MARINA is due to a better design, or this additional assumption. The reviewer also finds the authors statement that "their analysis focuses on MARINA because it is SOTA" confusing. Different from NLP and vision community where standard benchmarks are usually used to evaluate new models, He/she is confused by what it means for a newly proposed optimization algorithm to be SOTA.
5) Since the paper proposes a specific algorithm named PermK, it s quite reasonable to question how it relates to some previously proposed sparsification methods with similar design such as (Jianqiao et al., 2017). However, the authors insist their main contribution is in theory, and the small scale experiments comparing with TopK and RandK are sufficient. The reviewer disagrees about this. As communication compression is usually need in larger scales (at least beyond MNIST), and TopK/RandK are not SOTA baselines of sparsification.

The authors are expected to address them for the clarifications in the final version.
Overall, this paper receives negative reviews due to limited technical novelty and contributions. The authors  rebuttal does not address all the raised concerns. As such, the area chair agrees with the reviewers and does not recommend it be accepted at this conference
The paper proposed a new VAE based generative model for generating molecular conformations from graphs. Reading the paper, the reviews and the rebuttal, it looks like this project is a work in progress and not yet ready for publication. Some reviewers indicate that the paper lacks significant technical novelty. 

This is a worthy application. I encourage the authors to keep working and resubmit to another venue when they feel the work is ready and they have addressed all points raised by the reviewers.
This paper explores ways in which *emergent communication* (EC) methods from representation learning can be evaluated extrinsically, by hooking them into downstream NLP tasks. Reviewers agree that the paper is thorough, and finds encouraging results.

This paper is borderline, and difficult to evaluate, even after very substantial discussion (some of it private). From my reading of the reviews and pieces of the paper, I m very sympathetic to wvqW s concern that none of the present day applications under study seem likely to benefit from this kind of emergent communication pretraining: *Natural* language pretraining, even transferring across natural languages, is for too strong a baseline, and it s not even conceptually clear how one could substantially outperform that baseline. I m very concerned that the results in this paper will be—misleadingly—cited as proof that EC research is already contributing to downstream progress in NLP.

However, the narrow claims in the paper itself seem to be sound, and two confident reviewers whom I trust argue strongly that the ideas results here are surprising and novel, and that the paper could be the starting point for productive discussion and future work in this area. I m recommending spotlight presentation in the hope that the paper will provoke a nuanced discussion in that setting.
The paper conducted a thorough experimental analysis of the attention map in the Conformer models for CTC based speech recognition models and connected it with phonetic and linguistic information in the speech. Using these insights, the paper presented some computation improvement and marginal quality gains. The authors actively conducted additional experiments to further justify the claims. The paper is strong in terms of the systematic way of in depth analysis and further development (i.e. sharing the attention map across layers for speedup). But as pointed out by the reviewers, it lacks some comparisons with other alternatives to justify the importance of sharing attention maps in reducing computations.  Also it would be better if there s justifications on how the observations generalize to other types of models (such as LAS, RNN T). 

The decision is mainly because of the thorough analysis conducted in the paper which can be a good contribution to the community.
The paper proposes an efficient RL based approach for solving the weighted maximum cut problem. The proposed approach shares high level insights with prior work such as ECO DQN (Barrett et al.) and S2V DQN; the key contribution is to demonstrate that the proposed cheap action decoding and stochastic policy strategy can improve the scalability without sacrificing much of the quality of the solution on the tasks considered in this paper.

The reviewers in general find the paper well presented, and especially note that the clear motivation for improving the efficiency of current GNN based RL baselines, particularly represented by ECO DQN. 

A common concern among the reviewers is that the original title is misleading; the authors acknowledge that they should properly position the paper to avoid confusion that they were to address general combinatorial optimization problems (as the current title suggests). Notably, many combinational optimization problems can be reduced to max cut as suggested in the authors’ responses; demonstrating the performance in (some of) these problems via a max cut reduction would be helpful to support the significance of this work.

Beyond the title and positioning of this work, there were also initial confusions among the committee in terms of the choice of both (RL or supervised) learning based and heuristic based baselines. The authors did an excellent job in clarifying many of the questions in terms of related work and baselines (the clarity of the work has improved over the rebuttal phase). However, despite the additional ablation study and newly added baselines, there remain concerns/questions in the choice of task domains (lack of hard problem instances where existing solvers, learning  or heuristics  based may fail due to (possibly higher) computational complexity). Given the empirical focus of the paper, this appears to be an important concern, and not all reviewers are convinced the current empirical results are significant to warrant acceptance of this work.
The paper presents an approach to learn the surrogate loss for complex prediction tasks where the task loss is non differentiable and non decomposable. The novelty of the approach is to rely on differentiable sorting, optimizing the spearman correlation between the true loss and the surrogate. This leads to a pipeline that is simpler to integrate to existing works than approaches that try to learn a differentiable approximation to the task loss, and to better experimental results.

The paper is well written and the approach clearly presented. The reviewers liked the simplicity of the approach and the promising experimental results on a variety of challenging tasks (human pose estimation and machine reading).
The paper proposes a method for selecting a group of pretext tasks out of a set of candidates in order to optimize self training for downstream performance. The method relies on Hilbert Schmidt Independence Criterion (HSIC) and uses a few data samples to select weights for the given set of tasks The paper demonstrates that using the method for task selection can result in learning better representation for downstream tasks improving accuracy on speech, speaker and emotion recognition.

The reviewers had concerns mostly about the strength of the empirical results. In particular, they felt that the baselines are not strong enough. To the authors credit, the paper was augmented with some of the missing experiments that the reviewers pointed out (e.g., wav2vec plus naive task selection), but that did not persuade reviewers to change their recommendations.

The paper still misses the point that self supervised learning approaches can benefit from training larger models that result in better results. These comparisons are missing from the paper. It is established in other work that findings such as the use of pretext tasks often do not carry over to larger scales. Furthermore, the idea of pretraining a model specific to a downstream task is not inline of the philosophy of self supervised training that aims to train a single model that can be used for many different tasks.
Three experts reviewed the paper and all recommended acceptance. Based on the reviewers  feedback, the decision is to recommend the paper for acceptance. However, the reviewers did raise some valuable concerns that should be addressed in the final camera ready version of the paper. For example, the discussion about or comparison with related works, clarity of the writing, suggested experiments, etc. The authors are encouraged to make the necessary changes to the best of their ability. We congratulate the authors on the acceptance of their paper!
I agree with the reviewers that this work is not well presented, and it seriously lacks rigor and experimental support. The writing of this work also needs significant improvement. The authors made many claims without offering rigorous proofs, and hand waved their argument throughout without strong empirical support. In the end, the authors  response did not address the reviewers  concerns satisfactorily and no one is excited enough to defend the current draft. Please consider revising your draft according to the reviewers  comments.
The authors study the training settings that may affect active deep learning performance, including code/warm start, leveraging unlabeled data, and initial set selection, for each active learning strategy. The findings on several data sets help understand AL more, with some pieces of insights to inspire future research.

The reviewers were at best lukewarm about the work prior to the rebuttal. Some turned more positive but none were willing to strongly champion for the paper s acceptance, even after the authors provided a decent rebuttal. This leaves the paper to be a borderline case, and the recommendation comes from carefully checking the latest revision and calibrating its score with other submissions.

The reviewers are generally positive about the breadth of the study, the potential impact of the codebase and the systematic study that can inspire future works. Some clarified issues include comments on future research directions and the labeling efficiency plot (which is, however, not analyzed deeper in the main text), and results on additional settings like transfer learning (somewhat preliminary). In the end, two remaining concerns surround whether the technical contribution and the conclusions are sufficiently solid, including

* limited insights: Some reviewers comment that the insights are on the lighter side. The authors identify several issues that may affect the performance of the underlying tasks of active learning, and find that the best setting differs across different active learning strategies. But given that the paper offers at best "best practices of training models on actively queried labels", it is not clear whether the authors achieve their claimed goal of "compare different strategies in a fair way" in particular, the conclusion for this particular comparison seems to be missing (e.g. which is recommended in practice, BADGE or LL4AL or others?). Also, given that only three data sets (5 after rebuttal) have been studied in this work (see item below), the "generalization ability" of the conclusions in this paper cannot be clearly established. While the authors provided some additional pieces in the rebuttal, the pieces can use more study to be fully conclusive. Some reviewers are also concerned that the conclusions are rather scattered.

From a practical perspective, it appears to be a chicken egg problem on whether to fix the active strategy first (and then train the model with the best setting/practice), or fix the training setting first (and then select the best strategy). The authors may want to add more arguments on why they focus on the former rather than the latter.

* limited experiments: several reviewers point out that the few data sets used could not fully justify the "best practice", and demand data sets like ImageNet. The authors offered some new results on TinyImageNet and CIFAR100, but those are not studied as deeply as other data sets at the current point. A more careful study on the two (and other) data sets are thus strongly recommended.
The reviewers were split on this paper: the positive review appreciated (a) how adaptive weighing can be viewed as part of energy minimization, (b) the flexibility of the model to work with different model backbones, (c) the demonstration that even in no noise settings the method generates noticeable improvements. However, all reviews saw important shortcomings in the (a) few out of distribution results, (b) limited ablation studies, (c) clarity of the writing, particularly in notation, (d) explanations of experimental results (e.g., why using pseudolabels sometimes deteriorates performance), (e) assumptions behind the proposed method, (f) lack of self training baselines, (g) limited technical novelty. Ultimately, the number and severity of the shortcomings outweigh the positive parts of the paper. If the authors take the reviewer’s recommendations into account the paper will be a much stronger submission.
The paper provides a thorough study of the evolution of Hessian depending on a wide variety of aspects such as initialization, architectural choices, and common training heuristics. The paper makes a number of interesting observations. Some of them are not really new but overall, the experimental evaluation of the paper makes it a valuable resource for the community.

The reviewers are overall quite positive. One reviewer notes that more investigation of the behavior of batch normalization is required. I encourage the author to address this concern in the final manuscript. There is a lot of recent work on batch normalization that might be worth discussing, e.g.:
Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs
Jonathan Frankle, David J. Schwab, Ari S. Morcos

Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks
Hadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, Aurelien Lucchi

A Quantitative Analysis of the Effect of Batch Normalization on Gradient Descent
Yongqiang Cai, Qianxiao Li, Zuowei Shen
The paper proposes a method for learning identity preserving transformations through a set of learned Lie group operators. It builds upon previous work ( (Connor & Rozell, 2020; Connor et al., 2021) addressing two points: (i) how to select semantically related pairs of points, (ii) how to identify which operators are appropriate for a given local region of the manifold. Authors use nearest neighbors computed via the penultimate layers of a pretrained network to address (i), and learn a separate network q(c|z) that predicts the coefficients given a latent input z. Reviewers have two main concerns with the paper   limited novelty over earlier work that learns the Lie group operators; and the complicated nature of the method which needs training in three stages and uses a pretrained ResNet for finding nearest neighbors. Lack of comparison with relevant baselines is also pointed out by the reviewers. Given these issues, the paper is unfortunately not suitable for publication in ICLR at this point.
This paper presents a new benchmark task for models similar to CLIP for evaluating how visual word forms interfere with the visual recognition of objects in images when the former are superimposed on the latter ones. Specifically, by superimposing words belonging to different categories  (e.g., hypernyms vs basic labels) the authors study the misclassification rates of CLIP under different degrees of varying similarity between the original and superimposes labels. 

All reviewers agreed that this is a novel and interesting study which, by productively using insights from cognitive science literature on language biases, aims at shedding light on the inner workings of a popular artificial model. The main concern raised by reviewer P83Y was regarding the claims around misclassification rates. Indeed, since CLIP was not taught (e.g., by fune tuning or few shot prompting) which of the two labels (i.e., the written or the visual) is the correct one, it s not fair to assess its performance on this way. While this is strictly true, the experimental protocols presented in Sections 4.3/4/5 are still a valid way to assess representational inference. Moreover, the authors have followed P83Y suggestions and incorporated a few shot prompting experiment in Section 4.6.

All in all, I think this will make for an addition to the ICLR program and thus I m recommending accepting this paper.

(Minor comment: WKSS rightly pointing that this paper has, at best, a loose connection to compositionality. The authors changed compositionality  > representations which is a better fit, so please make sure to change the title also in Openreview when prompted.)
The paper presents a backdoor attack approach against pre trained models that may affect different downstream languages tasks with the same trigger. The paper shows that the downstream models can inherit security holes from upstream pre trained models. 

The paper is on the borderline and disagreement remains after discussion and author responses. In general, the new setting introduced in the paper is interesting and well motivated. However, the options split in how realistic the setting is (e.g., use of uncommon trigger), the evaluation of stealthiness, and the novelty of the idea. After checking the paper, I believe the ideas and insights are justifiable for an ICLR paper and they differ significantly enough from the prior work. I do agree with reviewers that they are some 
limitations of the proposed techniques (mostly inherited from the prior work it based on). However, as backdoor attack in NLP is a relative new area, I would be more lenient on these weaknesses. 

The reviewers also provide constructive suggestions on how to improve the evaluation and writing. I hope the authors can address all the comments in the next revision.
This paper proposes a theory for double descent phenomena in denoting deep neural networks. There are two major concerns: (1) The assumption that the data lie in a low dimensional subspace is quite strong, and needs to be weaken or better justified. (2) The theory only works for r 1, where the rank is one. For general rank, how to apply the proposed analysis is hand wavy and not convincing. The paper can be significantly strengthen if these two issues could be addressed.
The paper examines a sum over paths representation of ReLU networks, for which learning can be broken into two parts: learning the gates, and learning the weights given the gates, the latter of which being described by the Neural Path Kernel. The paper introduces a dual architecture, Deep Linear Gated Networks (DLGN) that parameterizes these two processes separately. The DLGN is argued to aid in interpretability of ReLU networks, with a main conclusion being that the neural network is learned path by path instead of layer by layer.

The reviewers generally found strength in the motivation and perspective and thought that the DLGN could serve as a useful architecture for aiding interpretability. Some reviewers found the presentation hard to follow, and others were not entirely convinced by the ultimate conclusions. Overall, the reviewers opinions were mixed.

I believe the ICLR community would generally find interest in the DLGN and the interpretations it might afford to deep ReLU networks. However, the number and strength of the conclusions obtained in the current analysis are rather weak. The conclusion that networks learn path by path instead of layer by layer was emphasized but the implications were not highlighted, and it remains unclear to me and at least some reviewers what the concrete significance of this observation actually is. Another major claim is that the DLGN recovers more than 83.5% of the performance of state of the art DNNs, but a priori it is not obvious what this number means, or if it is even good or bad performance. A more detailed analysis with additional common baselines, ablations, etc., would really help readers understand the significance of the performance gap.

Overall, this is an interesting direction with significant potential, but for the above reasons I cannot recommend the current version for acceptance.
This paper addresses unique windowing schemes for the input of an LSTM model for time series forecasting, in particular an exponential partitioning, where bin sizes increase as moving further from the current time point. Although the basic idea is interesting and motivating and experimental results are strong; as reviewers pointed out, technical significance and novelty are limited because of lack of theoretical or conceptual justification and motivation the proposed approach.  The authors’ claim is primarily based on experiments results. Other critical issues include the lack of comparison with recent advances in specifically designed to attend to longer history length or the discussion of modern approaches. other issues include presentation (e.g., grammatical errors) and the use of acronyms before introducing them.
This paper aims to address the catastrophic overfitting issue in single step adversarial training. Specifically, this paper finds that 1) using larger random noise initialization and 2) avoiding clipping adversarial perturbations are the two keys for stabilizing single step adversarial training. 

Overall, the reviewers find this paper is well written and the empirical results look promising. The reviewers originally misunderstood certain technical details of this paper, but got clarified in the discussion period.  However, the biggest concern shared by the reviewers is that the motivation of using larger random noise initialization and avoiding clipping adversarial perturbation is pretty unclear they all fail to (either empirically or theoretically) understand how and why these two techniques are helpful to preventing catastrophic overfitting. Given the main contribution of this paper is a revisiting of existing techniques, it is a legitimate concern from the reviewer side for demanding the in depth empirical analysis or the theoretical proof to help them better understand the proposed method; otherwise, the novelty contribution of this paper may get trivialized. 

I encourage the authors to delve deeper into the proposed method and make a stronger submission next time.
This paper analyzes the extent to which parameterized layers within a CNN can be replaced by parameter free layers, with specific focus on utilizing max pooling as a building block.  After the author response and discussion, all reviewers favor accepting the paper.  The AC agrees that its empirical results open a potentially interesting discussion on network design.
The focus of the submission is the estimation of the Shannon differential entropy (DE). The authors propose a differentiable DE estimator referred to as KNIFE (Kernelized Neural diFFerential Estimator): it is a plug in method (5) using KDE (kernel density estimation; (4)). KNIFE has parameters including the locations (a), weights (w) and covariances (A) in KDE, which are tuned according to the upper bound heuristic in (6). The approach is illustrated on toy examples and in the context of training neural networks.

Estimating information theoretical quantities is a current topic of machine learning. Unfortunately, as assessed by the reviewers
1) the submission lacks context and comparison to available entropy estimators,
2) the estimator closely follows Schraudolph (2004); the technical novelty is quite limited.

More work and major revision are required.
This paper propose a reparametrization approach for pruning residual networks. The proposed approach replace the skip layer connections with feedforward layers, and show the equivalence to the original network. However, the current presentation is not very clear on the advantage of the proposed approach for pruning. As two networks are equivalent, pruning the reparameterized network can be transferred to pruning the residual network. The authors need to clarify how their reparametrized network is different from the residual network when being pruned. More ablation studies are also need to better justify their claim.
As an empirical paper, this paper studies uncertainty estimations with respect to various architectures and learning schemes. Three reviewers suggested acceptance based on the strength of the paper (fairly extensive experiments were conducted, and some new observations were discovered, such as the superiority of ViT). On the other hand, two reviewers proposed rejection due to lack of rigor in writing and lack of novelty. No consensus was reached through additional discussion. In particular, the reviewer s point that the experiment was not well controlled different models were trained with different hyperparameters etc  seems quite important, and it weakens the significance of the contribution of the paper. 

All reviewers agreed that it is a potentially interesting and important paper. I encourage the authors to resubmit in the future after carefully addressing the reviewers  concerns.
The theory and results presented in this paper provide a new method to avoid collapse in contrastive learning.  All but one reviewer recommend acceptance.  The lone negative reviewer is concerned with the limited experiments, but the other reviewers, and the AC, find the experimentation convincing enough to warrant acceptance.
The authors propose a flexible variational posterior approximation, relaxing unrealistic factorization and strong parametric constraints that are standard. There was a mixed reception from reviewers. Overall, the paper is on the borderline. The presentation and empirical investigation could be changed so that the nice contributions in the paper are more easily recognized. Indeed, after rebuttal several reviewers still felt like their concerns were not fully addressed. One reviewer was concerned about the evaluation metrics, and wanted to see Stein discrepancy instead of ESS, and did not feel the ESS was sufficiently motivated (as described in updated comments). Another reviewer felt the uncertainty of the predictive distribution was sufficiently well evaluated. Another reviewer generally satisfied by the response. The decision could go either way, but the paper would probably be more widely appreciated by a significant revision, carefully taking into account the questions of the reviewers. The authors are encouraged to accommodate reviewer questions in future versions of the paper.
This paper proposes a new theory for modified DRM and PINN for solving elliptical PDEs, and delivers valuable advances on important topics.
This work adapts the widely used DP learning algorithm to language models. Reviewers all agreed that this work tackles an important problem with clear motivation and thorough experiments, and achieved strong performance (memory reduction and effectiveness) on NLP tasks.  Thus, we recommend an acceptance.
This paper presents a package for "Dynamic Fine grained Structured Sparse Attention Mechanism" (DFSSATTEN), which aims to improve the computational efficiency of attention mechanisms by leveraging the specific sparse pattern supported by sparse tensor cores of NVIDIA A100. DFSSATTEN shows theoretical and empirical advantage in terms of performance and speedup compared to various baselines, with 1.27~1.89x speedup over the vanilla attention network across different sequence lengths.

Reviewers praised the simplicity of the method and the clean code implementation. Speeding up attention mechanisms is an important problem is leveraging sparse tensor cores for attention speedup is a sensible idea. The practical speedups are significant (1.27~1.89x over the vanilla attention across different sequence lengths). However, they also pointed out some weaknesses: the fact that the proposed method is very specific to the particular sparse pattern offered by NVIDIA A100, and not easily generalizable to other future hardware; the fact that the method focuses on inference acceleration and not training from scratch (not completely clear in the paper), which limits its scope; and the fact that the method still has O(N^2) complexity (it still requires the computation of QK^T, which has quadratic memory and computation cost), and therefore it does not really address the quadratic bottleneck of transformers, unlike other existing work in efficient transformers for long sequences. 

I tend to agree with the reviewers and, even though the package can be potentially useful to other researchers, the scope seems limited and the paper seems a bit thin to deserve publication at ICLR.

Other comments and suggestions:
  When talking about linear transformers, you should cite [1], which predates Performers
  It is not clear to me why 1:2 and 2:4 are called "fine grained *structured* sparsity"
  Citations for the systems in Tab 4 are missing
  When comparing to other methods, it would be include to include their Pareto curves since those methods have tradeoffs in terms of sparsity / approximation error (or downstream accuracy).

[1] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, François Fleuret (https://arxiv.org/abs/2006.16236)
Dear authors,

I have carefully read the reviews, rebuttals, and the subsequent discussion. Most reviews provide high quality feedback, and the reviewers  combined opinion is strongly oriented towards recommending rejection. I have to concur with this recommendation. While essentially all reviewers agreed that the paper is well written, they also raised several key concerns. I will reiterate (and further elaborate) on some of them here:

1) I do not agree with the authors  claim that there is a significant difference between client sampling in FL and data sampling in SGD in terms of the underlying mathematics; at least not in the present form. The mathematical formulation of both problems is the same. While in SGD it is possible to access local information and construct more powerful sampling strategies using this information, it is also possible to forgo using this information and propose simpler data agnostic strategies. Such strategies have been studies in the SGD literature before. I recommend the literature on *arbitrary sampling* pioneered by Richtarik and Takac ("Iteration complexity of randomized block coordinate descent methods for minimizing a composite function", Mathematical Programming, 2014) in the context of randomized coordinate descent. The same sampling approach was later adopted for SGD. The paper by Csiba and Richtarik (Importance sampling for minibatches, JMLR 2018) suggested by one of the reviewers is relevant here as it adopts the arbitrary sampling approach to (variance reduced) SGD. Such an arbitrary sampling paradigm is more general than the unbiased sampling strategy you study here (indeed, arbitrary sampling includes also biased samplings). The work of Chen et al mentioned by a reviewer is also more relevant than you appreciate. This work also mentions a couple decomposition statements. They work with the arbitrary sampling framework in the FL setting   and this seems more general than your framework since it includes biased samplings as well. Note that they then proceed to compute the optimal sampling out of all samplings, while you do not attempt to theoretically capture what samplings are best. This prior works thus addresses a similar problem, and goes deeper in this aspect. Also, the parameters $v_i$ in their Lemma 1 are simply just statistics of the sampling   and are unrelated to the data. Also, Lipschitz smoothness constant of the aggregate loss over all local data samples is not hard to estimate, and is needed anyway to set the stepsize correctly. So, your comments about the unavailability of these quantities in the FL setting seem incorrect.

2) Theorem 1 is indeed just a simple calculation / observation rather than a result. I agree with the reviewer who said that the value of this simple observation, without a deeper study of its consequences, and an explanation of how the consequences lead to new results that are in some sense interesting, is quite limited. As mentioned by one reviewer, sophisticated methods are often non monotonic: they do *not* attempt to greedily reduce some simple potential (e.g., distance to the minimizer) as such a strategy may be suboptimal from a total convergence point of view. This observation by the reviewer seems to have been misunderstood by the authors. This limits the impact of Theorem 1 as Theorem 1 assumes that one is interested in a greedy method.

3) The bounded variance and bounded dissimilarity assumption *are* strong. The suggestion by one of the reviewers to consider a work on more accurate ways of modeling stochastic gradients (Khaled et al) was appropriate. I suggest the authors read the paper to see detailed reasoning explaining why the types of assumptions you make in this paper are problematic. The fact that some other papers use such problematic assumptions, even if they are well known, is not evidence that these assumptions are not problematic. It is merely evidence that many papers share the same issue. I also have to oppose the authors  view that non peer reviewed work should not be brought up. In my view, this is a deeply problematic and unscientific attitude to research that is available online. Peer review does not imply correctness, and vice versa. 

4) Experimental comparison with any other methods is missing. Why do you not compare with the optimal sampling strategy of Chen et al, for example? Does your framework suggest a better strategy in some sense? If yes, show it. If no, then in what sense are your sampling strategies interesting? In any case, all have been considered in the arbitrary sampling framework before as far as I can see.

5) There are more issues with have been identified by the reviewers. I strongly recommend the authors to take all of them seriously in their revision.

In summary, this is a solid paper. However, it has some serious issues and for this reason I cannot recommend it for acceptance. Having said that, I thank the authors for their submission and wish them best of luck in future research with this project.

Area Chair
This paper studies optimization of over parametrized neural networks in the mean field scaling. Specifically, when the input dimension in larger than the number of training samples, the paper shows that the training loss converges to 0 at a linear rate under gradient flow. It s possible to extend the result by random feature layers to handle the case when input dimension is low. Empirically the dynamics in this paper seems to achieve better generalization performance than the NTK counterpart, but no theoretical result is known. Overall this is a solid contribution to the hard problem of analyzing the training dynamics of mean field regime. There was some debate between reviewers on what is the definition of "feature learning" and I recommend the authors to give an explicit definition of what they mean (and potentially use a different term).
The paper presents a new technique that infers the endogenous states of an RL problem, as well as the corresponding model and optimal policy.  A bound is derived that shows that the amount of data needed depends only on the number of endogenous states, while being independent of the number of exogenous states and the complexity of the observation space.  This is remarkable since this is the first technique that is shown to have a complexity that depends only on the number of endogenous states.  Furthermore, the bound derived is not just a theoretical bound.  It is a practical bound in the sense that it is used in the associated algorithm, which is demonstrated effectively on two problems.  Perhaps the main weakness of the paper is that no intuition is provided in the main paper to explain why the sample complexity can be made independent of the number of exogenous states and the complexity of the observation space.  The reader has to look at the proof in the supplementary material.  Nevertheless, this is remarkable work.
The paper proposes a mechanism for A* planning with learned policy and value functions. The experiments (restricted to the Sokoban domain) show that the runtime of guided search follows a heavy tailed distribution, suggesting that in many cases, the problem is either solved quickly or takes a long time. An abstract model is proposed to explain this distribution, and a number of mechanisms are proposed to overcome its challenges.

The reviewers thought the paper had some interesting ideas but found the experimental section to be especially weak. While the paper starts out with quite general claims, the experiments only consider a single domain. Also, key details about the experiments were missing. Finally, the writing feels rushed   the original submission had many typos and lacked proofs for two theorems. 

I agree with these objections and recommend rejection. Please revise the paper following the reviews and resubmit to a different deadline.
The paper proposes a method for time series forecasting based on a hierarchical deep learning approach. Three reviewers submitted reviews, with two marginally accept and one marginally reject. The paper was therefore borderline, but the issues raised by the marginal reject reviewer on the justification for the design choice of a deep latent model and the experimental setup appear worth addressing in a revision resubmitted to another conference.
This paper gives a new theoretical framework to characterize the expressive power of graph neural networks that describes GNN by tensor language (TL) and then makes it possible to analyze its expressive power through the lens of TL. The authors connect the expressive ability of TL to the color refinement algorithms and (vertex/graph) k WL algorithms. By doing so, the several existing results can be recovered in a unifying manner. In addition to that, the function approximation ability is also investigated. 

The paper gives a novel theoretical framework that gives a clear perspective to the problem of expressive power of GNN, which would be quite beneficial to the community and open up a new research direction. The reviewers have raised several questions on the paper, but the authors addressed all the concerns properly. Therefore, I recommend acceptance to ICLR2022.
This paper proposes a dynamic programming strategy for faster approximate generation in denoising diffusion probabilistic models.

All reviewers appreciated the paper, but they are not overly excited. 

Two reviewers are focused on the log likelihood not being the objective for image quality. This AC does not really buy this argument. 

The method and story around are well rounded and finished. So it is hard to think of any major modifications that will change the overall story a lot. One could therefore argue for acceptance as it stands. On the other hand this is difficult to argue for given the below acceptance level scores. 

So the final recommendation is reject with a strong encouragement to submit to the next conference. Updating the paper with preemptive arguments on why the ELBO and not FID is the right thing to consider.
By the scores, this submission is quite borderline. This paper introduces stochastic weight averaging into a few shot learning setting, The reviewers all agreed the work was sound; discussion after the author response focused on the theoretical justifications, degree of novelty and potential impact, and the empirical support. 

The primary concerns were that the work was slightly too incremental to obviously merit publication at this stage: though the empirical results were sound, they mostly follow the existing observation that SWA tends to be beneficial for generalization in other settings; apparently in few shot learning as well. The positives would be that this is simple enough that it could become a general "best practice" in few shot learning baselines, and as such communicating this is important.

The other discussion focused around the theoretical justification relating SWA to low rank solutions. While empirically it does seem that the solutions found by SWA lead to low rank representations, this is not really adequately explored, and it s not clear enough why this should be expected to happen. I think if this relationship between SWA and low rank representations were more clearly explored then the paper would be a strong accept.

As it stands, it is quite borderline. Based on the scores (5,5,6), the recommendation is to reject, but it certainly could be included as well, as it has solid execution and is a clear topical fit for ICLR.
The paper presents a novel approximate second order optimization method for convex and nonconvex optimization problems. The search direction is obtained by preconditioning the gradient information with a diagonal approximation of the Hessian via Hutchinson s method and exponential averaging. The learning rate is updated using an estimate of the smoothness parameter.

The merit of the paper has to be evaluated from the theoretical and empirical point of view.

From the internal discussion, the reviewers agreed that the new algorithm is a mix a known methods, mainly present in AdaHessian, with a small tweak on the exponential average. Moreover, the theoretical guarantees do not seem to capture the empirical performance of the algorithm nor they provide any hint on how to set the algorithm s hyperparameters. For example, in Theorem 4.6 the optimal setting of $\beta_2$ is 1. That said, the most important theoretical contribution seems to lie in the fact that AdaHessian did not have any formal guarantee. Hence, this paper is the first one to show a formal guarantee this type of algorithms.

From the empirical point of view, the empirical evidence is very limited for the today standards in empirical machine learning papers. The reviewers and me do not actually believe that the proposed algorithm dominates the state of the art optimization algorithms used in machine learning. However, in the internal discussion we agreed that the algorithm has still potential and it should be added to the pool of optimization algorithms people can try.

Overall, considering the paper in a holistic way, there seems to be enough novelty and results to be accepted at this conference.

That said, I would urge the authors to take into account reviewers comments (and I also add some personal ones here). In particular, a frank discussion of current theoretical analysis and empirical evaluation is needed.

Some specific comments:
  AdaGrad was proposed by two different groups at COLT 2010, so both papers should be cited. So, please add a citation to: 
McMahan and Streeter. Adaptive bound optimization for online convex optimization. COLT 2010.
  Remark 4.7, second item: Neither Reddi et al.(2019) nor Duchi et al. (2011) *assume* bounded iterates, that must be proved not assumed. Instead, they explicitly project onto a domain that they assumed to be bounded.
  The convergence of the gradient to zero does not imply convergence to a critical point. To prove convergence to a critical point you should prove that the iterates converge, that in general is false even for lower bounded functions. Indeed, consider $f(x) log(1+exp( x))$, the iterates would actually diverge while the gradient still go to zero.
This paper proposes an algorithm for hyperparameters optimization that exploits a formulation as an MDP and thus makes use of a model based reinforcement learning approach.

The formulation of HPO as an MDP although not novel (Jomaa et al. is not the only one to have considered this case, and the connection between the two was already known in the community) is indeed an interesting topic that could be impactful for the community. Unfortunately, the current manuscript is not providing much new insight into the topic.

After carefully reading the paper, I agree with Reviewers cKwe and 8eE4 that the current manuscript has several points of concern:
1) the formulation as a sequential decision making problem is not fully elaborated 
2) lacking comparison to look ahead (i.e., non myopic) HPO algorithms (there is plenty of literature on Bayesian Optimization for doing this). This also makes it difficult to understand if the performance benefits come from the look ahead or from the MDP formulation 
3) the writing is generally understandable, but some of the important design choices and details of the algorithms are not easy to find in the manuscript   improving the clarity of the text would be very beneficial.

I encourage the authors to incorporate the feedback from the reviewer and to polish this paper into the shiny gem that it deserves to be.

Suggestions: 
  The MDP formulation for HPO might actually prove very beneficial for hyperparameters control (i.e., dynamically adjusting parameters during the learning process) where there is a real transition function rather than hyperparameters optimization. Might be worth reading https://arxiv.org/abs/2102.13651 which attempts to do hyperparameters control in the context of MBRL. 
  Adding better visuals to explain formulation and algorithm might go a long way.
  Tables 1 and 2 could be replaced by learning curves for a more intuitive way of visualizing the results.
This paper studies efficient algorithms for distributional reinforcement learning. The motivation stems from the need of risk neutrality, since other existing approaches might have one sided risk tendencies. The algorithms proposed in this paper are based on sampling from a distributional perturbation rather than using optimism in the face of uncertainty. Both theoretical guarantees and empirical results have been provided to validate the effectiveness of the proposed algorithms. While this is certainly an important and interesting direction, I agree with the reviewers that it is unclear from the theory in this paper why distributional perturbation is helpful.
Addressed semi supervised learning with the MNAR setting.  Well written paper.
Several additional experiments were reported in response to the reviewer questions.  
General agreement amongst reviewers.
Meta Review of Federated Learning with Heterogeneous Architectures using Graph HyperNetworks

This work investigates a method for federated learning in a neural architecture agnostic setting. They do this by using a graph hypernetwork to predict the weights of given neural network architectures (which is not exactly known at the onset). The authors conduct federated learning experiments to demonstrate good performance on several real datasets, and also showed that the trained GHN model can generalize (somewhat) to unseen architectures (which are mainly in the ResNet family). Personally, as AC, I find the results very promising, and the experiments show that GHNs are highly applicable to real world applications. But the reviewers outline several weaknesses in the discussion that makes it difficult to recommend acceptance of this paper for ICLR 2022.

The main weaknesses of the work are that application is mainly focused on a narrow family of ResNet architectures (can it be shown to go beyond this? If not, can the writing be improved to show that this is useful enough for many applications?) Reviewer U48w suggested improvements to the generalization experiments, and other details that can be addressed in the writing. Reviewer Tk9o mentioned that this work can be seen as a straightforward application of GHNs (limited novelty), while other reviewers do acknowledge the novelty of the work. I recommend improving the writing to clearly address this and defend why this is not a straightforward application of previous work. With these improvements, I m confident that this work will be accepted at a future ML conference or journal.

Even though I cannot recommend acceptance, both myself and other reviewers are looking forward to seeing improved versions of this work for publication in the future. As jPp2 also noted, “Previous works on federated learning either focus on the mechanism of parameter aggregation or the aspect of privacy. This paper opens a new direction in FL where clients may not be willing to share their unique model designs. From this perspective, I think this paper has promising impact on the research field of FL.” Good luck!
The paper proposes a multi agent RL framework that make decisions in a more human like manner by incorporating rational inattention. The approach is evaluated on two game theoretic problems. The reviewers agree that the topic of the paper is interesting. However, there are concerns about the significance of the proposed approach. As the method incorporates human inspired limitations, it s aim is not to outperform SOTA RL methods on regularly considered domains; at the same time, as the approach is only evaluated on two simulation based tasks, it is unclear how it would perform in more realistic scenarios that may benefit from human like decision making. For these reasons, I recommend rejection.
The paper introduces a technique to improve density ratio estimation. This is an important problem and very relevant to the ICLR conference. The main idea is to consider density ratios with respect to intermediate distributions to “scale” the densities and make the ratios easier to estimate by training a suitable discriminative model (classifier). Reviewers found the idea interesting but there was a consensus the paper is not ready for publication.
This paper examines the time dependent generalization behavior of high dimensional student teacher linear regression models. It introduces a simple two scale covariance model and examines the exact solutions for the dynamics, finding a tradeoff between the fast  and slow learning features, leading to epoch wise double descent. Qualitative comparisons are made with the SGD dynamics of ResNe18 on CIFAR 10.

The reviewers offer split opinions on this work, with most reviewers finding strength in exhibiting the complex behavior of epoch wise double descent in a simple and analytically tractable setting. Weaknesses highlighted in the discussion include clarity, discussion of prior work, and rigor of the analyses.

I believe a clear demonstration and analytical explanation for epoch wise double descent would certainly be of interest to the ICLR community, and I concur with the reviewers who emphasize these strengths of the paper. However, as one reviewer mentioned, this paper is primarily a theoretical work, and as such, the main theoretical advancements over prior work should be clear, and the novel results should be sufficiently rigorous. In this regard, the paper is lacking, as detailed below.

First of all, the discussion of SGD is imprecise, with no explicit definition of the optimization method that is actually being performed. What is the batch size? How is the sampling performed? What is the learning rate/schedule? The formulas in Secs. 2.1 2.2 suggest that full batch gradient descent is being performed. In Sec. 2.3, stochasticity from SGD is induced via a Gibbs distribution. However, contrary to the discussion, I don t think that this is a "well known" **result** (though of course it is a well known **model**), and in high dimensions I am not sure it is even correct (see e.g. [1]).

Second of all, even assuming the Gibbs distribution, the substitution on line (23) is only justified in words, whereas the cited results from Ali et al., 2020, only provide a bound. What is meant by "$\approx$"? Some discussion is given about this step of the derivation, but more precise statements would really help make the argument convincing.

Finally, the derivations seem to rely on the replica method from statistical physics, which is not rigorous. While I am generally supportive of such methods for technically challenging problems that do not readily admit alternative analyses, given the simplicity of the linear model setup here, I believe a more rigorous approach would not be prohibitively difficult. At the very least, some acknowledgement should be given about the lack of rigor in the derivation.

Overall, this paper presents a simple and analytically tractable model that sheds light on the importance phenomenon of epoch wise double descent. Unfortunately, the presentation is not sufficiently clear and the derivations not sufficiently rigorous to merit publication at this time.

[1] Paquette, Courtney et al. “SGD in the Large: Average case Analysis, Asymptotics, and Stepsize Criticality.” COLT (2021).
This paper proposes a new approach to online 3D bin packing with deep reinforcement learning. It received mixed reviews. AC finds that the responses from authors have addressed the concerns satisfactorily.
This is an exciting paper that provide the efficient algorithms for exact sampling from NDPPs along with theoretical results that are very pertinent in and out themselves. The AC agree with the reviewers that the authors satisfactorily addressed the concerns raised in the reviews, and is convinced that the revised version will be greatly appreciated by the community. We very much encourage the authors to pursue this line of work and in particular to overcome the practical restriction to the ONDPP subclass.
This paper received a majority voting of rejection. During the internal discussion, all reviewers insisted their original scores. I have read all the materials of this paper including manuscript, appendix, comments and response. Based on collected information from all reviewers and my personal judgement, I can make the initial recommendation on this paper, *rejection*. Here are the comments that I summarized, which include my opinion and evidence.

**Research Problem**

In this paper, the authors consider a novel scenario that feature selection in the contrastive setting, where an extra *background* dataset is utilized to remove the background noisy features. However, this problem can be easily handled with a fully supervised feature selection method, where the samples in the *target* datasets are annotated as 1 and the samples in the *background* datasets are annotated as 0. Therefore, the research problem addressed in this paper is not novel. Reviewer UFq8 and ft7b held the same opinion. 

**Technical Points**

The technical part could be more informative. The whole framework is based on auto encoder based self reconstruction, where the feature selection is finished by the recent CAE model. In my eyes, the major contribution of this paper lie in learning $g_z$, the background representation function. To achieve this, the authors proposed three strategies, *joint*, *pretraining* and *gates*. The *pretraining* idea does not involve any information from the target dataset, where the background representation function is a general one and it has no relationship with the target dataset. I believe the concept of background should be defined based on the target dataset. The *joint* idea suffers from the information leak, which was pointed by the authors. We can also see the inferior performance of the joint model, comparing with two other models. Unfortunately, the philosophy of *gates* is unclear.

**Experimental Evaluation**

(1) The authors only compared with one supervised method on the semi synthetic dataset. No results of supervised methods on real world datasets were reported. (2) The performance with different numbers of selected features were not reported.
An interesting paper, with non trivial results. The reviewers all agree that the paper is above bar (with two of them indicating strong vote for acceptance). The simplicity of the proposed approach (noted by some of the reviewers) is in my view a positive. Overall, a worthy contribution.
This paper presents a novel regularization technique for CNNs based on swapping feature vectors in the final layer. It is demonstrated that this simple technique helps with generalization in supervised learning and RL with image inputs.
Following the author rebuttal, all reviewers agreed that the simplicity of this method and the nice empirical performance it obtains is important to report to the community. In this respect, I agree with the reviewers, and recommend acceptance.

One important issue that came up during the discussion is how much this work is related to RL, and the authors SL experiments helped to put the contribution in a broader context. Indeed, one way to see the results of this work is that if such performance improvement is obtained in the Procgen benchmark with just image based regularization, perhaps this benchmark is not very suitable for studying generalization in RL (where we expect that more sophisticated techniques would be required). In addition, I can think of RL domains (e.g., Tetris, which was mentioned in the discussion) where I would not expect the proposed method to help. It would be good if the authors discuss these issues in some capacity in their final version.

Please take all reviewer comments into account when preparing the final version.
The paper studies how adaptive methods help train GANs to achieve better FID scores. It empirically shows that the adaptive magnitude in ADAM is the reason for ADAM s wide adoption for GAN training. The paper receives three reviews: one ranked the paper "accept, good paper" and two ranked the paper "marginally below the acceptance threshold". The supportive reviewer likes the findings in the paper interesting but does not provide enough explanation on the significance of the findings. On the other hand, the negative reviewers raise several concerns, including the GAN architectures used in the paper are outdated and the achieved performance gain is not major. As the paper focuses on performance instead of convergence, the meta reviewer feels it would be better to include results on SOTA GAN architectures. The provided rebuttal does not lead to any review score change. Consolidating the review and rebuttal, the meta reviewer feels the paper needed to be improved to meet the bar and would not recommend its acceptance.
The paper proposes a supernet learning strategy for NAS based on meta learning to tackle the knowledge forgetting issue. Forgetting happens when training a sampled sub model to optimize the shared parameters overrides the previous knowledge learned by the other sub models. The main idea of the paper is to consider training of each subnetwork as a task, and then apply MAML to ensure efficient cross task adaptivity. While the reviewers found the proposed method mainly an application of the existing meta learning strategies to one shot NAS, additional experimental results provided by the authors mostly convinced them about the effectiveness of the proposed method.
The paper describes an approach for automatically generating CAD sketches, including both the primitives that describe the drawing, as well as the constraints that describe relationships between the primitives that need to be maintained even if the primitives are changed. This is an important problem that is starting to receive a lot of attention from the literature. 

Overall, the paper is very well executed and the results are quite compelling. 

There were some concerns about the relationship with the work by Willis et al. and other papers that were published around the time when this paper was submitted. There is still some novelty in this paper relative to those works as argued in appendix H, but it would have been really good to have a more quantitative comparison. However, the authors pointed out that this work was concurrent as opposed to prior work as per the ICLR reviewer guidelines. 

Overall, given the quality of this paper and the guidance given in the ICLR reviewer guide, most reviewers agree with the meta reviewer that this paper should be accepted (the lowest reviewer still indicated it is above the acceptance threshold). However, there is some discomfort around not having an explicit comparison with very closely related work that ultimately was published before this paper.
This paper proposes a knowledge distillation strategy to enable the use of a large server side model in federated learning while satisfying the computation constraints of resource limited clients. The problem is relevant and well motivated, and the paper presents compelling experimental results to support the proposed strategy. However, reviewers had the following major comments suggestions/:
1) The theoretical analysis section needs improvement in terms of the technical depth and rigor
2) Better explanation of how the proposed strategy compares with previous works/baselines
3) Considering the privacy and scalability properties of the proposed strategy.

The paper generated lots of constructive post rebuttal discussions between the authors and the reviewers, and I believe the authors received several ideas to improve the work and appreciated the reviews. One of the reviewers increased their score. However, based on the current scores, I still recommend rejection. I do think the paper has promise, and with improvements, the revised version will make an excellent contribution.
This paper identifies a limitation with current attention in transformers where they scoring with query key pairs is strongly tied to retrieving the value and proposes a more flexible configuration that subsumes the previous setup but provides more flexibility. The authors shows this leads to improvements in various settings.

Overall, all reviewers seem to agree there is interesting insight and results in this paper and it merits publication. Also the discussion helped stress important points regarding weight sharing and more. One concern is that the model was not evaluated on standard NLP/vision datasets (I assume alluding to GLUE/SuperGlue/SQuAD, etc.), and authors seem to hint that pre training this is an issue for them computationally. This leaves open whether this indeed can and should replace the standard attention mechanism across the board, but is still very worthy of publication.
This paper provides a normal map inspired implicit surface representation involving a smooth surface whose high frequency detail comes from normal displacements.  Reviewers were impressed with the results and theoretical discussion in the paper.  The AC agrees.

The authors were responsive to reviewer feedback and addressed some questions about parameter choice during the rebuttal phase, including new experiments/discussion in the supplementary document.  Note the response to reviewer WHEF notes that the authors will be releasing data/code; the AC strongly hopes the authors are true to their word in that regard.

The AC chose to disregard some comments from reviewer G54X regarding tests with noise, as this method appears to be tuned to computer graphics applications; the level of empirical work here aligns with past work in the area.  Of course the authors are encouraged to include some tests responding to the reviewer comments in the camera ready.  The AC also found the score from reviewer WHEF to be somewhat uncalibrated with the tone of their review, but of course their assessment is quite positive nonetheless.


One small comment:  The abstract appears a bit strangely on the OpenReview site because of line breaks; if possible, please remove the line breaks.

Another small comment:  The "spectral shape representation" phrase used a bit in the discussion below might not be advisable, as this phrase typically refers to the intrinsic spectrum of a shape (e.g. Laplace Beltrami analysis)
The paper proposes a learning framework that allows for planning in continuous action spaces using tree search. Key to the approach is performing tree search over a discrete set of learned affordances that provide a compact abstraction of the action space that facilitates planning. The affordances are learned by passing gradients through a model based planner that uses learned models of the dynamics, reward, and state value functions. Experimental evaluations demonstrate the ability to perform tree search based planning using the learned affordances in a variety of domains for which tree search would otherwise be difficult.

The paper is topical, both with regards to its consideration of affordances as temporal abstractions that facilitate planning as well as the broader notion of integrating planning and learning. Several reviewers agree that the means by which affordances are learned by passing gradients through the planner is both interesting and novel. The reviewers also emphasize that the paper is well written and easy to follow, and that the approach is reproducible as a result. The reviewers raised a few concerns with the initial submission, notably the need for experimental comparisons to other recent baselines, which are important to clarifying the significance of the contributions, and the susceptibility to collapse in the affordance distribution. The authors clarified some of these questions and proposed adding comparisons to other baselines (e.g., DREAMER, for which there is already a comparison in the appendix), however it is not clear whether the submission was updated accordingly. The authors are encouraged to take this feedback into account and to include a more thorough experimental evaluation in any future version of the paper.
The authors introduce a novel probabilistic hierarchical clustering method for graphs. In particular they design an end to end gradient based learning to optimize the Dasgupta cost and Tree Sampling Divergence cost at the same time.

Overall the paper presents solid results both from a theoretical and experimental perspective so I think it is a good fit for the conference and I suggest accepting it.
The topic of the paper is the use of partial information decomposition (PID) for the analysis of interactions in latent representations.

All reviewers ended up appreciating the paper after a good extensive discussion with the authors. The numerical investigation is somewhat on the short side. One reviewer asks for more ablation studies and one reviewer asks for more investigation on real datasets to show the advantage of the method.

The paper is borderline. The theoretical development is fine. But one could argue that the paper could benefit from some more work on the experiments. However, the main points of the method is in place and further validation of the method can be left for future contributions.
The authors propose a new set of metrics for evaluation of generative models based on the well established precision recall framework, and an additional dimension quantifying the degree of memorization. The authors evaluated the proposed approach in several settings and compared it to a subset of the classic evaluation measures in this space. The reviewers agreed that this is an important and challenging problem relevant to the generative modeling community at large. The paper is well written and the proposed method and motivation are clearly explained. 

The initial reviews were borderline, and after the discussion phase we have 2 borderline accepts, one strong accept, and one strong reject. After reading the manuscript, the rebuttal, and the discussion, I feel that the work should not be accepted on the grounds of insufficient empirical validation. Establishing a new evaluation metric is a very challenging task   one needs to demonstrate the pitfalls of existing metrics, as well as how the new metric is capturing the missing dimensions in a thorough empirical validation. While the former was somewhat shown in this work (and in many other works), the latter was not fully demonstrated. The primary reason is the use of a non standard benchmark to evaluate the utility of the proposed metrics. I agree that covering a broader set of tasks and models makes sense in general, but it shouldn’t be done at the cost of existing, well understood benchmarks. I expected to see a thorough comparison with [1], one of the most practical metrics used today which can be easily extended to all settings considered in this work (notwithstanding the drawbacks outlined in [2]). What are the additional insights? What is [1] failing to capture in practical instances? Does the rank correlation change with respect to modern models across classic datasets (beyond MNIST and CIFAR10)? This would remove confounding variables and significantly strengthen the paper.

My final assessment is that this work is borderline, but below the acceptance bar for ICLR. I strongly suggest the authors to showcase the additional improvements over methods such as [1] in practical and well understood settings commonly used to benchmark generative models (e.g. on images). The experiments suggested by the reviewers are a step in the right direction, but not sufficient.

[1] Improved Precision and Recall Metric for Assessing Generative Models. Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, Timo Aila. NeurIPS ’19

[2] Evaluating generative models using divergence frontiers. 
Josip Djolonga, Mario Lučić, Marco Cuturi, Olivier Frederic Bachem, Olivier Bousquet, Sylvain Gelly. AISTATS ‘20
This paper proposes an alternative for constructing convolution kernels: instead of uniform spatial resolution, it proposes a spatially varying resolution with higher precision at the center of the kernel. The resolution decreases logarithmically as a function of the distance to the center. All reviewers agree that the idea is interesting, but in its current form, the submission is not mature enough to be published.

In particular, reviewers raised some concerns about computational efficiency of the method. The authors explain that their method runs slower than conventional convolution because the implementation uses of the shell conventional convolution modules, and they speculate that the speed can be accelerated if the method is directly implemented with CUDA or by directly adapting the underlying code of convolutions in the integrated framework. While this is a reasonable argument, it is not actually verified. This it is not clear if there would be other road blockers to achieve the promised performance. It would be great if authors could present actual performance of the method using either of their suggested solutions (CUDA or modifying code of convolutions).

In addition, reviewers raised concerns about some aspects of the evaluation setup, where test data is used to report the best performance. Authors respond that baselines are trained in the same fashion, hence the comparison is still fair. However, the reviewers were not convinced by this response. In concordance, I also think the use of test data during training is misleading, even if all methods use the same strategy, because this may tell us more about which approach can better (over)fit to the data as opposed to how well the methods are able to generalize to unseen samples.

Another concern relates to the diminishing return in the performance as networks get larger. The authors respond that this might be because only the first layer uses the proposed log polar convolution, speculating the problem will go away if the proposed approach is used in all layers. However, this is not empirically verified again and remains unclear if this is indeed the reason.

I suggest authors resubmit after accommodating the provided feedback.
This paper tackles the challenge of continual learning. It approaches the problem by combining a Gaussian Mixture Model (GMM) to model concepts in a latent space and and a decoder system to generate new data points for pseudo rehearsal and maintenance of previous information. When new concepts arrive, the GMM can be updated with rehearsal serving to prevent forgetting. The authors show competitive results on incremental learning of MNIST and FMNIST.

The scores were mostly below threshold, with one above threshold (5,3,5,6). The reviewers generally agreed the approach was interesting and they appreciated the theoretical treatments. However, there were a number of concerns, the central ones being the lack of clarity and the lack of convincing empirical demonstrations of scalability. The authors attempted to address the concerns, but they were not able to show good performance on larger datasets. The suggested this was due to the complexity of the encoding model, but they were unable to demonstrate this concretely. The reviewers  scores did not change, though, and the consensus was that this paper was not quite ready for publication. Given these considerations, and an average final score of 4.75, a decision of reject was reached.
This paper proposes Self Ensemble Adversarial Training (SEAT) for yielding a robust classifier by averaging weights of history models. The solution is different from an ensemble of predictions of different adversarially trained models. The authors also provided theoretical and empirical evidence that the proposed self ensemble method yields a smoother loss landscape and better robustness than both individual models and an ensemble of predictions from different classifiers.

The paper receives a mixed rating of 8 6 6 5 (after private discussion; initially it was 8 6 5 3), and all reviewers actively engaged in discussion. From the three positive reviewers, it is in general consensus that this paper has a clear motivation, is easy to follow, and owns reasonable (not exceptional) novelty. The negative reviewer poses a number of concerns, citing the absence of adaptive attack evaluation, the unclear difference between vanilla EMA and SEAT, and the proof of Proposition 1. The authors provided detailed responses and the negative reviewer was partially convinced (not fully) after viewing other comments. 

AC carefully reads all discussions and feels this fall into a borderline case. The authors did solid work and there is no fatal concern as AC can see. The majority sentiment is that this is a good paper, just not an exciting one. Hence, the current recommendation is a borderline acceptance.
This manuscript proposes and analyses a weighting approach to improve the conformance of adversarial training in federated learning. The authors observe that adversarial training seems to degrade during the late stages of training, and suggest that this degradation is a consequence of exacerbated cross device bias in federated averaging. They suggest and analyze a weighted scheme to fix this issue.

During the review, the main concerns are related to the novelty of the work compared to existing work, the clarity of the technical contributions, and unclear technical statements. The authors respond to these concerns and partially satisfy the reviewers. After discussion, reviewers remain mixed, with multiple weak rejects and one strong accept. No fatal flaws are noted. 

The opinion of the area chair is that while there are no fatal flaws, there is very limited enthusiasm for this paper. This limited enthusiasm seems to be a result of intuition for observed phenomena that seem incorrect or insufficient to reviewers. Overall, I think this paper outlines and addresses an interesting issue of real concern. Flaws in the intuition building/explanation, and issues with clarity of presentation need to be improved for this work to have some impact.
This paper demonstrates that current post hoc methods to explain black box models are not robust to spurious signals based on three metrics especially when the spurious signals are implicit or unknown.  Technical novelty is limited because the paper presents primarily empirical results instead of novel machine learning techniques. However, the problem is very important and timely, and significance to the field and potential impact of the presented results to advance the field are high as reviewers emphasized. There are ways to further improve the paper, including the clarity of presentation, although the authors improved in the revised manuscript.  Overall, this paper deserves borderline acceptance.
This paper introduces a dataset, based on preexisting standardized tests, of elimination/grid completion style logical reasoning puzzles expressed in text; available in both Chinese and English (with some of the text coming from semi automatic translation). The early pretrained MLMs BERT and RoBERTa perform poorly. 

This paper is solidly borderline. Reviewers had some concerns about the motivation and novelty the work, but I think that there is a plausible enough story for where this data will have value that I m not comfortable rejecting it only on this basis. More worryingly, the initial submission had some fairly serious writing quality and clarity issues, which impacted both the paper *and* the data. It seems like the authors made significant progress on this in the revision and engaged substantially during the discussion, but reviewers were not fully satisfied that the paper was up to ICLR standards, either as an initial submission or after revisions. This is a small detail, but it s a bad sign for the carefulness of the work that the OpenReview abstract is still unreadable, even after a request from a reviewer.
This paper studied Bayesian active regression with Gaussian processes, and proposed two intuitive algorithms inspired by the classical disagreement based and uncertainty sampling criteria. The reviewers appreciate the motivation and overall idea of taking a fully Bayesian approach by utilizing the joint posterior of the hyperparameters for active learning. However, there are shared concerns among the reviewers in the clarify and consistency of several key technical components, including discussion around bias variance tradeoff and its connection to the fully Bayesian approach, as well as in the experimental details, which make the current package insufficient for publication. 

Reviewers provide very useful feedback (in particular with a very extensive review by Reviewer hDWW) for improving the current work. The authors acknowledge in their responses that these are valid concerns and they would address these issues in a further version of this work.
All reviewers unanimously recommending rejecting this submission and I concur with that recommendation. However, many reviewers were quite pleased with the premise and basic concept of the submission and would have liked to see a clearer version with a bit more in terms of experiments. 

I agree with the submission that the most interesting architecture search research is about the search space, not the search algorithm.
The submission uses measurements of the data Jacobian matrix at different points to construct an extended data Jacobian matrix that then is projected and serves as input to a contrastive embedding learning algorithm. The resulting architecture embeddings can be used for many different things, including architecture search.

Ultimately, I am recommending rejecting this submission not because of one single overriding weakness, but because the totality of issues the reviewers raised make it clear the submission is not strong enough to publish in its current form. I encourage the authors to continue this line of work and produce a stronger submission in the future to ICLR or another venue.
This paper presents a study of on policy data in the context of model based reinforcement learning and proposes a way to ameliorate the resulting model errors.

This is a timely and interesting contribution, and all reviewers agree on the quality of the manuscript.
Please incorporate all the remaining feedback from the reviewers.

Minor comment: There might be interesting points of contact between this work and the concept of objective mismatch (https://arxiv.org/abs/2002.04523)
I recommend this paper for acceptance but I do so with significant reservations. Since this metareview will be public for all time, I direct this metareview to future readers of this paper so that they can weigh its merits and drawbacks in a clear minded way.

This paper proposes a "dual lottery ticket hypothesis." For those unfamiliar, the original lottery ticket hypothesis (Frankle & Carbin, ICLR 2019) states approximately that any randomly initialized neural network contains a subnetwork that can be trained in isolation to full accuracy in the same number of steps as the original network. That is, $\forall$ neural networks, $\exists$ a subnetwork such that $Accuracy(Train($subnetwork$)) \geq Accuracy(Train($network$))$ for a standard, fixed training procedure $Train$. (For the sake of posterity, note that this claim was supported on small scale neural networks but there is not evidence that it holds in general; only that it holds on the state of networks *early* in training. See *Linear Mode Connectivity and the Lottery Ticket Hypothesis* by Frankle et al. 2020.) To support this claim, Frankle & Carbin develop a procedure that finds such subnetworks, demonstrating that they exist in certain settings.

As far as I understand, the dual lottery ticket hypothesis states that, $\forall$ subnetworks of a neural network, $\exists$ a setting of the weights such that $Accuracy(Train($subnetwork$)) \geq Accuracy(Train($network$))$. Like the original lottery ticket paper, this paper shows that such subnetworks exist: it trains the subnetwork with an L2 penalty on all of the weights except those of the subnetwork, allowing them to gradually fade away and leaving a new setting of the weights for the subnetwork that then allows it to train in isolation to full accuracy (like those subnetworks found in the original lottery ticket hypothesis paper).

The reason that I have reservations about this approach is that the subnetwork found by the dual lottery ticket hypothesis procedure contains fully trained weights. This is novel but   to me   much less surprising and interesting: a randomly sparse subnetwork can be set with trained weights such that, after all of the other weights are fully pruned away, it can recover full accuracy. On the one hand, this is almost reminiscent of a standard pruning procedure where the network is both trained and pruned until a sparse subnetwork reaches full accuracy, with the dense network needed for much or all of training. On the other hand, the impressive part is that this can be done with a *randomly selected* sparse network rather than one chosen by a pruning heuristic. To me, that is the most interesting part of the paper. (And, for those readers wondering why specifically this paper is distinct from standard pruning, this is it.)

I wonder about the significance of this finding given that the subnetwork is set by training (not by random initialization or a tiny amount of training as in work on the lottery ticket hypothesis), but it s a novel idea and I think future scholars and future research should be the judge of that significance, not me or the reviewers. The novelty alone merits publication, and we will have to wait and see about the significance. Thus, I weigh in favor of acceptance, although with reservations.
The submission describes a method for tuning machine learning pipeline hyperparameters using transfer learning from related tuning tasks. In particular, the method uses learned meta features to construct a covariance function for a GP.

This was an extremely difficult case and could have gone either way. It was the closest case for any paper I serve as the AC for. Two of the reviewers recommended rejecting the paper and three recommended accepting, although during discussion one of the reviewers recommending accepting the paper seemed to actually be more on the reject side.

Ultimately, I have decided to recommend rejecting this submission. However, if either the clarity (especially concerning the neural network setup) or the experiments were somewhat improved I would have recommended accepting it. I view clarity as an extremely important factor when weighing whether a submission should be accepted. I concur with the reviewers on the following weaknesses of the experiments: (1) the lack of an ablation test when considering ad hoc meta features and (2) the experimental evaluation is based on mostly aggregated metrics.

I know this recommendation must be disappointing, but I encourage the authors to polish the work a bit more and resubmit it somewhere.
This paper proposes a generalization of the standard Transformer attention mechanism in which keys and queries represent abstract concepts (which must be specified a priori). This in turn yields "concept embeddings" (and logits) as intermediate network outputs, providing a sort of interpretability. Reviewers agreed that this is a simple (in a good way) and interesting approach, and may lead to follow up work that builds on this architecture. 

Some concerns regarding the relation of this method to prior work—in particular the "Concept Bottleneck" model—were raised and addressed in discussion; the authors might incorporate additional discussion in future drafts of the work.
This paper presents a meta learning framework to learn novel visual concepts with few examples. The proposed FALCON model uses an embedding prediction module to infer novel concept embeddings. This is done via paired image and text data as well as supplementary sentences. The resulting systems shows improvements on a series of datasets with synthetic and real images. The reviewers were supportive of this submission and praised the novelty, central ideas and experimental setups.

Concerns included:\
(a) [2P5Z] Justifying the formulations in this paper and situating it with past work   "Why is this an ecologically valid problem formulation?", "why a meta learning approach is the best formulation to tackle the problem?", "Why the box embedding space?"\
(b) [W3YC] More details required about the dataset and approach.\
(c) [98FU] Failure patterns

The authors provided detailed responses to these concerns. Concern (a), (b) and (c) were well addressed in the rebuttal and paper, and led to in increase in the reviewers rating.

Given the above, I recommend acceptance. But I do urge the authors to add the details provided in the rebuttal into the main paper. In particular, the concerns/suggestions by reviewer 2P5Z can hugely help in improving the paper and informing the reader.
The paper extends MuZero to stochastic (but observable) MDPs. To represent stochastic dynamics, it splits transitions into two parts: a deterministic transition to an afterstate (incorporating all observations and actions up to the current time), followed by a stochastic outcome (accounting for new randomness that follows the last action). The transition to an afterstate is similar in spirit to ordinary MuZero s dynamics model; the stochastic outcome is learned by a VQ VAE. At planning time, MuZero retains the MCTS lookahead from ordinary MuZero. Stochastic MuZero achieves impressive results: e.g., it maintains the original MuZero s strong performance on the deterministic game of Go, while improving on MuZero significantly (and achieving superhuman performance) on the stochastic game of backgammon.

This is a strong paper overall: it presents a convincing and successful extension of the already influential MuZero work, along with large scale computational experiments confirming the utility of the approach. There are nonetheless a few weaknesses: first, compared to the original AlphaZero and MuZero work, it is perhaps less surprising that the given approach is successful, since it is more closely related to prior work. Second, due to the large scale computational infrastructure needed, it is only possible to run some of the experiments once. This is not in itself a problem, but care needs to be taken in interpreting the results of such single run experiments: e.g., any figures that show results of single run experiments should have a clear warning label, and any statements such as "stochastic MuZero performs better than original MuZero" should be tempered with a caveat about how reliable these conclusions are likely to be. Section 5.4 (which runs shorter experiments using three random seeds each) makes a start at evaluating reliability, but (a) the headline results in previous sections do not contain any caveats or pointers to 5.4, and (b) 5.4 should explicitly acknowledge that it cannot hope to detect even quite common failure cases with so few seeds.
This paper presents a method for using transformer models to perform approximate Bayesian inference, in the sense of approximating the posterior predictive distribution for a test example.  This seems similar to doing amortized variational inference using a transformer model.  The reviewers all found the paper to be clearly written, interesting, novel and compelling.  Two of the reviewers found the results "impressive".  There is some concern of over claiming (is it really Bayesian?, are the authors making too broad statements based on very simple case studies?).  The presented method is also not scalable O(n^2), so the setting is restricted to very small datasets and models. 
 However, the reviewers didn t seem especially concerned by this.  The reviews were mixed but leaning positive (8, 6, 5) and the positive reviews are more substantial.  Therefore the recommendation is to accept, but please incorporate the reviewer feedback and additional discussion about related methods (discussion below) into the camera ready.
None of the reviewers championed the paper. Many weaknesses were shared across the reviewers: none of the individual contributions is individually novel, paper is not well written and the results do not show significant improvement over the prior state of the art. No rebuttal was provided. The AC agrees with the reviewers that the paper is not ready for publication at ILCR.
The paper contributes a theoretical understanding of training over parametrized deep neural networks using gradient descent with respect to square loss in the NTK regime. Besides giving guarantees on the classification accuracy using square loss, authors reveal several interesting properties in this regime including robustness and calibration. 

The problem studied here is exciting and very relevant. The current version, unfortunately, has some shortcomings. For example, under a margin assumption, the authors show that the least squares solution finds something with the margin and, therefore, it yields “robustness.” There is no quantification of how “robust” is the trained model, what is the threat model, what if the noise budget is larger than the attained margin. In general, the analysis lacks any careful finer characterization or quantification of the claimed properties. Besides, as was pointed out, the setting of the neural tangent kernel regime is somewhat limited and to some extent impractical. The assumptions under which the results hold further make the setting of the paper significantly restrictive. 

The writing can be improved with more emphasis on the novelty and significance of the contributions. Currently, all of the assumptions are buried in the appendix and the main paper is not even self contained. I believe the comments from the reviewers have already helped improve the quality of the paper. I encourage the authors to further incorporate the feedback and work towards a stronger submission.
This paper proposed a personalized federated learning algorithm which takes into account the similarity of gradient of different users to update the model. Although the ideas presented are intuitive, the algorithms have fundamental limitations, for example, they may cause large overhead of memory, communication and computation, and are unsuitable for privacy preserving machine learning. In addition, there are no rigorous analysis and the experiments are not convincing. This is a clear rejection.
This paper proposes a method to learn representations in MBRL by exploiting sparsity in the model to improve data efficiency. The key idea is to build a representation for which the model is invariant.
The idea is quite interesting, but one weakness of the current draft is that there is a disconnect between the presented theory (linear case) and the relevant experimental setup (non linear).
The paper is overall well written but would still benefit from a revision to improve clarity as pointed out by the reviewers.
The experimental results are inconclusive due to the choice of weak baselines.
The paper proposes an approach and specific training algorithm to defend against membership inference attacks (MIA) in machine learning models. Existing MIA attacks are relatively simple and rely on the test loss distribution at the query point and therefore the proposed algorithm sets a positive target mean training loss value and applies gradient ascent if the average loss of current training batch is smaller than it (in addition to the standard gradient descent step). The submission gives extensive experimental results demonstrating advantage over existing defense methods on several benchmarks. The primary limitation of the work is that it defends only against rather naive existing attacks which do not examine the model (but rely only on the loss functions).
PAPER: This paper introduces an extension of the HuBERT audio only model for the audio visual setting, allowing for self supervised pre training of multimodal model which also performs well on the unimodal tasks (lip reading and ASR). The paper applies the idea of modality dropout to their multimodal pre training setup and introduce the idea of masking with substitution as a way to improve visual representation learning. A strong aspect of the paper is its experimental section, showing strong improvement for lip reading tasks, bringing a new state of the art performance. The experiments also show improvement for ASR task. 
DISCUSSION: All reviewers seemed to appreciate the experimental results, with new state of the art performance on both unimodal task, when performing multimodal pre training. The paper does bring some technical novelty, but primarily because of its application to the audio visual domain. The modality dropout idea was already explored for other audio visual tasks such as speech driven face animation (Abdelaziz et al., ICMI 2020), but the idea of “masking by substitution” seems novel and helps learning better visual representations. The authors were able to address many questions and concerns expressed by reviewers. All reviewers took the time to read these responses and acknowledge them.
SUMMARY: This paper brings an interesting extension of the audio only HuBERT model for the audio visual setting. The strength of the paper is in its evaluation, with strong performances, establishing many new state of the art results.  All reviewers supported the acceptance of this paper.
Three experts reviewed this paper and all recommended rejection. The rebuttal did not change the reviewers  recommendations. The reviewers was not excited by the proposed probabilistic framework and raised many concerns regarding the comparison with baselines and competing methods, limited size of datasets, and limited scope of one dataset for one task. Considering the reviewers  concerns, we regret that the paper cannot be recommended for acceptance at this time.  The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
This paper proposes an approach to improve cross domain generalization in few shot learning, using an objective that attempts to fight overfitting on the observed domain at any given iteration while maintaining the general learned information so far from all domains. The approach uses a domain cycling procedure, where each iteration sees a single domain and, pseudo labels coming from predictions of a previous iterate of the model and from a parameter averaged general model are used in a combined training objective.

Three of the reviewers support acceptance (one strongly), while the fourth leans weakly towards rejection, despite an extensive response from the authors that include new results. One concern was a lack of comparison on Meta Dataset, which the authors went some way towards addressing during the rebuttal, though they also argued Meta Dataset couldn t really support the kind of cross domain evaluation they were targeting. The reviewer was not convinced by the authors  argument, and I too am not, in particular when you consider that Meta Dataset evaluations now often include evaluations on MNIST and CIFAR 10/100, in addition to MS COCO and TrafficSigns (all not included in the training split of Meta Dataset). That said, the experimental protocol favored in the authors  experiments certainly is sound and challenging for cross domain generalization, so I d hesitate to penalize them for that alternative choice.

Overall, I find the ideas behind this work neat, interesting and well motivated. Even if the basic ideas aren t completely novel, I found their combination thought provoking and creative.

Therefore, in the end, I feel this work will be beneficial to the body of literature on few shot learning and would merit to appear at ICLR.
This paper proposes a new class of divergences that are also sensitive to the variance of the estimator. The proposed additional variance penalty term introduces a bias term and acts directly on each component of the statistical estimator. By choosing the penalty parameter one can trade bias versus variance. 

The results on synthetic examples look promising and suggest that with this technique, it is feasible to decrease the estimation error relative to the baseline statistical estimator. This is demonstrated to be particularly pronounced for certain Renyi divergences in the large order parameter alpha regime. Two applications (detection of subpopulations and disentangled representation learning in speech) are provided.

The opinions about the work were fairly divided. Both positive reviews have lower confidence and are rather short and do not fully justify the high rating. Two high confidence reviews are negative and raise several critical points. In a nutshell, reviewer JtE9 complains mainly about the insufficient experimental evaluation while 3SLV raises several concerns regarding readability, mathematical notation, lacking details of the proofs, as well as technicalities regarding the consistency. The authors have partially answered the concerns.

While there seems to be a consensus that the paper is interesting and makes a valid contribution, the introduction of the VP term defines a new estimation problem and both the choice and interpretation of lambda becomes critical. In particular, the key question is understanding the effect of lambda for various tasks where divergence estimation is crucial and I am not fully convinced if the chosen applications are the best for convincingly demonstrating the utility as these require somewhat application specific motivation. I would rather see result of standard benchmark datasets (such as estimating the divergence between two subsets of MNIST images to detect subtle distribution shifts).

The synthetic experiments are good but this section could be improved as well to get the message accross. Rather than delving directly to the findings, this section could first justify what needs to be measured and what are the control variables (number of samples, Renyi order etc)

In light of the comments raised by the reviewers, I feel that this paper can benefit from a further iteration and clarification of the experimental section before being accepted to a venue like ICLR.
This is an interesting submission, which was overall well received by the reviewers. I would recommend the authors to discuss further the vast modern litterature on efficient computation of Wasserstein distances and their minimization (see, e.g. Peyré and Cuturi 2019, and references therein)
This paper has potential impact in the theorem proving community, and demonstrated the possibility of using LMs for theorem proving in Lean, and is good enough to use "in the real world" through an interactive theorem proving tool. 
The reviewers wish their data/models were public to address some concerns raised by the reviewers, but we think the community can benefit from this work.
This paper makes a key observation that the gradient based method gets more likely to suffer from poor local optima in multi agent reinforcement learning (MARL) with more agents particularly in the offline setting.  The paper proposes the use of zeroth order optimization method to avoid local optima.  Specifically, it samples multiple actions and regularize the policy to get closer to the optimal action among those.  The use of such zeroth order method to avoid poor local optima is not particularly new, although finding its effective in MARL and the empirical support are valuable.  The main discussion point was the insufficiency of experimental support, and the additional experiments during the discussion have addressed the original concerns of the reviewers to some extent.  Overall, given the limited novelty and inefficiency of support (either theoretical or empirical), the paper is slightly below the borderline.
The paper studies the benign overfitting phenomenon for linear models with adversarial training. The main issue is that the result is quite expected for experts versed in the benign overfitting papers, and indeed the reviewers pointed out that they could not see much technical novelty. However, even more importantly, the original benign overfitting papers had the advantage of proposing of simpler model (linear!!!) with the same behavior as the complex ones in practice. This is not the case here, as the result diverge from empirical observations on deep networks. The authors argue that it is a valuable finding that the empirical observation is not "universal", but this is a somewhat moot point as linear models are a priori very very different from the setting in which these empirical observations were made. For these reasons I believe the paper does not meet the bar for ICLR (yet it could still be publishable elsewhere).
The paper proposes an RL technique for dealing with the problem of network (graph) rewiring for robustness against attacks. Graph rewiring has been studied in a variety of fields, including graph theory (graph abstraction), graph ML (adversarial robustness, performance of GNNs), and combinatorial optimization. Reviewers had concerns with novelty, the correctness of some of the statements, and empirical evalution (in particular, baselines and scalability). While the rebuttal addressed some of the concerns, the overall feel about the paper is lukewarm and the AC believes the paper is below the bar.
The paper presents multi agent RL framework that uses the divergence between the learned policies and a target policy as a penalty that pushes the agent to learn cooperative strategies. The proposed method is built on top of an existing one (DAPO, Wang et al., 2019). Empirical experiments clearly show the advantage of the proposed method.

The reviews for this paper are mixed and borderline. The reviewers appreciate the experiments reported in the paper and that indicate the advantage of the proposed method. But two reviewers do not think that the proposed analysis is sufficiently novel compared to an existing one (DAPO). The responses provided by the authors were appreciated, but did not dissipate these concerns.
The paper formalizes the adversarial attack problem for transductive defenses, where the model is sequentially updated with a batch of (adversarial) test inputs. The paper comes up with a quite generic attack scheme and their instantiation of this scheme shows that RMC and DENT are not robust respectively not more robust than the underlying adversarially robust base model.
  
Positive 
  formal treatment of attacks on transductive defenses including discussion about different types of attacker knowledge
  the attack model is quite generic and could work for future transductive defenses and thus is a useful baseline attack which could be suggested to be used by future transductive defenses for robustness evaluation. In particular, as the standard AutoAttack is not designed for transductive defenses and thus can overestimate adversarial robustness

Negative
  the description is sometimes overly technical and some (important) details had to be clarified
  the technical novelty of the attack is limited
  the overall accuracy but also robust accuracy depends on the chosen batch. Therefore the authors should report mean and standard deviation over several different random draws of batches 
  the Transductive Adversarial Training Defense seems to consist of adversarial retraining from scratch after each incoming batch. This is excessively costly and not practical.

Minor:
  The batch size is an important parameter which apparently is assumed to be known in this work

The paper is borderline. Two reviewers argue for rejection, two for acceptance. Only one reviewer engaged in the discussion. 
In my point of view the positive point of having a reference for correct evaluation of adversarial robustness of transductive defenses weighs more than the raised negative points which can be fixed (at least partially). Thus I think that this paper is a valuable contribution to the field of adversarial robustness.
Dear Authors,

The paper was received nicely and discussed during the rebuttal period.
There is consensus among the reviewers that the paper should be accepted:

   This paper does contribute solidly to a timely topic of theoretical understanding of sparisty recovery with deep unroling. 
   The original version had very limited experiments and only synthetic ones, which raised concerns about whether the setting is motivated and whether the algorithm works on actual real data. The revision fixed that to an extent with some experiments on real data.

Yet, there are still some concerns that we suggest to be tackled for the final version:
  The capacity analysis is carried out inside a strongly convex regime while the algorithm is advocated for nonconvex sparsity recovery (see, e.g., the Discussion at the end of Section 2.1 );
  The analysis is relatively loosely connected to the adopted fist order optimization procedure;
  While the depth of network plays a role in the upper bound of Equation (15), its real impact on generalization gap looks quite limited.

The above are just suggestions to be looked more carefully, but there are not necessary. 

The current consensus is that the paper deserves publication.

Best
AC
The paper studies the problem of OOD classification: the test data and training data distribution can have different spurious feature class dependencies.

The reviewers have stated that the proposed procedure is a natural choice, with simple implementation. Another positive point is that it could easily be incorporated in many off the shelf machine learning training algorithms.

Yet, the technical novelty was mentioned to be limited. The bilevel optimization point of view and the connection with min max optimization problems raised some concerns, as the vocabulary used could be misleading.
It was also raised that the paper lacks theoretical supports: no formal analysis, most explanations are ad hoc, etc.
The paper studies the adversarial robustness of vision transformers. The authors conclude that vision transformers are generally more adversarially robust than the convolutional neural networks. Several interesting empirical conclusions are made for the robustness property of vision transformers. Sufficient empirical experiments are conducted. Overall, the paper is well written, well organized, and interesting. However, there are some concerns about the current version. (1) There are some concurrent works having similar empirical findings, which have been formally published and would weaken the interest of readers in the paper. (2) The reviews suggest that the authors use the insights from the paper to design more robust and effective vision transformers. The four reviewers have unanimous recommendations below the acceptance threshold. We therefore cannot recommend acceptance. However, we believe that by taking the comments, the next version would be a very strong paper.
This paper introduces a new approach for risk sensitive RL by using an objective that depends on the full distribution and can apply a weight to the resulting trajectory. The reviewers thought that focusing on more general and expressive objectives for RL is well motivated. However, they had a number of concerns of the current paper state, including its clarity in a number of sections and its relation to other work in risk sensitive RL. The authors provided thoughtful responses but some concerns lingered around the prior concerns.
This paper describes how to apply a combination of case based reasoning and RL methods to improve the performance of agents in text adventure games.  The reviewers unanimously recommend acceptance.  This work is both insightful and practical.  This is a valuable contribution.  Well done!
The paper presents an approach to select visual tokens in images and reorganize them for the object classification, within Transformers. All four reviewers find the paper interesting and novel, and they are also very positive about the experimental results. The authors also addressed minor concerns of the reviewers successfully through the discussion phase, clarifying details and adding experiments.

We recommend accepting the paper.
This work presents  a new sample based policy extragradient algorithm for finding an approximate Nash equilibrium in tabular two player zero sum Markov games with improved sample complexity guarantees. While originally the reviewers had concerns regarding the novelty and technical difficulty of the paper, these were successfully resolved during the rebuttal, and now all reviewers agree that this is an interesting contribution. Hence, I recommend acceptance of the paper.

In the final version the authors should make the following changes:
  Please mention early on (e.g., in the abstract and the introduction, as well as in the definition of the Markov game) that you consider a tabular problem (finite state and action spaces). Furthermore, it would be important to define informally the quantities in the bound in the abstract and when presenting Table 1. 
  While not entirely uncommon, Assumption 1 is quite strong, requiring mixing for any policies. It would be great if the authors could also add a comment on this, emphasizing that this is the case, as well as explaining how weakening the assumption would introduce problems (as explained in the response to Reviewer RwGu).
  The comparison to the lower bound of Zhang et al. (2020) should also be included, as discussed in the response to Reviewer 5TU3.
  Please discuss Assumption 2 in relation to the work of Wei et al. (2021), and rephrase the relation to the latter paper accordingly, as promised in the discussion with Reviewer Hsr5.
This paper presents an extension of the Predictive State Representation (PSRs) to multi agent systems, with a dynamic interaction graph represents each agent’s predictive state based on its “neighborhood” agents. Three types of agent networks are considered: static complete graphs (all agents affect all others experience); static non complete graphs (only some agents affect one another); and dynamic non complete graphs (agents affect one another in a time varying way). A number of theoretical results are presented, including PAC bounds for the approximations in the framework. The paper also contains a number of experiments that clearly show the advantages of the proposed technique over some related methods.

The reviewers unanimously agree that this is a strong paper, with a solid theoretical and empirical analysis.
The paper considers learning a fair classifier under distribution shift. The proposal involves an additional MMD penalty between the model curvatures on the data subgroups defined by the sensitive attribute. Reviewers generally found the problem setting to be well motivated, and the paper to have interesting ideas. Some concerns were raised in the initial set of reviews:

(1) _Relation between local curvatures and fairness robustness_. The concern was that the paper does not make sufficiently clear how similarity of the distributions of local curvatures ensures fairness robustness, and that there is no explicit definition of fairness robust to distribution shift.

(2) _Comparison to related work_. The concern was that works such as FARF as also considering the issue of distribution shift.

(3) _Technical novelty_. The concern was that technical depth of the proposal may be limited, as it builds on existing ideas (e.g., adversarial learning, Hessian to measure curvature).

(4) _Significance of results_. The concern was that the improvements of the proposed method are not significant, statistically and/or practically.

For point (1), the response clarified that the proposal is to ensure that the local curvature (and hence robustness) across data subgroups is similar. The relevant reviewer was still unclear as to whether this ensures what one might intuitively consider "robust fairness". On my review of the paper, I do concur that from the Introduction, and the para preceding Eqn 4, it appears that one natural notion is

$ \sup_{\mathbb{Q} \in \mathcal{U}( \mathbb{P} )} \Delta( \mathbb{Q}( \hat{Y}, Y \mid A   0 ), \mathbb{Q}( \hat{Y}, Y \mid A   1 ) ) $

where $\mathbb{P}$ is the observed data distribution, $\mathcal{U}$ is some uncertainty set, and $\Delta$ is some fairness measure (e.g., DP). Assuming this is indeed the ideal, it would be useful to mathematically contrast it to the proposal adopted in the present paper. The para preceding Eqn 4 correctly notes that the above notion would require specifying $\mathcal{U}$. This may be challenging, but an apparently reasonable strategy that follows the distributionally robust optimization literature would be to use a specific ball around the training distribution (e.g., all distributions with bounded KL divergence against $\mathbb{P}$). Further, it is of interest to ask whether the proposed objective in any way approximate this one; put another way, is there any implicit assumption made as to which class of distributions one is likely to encounter?

Further discussion would also be useful on the following alternative to the objective presented in the paper: rather than match the curvatures for the subgroups, simply minimise their unweighted average. This ought also to ensure robustness under the two different distributions; page 2 hints that this might not work owing to the different scales of these terms (i.e., the minority subgroup being much less robust), but the point does not seem to be discussed very explicitly subsequently.

For point (2), the response noted that FARF is designed for online learning, whereas the present paper involves a single, static training set drawn iid from a single distribution. In the present paper, the drift happens at test time, and the learner has no access to samples from this distribution. The authors argued that FARF can be applied as is to this setting. From my reading of this and the FARF paper, I agree that while the latter should be cited, it is not clearly applicable to the present setting.

This said, the present paper primarily focusses on the covariate shift setting, for which there have been some relevant recent works; see:

Singh et al., "Fairness Violations and Mitigation under Covariate Shift", FAccT  21.

Rezaei et al., "Robust Fairness under Covariate Shift", AAAI  21.

The former uses tools from joint causal graphs, while the latter assumes access to an unlabelled sample for the target distribution. The present work is certainly different in technical details, but at a minimum it seems prudent to acknowledge that there are relevant works on ensuring fairness outside the observed training distribution, and thus tone down statements such as "As a pioneer work...". There also seems scope to compare against the latter, e.g., to see how valuable having a few samples from the target domain are.

Another work relevant to the spirit of ensuring fairness beyond the observed data is

Mandal et al., "Ensuring Fairness Beyond the Training Data", NeurIPS 2020.

This is in line with the distributionally robust objective suggested in point (1), where one considers test distributions that can be arbitrary re weightings of the training distribution.

For point (3), from my reading, the technical content is reasonable. I would however have liked more mathematical discussions on point (1) above, which is important as it is the foundation of the strategy followed.

For point (4), the response asserts their improvements are significant practically and statistically. From my reading, I am inclined to agree with this claim. I would however note that another reviewer raised the question of whether Gaussian and uniform noise are reflective of real world distribution shifts. I concur with this concern; this part of the paper seems a little disappointing. The response mentioned results on a new setting with more realistic shift, which we suggest is incorporated into future versions of the paper.

Overall, the paper has some interesting ideas for a topical and important problem. At the same time, there is scope for tightening the work per the comments above, particularly on points (1) and (2), and to some extent (4). We believe that addressing these would help properly situate the work, and thus increase its clarity and potential impact. We thus encourage the authors to consider incorporating these for a future submission.
In my opinion, this is a cool idea, but could use a few more test settings to evaluate the general applicability of their method. It would be interesting to see if the method generalizes to a non reference based task.

Strengths:
Novel method that explores the interaction of color masks for learning to prompt about regions in images by identifying the color region they correspond to
Paper contains extensive ablation studies & discussions

Weaknesses:
Experimental results are run on uncommon benchmarks, making it difficult to compare to SOTA V+L methods
Consequently, it’s not clear that this method would generalize beyond visual grounding to tasks such as VQA or captioning
This paper introduces a differentiable yarn level model of fabrics. The model is more detailed and physically realistic than proposed in earlier work, which may allow for applications to manufacturing guidance and textile design.
The paper is generally well written and contains detailed problem formulation and derivations.
Experiments show it is possible to successfully learn a control policy and material parameters using the differentiable model.
This paper proposes a method for interpreting structured output model. All the reviews are negative. The reviewers find the paper difficult to read, and lacking in novelty, technical contribution and empirical evaluation.
The paper proposes an interesting way of prioritizing samples in replay that is compatible with many RL methods. It is evaluated experimentally on different tasks and with different RL algorithms.

The reviewers highly appreciated the revised paper and the detailed replies and discussions.
While this iteration improved the paper substantially, it is still not ready for publication in its current form. In particular:
  The paper is still not self contained enough
  The reviewers are still not convinced about the statistical significance
  More tasks should be added
  PER needs to be added as a baseline
The authors promised those changes for the final version, but those are so substantive that the paper will need to go thorough another complete review cycle. Hence, we d like to encourage the authors to re submit at a different venue.

P.S.: Careful with double blind submissions, acknowledgements should not be included.
The paper presents a variant of sliced wasserstein distance , where the slicing operation is performed with a neural network. The resulting distance is studied and experiments on synthetic data and as cost in generative modeling are performed.

While the idea of the paper is not that novel, the work is overall well executed. Reviewers agreed that the paper is borderline weak accept. Accept as a poster.
Strength
* The paper is relatively clearly written.
* The proposed method appears to be sound.

Weakness
* The novelty of the work seems to be limited.  
* The experiment part needs significant improvements.  The comparison with existing methods may not be fair.  Evaluation of efficiency should be given. There are also detailed investigations that need to be conducted, as indicated by the reviewers.
* There are technical issues that need to be addressed.
The paper proposes a method to improve PROVEN, which gives a certification for probabilistic robustness. However, reviewers think the paper is below the acceptance bar due to unclear motivation and insufficient experiments. In particular, a clear use case of probabilistic robustness certification is crucial for the paper.
The reviews received for this paper raise several critical concerns to which the authors have not provided a response. Thus, in its present form, the paper is not ready for publication.
The paper explores the usefulness of intermediate layers for linear probing, aiming at improving out of distribution transfer with significantly less cost than fine tuning. Two reviewers recommended borderline acceptance, while two others recommended borderline rejection as final rating. The main concerns raised by the reviewers were the limited novelty of the proposed method (e.g., compared to Elmo), unconvincing results in the natural and structure categories of VTAB, and lack of experiments to justify the claims, as well as the demonstration of the method in other tasks beyond image classification. The rebuttal has clarified several other questions. The AC really likes the simplicity of the approach, and also finds the problem of improving the efficiency of transfer learning very important. In addition, the paper is very well written and easy to follow, as acknowledged by all reviewers. However, the AC agrees with R2 and R3 that the paper, in its current form, does not pass the bar of ICLR, unfortunately. First, the novelty is limited, as pointed out by R1, R2, and R3. In addition to the related works mentioned in the reviews like Elmo, note that the idea of selecting intermediate features, concatenating them, and running a linear classifier for OOD transfer has also been explored in [Yunhui Guo et al, A broader study of cross domain few shot learning, ECCV 2020]. Second, while the approach has advantages in terms of efficiency, the accuracy drop (compared to fine tuning) for in domain tasks limits its applicability. Finally, even though the AC agrees with the authors this is not a requirement, a more comprehensive set of experiments on more tasks would make the paper stronger, especially given that the novelty is incremental. The authors are encouraged to improve the paper for another top conference.
The paper studies the problem of finding an optimal memory less policy for POMDPs.  This work makes an important theoretical contribution.  The reviewers are unanimous in recommending the acceptance of the paper.  Well done!
The paper studies the join Q value decomposition problem in MARL. Some of the results are interesting, e.g., the True Global Max condition and several experiments. However, the majority of the reviews are negative due to the current presentation of the paper. We encourage the authors address all the reviewers  comments and submit a new version to the next conference.
Summary: This paper studies the neural contextual bandit problem, and proposes a neural based bandit approach with a novel exploration strategy, called EE Net. Besides utilizing a neural network (Exploitation network) to learn the reward function, EE Net also uses another neural network (Exploration network) to adaptively learn potential gains compared to currently estimated reward.

Discussions: The reviewers appreciated the novelty and the quality of the ideas and results in this paper. Most questions were about details in algorithm design choices and in the analysis. The authors have addressed these questions and updated their draft. The reviewers have now reached a consensus and recommend accepting this paper.

Recommendation: Accept.
The paper considers the Equitable and Optimal Transport (EOT) problem which is arises in fair division of goods and multi resource allocation. The resulting problem is a linear program, which is polynomial time solvable; however, the existing polynomial time solvers either do not scale well with the dimension or are dual methods with entropic regularization for which it is unclear how to extract a primal solution. The paper shows how to extract a primal solution and also provides complexity analysis of a recently proposed projected alternating minimization method (PAM). The paper further provides a Nesterov accelerated variant of PAM. 

Overall, the paper is a meaningful contribution and was considered borderline. On one side, EOT seems like an interesting problem, the paper is well presented, and the provided complexity results are technically sound. On the other hand, the reviewers felt that the EOT problem was not motivated enough, that the techniques for proving the results were mostly standard, and that the numerical experiments were insufficient. Even though the authors provided additional numerical experiments, I did not find the responses regarding motivation (particularly in the context of ML applications) and technical novelty convincing enough. The paper could have gone in either direction, but as there was ultimately no particularly strong support from any of the reviewers, I recommend rejection. The authors are advised to carefully revise the paper and resubmit.
The reviewers have raised relevant concerns that preclude acceptance and the authors have not provided a response. At this time, all reviewers concur that this paper should be rejected and I agree.
This paper studies the dependency of SGD convergence on order of examples. The main observation of the paper is: if the averages of consecutive stochastic gradients converge faster to the full gradient, then the SGD with the corresponding sampling strategy will have a faster convergence rate. For different sampling strategies, sampling with replacement has slower convergence in stochastic gradient, where sampling without replacement can converge faster. The paper also proposes two new algorithms that can improve convergence rates in some interesting settings. The reviewers find the analysis clean and the new algorithms are interesting. There is some concern on the dependency on n or d for the faster rate, which should be discussed more clearly in the final version of the paper. Overall this is a solid contribution to the example selection problem.
This paper proposes an alternative method to improve UNMT by using only a pre trained generative language model to bootstrap the process. In all, the reviewers think the proposed method is reasonable. 

However, the empirical part is not convincing.  Most reviewers think that evaluating the method on one language pair (En Fr) is not enough to show the effect of the proposed method. In addition, some reviewers argue the clarity of this paper.

In all, I think the proposed method is meaningful. However, the current version is not ready to be published in this ICLR. I hope the authors can improve their paper according to the reviews.
This paper proposes a new contribution in the recent literature on learning distributions of sketches. While all reviewers have recognized the overall good quality of the presentation, two factors seem to weight heavily on a negative decision: clarifications on the contribution s scope (presented as a tool for general Hessians in the introduction, but ultimately only applied to least square errors of linear predictors, to recover an explicit factorization of the Hessian matrix) and links with existing literature; weakness of experiments whose small scale does not justify using sketches in the first place. Since this is a "learning" approach, I am particularly sensitive to the latter point, and therefore am inclined to reject, but I encourage the authors to address these two issues with the current draft.
This paper analyzes popular metric based few shot learning (FSL) methods from the perspective of computational geometry. Namely, viewing prototypical networks as Voronoi diagrams (VDs). This lends itself to incorporating extensions based on the recently proposed CIVD that allows for multiple centers per cell. The paper then discusses various aspects of the FSL pipeline (data augmentation, feature transformations, geometries and representations), referred to as heterogeneities, that can be efficiently ensembled via a cluster to cluster VD (CCVD). The resulting model produces state of the art results.

Initial concerns from the reviewers pointed to a potential lack of novelty (since it can be seen as applying existing ideas to FSL), lack of self containment in the main paper, weak positioning in the context of other FSL methods (which ones can be interpreted under the VD framework) and a potentially impractical computational complexity. The discussion period settled these issues, with the paper receiving several updates, and the reviewers all ended up recommending acceptance.

Personally, I would like to see an addition to Figure 1 with the resulting decision boundaries from CIVD and CCVD. I think that this would greatly improve the ability of the reader to reason about the approach intuitively. Also, as a minor comment, I think that the argmax below eqs 1 and 7 should either be an argmin, or the distances should be negated. Otherwise, I think this is a valuable contribution to the FSL literature.
This paper proposes and studies a variant of policy optimization mirror descent policy optimization (MDPO) which was inspired by the mirror descent algorithm in the optimization literature. The proposed algorithm attempts to find a policy parameter that maximizes the expected regularized advantage function,  where the regularization term is based on the KL divergence between the new policy iterate and the current policy iterate. The main contributions are algorithmic and empirical, with detailed discussions provided to illuminate the connection between MDPO and other existing policy optimization paradigms like TRPO, PPO, etc. The paper provides an interesting and useful contribution to the growing literature of policy optimization.
This paper proposes to use an energy based model for a multi objective molecular generation. The energy function is parameterized by relational graph convolutional network (R GCN) so that it has a permutation invariance property. The model is trained by contrastive divergence and the generation is performed by Langevin dynamics. Experiments on single and multi objective molecule generation are conducted to verify the effectiveness of the proposed framework. The paper is well written, and the experiments are comprehensive. The major shortcoming of the paper is its limited novelty, since using EBM for graph generation is a straightforward application of the existing deep EBM framework. The contribution is marginal.  

During the discussion, two of the reviewers pointed out that the contribution is limited and marginal. Two reviewers pointed out that the performance gain obtained by the proposed model is marginal and not significant. One reviewer has a concern about the computational cost of MCMC. However, the authors didn’t provide a rebuttal to address the concerns raised by the reviewers. Given the fact that all the concerns from the reviewers remain, and the contribution and performance gain of the work are marginal, the AC recommends rejecting the paper.
This paper makes significant advances in offline reinforcement learning by proposing a new approach of being pessimistic to deal with uncertainties in the offline data.  The proposed approach uses bootstrapped Q functions to quantify the uncertainty, which by itself is not new, and introduces additional data based on the pseudo target that is penalized by the uncertainty quantification.  The use of such additional data is the first of a kind, and the paper provides theoretical support for the case of linear MDP and empirical support with the D4RL benchmark.  The reviewers had originally raised concerns or confusions regarding theoretical analysis and experiments.  The authors have well responded to them, and no major concerns remain.
This paper proposes a method for finding the action space in reinforcement learning problems, characterizing the search space into dispensable and indispensable actions through a Monte Carlo approximation.

Reviewers are unanimous that the paper is not fit for publication at this stage. While it tackles an interesting problem and seems to be novel, the presentation leaves much to be desired; this area chair also had a hard time figuring out how the different parts of the paper fit together. Additionally, the use of a unique problem makes it hard to judge the contribution of the algorithm.
The paper formulates fluid simulation as an image to image prediction task and proposes to solve the problem using a cGAN formulation. The objective is to develop fast approximate solutions for the modeling of fluid dynamics, here Navier Stokes for incompressible flows. The images correspond to the discretization of velocity and pressure fields. Experiments are performed on a simulation for a Karman vortex street. 

All the reviewers expressed concerns w.r.t. the absence of references and comparisons with closely related work in the recent but abundant literature on NN for modeling PDE dynamics, the lack of novelty and the insufficient experimental design, description and discussion.
This paper aims for detecting not only clean OOD data, but also their adversarially manipulated ones. The authors propose a method for this goal, with no/marginal loss in clean test accuracy (say, Acc) and clean OOD detection accuracy (say, AUC), while existing methods for targeting the same goal suffers from low Acc and AUC. 3 reviewers are positive and 2 reviewers are negative. Reviewers and AC think that the proposed idea of merging a certified binary classifier for in versus out distribution with a classifier for the in distribution task is interesting. However, AC thinks that experimental results are arguable as pointed out by reviewers. For example, in CIFAR 10, the proposed method outperforms the baseline (GOOD) with respect to Acc and AUC, but often significantly underperforms it with respect to GAUC (guaranteed AUC) or AAUC (adversarial AUC). Then, the question is which metric is more important? It is arguable to say whether Acc is more important than GAUC or AAUC. But, at least, AC thinks that AUC and AAUC (or GAUC) are equally important as adversarially manipulated OOD data is nothing but another OOD data made from the original clean OOD data. Hence, the superiority of the proposed method over the baseline is arguable in the experiments, and AC tends to suggest rejection. 

ps ... AC is also a bit skeptical on the motivation of this paper. What is the value of obtaining "guaranteed AUC"? It is not the "real/true" worst case OOD performance, as it varies with respect to the tested clean OOD data. Namely, it is the worst case OOD performance just in a certain "subset" of OOD data, i.e., adversarially manipulated OOD data made from a certain clean OOD data. Hence, AC is curious about what is the value of establishing such a "partial" lower bound (rather than "true" lower bound considering all possible OOD data). AC thinks that the problem setup studied in this paper (and some previous papers) looks interesting/reasonable at the first glance, but feels somewhat artificial after a deeper look.
The paper introduces a method to learn rotations of a quantized embedding end to end. The proposed technique seems novel, although the technical/algorithm novelty seems to be somewhat marginal. 
The empirical results are promising, although do not quite match some of the claims by the authors. 
Hopefully the reviewer feedback would help in producing an even more influential paper.
The paper examines neural architecture search for multi task networks, by associating model hyperparameters with a coding space and building an MLP predictor for mapping codes to task performance.  After the discussion phase, reviewers are marginally in favor or accepting the paper, pointing to the extensive experimental results as a convincing contribution.
This paper proposes an expansion strategy for both task agnostic and task boundary aware CL. The authors demonstrate the quality of their method using two standard scenarios with the Split MNIST and CIFAR datasets. 

Enabling CL for task agnostic and task boundary aware is important and an active area of research. The proposed approach is an interesting method that adds an expert for each new task. Experts are then combined (Mixture of Experts) for prediction. One disadvantage of a MoE approach is that the model size and compute will grow linearly with the number of tasks. This effect is partly limited in the paper as the authors show that experts can be small neural networks.

There was a bit of confusion in the original reviews regarding the exact setting this paper works under. As far as I understand this paper mostly deals with the class incremental setting (task IDs available at training time, but not at test time). The task agnostic setting (task IDs never given) is also explored in Section 5.1. I think this confusion is partly a reflection of the state of the CL literature and the authors provided clear and concise replies to the reviewers.

The main limitation that remains is regarding the experiments. I agree with the reviewers that the current experiments seem somewhat preliminary and showing results on larger scale datasets and/or compared to a wider diversity of baselines is important. Reviewer sgG4 made precise comments about this. Other minor comments by the reviewers including providing a detailed report of the memory usage and computational costs of the various methods (partly done in Figure 5.3).

I think this method is interesting and could be impactful. I strongly encourage the authors to polish their manuscript and consider adding some of the additional empirical results that were suggested.
This paper proposes a prototypical contrastive predictive coding by combining the prototypical method and contrastive learning, and presents its efficient implementation for three distillation tasks: supervised model compression, self supervised model compression, and self supervised learning via self distillation. The paper is well written, and the effectiveness of the proposed method is validated through extensive experiments.  Reviewers generally agree the paper has clear merits despite some weaknesses for improvement. Overall, I would like to recommend it for acceptance and encourage authors to incorporate all the review comments and suggestions in the final version.
The authors address a very important question pertaining to the relevance of morphological complexity in the ability of transformer based conditional language models. Through extensive (controlled) experiments using 6 languages they answer as well as raise very interesting questions about the role of morphology/segmentation/vocab size which mat spawn more work in this area.

All the reviewers were positive about the paper and agreed that the paper made significant contributions which would be useful to the community. More importantly, the authors and reviewers engaged in meaningful and insightful discussions through the discussion phase. The authors did a thorough job of addressing all reviewer concerns and changing the draft of the paper accordingly. 

I have no hesitation in recommending that this paper should be accepted.
This paper proposes a computationally efficient method to detect adversarial examples in reinforcement learning models. The detection method is based on the curvature of the loss landscape around the inputs, which is shown to have larger negative value for clean examples compared to adversarial ones. The experiments on Atari environment models show the effectiveness of the method.

The paper is well written and backs up the experimental results with mathematical intuition and analysis. 

However, the baseline of Roth et al. and all attack methods used have been designed for image classifiers. 

If the authors decide to focus on RL, the attack methods should be tailored to RL. The word  “worst case” in the title is misleading, since the attacks used in the paper are not optimal for RL algorithms. This reduces the credibility of the claimed successful detection. 

If the authors decide to frame this work as introducing a new property of adversarial examples which can be applied to other tasks, the authors should test this method on other tasks such as benchmark image classification datasets (for example CIFAR10). 

With the current experiment section, it is unclear whether this method works in RL applications since the authors use attack methods designed for image classifiers rather than RL algorithms (Please refer to the following papers for some existing RL attack methods). It is also unclear whether this paper introduces a new property of adversarial examples that is general. 

  Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on neural network policies
  Ezgi Korkmaz. Nesterov momentum adversarial perturbations in the deep reinforcement learning domain. 
  Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust deep reinforcement learning with adversarial attacks.
  Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho Jui Hsieh. Robust deep reinforcement learning against adversarial perturbations on state observations.
  Huan Zhang, Hongge Chen, Duane S Boning, and Cho Jui Hsieh. Robust reinforcement learning on state observations with learned optimal adversary.
This paper extends a recent approximate dynamic programming method (i.e., DP with neural networks) for a ride sharing problem.
An elegant trick is proposed to obtain a more expressive function approximation without suffering a combinatorial explosion of the action space. While the idea is somewhat ad hoc in its implementation, and limited in novelty w.r.t. the ADP work that the paper builds on, the empirical performance improvement on the ride sharing problem is clear. 
Initially, the reviewers also raised several clarity and presentation issues, but the authors did a good job in addressing them in their rebuttal.

The reviewers gave scores of 5,8,5. The main critique is limited novelty.
During the discussion, we focused on the novelty of the approach, whether the ideas can be generalized beyond the very specific ride sharing problem, and whether the work is strong enough if viewed as an application paper.
The conclusion, which my final decision is based on, is that currently, the contribution is very specific to the ride sharing problem, and it is not clear whether this idea can be extended to more general optimization problems. This means that the scope of the algorithmic approach, taken with respect to the ICLR audience, is rather narrow. On the other hand, the current presentation does not meet the bar of a strong application paper, as there is not enough novelty in the problem and data.

My advice to the authors is to broaden their investigation and evaluation. Another option would be to target a venue that is more focused on the ride sharing problem.
The main contribution of this paper is that it points out incorrect claims in the literature of multi agent RL and provides new insight on the failure modes of current methods. Specifically, this paper investigates the inconsistency problem in LOLA (meaning it assumes the other agent as a naive learner, thus not converging to SFPs in some games). It then shows problems with two fixes in the literature: 1) HOLA addresses the inconsistency problem only when it converges; otherwise, HOLA does not resolve the issue. 2) GCD does not resolve the issue although it claims to do so. This paper then proposes a method COLA that fixes the inconsistency issue, which outperforms HOLA when it diverges. Reviewers generally agree that the insight from this work is interesting and important for the field. However, there were some concern on both the theory and the experiments. While the updated version addresses some of the concerns, it also made significant changes to both the theoretical and the empirical sections, and would benefit from another round of close review. Thus, I think the current version of this work is borderline.
This paper introduces a quantum pyramidal circuit for the computation of orthogonal layers in neural networks and implements the algorithm on simulators and on a quantum computer to illustrate its effectiveness. It also obtains an O(n^2) classical algorithm for forward and backpropagation.

The reviewers generally found strength in the derivations and implementation on real quantum machines. Some reviewers regarded the contributions as strong and novel, while others expressed skepticism about the novelty and the robustness of the algorithm. Having read the paper in detail, I concur with the several reviewers who found the literature review of classical orthogonal NNs to be lacking. In particular, one reviewer highlights similarities with Householder reflections and Givens rotations, for which substantial literature already exists. Without a proper comparison to this existing work, it is not possible to properly assess the novelty or relative contributions of the current paper.

Beyond an extended discussion of related work, the paper would also benefit from improved experimental analysis. While the paper is framed around the quantum algorithm, the main contributions are described as a novel and efficient classical algorithm. This would indeed be a contribution of interest to the broader (non quantum) ICLR community, but there is no experimental evidence supporting the utility of the proposed methods. An analysis that compares the classical algorithm to the numerous prior works that parameterize orthogonal layers would be an essential addition. As it stands, I cannot recommend the paper for publication.
The paper introduces a convolutional like operator called optimized separable convolution, which scales well in the number of channels, C. The paper studies the classification performance of ResNet with the optimized separable convolution, and with other choices of the convolutional operators. The paper finds the introduced convolutional  operator to be more parameter efficient than competing operators, however only by a relatively small margin, and this also comes at the cost of computational performance. 

The reviewers appreciated that the proposed convolutional operation is more parameter efficient and that any improvement in convolutional networks potentially benefits a large array of methods and tools. The reviewers  criticize that the operation only offers marginal gains at the cost of slower runtimes, and agree that the contribution is only marginally significant and therefore below the acceptance threshold.

I agree with the reasoning of the reviewers that the extra computational cost is not worth the marginal improvement, and therefore recommend to reject the paper. Also, the authors didn t respond to the comments of the reviewers and their reasonable questions.
This paper propose two new neural network (NN) architectures, namely TNN, and SQANN. The paper claims that these networks are resistant to catastrophic forgetting, are interpretable, and are highly accurate. While the reviewers agree that the idea of making neurons reflect training data is novel, some concerns remain post rebuttal. Most of the reviewers opine that the statements of theorems are unclear, confusing, and hard to interpret (even after the rebuttal and update), thus making it hard to appreciate the contributions of this work. Given this, we are unable to recommend this paper for acceptance at this time. We hope the authors find reviewer feedback useful.
This paper provides a method for offline RL in settings where the environment may exhibit significant similar structure, such as one part having nearly the same dynamics as other parts. The work is motivated in part by healthcare settings. The reviewers appreciated the potential applications to areas like healthcare but also thought there is a strong body of related work (e.g. transfer learning, meta RL and other related papers) and it was unclear how novel the approach was within that related work, or how it would compare. The authors did not respond to the reviewers’ reviews. We hope their input is useful to the authors’ in revising their work for the future.
The paper evaluates the generalization capabilities of model based agents, in particular, MuZero, compared with model free agents. Reviewers agree that the paper is well written and the topic is interesting. The ablation study is especially interesting, as it disentangles the effect of different algorithmic components. Some concerns are raised about the significance of this work, as the scope is limited to an empirical study and the results are not necessarily very surprising. 

Since the paper presents clear results on an important and relevant topic, I recommend acceptance.
This paper extends Randomized least square value iteration (RLSVI), which is a method for exploration exploitation tradeoff that is suitable for linear FA, to the deep RL setting. A key component is using Hypermodels of Dwaracherla et al. (ICLR, 2020) to generate the weights of last layer of the DNN. This generates a learnable randomness required in an RLSVI like procedure.
The paper provides some theoretical results regarding Hypermodels, and provides extensive experiments to show that their method is a competitive one in solving exploration problems.

The majority of the reviewers are positive about this work. The concerns include the incremental nature of this work and the empirical results. In my opinion, the algorithmic contribution is reasonable, but somehow incremental. The theoretical results are minor, but acceptable. The empirical results are extensive, though they have some shortcomings.
I explain the algorithmic and empirical contributions below:


**Algorithmic Contribution:**
Similar formulation has been done by  Dwaracherla et al. But that work does not consider the RL setting, and instead focuses on the bandit setting. This work provides such an extension. A straightforward application of Hypermodels does not work for DRL, but some simple, yet crucial, tricks needs to be applied to make it work. The trick is to use Hypermodel to generate the weights of the last layer, instead of all layers. Although this is simple, the fact that it enables the method to work for DRL paper is significant.


**Empirical Results:**
The empirical results are quite extensive. There are two main issues with them though:

a) Many experiments on the Atari Suite are terminated after 20M samples. This is shorter than usual.
b) The experiments are only repeated for 3 runs (seeds)   except one, which used 5 runs.

The authors  answer for (a) is that they have a limited compute budget, and running for 200M samples would cost them about $20K. Also they argue that 20M samples is enough to show the benefit in a better exploration method, as shown by some other papers.

After some inquiries, it seems that this $20K value has the correct order to run the experiments on a cloud (maybe within a factor of 2 or 3). Given this prohibitive cost, I am willing to accept that 20M samples might be sufficient for proving the main points of this paper.  I am not giving a large weight to this in my evaluation.

The main concern for me, however, is having only 3 independent runs of the algorithms, especially given that the issue under study is the efficiency of exploration exploitation tradeoff, and that the proposed method has a lot of randomness built in. This is the main weakness of the empirical results in my opinion, and not the 20M samples issue.

For instance, Figure 3 (Human normalized score over 56 environments in Atari 2600 suite) does not have any confidence interval information. And Figure 4 has some shaded areas around the curves, but it is not clear whether it is standard deviation, standard error, or some other quantification of uncertainty.

Even though this is a borderline paper, I recommend acceptance of this work under the expectation that the authors should improve their empirical studies. In particular, I recommend much larger number of independent runs (seeds), maybe around 10 or so, with proper information about the uncertainty of the estimates. If running this for all games is prohibitive, showing the performance with more runs on a subset of the games is sufficient. Also I encourage the authors to consider NeurIPS 2021 paper "Deep Reinforcement Learning at the Edge of the Statistical Precipice".
The work presented in this study gives a theoretical finite sample generalisation performance of stochastic gradient descent on linear models, for different batch sizes and feature structures. This approach enable the authors to predict the training and test losses of neural networks on real data.

While there were some parts that were initially mis understood by some reviewers in the initial version of the papers, the extensive discussions between the authors and the reviewers led to several updates, both in the reference to prior work, but also in the presentation clarity. The wide impact and relevance to ICLR of this type of contribution made us recommend this work for acceptance at ICLR.
Reviewers agree that the paper is well motivated and the proposed method is somewhat interesting and well experimented. However, reviewers feel that the paper relies on many existing methods and does not appear to be novel enough.
This was a somewhat unusual submission in that the authors tried to motivate their paper by pointing to a separate anonymous manuscript.  However, the authors didn t seem to want to confirm they would merge the manuscripts when asked about this. It was thought that in fairness the submitted manuscript should be judged on its own. After discussion, it was agreed that the submitted paper on its own, did not generate enough enthusiasm to merit acceptance.
The paper points out how set equivariant functions limit the types of functions that can be represented on multisets. They develop an new notion of multiset equivariance to address this limitation. The paper improves an existing multi set equivariant Deep Set Prediction Network through implicit differentiation, which is an area of rising interest. The reviewers and I note that the paper is well written.
This paper proposes to use implicit neural representations to model how our surroundings affect the sounds reverberating within. Concretely, the proposed approach can produce impulse responses that capture environment reverberations between any two points in a scene.

Reviewers praised the novelty and originality of the idea (and I concur), but raised concerns about the clarity of the writing (especially w.r.t. modelling the phase component), lack of detail, insufficient or inadequate experiments and overclaiming of results. (There were also concerns about overclaiming of contributions, but I am inclined to agree with the authors that this isn t really the case.)

The authors have clearly taken the time to try to address these concerns, and I commend them on their willingness to engage with the reviewers  comments and suggestions. While one of the reviewers raised their score to "accept", I am inclined to agree with the other reviewers and recommend rejection. The required degree of revision is substantial, and therefore difficult to assess within a single review cycle. I believe this work must undergo another thorough assessment in its revised form, before it can be accepted for publication.
This paper provides generalization bounds for meta learning based on a notion of task relatedness. The result is natural and interesting intuitively, when tasks are similar, then meta learning algorithms should be able to utilize all data points across all tasks. The theoretical contribution is novel, and the results also provide more practical insight into the performances of some models.
Most of the existing GNN based methods model the node labels independently and ignore the joint dependency of node labels. The CRF based methods work in this setting, but they are hard to learn. Hence, this paper proposes to ease the learning difficulty by solving the proxy problem and simplifying the max min problem.

The SMN model proposed in this work is much cheaper than the CRF method. For parameters, since the node GNN and edge GNN share parameters in layers, only a few amounts extra parameters are introduced. As for the training time, it just doubles the general GNNs. Compared with CRF methods, the cost saved by SMN is significant.

Empirically, SMN works well in most settings, in terms of both node level accuracy and graph level accuracy, the different backbones, and different datasets. Meanwhile, the authors provide results to show the effect of refinement, the shared GNNs, the different learning methods, convergence, and a tiny case study. The experimental results are significant and well organized.

After the rebuttal and discussion, all reviewers are in a favor of accepting this submission.
This paper proposes the use of Gaussian process regression embedded into a neural network architecture for few shot segmentation. In more detail, support and query images and support masks are fed through their encoders and their corresponding features are then used for Gaussian process regression to infer the distribution of the query mask encoding given the support set and the query images. The mean and the variance characterizing the GP predictive distribution is then fed into a CNN based decoder to make the final prediction (segmentation).  The method is evaluated on PASCAL 5^i and COCO 20^i datasets, showing the superiority of the proposed approach wrt several competitive baselines. 

Overall, the reviewers found the approach of using GPs within the proposed architecture interesting and somewhat significant and novel to the few shot segmentation community. Technically, the proposed method does not develop a new algorithm and simply uses standard Gaussian process regression. The authors seemed to have addressed several concerns raised by the reviewers including the ablation study evaluating the influence of the GP module. However, the reviewers felt that there were quite a few changes/clarifications to the paper and new results that were not highlighted in the revised version, which made it difficult to provide a new assessment of the paper. Furthermore, the reviewers also thought that the authors did not provide convincing explanations in terms of the improvements from 1 shot to 5 shots, the not so good results when the model was trained with standard SGD without loss weighting and the rationale behind the success of the 5 shot setting.
This paper addresses the problem of program synthesis given input/output examples and a domain specific language using a bottom up approach. The paper proposes the use of a neural architecture that exploits the search context (all the programs considered so far and their execution results) to decide which program to evaluate next. The model is trained on policy using beam aware training and the method is evaluated on string manipulation and inductive logic programming benchmarks. The results show that the proposed method outperforms previous work in terms of the number of programs evaluated and accuracy.  

Overall, the reviewers found the paper to be well written and the idea proposed to be significantly novel and interesting to be presented at the conference and I agree. Several limitations were pointed out by the reviewers in terms of (i) actual run time performance, (ii) the incompleteness of the search algorithm and the (iii) reproducibility of the approach. I believe the authors have addressed these points satisfactorily in their comments.
The authors provide a framework for unsupervised clarification based on minimizing a between cluster discriminative similarity. It is more flexible than existing methods whose kernel similarity implicitly assumes uniform weights, and the authors connect to ideas such as max margin and weighted kernel approaches. This yields a clustering algorithm naturally that alternates between updating class labels and similarity weights. Moreover the reviewers (and I) appreciate the analysis of generalization error through Rademacher complexity arguments and detailed author responses. I might add while the paper draws connections to weighted kernel methods and have since added references to sparse subspace clustering etc, there is recent interest in using similar arguments to derive error bounds and uniform concentration results for center based methods that might be included in the survey of related work, for instance recent work from Swagatam Das and collaborators. The authors have importantly added details on the optimization using SMO, and the revision should include these details in a clear exposition together with the computational complexity discussion mentioned in their response.
This paper presents an approach for machine learning to fix programming errors via edits to abstract syntax trees. The main contributions are a pretraining scheme based on masking out subtrees and some minor architectural modifications compared to previous work. Reviewers found the paper to contain a significant amount of work, but there are some questions about significance relative to previous work that framed the problem similarly, and about experimental methodology. Authors did a great deal of work in the rebuttal to address many of the experimental methodology questions, but this also introduced substantial unreviewed changes to the model, the pretraining approach, and the experiments. In total, the remaining concerns about significance and the substantial changes lead us to recommend that this paper be revised and resubmitted to the next conference.
The paper provides a neat idea about explaining (linear) predictors based on designing ways of perturbing parameters. It is focused on linear models (which can still lead to non linear classifiers), but it is a relevant case, particularly for explainability.
This paper proposes algorithms for learning (coarse) correlated equilibrium in multi agent general sum Markov games, with improved sample complexities that are polynomial in the maximum size of the action sets of different players. This is a very solid work along the line of multi agent reinforcement learning and there is unanimous support to accept this paper. Thus, I recommend acceptance.
The Authors study the emergence of systematic generalization in neural networks. The paper studies a timely topic and presents a set of concrete results. For example, reviewer ZgRW emphasizes that a key strength of the paper is constructing simple datasets where systematicity emerges. I think indeed it is valuable, as systematicity is sometimes poorly defined and understood, so building a theoretical testbed might be very helpful.

However, the reviewers found important issues, which the rebuttal was unable to address. Perhaps the key issue (raised e.g. by reviewer 9QCY) is that results do not clearly generalize to more practically relevant settings. What is somewhat missing is a clear set of guidelines or implications for how to improve systematicity in more practically relevant neural networks.

Based on this and other issues raised by the reviewers, unfortunately, I have to recommend rejecting the paper. Thank you for your submission, and I hope that the review process will help you improve the work.
The paper proposes a method for learning state value functions from (s,s ,r) tuples, founded on the theoretical analysis in MDP setting. The extensive evaluation in several environments shows the benefit of the algorithm.

The consensus among the reviewers, and I concur, that the paper proposes an interesting and novel method. It is cleanly presented, and well founded. The evaluations across range of environments, including robot manipulation validate the method.

During the rebuttal, the authors provided additional evaluation, added a discussion on the latent MDPs, and made numerous clarification, addressing most / all reviewers  questions.
This paper finally received divergent and borderline reviews with two positive (6) and two negative (3) rates. After the thorough reviews by ACs ourselves, we would like to decide to reject this work at this time, even though this submission has a lot of potentials including intensive analyses on instance segmentation frameworks and architectures.

We first would like to appreciate comprehensive author’s responses and additional empirical results. They should be extremely helpful to make this submission stronger. Here are some of our suggested points for improvement: (i) The novelty, significance, and practical implications of this work (compared to previous analysis work) may need to be better presented in a more persuasive way. (ii) Nuance of stylization transformation can be better explained compared to other types of perturbations or transformations. (iii) Empirical fairness can be better justified. (iv) Since the paper is written in a highly condensed way, some of reduction may improve the readability. (v) Finally, given that this paper focuses on empirical study about instance segmentation, it may be more appreciated in a computer vision venue.
This paper proposes InfoMax Termination Critic (IMTC), a new approach for learning option termination conditions with the aim of discovering more diverse options. IMTC relies on a scalable approximation of the gradient of a mutual information objective with respect to the termination function parameters.

Reviewers liked the motivation and the simplicity of the approach. While there were some initial concerns regarding the similarity of IMTC and VIC, the authors did a good job of clarifying the differences and providing additional results in the rebuttal. While two reviewers raised their scores based on the rebuttal, this left reviewers split on whether to accept or reject the paper.

Given that the paper’s main contributions are evaluated empirically I based my decision on the strength of the evaluation. The main claim in the paper is that IMTC significantly improves the diversity of the learned options when combined with intrinsic control methods like VIC and RVIC. The main supporting evidence of this claim is a visualization of the option policies and termination probabilities reached by VIC and RVIC. There are several issues with this comparison:
* This is a poor visualization of the kind of option diversity the paper aims to obtain. Given that mutual information based objectives used by VIC, RVIC and IMTC aim to optimize diversity in the final states reached by the options, visualizing the distribution of final states or the trajectories produced by the options is more meaningful.
* The VIC and RVIC baselines are evaluated with a fixed option termination probability of 0.1 which biases the comparison in favor of IMTC because IMTC is able to choose when and where to terminate while VIC and RVIC with random termination get to control neither. Using fixed option duration with MI based option discovery methods like VIC, DIAYN and RVIC is more standard and is known to produce options with very clear terminal state clusters which are well separated for different options. Fixed option duration allows VIC and RVIC precise control of where they will terminate since option duration is fixed, hence it should have been included in the comparison.
* As mentioned in point 2 above, it is well established that VIC tends to learn options with well clustered end states, especially in simple gridworld domains like in Figure 3 (see VIC, DIAYN and RVIC papers). The authors seem to obtain different qualitative results raising questions.

Overall, I don’t think the qualitative experiments show that IMTC is able to improve the diversity of options discovered by VIC or RVIC due to issues with how the experiments are done (random option duration for VIC and RVIC) and how the results are presented (visualizing action probabilities instead of final states). Given these concerns and the split among the reviewers I recommend rejecting the paper in its current form.
This work concerns Automatic Music Transcription (AMT)   transcribing notes given the audio of the music. The paper demonstrates that a single general purpose transformer model can perform AMT for many instruments across several different transcription datasets. The method represents the first unified AMT model that can transcribe music audio with an arbitrary number of instruments.

All reviewers rated this paper highly and are excited about seeing it at the conference. One reviewer noted that "This paper seems to be a great milestone in the AMT research. It is probably the first unified AMT model that can take music audio with an arbitrary number of instruments."

The reviewers had some suggestions and comments, which appear to be addressed by the authors.
Four experts reviewed the paper and provided mixed recommendations. All reviewers found the experimental results strong, but they have different views about the technical novelty. Three reviewers considered the technical novelty as a weakness of the paper, but Reviewer z4BR was less concerned about it than the other two. After AC carefully read the paper and the authors  responses, AC agreed with the reviewers that the combination of InfoLOOB and modern Hopfield networks, which were both existing works, is incremental despite the empirical results. Besides, AC agreed with Reviewer jmHN that the theoretical results are not significant enough and could be moved to Appendices. While the empirical results are strong, they could not answer how the trend would change with bigger models and bigger datasets. Hence, while the paper clearly has merit, the decision is not to recommend acceptance. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
ICLR is selective and reviewers are not sufficiently enthusiastic about this paper. In particular, they point out closely related methods that should be cited and compared to as baselines. The reviews are of good quality, and the authors did not respond.
This work suggests an extension of diffusion based generative models, where both the forward and reverse process have learnable parameters (rather than just the reverse process). This is then applied to speech synthesis, with high fidelity audio generated in very few sampling steps compared to what is typical for this class of models. The proposed model is specifically compared to other diffusion based approaches for speech synthesis in terms of inference speed.

Reviewers highlighted the novelty of the idea and the convincing experimental results. Concerns were raised about the accessibility and clarity of the presentation (structure, too many technical details), lack of a related work section, and the methodology used to compare the proposed model against baselines. The authors have attempted to address these issues, and two reviewers raised their scores as a result. All reviewers now recommend acceptance.

I am therefore recommending acceptance as well, but I would like to encourage the authors to polish the presentation further, in order to make the work maximally accessible to a wide audience.
This paper proposes an importance sampling estimator for probabilities of observations of SDEs. The proposed approach has several advantages over conventional methods: it does not require an SDE solver, it has lower gradient variance, and shows nice results with a Gaussian process representation of the function. Reviewers were somewhat split on this paper, with some concerns that experiments were limited. On balance, however, the paper makes several nice contributions, the experiments are in line with related works, and the authors did a good job of clarifying Theorem 1 in the rebuttal. We note that Reviewer K19Y changed their opinion to accept (although they forgot to update the score). Please carefully account for all reviewer comments in the final version.
This paper suggests an architecture with a deterministic initialization which has only 0/1 values.
The reviewers were mostly (marginally) negative, mainly because of the low novelty and significance of this work. 

Specifically, the main novelty issues were:
1) Improving convergence speed and removing BatchNorm: was already done, in a quite similar manner, and it achieves better or similar results. (Fixup , ReZero: https://arxiv.org/abs/1901.09321, https://arxiv.org/abs/2003.04887, and few others as well)
2) Initializing a network with deterministic initialization: was also done (ConstNet, https://arxiv.org/abs/2007.01038). I think the main difference from the previous work is the additional Hadamard connections, which help break the symmetry. However, it is unclear what is the benefit of this modification, as the previous work could train without it (albeit on CIFAR).

Specifically, the main significance issues were:
1) Reducing standard deviation: The authors  response confirmed there is no statistically significant benefit (p ~ 0.1) for variance reduction when comparing with Kaiming initialization for ImageNet.
2) General network performance: The results do not seem better than the baseline (Xavier init is not a proper baseline in a network with ReLUs).
3) Sparsity claims: The network appears to be losing accuracy even with 20% sparsity, which isn t even useful for efficiency. For comparison, the lottery ticket hypothesis showed you can get to 90% sparsity and get better results. So, this is a nice observation, but not a major contribution.

Therefore, I recommend the authors to better distinguish themselves from previous works (What are the changes? Why are these important?), and improve their empirical results so they highlight the usefulness of the suggested method (e.g., improve the SOTA in some benchmark).
*Summary:* Study isolated orientations of weights for networks with small initialization depending on multiplicity of activation functions. 

*Strengths:* 
  Interesting analysis of properties in early stages depending on activations. 

*Weaknesses:* 
  Reviewers found the settings limited. 
  Reviewers found experiments limited.  

*Discussion:*

In response to ejGJ authors reiterate scope of covered cases and submit to consideration that their experiments should be adequate for basic research. Reviewer acknowledges the response, but maintains their assessment (limited scope of theory, limited experiments). KucV found the experimental part limited in scope, the settings unclear (notion of early stage, compatibility with theory), and review of previous works lacking. KucV’s sincerely acknowledged authors for their efforts to address their comments and improving the manuscript, and raised their score, but maintained the experimental analysis is not fully convincing and unclear, and the comparison with prior work insufficient. zuZq also expressed concerns with the experiments and the notions and settings under consideration. They also raised questions about the comparison with standard initialization. Authors made efforts to address zuZq concerns. zuZq acknowledged this but maintained initial position that the article is just marginally above threshold. jDJ5 found the paper well written and the conclusion insightful. However, also raised concerns about the experiments the settings under consideration. Authors made efforts to address jDJ5’s concerns, who appreciated this but was not convinced to raise their score. 

*Conclusion:*  
Two reviewers consider this article marginally above and two more marginally below the acceptance threshold. I find the article draws an interesting connection pertaining an interesting topic. However, the reviews and discussion conclude that the article lacks in several regards that in my view still could and should be improved. Therefore I am recommending reject at this time. I encourage the authors to revise and resubmit.
This paper investigates the dereverberation problem from the audio visual perspective.  The geometry of the environment is represented by RGB and depth images.  The authors propose a so called visually informed dereverberation of audio (VIDA) model and also create a dataset consisting of both synthetic and real data to verify the effectiveness of the model.  Experiments are conducted on speech enhancement, speech recognition and speaker identification tasks.  The authors compare VIDA with audio only dereverberation as well as various established baseline systems in the community.  

The audio visual way of coping with dereverberation using visual representation of the acoustic environment seems to be interesting. The authors  rebuttal has cleared most of the concerns raised by the reviewers but there are still numerous lingering concerns which affect its acceptance.  First of all,  most of the reviewers consider the novelty not overwhelmingly significant.  Second, the contribution of the visual input seems to be only marginal compared to the audio only dereverberation. Results on real data are also mixed.  Some of the reported p values are extremely small, which raises questions whether it is due to the size of the test set.  Third, there are noticeable artifacts in some of the samples in the demo.  Fourth,  there are numerous issues in the paper that are worth further in depth investigation. For instance, it would be helpful to show in which way exactly the RGB and depth images helps.
The reviewers overall were quite happy after the rebuttal phase, in which the authors considerably improved the presentation quality and addressed reviewer concerns, and recommended acceptance. The reviewers agreed that while the theory was short and relied on various possibly restrictive assumptions and maybe was largely an improvement in constant factors, it extended prior work (some of which was in ICLR) and was interesting and motivated the experiments which were notably faster than existing methods.
The paper explores "Astuteness of explainer", to measure reliability of the explanations. There were concerns about the overlap of the proposed work with existing literature.  It was felt that both theory and experiments need more development
Verifying robustness of neural networks is an important application in machine learning. The submission takes on this challenge via the interval bound propagation (IBP) framework and provides a theoretical analysis on the training procedure. They establish, in the large network with case, that the certification via IBP reflects the robustness of the neural network. Despite the tensions between the changing architecture and the required accuracy, the results are insightful. The AC recommends the authors to revise the paper, correcting the significant amounts of typos and improve the presentation for its final version.
The authors consider an interesting approach for modeling analogical relations through Abelian group networks. While the conceptual contributions in the work, the explicit introduction of Abelian relations in particular, were generally appreciated, the reviewers found the numerical results provided in the paper lacking. In addition, several issues regarding the scope of the problems to which the proposed approach applies have been raised. Thus, given this, and the exchanges between the reviewers and the authors, in its present form, the paper cannot be recommended for acceptance. The authors are encouraged to incorporate the valuable feedback provided by the knowledgeable reviewers.
The paper develops a diffusion process based generative model that perturbs the data using a critically damped Langevin diffusion. The diffusion is set up through an auxiliary velocity term like in Hamiltonian dynamics. The idea is that picking a process that diffuses faster will lead to better results.The paper then constructs a new score matching objective adapted to this diffusion, along with a sampling scheme for critically damped Langevin score based generative models. The idea of a faster diffusion to make generative models is a good one. The paper is a solid accept.  

Reviewer tK3A was lukewarm as evidenced by their original 2 for empirical novelty that moved to a 3. From my look, it felt like a straightforward application of ideas in one domain, sampling, to another, generative modeling. It s a good paper, but it does not stand out relative to other accepts.
The authors improve upon existing algorithms for complete neural network verification by combining recent advances in bounding algorithms (better bounding algorithms under branching constraints and relaxations involving multiple neurons) and developing novel branching heuristics. They show the efficacy of their method on a number of rigorous experiments, outperforming SOTA solvers for neural network verification on several benchmark datasets.

All reviewers agree that the paper makes valuable contributions and minor concerns were addressed adequately during the rebuttal phase. Hence I recommend that the paper be accepted.
This paper has been reviewed by four experts. Their independent evaluations were consistent, all recommended rejection. I agree with that assessment as this paper is not ready for publication at ICLR in its current form. The reviewers have provided the authors with ample constructive feedback and the authors have been encouraged to consider this feedback if they choose to continue the work on this topic.
The paper considers input dependent randomized smoothing to obtain certified robust classification. The main contribution is the derivation of necessary conditions on how the variance of the smoothing distributions (assumed to be spherically symmetric Gaussian distributions) has to change to achieve certified robustness. All reviewers like this result, as it provides guidance on designing input dependent smoothing, which is an interesting result for the community and certainly helps future research.

On the negative side, the smoothing method derived based on the theory provides little (if at all) improvement in practice, it cannot be scaled to higher dimensions, it does not address the problems it claims to address (the "waterfall" effect, as also admitted by the authors in the discussion), and the presentation should be significantly improved. 

The paper received mixed reviews. While I think that the presented theoretical results are useful and interesting, the problems mentioned above make me to side with the negative reviewers and suggest rejection of the paper at this point (although this was not an easy decision).

While this is only lightly touched in the reviews, I strongly recommend the authors to make the presentation of the theoretical results more comprehensible. It is quite hard to follow the paper as notation is introduced continuously in an ad hoc and confusing way (e.g., in the proof of Theorem 2, $a$ denotes $\delta$ and $\|\delta\|$), and things are often not adequately defined (e.g., the certified robust radius is not defined formally; in Lemma 1, $x$ is undefined and used for $x_0$ as well as a free parameter, $\chi_N^2$ is only implicitly defined, etc.)
Although scores are somewhat mixed, even ignoring the most negative review the overall score would still be somewhat below the acceptance threshold.

The authors and reviewers had a robust discussion, mostly about the novelty, experimental setting, and the significance of the results. Although the discussion ultimately did not reach a consensus, I think there are valid points on both sides. E.g. I somewhat disagree with the reviewer that the paper is too application focused for ICLR, though several other points remain valid. The overall message that the experiments seem not totally convincing was highlighted by multiple reviewers.
This paper proposes a more generalized form of certified robustness and attempts to provide new results on applying randomized smoothing to semantic transformations such as different types of blurs or distortions. The main idea is to use an image to image neural network to approximate semantic transformations, and then certify robustness based on bounds on that neural network. The authors provide empirical results on standard benchmark datasets like MNIST and CIFAR showing that their method can achieve improved results on some transformations compared to prior work.

The review committee appreciates the authors taking the time to attempt to respond to the concerns of all reviewers, and for updating and improving their work during the rebuttal process. The committee is glad to see that they do provide empirical evidence of improvement to common corruption robustness, compared to AugMix (one of the state of the art approaches for standard common corruption robustness) and TSS.


However, the reviewers still have concerns about the novelty of the paper. The main novelty is not improvement for resolvable transformations (prior works that the authors cite perform about the same or better), but rather, is the ability to handle non resolvable transformations. The reviewers agree that robustness to non resolvable transformations is important; however, the reviewers think certified robustness to non resolvable transformations is not meaningful, because they are only being certified with respect to a neural network that is trained to approximate those non resolvable transformations. Without MTurk studies to confirm how good the neural network s non resolvable transforms are, the reviewers do not find certified robustness here meaningful.
Thank you for your submission to ICLR.  The reviewers and I are in agreement that the paper presents a substantial contribution to the field at the intersection of differentiable simulation and ML methods.  In particular, the half inverse method is compelling, non obvious, and hints of a nice path forward towards the goal of practical differentiable simulations within models.  Overall I m happy to recommend the paper be accepted.
This paper explores the use of recurrent neural networks to model neural activity time series data. The hope is that computationally demanding biophysical models of neural circuits could be replaced by RNNs when the goal is simply to capture the right input output functions. The authors show that they can fit RNNs to the behaviour of a complex, biophysical model of the C elegans nervous system, and they explore the space of hyperparameter and network choices that lead to the best fits.

The reviews for this paper were borderline, with scores of 3, 6, and 8. On the positive side, the reviewers agreed that the paper is very effective in demonstrating that the input output behaviour of the biophysical model of C elegans can be replicated by RNNs. But, on the negative side there were concerns about the limited nature of the empirical results, lack of details about the simulation, too much emphasis in describing well known RNN architectures, and lack of systematic strategy for applying this technique in other systems. The rebuttals did not change the borderline scores.

Thus, this is an instance where the AC must be a bit more involved in the decision. After reading the paper and reviews, the AC felt that this work was not sufficiently general in its application. Ultimately, using artificial neural networks to fit neural data is common practice nowadays, so really, this paper serves as a proof of concept for replacing a complex biophysical model with a simpler RNN. But, given that RNNs are quite good at modelling sequence data, it s not terribly surprising that this works. Moreover, though the authors do a very careful search over network design decisions, they don t provide a systematic strategy for others to employ if they so wished. Also, the authors do not provide much insight into what the RNNs learn that might help us to better understand the modelled neural circuits. And most importantly, this only demonstrates the effectiveness for systems where we have biophysical models with well established accuracy, which is not the case for most neural circuits. Given these considerations, a reject decision was reached.
This paper presents the problem of robust inaccuracy (model predictions being robust to perturbations but inaccurate on datapoints), and present methods to maximize robustness while avoiding robust inaccuracy. Furthermore, they develop an abstention mechanism based on robustness to prevent prediction on points where the model is not robust. Results show improvement in adversarial robustness to standard attacks with only small reduction in natural accuracy. 

Reviewers were mixed on the clarity and importance of this submission. A major concern raised was on the importance of robust inaccuracy, motivation for avoiding it, and novelty of the proposed method. Other abstention mechanisms are available and one does not solely need to rely on robustness. Additionally, results are often presented on a pareto front and the method does not strictly dominate prior approaches. Authors addressed many of the clarity concerns in their updated revision, and reviewers commented on the high quality of analysis performed in the experiments. But several reviewers still found the draft and description of the robust inaccuracy problem insufficiently motivated and the methodology not well explained. Given lingering concerns over clarity and motivation (in spite of a revision that exceeds the page limit), I cannot recommend this paper for acceptance.
Overall, the work is borderline with no reviewer feeling strongly for or against the paper.

The paper is well written and proposes a simple approach, along with code for reproducibility. Criticism stems primarily in the work s technical novelty, being an incremental improvement of ideas from ANP and BANP, and related work like Neural Bootstrapper. In addition, the experimental validation involves regression on 1 to 2D functions, Bayesopt on synthetic functions, and contextual bandits on the synthetic wheel bandit problem. This is fairly toy, and multiple reviewers raise unaddressed concerns on the regression experiments. Ignoring orginality in and of itself (which is overvalued in conferences), the work does not yet provide a sufficiently convincing demonstration of its practical importance.

I recommend the authors use the reviewers  feedback to enhance their preprint should they aim to submit to a later venue.
All but one of the reviewers recommended rejecting this submission. The reviewer recommending acceptance (PBhC) was not confident in their assessment and was unwilling to champion the paper during the discussion phase, making it very difficult for me to unilaterally overrule the de facto reviewer consensus and recommend accepting the submission. Although some of the reviewers recommending rejecting the submission made relatively weak arguments, others raised more compelling points in favor of rejecting the paper. The discussion and reviews convinced me that the preponderance of the evidence indicated that I should recommend rejecting on the merits of the case anyway. Ultimately, I am recommending rejecting this submission, primarily because I do not believe the empirical contributions are strong enough, nor are they polished enough. Holistically, it is hard to see what impact this work can have without improved empirical evidence, given how little guidance the theoretical results give to practitioners. That said, I hope the authors iterate some more on the experiments and refocus the narrative a bit in that direction.

The paper exhibits a problem where gradient descent with momentum provably generalizes better than gradient descent without momentum. Given that momentum does not universally improve the out of sample error of neural networks trained with gradient descent, we should strongly suspect that there also exist problems where adding momentum to gradient descent degrades out of sample performance. Therefore, what actionable insights do we have? The paper suggests that perhaps the details of the problem (constructed in the submission) where momentum helps gives us an ability to predict when momentum will be helpful in practice, but we would need to see several more successful predictions of this form on typical datasets from the literature or other real (non synthetic) datasets. Furthermore, has the literature and this submission even demonstrated convincingly enough that momentum improving out of sample error for the same training loss is a common occurrence? And has this submission even made a convincing empirical case on CIFAR10, let alone a larger selection of problems? The latter question would be sufficient to reject the submission, but resolving it favorably would not, in my view, be sufficient to accept the submission without also more evidence for the prevalence of this momentum generalization phenomenon or without demonstrating successful predictions about relative generalization performance on more problems.

Has the literature established that gradient descent or minibatch stochastic gradient descent often generalizes better when using momentum? The paper says "While these works shed light on how momentum acts on neural network training, they fail to capture the generalization improvement induced by momentum (Sutskever et al., 2013)." but Sutskever et al. to my recollection only measures training set loss and never properly considers questions of generalization. Certainly, in many places in the literature we see momentum get better validation error, but rarely do we get information on whether it does so for the same training loss and a priori we should suspect optimization speed is the primary effect at play. The paper also claims "Although it is well accepted that Momentum improve generalization in deep learning...", but the submission does not provide enough evidence that this is well accepted. The results of Leclerc & Madry (2020) are equivocal and may well be confounded by batch norm, but would need to be investigated further. So no, at least with the citations in this submission, it is far from well established that momentum often improves generalization performance, i.e. that momentum results in better validation loss for the same training loss. Of course it won t always do this, but we should observe it regularly in the wild (the more dramatically the better) for this to be interesting.

Ok, but what about the experiments on CIFAR10? These experiments are hard to interpret because they seem to compare misclassification error (zero one loss) with the actual optimization objective of cross entropy error. These issues may be resolvable, but in their current form leave open too many loose ends. Just because two training runs both get zero classification errors on the training set does not mean that they do not differ in the log loss and even a small difference in log loss might explain a large difference in out of sample classification error. Although we often use these quantities as proxies for each other, that isn t quite safe and a better way to conduct this measurement would be to select an iterate of GD without momentum that has an almost identical (but slightly better) training cross entropy loss than a specific iterate of GD with momentum and then compare the cross entropy loss on the validation set, repeating for many different runs and iterates.

In the final analysis, stochastic gradient descent without momentum rarely gets used in practice and full gradient descent even more rarely, so this submission needs to do a better job of making a case for the impact it will have on researchers in this field. Perhaps a stronger case can be made, but I do not quite find the current version sufficiently compelling.
This paper proposed a self supervised speech pre training approach, by the name of SPIRAL, to learning perturbation invariant representations in a teacher student setting.  The authors introduced a variety of techniques to improve the performance and stabilize the training.  Compared to the popular unsupervised learning model wav2vec 2.0, better WERs were reported using SPIRAL with a reduced training cost.  All reviewers considered the work solid with sufficient novelty but also raised concerns regarding the generalization under unseen real world noisy conditions and missing decoding details.  The authors responded with new Chime 3 results  and updated LM decoding results.  The new results show that, after a bug fix, SPIRAL can outperform wav2vec 2.0 when no external LM is used.  

Overall the proposed approach is technically novel.  The experiments are extensive and the results are compelling. In addition, the training time can be significantly reduced compared to wav2vec 2.0. All reviewers are supportive.  So I would recommend accept.
The reviewers agree that this is an interesting treatise on some relationships between SGD fine tuning and evolutionary algorithms. All reviewers have requested some experimental validation or demonstration of the theory developed in this paper, which is not currently included. Whilst the computational requirements (and time required) may be long, this will significantly assist the many readers of the paper and save them from having to run such an experiment many times themselves.  The reviewers provided a number of suggestions of how this might be done. The reviewers also highlighted a number of specific improvements that can be made to the writing of the paper.
This manuscript proposes a ranking approach to identify Byzantine agents in federated learning. Distinct from existing methods, the mitigation is implemented by computing ranks for each gradient, then computing rank statistics across agents. The primary intuition is that adversarial agents can be identified by examining these rank statistics.

There are three reviewers, all of whom agree that the method addresses an interesting and timely issue   giving the growing interest in both Byzantine robust learning and federated learning in the community. However, reviewers are mixed on the paper score   with a strong accept a weak accept, and a strong reject. Common issues raised include the generality of the approach beyond the outlined attacks, 
Other issues brought up, but addressed in the rebuttal include some weaknesses in the evaluation and comparison to additional baselines. There is also an interesting discussion of using higher order statistics, which does not seem to help the methods when evaluated by the authors. Nevertheless, after reviews and discussion, the reviewers are mixed at the end of the discussion.

The area chair finds, first, that the paper is much improved, and much more applicable in the updated form than in the original version. However, the area chair agrees with the reviewer who notes that the moniker "Byzantine robust" implies the methods should be provably robust to worst case adversaries, not only to a selected set of adversaries with pre selected attacks. The specified setting may be too narrow for interest by the community. To this end,  the area chair suspects that the method may be robust to a more general set of attacks than noted   working to outline sufficient conditions for robustness would significantly strengthen this work. The asymptotic nature of the robustness guarantees is also of concern.

An additional concern of the area chair is that the system setting investigated assumes gradient communication and IID data across devices. While this is not an issue on its own, the setting is closer to distributed learning than federated learning, where one generally communicates model updates, or model differences after multiple local updates, and not gradients. This difference can have a significant effect on robustness methods that depend on identifying benign vs. adversarial statistics of parameters. Non IID data is also common in the federated setting, though this is less concerning, as robust methods for non IID settings are only now emerging. A simple fix for this issue would be to rename the setting from "Federated" to "Distributed."

Authors are encouraged to address the highlighted technical concerns in any future submission of this work. The primary concern may simply be a naming issue (i.e., removing "Byzantine" might fix this concern. Nevertheless, taken together, the opinion of the area chair is that the manuscript is not ready for publication. Again, the area chair believes that many of the issues noted can be fixed, the paper can be strengthened, and this paper may be publishable with limited additional work.
The paper proposes a method to accelerate training of an architectural hybrid of Transformers and CNNs: first train a CNN and then use the learned parameters to initialize a more general Transformed CNN (T CNN) model; subsequently continue training the T CNN.

Reviewers ratings are marginal, with three "marginally above threshold" and one "marginally below threshold".  However, no reviewer makes a compelling argument for acceptance, and all reviewers point to significant weaknesses in the work.  Reviewer ojmG: "novelty of the proposed method is limited" and "do not always reach the performance of end to end Transformers".  Reviewer Q4Pp: "experiments are very limited" and also (after rebuttal): "it would good to provide some experiments on a dataset different to ImageNet".  Reviewer ZjBY: "proposed model is not compared with many of the existing model architectures" and (after rebuttal): "would benefit from additional experimental analysis".  Reviewer zV42: "limited novelty prevents me from giving a higher rating".

In summary, while reviewer ratings span either side of above/below the acceptance threshold, the reviewer comments point to limited novelty and limited experimental impact.  Results appear not particularly surprising or significant: while the method provide some savings in training time, it does not seem to ultimately improve top accuracy on tasks and still lags behind the latest vision transformer architectures.  The author response did not substantially change reviewer opinion.  The AC has also taken a detailed look at the paper and does not believe the contribution to be of sufficient significance to warrant acceptance.
All reviewers were clear in their opinion that the paper deserves to be accepted. One reviewer also indicated a wish to increase the score from 6 to 7 but was not able to do that, so it isn t reflected in the final score. The reviewers appreciated the methodological contribution made by the paper.
This paper presents an approach to synthesize programmatic policies, utilizing a continuous relaxation of program semantics and a parameterization of the full program derivation tree, to make it possible to learn both the program parameters and program structures jointly using policy gradient without the need to imitate an oracle.  The parameterization of the full program derivation tree that can represent all programs up to a certain depth is interesting and novel.  In its current form this won’t scale to large programs that require large tree depth, but is a promising first step in this direction.  The learned programmatic policies are more structured and interpretable, and also demonstrated competitive performance against other commonly used RL algorithms.  During the reviewing process the authors have actively engaged in the interaction with the reviewers and addressed all the concerns, and all reviewers unanimously recommend acceptance.
The paper proposes a Transformer based model called SCformer to perform long sequence time series forecasting by computing efficient segment correlation attention. The reviewers think the method lacks novelty and the experiments need a detailed ablation study.
This paper proposes a new ensemble training method for improving adversarial robustness to multiple attacks (e.g., $\ell_2$, $\ell_1$ and $\ell_\infty$). Specifically, authors adopt the recent Multi Input Multi Output (MIMO) ensemble architecture for computational efficiency. Then, the authors construct the adversarial examples using the outputs of multiple attacks simultaneously. With these examples, standard adversarial training is conducted on MIMO ensemble.

All reviewers are on the negative side. AC agrees with reviewers’ concerns on limited novelty and insufficient empirical evaluation. AC also thinks that the improvement is not that significant compared to the existing method, especially concerning the real world dataset. Overall, AC recommends rejection.
All reviewers raise issues with the proposed method and whether it is a) applicable to non synthetic tasks/datasets; b) how
the input could be broken down into intermediate subproblems in a principled way and whether this would substantially 
make the proposal slower than the vanilla encoder/decoder framework; c) awareness of previous work. It is a same the authors
did not provide a response, however the reviewers have provided useful feedback they could use to improve their submission.
This paper received 5 quality reviewers, where 3 of them rated 5 and 2 rated 3. While this work has merits, many concerns are raised by various reviewers. The AC agrees with the reviewers that this paper is not ready for publication at its current form.
This submission introduces a theoretical model to explain how "in context learning" (i.e. the ability to output a correct prediction based on inputs for a task that the model was not explicitly trained on) is possible. The model uses a mixture of HMMs and shows that in context learning is a natural consequence of Bayesian inference under that model. Overall, reviewers agreed that the contribution was useful and timely, and were somewhat convinced by the theoretical arguments. However, there was some broad concern with the framing of the paper. Namely,
1) The paper claims that prompted data is OOD w.r.t. the pre training distribution. In fact, this is almost certainly not the case for many tasks and datasets. Indeed, it is highly plausible that data very similar to the example given by the paper (identifying the nationality of different celebrities) appears in the pre training dataset of large LMs. Other examples include the popular "tldr;" task format for summarization which is incredibly common on the internet, etc.
2) The paper does not sufficiently distinguish between insights gained in the toy setting considered by the theoretical model and insights that can be applied to large LMs. Most reviewers were concerned that there might not be any reason to think that the insights gained from the theoretical model would apply to large LMs. The paper, however, very much frames itself as developing insight into the behavior of large LMs.

I will recommend acceptance of this paper, but will stipulate that the above two issues should be fixed in the camera ready version. Namely, I would suggest that the authors do not refer to prompted forms of tasks/datasets as "OOD", and I would suggest that any claims about different insights are not applied to large LMs.
All three reviewers recommend acceptance. The paper introduces an interesting study and insights on the connection between local attention and dynamic depth wise convolution, in terms of sparse connectivity, weight sharing, and dynamic weight. The reviews included questions such as the novelty over [Cordonnier et al 2020] and the connection to Multi scale vision longformer, which were adequately addressed by the authors. The findings in this paper should be interesting to the ICLR community.
This paper investigates a semi supervised label refining approach to searching for similar voices for voice dubbing.  The apporach is based on generating refined labels using a clustering algorithm on the initial labels.  Therefore, better voice characteristics can be extracted and used to select a new voice in the target language that closely matches the voice characteristics of the source language.  Experiments are carried out on MassEffect as the main dataset and Skyrim as the second dataset and results show that the proposed approach slightly outperforms state of the art.  While the topic under investigation is interesting and has its value to the applications such as voice casting,  there are strong concerns raised by the reviewers.  Reviewers find the paper difficult to follow.  Some important pieces of information are either missing or only vaguely explained (e.g. non expert initial labels,  clear interpretation of p vectors, etc.), which greatly hinders a deep understanding of the work.  Some technical details such as network architecture and its training should be elaborated.  This paper needs some good improvement in order to get accepted.  No rebuttal is provided by the authors so all these concerns still stand.
The focus on this paper s proposed FILIP method is to perform word patch alignment by token wise similarity matrix through cross modal late interaction by modifying only contrastive loss, leading to training and inference efficiency. The authors also collected FILIP300M, a large scale cleaned image captioning dataset for FILIP’s V L pre training. FILIP achieves strong performance on zero shot image classification and image text matching tasks, and the paper also visualizes the ability of fine grained (visual textual token) classification and localization. Overall most of the reviewers appreciated the idea and the generalization results, but had some concerns about not enough technical novelty over the Khattab and Zaharia Colbert paper, which this paper adopts for multimodal tasks. Some reviewers also had concerns about the dataset release but the authors promise to address this. Some reviewers were also not fully convinced about the high storage requirements and scalability for some of the retrieval tasks that the authors tested.

NOTE: The authors are also asked to describe any ethical considerations or issues that arise in their large scale dataset collection in the camera ready version of the paper, see https://arxiv.org/abs/2110.01963 for examples.
The paper proposed a novel deep learning model specifically designed for periodic time series forecasting problems. The approach includes lay by layer expansion, residual learning, and periodic parametrization. The model outperforms state of the art baselines on several time series forecasting benchmarks.  The reviewers appreciate the extensive experimental results, but also suggested improvement on writing and comparison regarding the parameter efficiency of the model.
Overall, the reviewers thought this paper suggested an important problem.  However, there were many concens.  Particularly, the multiple reviewers felt it was unclear when the new approach is better than prior work. The reviewers had difficulty connecting the experiments to the paper s main claims.
The paper introduces a simple technique to improve non autoregressive generation by training the model to reconstruct model perturbed inputs in addition to inputs perturbed by a fixed noise source. 

Despite interest in the paper, we were worried about a number of aspects missing from section 3. During the rebuttal phase, however, the authors addressed most if not all comments and the section is now rather complete.

For its clarity, and the interesting results, we are recommending this version for acceptance. 

A comment on presentation:

The paper attempts to establish a connection with variational diffusion models, but the connection does not seem strong enough at this point. In a variational diffusion approach, the forward view would not involve $f_\theta$, for example. Also, given that the distribution of $\mathbf x_t$ depends on $\theta$, the gradient estimator used in the paper is a heuristic, and I d like to ask that the authors emphasise this clearly and early on in the draft.
The paper provides additional empirical evidence that self supervised learning methods can help disentangling factors of variation in a dataset. That said, the paper can benefit from better framing and perhaps comparison with existing work (e.g., https://arxiv.org/abs/2102.08850 and https://arxiv.org/abs/2007.00810). Furthermore, the authors acknowledge that there was a bug in their code, which I believe should at least lead to softening the claims about group disentanglement. Accordingly, please consider revising the paper and re submitting to other venues.
The paper is an interesting take on representation learning, using (prior) tasks to determine which information is important. The problem setting is somewhat difficult to pin down, so that that finding the correct comparisons is not obvious and opinions differ on many details of the setup. However, this is not a fault of the paper; it is a general problem the further one moves away from clean settings like classical supervised learning.

There was a lengthy and detailed back and forth between the authors and reviewers, where the authors clarified most of the points raised, extended their results, resulting in one reviewer switching from reject to accept.
This paper presents a method for distilling pretrained models (such as BERT) into a different student architecture (CMOW), and extend the CMOW architecture with a bidirectional component.  On a couple of datasets, results are comparable to DistilBERT a previous baseline. This paper is nice, but can be stronger with more empirical experiments on non GLUE tasks (TriviaQA, Natural Questions, SQUAD for example).  Furthermore, I agree with Reviewer M3tk that there are many empirical comparisons with baselines such as TinyBERT missing and the argument of not needing the teacher model to be super convincing.
This paper proposes a simple, theoretically motivated approach for post training quantization. The authors justify its effectiveness with both a sound theoretical analysis, and strong empirical results across many tasks and models, including a state of the art result for 2 bit quantized weights/activations. All reviewers agreed the paper is worth accepting, with 3/4 rating it as a clear accept following the discussion period, and the fourth reviewer not giving strong reasons not to accept.
This work proposes a hybrid autoregressive and adversarial model for sound synthesis (including but not limited to speech), conditioned on various types of control signals. Although recent adversarial approaches have gained favor over previously popular autoregressive approaches in this domain, because of their ability to produce audio signals much more quickly, the authors argue that these models tend to introduce certain types of artifacts which stem from an inability to learn accurate pitch and periodicity. They propose to address this by reintroducing some degree of autoregression, without compromising too much on inference speed.

Reviewers praised the presentation of this work, the thoroughness of the experimental evaluation, and the audio examples provided. A few concerns were also raised regarding related work and the clarity of some parts of the paper, which the authors have taken the time to address. After the discussion phase, all reviewers chose to recommend acceptance, and I will follow their recommendation.
This work reduces the time and memory complexity of Transformer for long sequences by using multiscale pooling to reduce attention from quadratic to linear complexity. Theoretical and experimental results show good results and are very competitive with the state of the art. The paper is well written and experiments are thorough. The additional results in the rebuttal also helped reduce some of the reviewers  concerns. Though the work is somewhat incremental and the experimental settings for the baselines are different, the thoroughness of the experiments and the good results make this a good addition to the conference. For the final version, the authors should provide code, provide error bars and details of the speedup of GLUE.
While fusing multiple heterogeneous neural networks into a single network looks like an interesting exploration, there are many major concerns raised by the reviewers:
1) The motivation why the proposed method works is not convincing. In other words, under what conditions the proposed would work or would not work is not clear.
2) The authors failed to provide either theoretical analysis or convincing empirical studies of the proposed method. In the rebuttal, the authors did not address the critical issues raised by the reviewers.
3) There are many other detailed problems about the proposed method as well as the experimental setup.

Therefore, by considering the above concerns, this submission does not meet the standard of publication at ICLR.
This paper proposes a VAE based hierarchical generative model (Latent Object Model) to model scenes with multiple objects. 
The paper would benefit from a substantial revision to improve text quality and clarity.
The experiments lack proper quantitative baselines and imputations; and the overall results are quite underwhelming relative to existing models.
The paper did not strike any reviewer as a critical addition to the literature, including various concerns regarding (1) the use of the general theory of relativity, (2) some components are well known in the past works.
This paper proposes an extra loss to add on top of the contrastive learning. The contrastive learning seek representations invariant to transformation, while the extra loss the authors proposed encourage representations to be equivariant to the transformation (i.e. retain information about transformation in later representations). While reviewers and I agree this is a sensible motivation, and acknowledge good results that authors have obtained, the fact that most, if not all, improvement is combing from the 4 way rotation transformation is a bit unsatisfactory. Furthermore, this additional loss was proposed before and is actually quite well known, so the actual novelty in the proposed technique is somewhat limited. Nevertheless, this paper provides a comprehensive evaluation, obtaining a reasonable improvement, and makes a good case for using an equivariant seeking loss. The authors are strongly encouraged to release their code (including training details for reproducing ImageNet results) as the improvements they present are central to the acceptance.
The authors propose to use implicit policies (similar to a conditional GAN) with a GAN inspired regularizer. Theoretically, they show an equivalence between policy matching and state action visitation matching. Finally, they evaluate their approach on D4RL and showed improved performance as well as ablations.

Reviewers did not find the theoretical contribution to be significant.  While the exact form may be novel, the general result has been shown in previous work and they only use the general result as a loose motivation for their approach. All reviewers acknowledge their empirical improvements as the primary strength of the paper. While a central component of their story is joint state action regularization, Reviewer Ht1b identified that their proposed approach does not appear to directly regularize the joint state action distribution, but rather behaves more similarly to existing policy constraint methods. I agree with Reviewer Ht1b and after much back and forth discussion (both Reviewer Ht1b and myself) with the authors, I have not been persuaded otherwise.

The paper has a lot of potential   strong empirical results, but the justification and explanation of the method needs to be rewritten in light of the policy constraint regularization or a stronger argument needs to be put forth in support of joint state action regularization. I don t think this diminishes the results though, but without this substantial revision, I cannot accept the paper at this time.
This paper proposes a self exciting temporal point process model with a non stationary triggering kernel to model complex dependencies in temporal and spatio temporal event data. The kernel is represented by its finite rank decomposition and a set of neural basis functions (feature functions). The proposed model has superior performance in comparison to other state of the arts methods. All the reviewers recognized that the model is interesting and advances the state of the art in a meaningful way. While they were some concerns regarding the experimental evaluation, particularly in terms of real data, and the presentation, the rebuttal/revision by the authors cleared up these concerns.
This paper addresses the problem of goal navigation in unseen environments by learning to build a local, then a registered, global occupancy and semantic map of object categories from reprojected RGB+D observations, while extrapolating (hallucinating) unseen observations from contextual semantic priors (e.g., "tables are usually surrounded by chairs"). It then uses a measure of epistemic uncertainty on different estimations (realisations) of that map as a navigation goal selection policy to perform active exploration, and controls the agent using a local goal driven policy; different information gain metrics are investigated. Essentially, the policy accumulates the predicted semantic maps and uses the uncertainty of the semantic mapping to select informative goals. The semantic map predictor is implemented as three U nets for occupancy extrapolation from depth projection, semantic segmentation of RGB, and semantic map inference from the ground projection and extrapolated depth projection maps. The whole method is evaluated on the Matterport3D environment.

Reviewers praised the well written and comprehensively evaluated study, the active learning formulation and the idea of epistemic information gain as a measure of uncertainty for goal selection, and the code availability. Reviewers  major concerns included the computational cost of ensemble based uncertainty estimation (Nmyk), and a missing submission to an active learderboard of the habitat challenge (Nmyk, CF2f). Reviewers CF2f and qy8x had a longer list of issues that have been addressed in the rebuttal.

Reviewers engaged in a discussion with the authors, and the scores are 5, 6 (though not updated) and 8. I believe that the paper just meets the conference acceptance bar and would advocate for its inclusion in the conference.
The paper extends the analysis of Telgarsky (2013) and Gunasekar et al. (2018) to the Breman proximal point algorithm and to mirror descent. Upper and lower bounds show a dependency on the condition number of the distance generating function used in the Bregman divergence.

The paper received lukewarm reviews, also because the topic does not seem to be a good match for this community. In fact, none of the algorithms analyzed seem to be commonly used as optimization algorithms for deep learning, despite of the applications mentioned by the authors.

So, I didn t take into account the concerns about the relevance of the results for deep learning people, the complains about missing references from the OMD literature, and the supposed restricted setting.

However, even ignoring the above issues, the paper seems to fall squarely on the borderline. Hence, I carefully read it.

It seems to me that the analysis heavily builds on previous work, in particular the seminal paper of Telgarsky (2013) and the Fenchel Young trick in Ji&Telgarsky (2019). The part on the Bregman divergence is novel, but technically speaking it is also straightforward for people in this sub community. For example, Lemma B.3 is very well known to any optimization person. Moreover, the curvature of the Bregman divergence is exactly the term one would expect to appear. So, the upper bound seems to be incremental compared to past work and it does not really add much to our understanding of this problem.
The matching lower bound is probably the only truly interesting result. However, it still does not exclude the possibility to achieve a better margin when measuring it in a different way. Indeed, measuring the margin according to the (dual) norm appearing in the strong convexity definition of Bregman divergence is not completely justified, but rather it seems a way to make the analysis work coherently.

Overall, given the overall lukewarm reviews and my evaluation of the limited novelty of the theoretical results, I recommend rejecting this paper.
The main identified issues were the limited contribution and use cases, poor writing and missing baseline comparisons and more needed experiments. These issues were not addressed satisfactorily by the rebuttal and hence, I believe the paper should be revised by the authors and undergo another review process at another conference. I therefore recommend rejection.
The paper tries to analyze the relationship between regularization, adversarial robustness, and transferability.

Pros:
  An interesting problem was tackled.

Cons:
  The main claim (Prop.3.1) is almost trivial.  Prop. 3.1 shows that "relative" transferability is smaller for stronger regulariation, which is just a slight generalization of the triangler inequality ||YT   YS|| <  ||YT   Y|| + ||YS   Y|| for any Y in Fig.2.  
  Experiments show negative correaltion between the relative transferability and accuracy, which is trivial.  Large regularization degrades the accuracy which increases the "relative transferability".  "Absolute" transferability in Appendix doesn t show clear negative correlations.
  Salmann et al. claimed that adversarially "trained" models transfer better, and did not claim that there are positive correlations between the transferability and robustness for general classifiers without adversarial training.  So the finding in this paper is not surprising nor against Salmann et al.

To prove that adversarial robustness is just a subproduct of regularization, the authors should show that the "absolute" transferability by adversarially trained classifier can be achieved by other regularization.  Defining relative transferability is fine if it is just a decomposition to conduct an analysis of the absolute transferability.  But no conclusion on the performance should be made from its analysis, because a trivial correlation will appear, i.e., (A B) and B should be negatively correlated unless A strongly correlates to B.  Also, this is highly misleading so that some reviewers seem to have misunderstood that the authors would have claimed that negative correlations between regularization and absolute transferability were observed in the original submission.

Overall, the paper requires major revision.
This paper introduces an ImageNet scale benchmark UIMNET for uncertainty estimation of deep image classifiers and evaluates prior works under the proposed benchmark. Two reviewers suggest reject, and one reviewer does acceptance. In the discussion period, the authors did not provide any response for many concerns of reviewers, e.g., weak baselines, weak novelty, and lack of justification for the current design. Hence, given the current status, AC recommends reject.
In this work, authors study query efficiency in the zeroth order setting of adversarial examples. Reviewers pointed out several weaknesses in the work. They mentioned the paper is not well organized and poorly written, experiments are not comprehensive and the practical significance of the proposed method is unclear. Although reviewers appreciated authors  efforts and responses in the discussion period, they felt that the paper is not above the accept threshold this round and still needs a bit more work.
This paper aims to explain the pretraining effectiveness of masked language model, based on the concept of  diversity of classes. They empirically study how a diversity regularizer, based on this theory can improve model performance, as an empirical support.

Before rebuttal, reviewers consistently found the empirical study rather preliminary, while authors, through rebuttals, argue the theoretical study should be highlighted as their main contribution, and expressed concerns that the lack of empirical rigor should not be a ground to reject. We agree with these concerns, but rebuttals and discussions failed to convince reviewers that assumptions and evaluations are proper for connecting the proposed theory to potential impacts in pretrained language model scenarios. Revising to make this connection clearer would address the reviewer disagreements in the future.
There were genuine differences of opinion here. I saw reviews of 8,6,5,5.
In these cases, I do try to check if the 8 has a really compelling argument and err on the side of accepting, but here I think both the positive and negative reviews have fair points, so I am inclined to recommend rejection here.

I think the good news is that a lot of the negative stuff was around scoping/writing/related work, and so it should be (relatively) easy to shore up this submission into something that will get better reviews in the next conference cycle.
This paper presents a new method that uses transformers to predict the result of pairwise competitions given each players’ history of past game plays. The reviewers thought this had notable potential benefits for practice. However the reviewers’ also had some significant concerns with the current work in terms of the evaluations used, which were primarily  correlation instead of prediction accuracy or calibration etc. There was also some concern about other aspects of the presentation. We hope that the reviewers’ responses are useful to the authors in revising their work for future submissions as this method has the potential to be very useful for many domains.
This paper presents a Feature Propagation (FP) method for dealing with missing features in graph learning tasks. The FP method is based on minimization of the Dirichlet energy and leads to a diffusion type differential equation on the graph. Empirical results demonstrated the effectiveness. However, after rebuttal major concerns still remain on the novelty and siginificance, in particular, the connection with label propogation should be better elaborated, which is crucial to understand the contributions of this paper. Considering that, I can t recommend accept the current manuscript. The authors are encouraged to further improve for a more solid publication in the future.
The paper proposes a model for large scale image retrieval. Unlike previous work that rely on local features, the proposed method aggregates local features into the so called Super features to improve their discriminability and expressiveness. To do so, the method proposes an iterative attention module (Local Feature Integracion Transformer, LIT), that outputs an ordered set of such features. By exploiting the fact that features are ordered, the paper proposes a contrastive loss on Super features that match across images. The paper presents a thorough empirical evaluation on several publicly available datasets including relevant baselines.

Overall the paper is well written and the empirical results are strong (including detailed ablations that motivate the design of the method). All reviewers and the AC appreciate the idea of applying the contrastive training at local feature level while only requiring image level labels.

Reviewer hp4Y points out that the proposed LIT is not particularly novel, but previous work are properly cited. Also this is not a major issue given that the motivation is very clear, it is well executed and the empirical results are strong. 

Reviewer uoYN had initial concerns regarding inconsistencies in the mathematical formulation of the method, which were resolved in a detail (and constructive) discussion with the authors.

All reviewers recommend accepting the paper, three of which consider the contribution to be strong. The AC agrees with this assessment and recommends accepting the paper.
All reviewers recognized the contribution of performing a theoretical study to investigate how the two technique lines (detection based methods and integral regression based methods) work for pose estimation. The study in this paper could be valuable for the researchers in the pose estimation domain, for the further research. The AC agrees with the reviewers and recommends accept for this paper.
Summary: this is a difficult paper to meta review, since it contains some insightful ideas and interesting experiments, while it also unfortunately contains omissions, confusions, and places where clarity is lacking (see below). One consistent theme is that the paper is too dismissive of prior work; the exposition is not as clear as it should be about what aspects of FORBES are present in previous papers, it uses too broad a brush to describe prior methods (resulting in too general statements about what these methods can t do), and it skips important chunks of the extensive literature on POMDP belief representation and tracking. As a result, the paper doesn’t do a good job concisely and accurately stating its contribution; there is still reasonable concern about how significant this contribution is. On the other hand, the experimental results for FORBES are interesting; the new method seems to represent a better combination of techniques than at least many existing works, at least to the resolution of the experiments’ statistical power. So the end question is whether interesting experimental results and a new combination of techniques are enough to outweigh the problems outlined above. In the end we believe that the correct outcome is rejection; but we have every expectation that a future version of the paper will resolve the difficulties outlined here and will appear in a future conference.

A brief note about the discussion: the original scores for this paper were lower. While some reviewers raised their score later in the discussion, a thorough reading of the discussion and the revised paper indicates that a substantial fraction of the issues leading to the lower scores still remain.

More details:

There is a lot of prior work on tracking belief states, which should be cited more thoroughly. The paper s intro makes it sound like diagonal Gaussians were the only previous alternative. At least, the intro should cite older work on MCMC methods like particle filters (e.g., Thrun’s book Probabilistic Robotics, or Arnaud Doucet’s work), and prior deep net papers that attempt non Gaussian representations, even if these don’t perform as well as hoped (see below for examples). It is also important to compare to RKHS representations of beliefs, such as Nishiyama, Boularias, Gretton, Fukumizu 2012; these handle multimodality, and can behave similarly to deep nets if they use the neural tangent kernel. Accurately comparing to prior work is one of the most important functions of a paper, so it doesn’t make sense to be unfairly critical of prior work or to skip it.

The paper is also unclear about the effects of Gaussian distributions at different places in a variational approximation. Because of this lack of clarity, the criticisms it levels at previous variational methods seem to be true only of some of them. 

In particular, the introduction should distinguish between two uses of Gaussian approximations: first for the belief itself, and second for the distribution of observations given a belief. Some prior works make only one of these approximations. For example, a non Gaussian distribution used as a belief state can predict multi modal future behaviors, even if we approximate observations under a given belief as Gaussian.

The introduction should also distinguish between two common places that a Gaussian could enter into a variational approximation: at the input or at the output of a network. A Gaussian latent at the input of a variational network (even if it has diagonal covariance) can result in a highly non Gaussian output distribution, while Gaussian noise added at the end will (if it is the only noise) lead to a Gaussian output. Again, some of the statements in the intro apply only to the latter use of a Gaussian, while some prior work focuses on the former use.

There is an important conceptual confusion in the paper about what it means to have a multimodal belief state: the paper presents the true belief as an inherent property of an environment, while in fact it is a property of an environment *model*. So, there can be two different equally accurate models of the same environment which differ in the belief representation; a simple example would be to use either a continuous state whose components are joint angles, or a discrete state obtained by finely discretizing this continuous one. In the first case the belief would be a distribution over the continuous space, while in the second it would be a categorical distribution (a point in a simplex). A consequence of such a difference is that beliefs can be multimodal in one representation and not another.

The importance of this confusion is that, since we are asking our network to learn a belief state, the learning process could potentially favor representations that lead to unimodal beliefs — so it’s not clear theoretically that forcing a unimodal belief representation is necessarily a disadvantage. The paper presents the situation as if the disadvantage is forced by theory, while instead the argument should be based on experiments: e.g., one could try to show that unimodal representations, even if given a higher latent dimension to work with, aren’t empirically able to capture the same information.

Some interesting prior deep net POMDP papers that might need better discussion:
* Han, Doya, and Tani ICLR 2020 (which isn’t cited here) puts the Gaussian latent variable as an input to the network for predicting beliefs (eq 2), resulting in a possibly highly non Gaussian output representing the belief.
* Tschiatschek et al, 2018 (also not cited) uses a Gaussian *mixture* as the variational distribution to approximate beliefs, again allowing multimodality.
* Igl et al. 2018 (which is cited only late in the current paper, and basically dismissed) uses a deep version of particle filters to allow non Gaussian distributions for both beliefs and observations.
* More work that is potentially relevant but not adequately compared (even if briefly cited): Gregor et al. (2019), DreamerV2, Ha & Schmidhuber’s World Models. Each of these makes at least some choices to try to handle at least some kinds of multimodality, so a clear explanation of differences that avoids the confusions mentioned above would be very helpful.
* In general, the results of the search “variational encoder POMDP” seem to include a number of papers not cited in the current paper; another useful search is “normalizing flow POMDP"

Finally, in the experiments section, the paper needs to correctly report the reliability of its conclusions. In some places (e.g., Fig. 5) there’s no mention of reliability or repeatability of conclusions; the paper just says that its evidence “support[s] the claim that FORBES can better capture the complex belief states”. In other places (e.g., Fig. 6, 7), the paper displays uncertainty representations based on only a few replications of an experiment (e.g., 3 seeds for Fig. 6, or 5 seeds when a reviewer requested extra experiments). The corresponding uncertainty estimates almost certainly are strongly biased too low (too certain); e.g., three runs would have less than a 50% chance of even seeing failure modes that happen with probability as high as 20% (0.8^3   0.512 > 0.5), meaning that the estimated standard deviation could be almost arbitrarily badly biased downward. To be clear, experiments with few replicates can still be highly useful and informative, and it’s true that some experiments are too expensive to run many times; but in such cases the paper should add appropriate caveats to its conclusions. For example, instead of reporting the sample standard deviation based on a normal model, the paper could report a confidence interval based on a more robust model or test, such as a Wilcoxon test. (To illustrate the difference, confidence intervals at typical significant levels like p 0.05 would be vacuous (infinitely wide) under Wilcoxon with 3 seeds, but much weaker p values would still yield non vacuous intervals.)

A few smaller questions:

The authors added a nice ablation study to compare to Dreamer; this is great to see. It would be good to discuss the connection to earlier methods such as PlaNet and Dreamer at places where the current method is similar or different (e.g., different from Dreamer in the belief state representation in sec 2.2, but similar in the RL framework in section 3.2). These comparisons would aid in the reader’s understanding of what is new in FORBES.

An unusual feature of FORBES is that the variational approximation to the belief at time t+1 is not a function of the belief at time t. Instead the belief inference network q_{\psi,\theta} takes as input the entire past trajectory, uses convolution and recurrence to reduce the variable length input to fixed dimension, and passes this fixed dimension representation through a normalizing flow mapping. It would be interesting to discuss the reason for this design decision. In particular, it seems like it would inhibit tracking — i.e., it could be hard to propagate information from one belief distribution to the immediate next one, particularly if there are a few unlikely observations scattered through a trajectory.

A minor point for clarity: in Fig 1 it s unclear what distributions the white and gray triangles refer to. They don t seem to correspond to a natural belief state: instead maybe they incorporate three simultaneous observations from the same starting belief? Correctly intersecting beliefs is an important issue though, so at a high level the point that the figure is trying to make fits well.

Another point for clarity: “there always exists a diffeomorphism that can turn one well behaved distribution into another”: this is true for some definition of ”well behaved”, but it’s misleading to say it this way. E.g., it is not true if the distributions in question can have atoms, or differ in dimension or topology; these exceptions are unfortunately important cases that do come up in practice.
This paper presents work on classification with a background class.  The reviewers appreciated the important, standard problem the paper considers.  However, concerns were raised regarding presentation, empirical evaluation, clarity, novelty, and signficance of the work.  The reviewers considered the authors  response in their subsequent discussions but felt the concerns were not adequately addressed.  Based on this feedback the paper is not yet ready for publication in ICLR.
This paper studies the problem of how to train an agent to understand relationships and dependencies among available (and potentially changing) actions in an RL environment to more efficiently solve a task. For instance, in the absence of a hammer for the task of putting up a painting on a wall, the agent could use an alternative tool like adhesive strips if available. The paper s main technical contribution is to use train a graph attention network to learn action space relationships under a given action representation. The paper demonstrates the effectiveness of this strategy on a range of environment benchmarks.

The reviewers initially brought up several lacunae in their assessment of the paper. These included the opaqueness in the explanation of the graph network structure, incremental nature of the improvement over the paper of Jain et al 2020, the lack of clear ablation studies and their message, comparisons with baselines drawn from other existing approaches potentially relevant to the setting, and the role of hyperparameters and their tuning.

In response, the author(s) provided detailed clarifications and additional experimental results. Namely, they clarified the details of the graph attention network, added ablation studies to help understand the role of this component, discussed the relevant and (in)applicability of other existing work, and supplied details about hyperparameter tuning. The author response was adequate to convince the reviewers to arrive at a consensus reflecting the positive impression of the paper.

In view of the unanimous opinion of the reviwers, I recommend acceptance of the paper.
The paper proposed Trained ML oracles to find the decent direction and step size in optimization. The process they call grafting. Reviewers raised several concerns about the reliability of ML oracles in general settings which is valid. The rebuttal could not convince the reviewers to change their opinion.  Ideally for an empirical only paper with heavy reliability on ML for critical decisions, to meet the high bar of ICLR there must be several experiments (5 10 datasets or more) on diverse datasets and settings. Also, there should be discussions on when and how the method fails and related discussions. In that sense the paper does not meet the bar for publication.
The paper proposed a novel assisted learning scenario which would likely be useful for organizational level learners (i.e. learners with sufficient computational resources but limited and imbalance data). The paper is generally well presented, but there are shared concerns amongst the reviewers in the significance of technical contributions: (1) Due to the asymptotic nature of the consistency results, the technical strength is not strongly supported with the existing theoretical analysis. (2) Although the problem setup is novel and seems interesting, the practical significance of the results is not well supported without a concrete real world application. (3) There are a few clarity issues raised in the reviews, which suggest that the paper could benefit from a major revision to address the above concerns.
This paper proposes two extensions of the TRPO algorithm in which the trust region is defined using the Wasserstein distance and the Sinkhorn divergence. The proposed methods do not restrict the policy to belong to a parametric distribution class and the authors provide 
closed form policy updates and a performance improvement bound for the Wasserstein policy optimization.
The authors provide an empirical evaluation of their approaches on tabular domains and some discrete locomotion tasks, comparing the performance with some state of the art policy optimization approaches.

After reading the authors  feedback and interacting with the authors, the reviewers did not reach a consensus: one of the reviewers votes for rejection, while the other three reviewers are slightly positive.
In particular, the reviewer that voted for rejection raised a number of concerns that have been discussed at length with the authors, who were able to clarify some of the issues, but some of the answers did not satisfy the reviewer.
I went through the paper and I found the paper solid from a technical point of view, but I share some of the reviewers  concerns and I think that the authors should better position their contribution with respect to the state of the art. 
Overall, this paper is borderline and I feel it needs still some work to deserve clear acceptance (which I think will be soon).
This paper received some additional discussion between the reviewers and the area chair. The reviewers were largely unswayed by the author responses. One concern was the level of technical novelty, feeling that this was largely a straightforward adaptation of DPSGD (as, admittedly, most works in the DP ML setting are). The primary technical contribution may be the sampling amplification theorem, which one reviewer felt was also straightforward from previous work. Other criticisms was that the privacy parameter epsilon is rather large, and that results are restricted to 1 layer GNNs. Generally, the work did not feel very novel to reviewers from either the privacy or the GNN community. However, they felt that the paper could benefit substantially from exploration and implementation of the comments made in the responses, so the authors are encouraged to pursue those directions. Some of the many suggestions from reviewer Xcpu may help the authors make the paper appeal more to the GNN community.
The paper proposes a new reinforcement learning actor critic type algorithm for parameterized policy spaces. The actor builds gradient estimates derived from perturbations of the policy (in the spirit of simultaneous perturbation stochastic approximation (SPSA) or Flaxman Kalai McMahan s "Gradient Descent without a Gradient" idea), while the critic is based on standard temporal difference (TD) learning. The algorithm is benchmarked, along with other well known techniques, on Mujuco based environments where it is seen to often perform well.

There were several concerns raised by the reviews initially, including the validity of the value function obtained by the rather non standard perturbation of the behavior policy suggested in the paper, the necessity of the zeroth order scheme, the impact of the hyperparameter N, the lack of clarity about the overall algorithmic flow, and the lack of more contemporary baselines such as SAC, A3C and TD3.

Most concerns appear to have been addressed by the author(s) in their detailed responses, and new explanations have been added with significant effort, to the credit of the author(s). While the paper breaks new ground in the conceptual sense, and the reviewers are borderline positive about the paper, I am afraid that parts of the paper, especially relating the the soundness of the algorithm, are still unclear and not concretely motivated. This, coupled with the low confidence levels expressed in the reviewers  evaluations, renders the paper s form too preliminary at this stage to merit acceptance.

For instance, I notice upon a careful reading of the paper the following issues:

(a) Equation (7) is derived by claiming that $V^\beta(s_t)$ is uncorrelated with the Gaussian noise $\epsilon$. However, I fail to see why this should hold, since the paper mentions, in the paragraph before equation (6) that $\beta   \pi_{\theta + \sigma \epsilon}$, so $\beta$ ostensibly clearly depends on $\epsilon$.

(b) The motivation behind the objective $J_{ZOAC}$ in (6), and the quantities involved in its definition, is rather opaque. For instance, the right side of (6) suggests an infinite horizon discounted reward criterion, whereas the expectation is taken with respect to $d^\beta$, the "stationary distribution" of the policy $\beta$. How/why is this justified? I would expect the use of the discounted occupancy measure here, instead of the (long term) stationary measure which washes out any near term trajectory effects.

(c) The paper mentions that $\epsilon$ is a sequence of random perturbations *per time step* in (6) as opposed to the usual ES perturbation of a one time perturbation. However, the size of the covariance matrix $I$ in (6) and (3) are not explicitly distinguished, leading to much confusion in the mind of the keen reader.

I hope that the author(s) can utilize the feedback from the reviews in order to put up a significantly clearer and solidly motivated paper in the next round, so that its conceptual merits can be proven without doubt. Thanks and best wishes.
This paper proposes a new federated learning method which uses the recently developed PAGE gradient estimator in the local updates, and provides convergence analysis for both convex and nonconvex loss functions. There are several technical questions raised by the reviewers that are not addressed by the author rebuttal. Given such technical issues and limited novelty and empirical evidence, I cannot recommend acceptance.
The authors explore the forgetting behavior of large scale pre trained models in continual learning (CL). The authors find that forgetting is mitigated by scale (or models and of pre training datasets). The authors also make preliminary observations to try to explain why this happens. This manuscript is somewhat in line with recent results around scaling laws and in a sense, this paper extends the study of scaling laws to the CL setup with a focus on catastrophic forgetting, traditionally the main desiderata of CL.

The initial reviewer assessments indicated that this paper was likely below the acceptance threshold of the conference. The main perceived limitations were: 
+ CL experiments on sequences of two tasks are limiting (in CL it s common to look at sequences of at least 10 tasks)
+ The effect of the pre training data on the results was not properly assessed. In particular, it is likely that the pre training data and downstream/test data were very similar.
+ Other aspects such as missing hyperparameter values and empirical settings were also raised.

The authors really came through and obtained lots of additional empirical results. In particular, the authors show that their results mostly hold on longer task sequences and even if the downstream task was very different from the pre training tasks. Further, the authors provided precise answers to all the reviewer comments and also ran a few more studies to answer more specific reviewer questions (including a few to answer some excellent suggestions from reviewer ZtJq). 

Overall, this is a good contribution and I imagine one that could have a significant impact in the field and give rise to follow up work. Congratulations! 

In preparing the final version of the manuscript, I would strongly suggest that the authors incorporate all results discussed in their replies in their paper. Further, and as was suggested by reviewer Dwjt, I think it would be very useful to add the study of the longer task sequences and of the different downstream tasks to the main paper and not the appendix as is done currently.
Exciting work at the intersection of continual learning and representation learning. The reviewers have all commented that the proposed work addresses a number of issues related to catastrophic forgetting, which is very encouraging. The work also shows that the representation learning with the proposed method is more general than the one learned with supervised CL. The reviewers have praised the work as being well written and with thorough experiments. There was a robust back and forth between the reviewers and the authors during the rebuttal period, in which the authors appear to have addressed most of the concerns. Given the insights, results and potential impact of this work, I think this work definitely should be published at ICLR.
This paper presents a variant of the WAE which uses a contrastive criterion to enforce the marginal distribution matching constraint. Experiments show faster convergence in terms of Wasserstein distance, more visually appealing samples, and better FID scores compared with other WAE models.

The original WAE framework leaves open the choice of approximation for enforcing marginal distribution matching, and the original paper gives two such algorithms. Therefore, it s pretty natural to replace this approximation with something else (such as the contrastive criterion used here), so a submission would need to show evidence that it s significantly better than other approaches. Reviewers have expressed various concerns about the experiments. None of them are major problems, but overall the method doesn t seem consistently better than other WAE methods; e.g., the FID score is worse than that of WAE GAN.

I encourage the authors to take the reviewers  comments into account in preparing the submission for future cycles.
The paper proposes a Bayesian approach to learning in contextual MDPs where the contexts can dynamically vary during the episode.
The authors did well in their rebuttal and alleviated most of the reviewers  concerns. During the discussion there was an agreement that the paper should be accepted.
Please take all reviewer comments into account when preparing the final version.
This paper regularizes deep neural networks via the Hessian trace.  The algorithm is based on Hutchison’s method, further accelerated via dropout.  Connection to the linear stability of dynamical system is discussed.  The proposed regularization shows favorably in the experimental results.

The idea of the method is clear.  The paper’s writing needs a lot of improvement because there are a number of grammatical errors.  The major technical concerns include: a) the experimental results are still not convincing; b) the explanation of favoring instability in the dynamical system that resorts to overfitting prevention (reviewer GDik).  I’ve read the rebuttal, but remain unconvinced.
This to me looks like quality work not yet adequately developed, and thus is borderline work.  The authors seem to have achieved a good result:  equalling SotA SEAL (although, one reviewer did preliminary experiments and could not match this) with a sophisticated algorithm using a variety of Bayesian tricks, a more scalable algorithm, and one potentially adapted to further tasks.  However, not all of these impressive feats are adequately demonstrated in this paper, though many had parts included in the rewrite.  So I d say the paper needs a rewrite and more focussed experimental work to broaden the presentation of empirial performance, for instance to node classification.
I certaintly appreciated the use of IBP and Dirichlet models within the system, so would love to see the work further developed.
The reviewers agreed in several aspects:  (1) more experimental work, for instance on better and larger benchmark data, (2) better presentation and discussion of the theory, (3) better discussion of the motivation for the model (as per reviewer D8S8), and oftentimes linked to the ablation study to support this, which you have done some of (4) additional connections to recent related work in graph representation learning on link prediction works
The authors have done a good job or addressing many of the reviewers concerns, ultimately lifting the paper from Reject to Borderline Negative, but I think more work is needed.
The paper adopts CVAE to generate OOD samples for training an outliner detector. It consists of two phases that train an OOD detector by leveraging the generated OOD data and shows it outperform other methods. According to reviewers’ discussion, there is a concern from the discussion: why CVAE works but other variants or cGAN doesn’t. The paper needs more motivation or evidence or ablations to support the generality of the work.
The paper studies how to build predictive models that are robust to nuisance induced spurious correlations present in the data.  It introduces nuisance randomized distillation (NuRD), constructed by reweighting the observed data, to break the nuisance label dependence and find the most informative representation to predict the label. Experiments on several datasets show that by using a classifier learned on this representation, NuRD is able to improve the classification performance by limiting the impact of nuisance variables. The main concerns were about the presentation and organization of the paper, which was heavily focused on the theoretical justifications but fell short in explaining the intuitions and implementation details. The revision and rebuttal have addressed some of these concerns and improved the overall exposition of the paper, based on which two reviewers raised their scores to 8. While there is still room to further improve the paper by providing more detailed discussions about the proposed algorithms, the AC considers the paper ready for publication under its current form.
The paper addresses an important problem of selecting inputs to drive an inductive program synthesis process. This is an important problem because inductive synthesis relies on carefully chosen inputs to ensure that the chosen inputs can provide sufficient information about what the desired program is. This paper proposes an approach where instead of simply asking the user to provide a set of input/output examples to the synthesizer, the user interacts with a query network that queries the user on the output of specific inputs and these input/output examples are then fed into an existing program synthesis engine.  

I think the ideas in the paper are very original and I agree with reviewer WAY8 that this paper should be accepted. I think the original version of this paper had several issues that led to the low scores from the other reviewers, but the paper improved significantly with the review process. 

That said, I do think that some of the concerns of the other two reviewers are valid, and some additional steps could be taken to address them. For example, the paper follows a long tradition in ML of making assumptions that are questionable but that make the math work nicely (e.g. choosing to represent things as gaussian distributions, or assuming independence for things that are clearly not independent). We are usually ok with such shortcuts if they are properly acknowledged and the resulting method proves to work well empirically, but the original set of experiments in the paper was extremely minimal. That said, the experiments added through the rebuttal process give me more confidence that even with the mathematical shortcuts, the method still works well.
This paper studies a learning scenario in which there exist 2 classes of examples: "predictable" and "noise". Learning theory is provided for this setting and a novel algorithm is devised that identifies predictable examples and makes predictions at the same time. A more practical algorithm is devised as well. Results are supported by experiments.

Reviewers have raised a number of concerns (ranging from how realistic this settings is to missing references). Overall they found this work interesting and relevant to ML community and appreciate the effort that authors have put in in their thoughtful response. However, after a thorough deliberation conference program committee decided that the paper is not sufficiently strong in its current form to be accepted.
The paper presents a defense against the gradient sign flip attacks on federated learning. The proposed method is novel, technically sound and well evaluated. The crucial issue of the paper is, however, that this defense is specific to gradient flip attacks. The authors show the robustness of their method against white box attacks adhering to this threat model and claim that "an adaptive white box attacker with access to all internals of TESSERACT, including dynamically determined threshold parameters, cannot bypass its defense". The latter statement does not seem to be well justified, and following the extensive discussion of the paper, the reviewers were still not convinced that the proposed method is secure by its design. The AC therefore feel that the specific arguments of the paper should be revised   or the claim of robustness further substantiated   in order for the paper to be accepted.  

Furthermore, as a comment related to ethical consideration, the AC remarks that the paper s acronym, Tesseract, is used by an open source OCR software (https://tesseract ocr.github.io/) as well as in a recent paper: Pendlebury et al., TESSERACT: Eliminating Experimental Bias in Malware Classification across Space and Time, USENIX Security 2019.

All of the above mentioned reservations essentially add up to a "major revision" recommendation which, given the decision logic of ACLR, translates into the rejection option.
This paper presents a method for unsupervised learning of disentangled representations by first training a VAE with a tangled set of latents, and then sequentially learning disentangled latent variables one at a time from the entangled initial VAE latent space. On several toy disentanglement benchmarks, the method is shown to perform competitively with previous VAE and GAN approaches. 

There were several concerns from reviewers around the clarity and description of the proposed one factor at a time (OAT) training procedure. While the updated draft addressed several typos and some clarity issues, multiple reviewers continued to find the method description problematic. There were additional concerns around the viability of the method on real world datasets where the number of factors are not known, and as the authors stated the proposed method can also result in one factor of variation encoded into mulitple latent variables, which hurts on many of the disentanglement metrics.  The addition of CelebA downstream task evaluation begins to address this concern of real world data, but more rigorous experiments (including more description of how models were selected) and discussion of the limtiations of the proposed method are needed. There is also no theoretical motivation as to why the proposed intervention based factor learning algorithm should recover the ground truth factors.

Given the concerns over experimental results, clarity, and lack of theoretical motivation, I suggest rejecting this paper in the current form.
This paper deals with the important practical problem of speeding up GNNs.
Although the proposed method based on LSH may be considered to be a rather too simple preprocessing, it would be worthwhile to share the practical idea with the community as far as the proposed method is shown effective enough.
However, as pointed out by several reviewers, it is concerned that the experimental validation of this paper is not sufficient.
Further and deeper validations will make this paper stronger.
The manuscript focuses on model robustness under distribution shift, specifically domain shifts and subpopulation shifts. Domain shift is where the test domain and train domain are disjoint. Subpopulation shift is where test distribution has different mixture proportion than train distribution. The assumption is that domain identification spuriously correlates with labels. The proposed framework learns an invariant representation by using mixup strategies and interpolates samples either with the same labels but different domains or with the same domain but different labels to. Experiments are performed on a variety of domain shift and subpopulation shift benchmarks, and results showed that the proposed framework is better than empirical risk minimization (ERM) and alternative data augmentation methods. Theoretical analysis is also provided and it is shown that, under certain conditions, the proposed framework has asymptotically smaller worst case classification errors than ERM and vanilla mixup.

Reviewers agreed on several positive aspects of the manuscript, including:
1. The manuscripts addresses a critical point that prevent models from generalization, namely spurious correlation; 
2. The proposed method is simple and easy to implement, and the empirical results are within expectation.

Reviewers also highlighted several major concerns, including:
1. Different recent approaches introduce methods that use some sort of mixup across domains in similar settings;
2. Ablation study on datasets without spurious correlations are missing;
3. Evaluation of domain invariance representations and prediction level invariance needs clarifications;

Authors clarified different motivations of the two selection strategies in relation to spurious correlation between domains and labels, and provided an ablation study on datasets with no spurious correlation. Post rebuttal, reviewers stayed with borderline ratings, and they have suggested further improvements: improving results analysis and the conclusion that “existing domain information may not fully reflect the spurious correlation”, understanding the implication and the reasons that invariance is achieved at the prediction level instead of at the representation level despite the original goal is to learn an invariant representation, and improving presentation of the manuscript including settings and assumptions.
The authors consider the problem of training a fair classifier on decentralized data, and compare three methods: training locally, training the proposed FedAvg algorithm with local fairness, and a global fairness approach.

The reviewers agreed that the setting was interesting and novel, but had concerns about the writing quality, experimental setup, and, most importantly, the organization of the paper, with several reviewers complaining that necessary information was relegated to the appendix.

Overall, this work is not quite ready for publication. With that said, the reviewers agreed that it was interesting and highly promising (it just needs refinement). Please seriously consider the reviewers  recommendations, which on the whole were very constructive and, if followed, should lead to a significant improvement in your manuscript.
This paper presents a method for unsupervised domain adaptation, focusing on the object detection problem. Under this framework, the paper proposes modules of domain adaptive instance normalization, global style alignment and local content alignment. The proposed method is evaluated on multiple datasets.

Several reviewers have pointed out that the paper lacks discussion and comparison to related methods. The paper has some merits but, the lack of a proper presentation, and the fact that there are not enough experimental results to support all claims in the paper, result in a submission that does not meet the bar of ICLR publication. Hence, the current paper is recommended to be not published at ICLR.
This paper was borderline, based on the reviews. The paper points out an interesting connection (somewhat known but not in this specific version) and good experimental results. However, numerous reviewers raised concerns that the paper was lacking a comparison to prior work connecting unsupervised learning and meta learning, most notably, Hsu et al. (2019).

After reading the revised version of the paper, the authors address this issue and also all the other reviewer comments. In relation to prior work they clarify that they focus on the contrastive unsupervised case and also do a good job in answering other reviewer concerns relative to novelty and results. 

I would also like to point out, as reviewers also did that the previous title was a bit aggressive and provocative. Gladly the authors agree to change it to a more scientific `The Close Relationship Between Contrastive Learning and Meta Learning”. 

Overall I think the authors have done a good effort on addressing the reviewer concerns and I think the paper would be interesting for ICLR readers.
This paper proposes to perform unsupervised grammar induction over image text pairs and used shared structure between the modalities to improve grammar induction on both sides. Authors find the paper clear, creative and interesting and recommend acceptance without hesitation.
This paper received a majority vote for acceptance from reviewers and me. I have read all the materials of this paper including manuscript, appendix, comments and response. Based on collected information from all reviewers and my personal judgement, I can make the recommendation on this paper, *acceptance*. Here are the comments that I summarized, which include my opinion and evidence.

**Research Motivation and Problem**

This paper is well motivated by the agnostic of CPE assumption that the support of the positive data distribution cannot be contained in the support of the negative data distribution. To tackle this problem, the authors built an auxiliary probability distribution such that the support of the positive data distribution is never contained in the support of the negative data distribution.

**Technical Contribution**

The technical part is simple and clear. The regrouping idea is also easy to implement. The theoretical justification is a good complement of the proposed ReCPE algorithm.

**ReCPE does not affect its base if the assumption already holds**

The authors employed the synthetic datasets to verify this point. This is a plus.

**Experimental Results**

The authors demonstrated their ReCPE algorithm can be used as a booster on seven base PU classifiers.

**Presentation**

The presentation has been much improved with the guidance of one reviewer. But I found two extra minor ones. (1) "PU Learning"  > "PU learning" at the beginning of the second paragraph on Page 1. (2) Two typos in "9 real word datasets." on Page 7  > "9 real world dataset.", where the footnote should be placed after the period.

**Layout**

(1) Too many lines in Table 1. It is suggested to remove the horizontal lines among the same dataset, (2) Appendix should go after the main manuscript, rather than a separate file.

No objection from reviewers was raised to again this recommendation.
This paper presents promising and ambitious work in the context of Byzantine tolerant learning in the decentralized setup. The reviews raised several critical points of concern: completeness of technical derivations and details, experimental results and comparisons to other work, excluding several state of the art attacks. The reviewers provided with a generous amount of feedback, that should be incorporated in the paper before publication. Unfortunately the camera ready timeline is quite sort for such an extensive feedback to be integrated in this paper.  The authors are urged to resubmit after taking into account all the suggestions that were made during this review cycle.
This paper proposes measures of consistency between back doored and clean models, proposes regularization using those consistency measures, and showcases that such trained models indeed exhibit better consistency. Also, it is demonstrated that the fine tuned model does not deviate too far from the original clean model. The reviewers  comments are all well addressed. Some concerns related to the notion of consistency and how it relates to the detection of backdoors are still left open, but the reviewers seem to be satisfied with the answers. Given the overwhelmingly positive reviews, I propose accept.
This is an interesting work, and I urge the authors to keep pushing this direction of research. Unfortunately, I feel like the manuscript, in its current format is not ready for acceptance.

The research direction is definitely under explored, which makes the evaluation of the work a bit tricky. Still I think that some of the points raised by the reviewers hold, for e.g. the need of additional baselines (to provide a bit of context for what is going on)I understand that the authors view their work as an improvement of the previously proposed DT network, however that is a recent architecture, not sufficiently established not to require additional baseline for comparisons. This combined with the novely of the dataset makes it really hard to judge the work. 

The write up might also require a bit of attention. In particular it seems a lot of important details of the work (or clarifications regarding the method) ended up in the appendix. A lot of the smaller things reviewer pointed out the authors rightfully so acknowledged in the rebuttal and propose to fix, however I feel this might end up requiring a bit of re organization of the manuscript rather that adding things at the end of the appendix. I also highlight (and agree) with the word "thinking" being overloaded in this scenario.

Ablation studies (some done as part of the rebuttal) might be also a key component to get this work over the finish line. E.g. the discussion around the progressive loss. I acknowledge that the authors did run some of those experiments, though I feel a more in depth look at the results and interpretation of them (e.g. not looking just at final performance, but at the behaviour of the system), and integrating them in the main manuscript could also provide considerable additional insight in the proposed architecture. 

My main worry is that in its current format, the paper might not end up having the impact it deserves and any of the changes above will greatly improve the quality and the attention the work will get in the community.
This paper modifies the conditional diffusion model guided by a classifier, as introduced by Dhariwal & Nichol 2021, by replacing the explicit classifier with an implicit classifier. This implicit classifier is derived under Bayes  rule and combined with the conditional diffusion model. This combination can be realized by mixing the score estimates of a conditional diffusion model and an unconditional diffusion model. A trade off between sample quality and diversity, in terms of the IS and FID scores, can be achieved by adjusting the mixing weight. The paper is clearly written and easy to follow. However, the reviewers do not consider the modification to be that significant in practice, as it still requires label guidance and also increases the computational complexity. From the AC s perspective, the practical significance could be enhanced if the authors can generalize their technique beyond assisting conditional diffusion models.
The authors present a new framework to make deep ensembles provide better coverage of the posterior and be less reliant on initialisation. The authors generally did a good job presenting their approach, avoiding dubious claims that deep ensembles are non Bayesian, and instead focusing on ways in which deep ensembles can be improved, practically and theoretically. It is worth noting in a revised version, however, that many approximate inference procedures do not have theoretical guarantees. The claim that deep ensembles have "arbitrary bad approximation guarantees" is vague and appears to single them out in a way that could confuse the reader. Regarding priors, it is also worth noting that Wilson & Izmailov (2020) provide evidence that the prior in weight space induces a prior in function space with useful properties, although the prior can be improved.

The authors do a good job of responding to reviewers, and describing limitations. Ultimately, however, the general opinion was not swayed to accept. In addition to reviewer concerns, the experimental evaluation could be substantially improved. There are several procedures that build on deep ensembles to capture uncertainty within modes. How does this procedure compare? Why are no likelihood evaluations considered? What about accuracy? In its present form, it s unclear what practical value the contributions are providing, besides possibly better OOD detection, but even that direction is explored in a relatively limited way. It could also be interesting to measure the distance of the predictive distribution to a good proxy for the Bayesian model average. Overall, there are the raw ingredients of a good paper here, and the authors are encouraged to continue with this work.
The paper introduces drop out probabilities which are adaptive to the similarity of model parameters between clients.
The reviewers liked the idea, however missed several aspects, such as a convergence analysis or at least discussion, as well as an analysis of additional cost of the adaptive step, and finally several concerns on the strength of the experimental setup and benchmarks.

Unfortunately consensus among the reviewers is that it remains below the bar even after the discussion phase.

We hope the detailed feedback helps to strengthen the paper for a future occasion.
The authors propose the resource constrained offline RL problem where the offline dataset contains extra features that are not available online. The goal is to use these extra features to improve performance during deployment. They propose a simple modification to TD3 BC in the continuous control setting and a simple modification to CQL in the discrete setting. They evaluate their proposed approaches on D4RL, RC D4RL (a novel dataset that they introduce for resource constrained offline RL), Atari, and a proprietary real life Ads problem.

Initial reviews identified the following concerns:
* While the exact problem is novel, the idea of having access to privileged features at training time that are not available at deployment has been explored in supervised learning and online RL. The reviewers were not clear how considering the offline RL setting interacts specifically with the privileged features to produce an interesting setting.
* The baseline simply trains on the limited feature set. Unsurprisingly, using the extra features can improve performance. In light of the previous point, reviewers asked for more substantial baselines, suggesting BC on the teacher and predicting the missing features as some possibilities.
* The set of tasks was too limited.

The authors provided a substantial response:
* Experiments on Ads data 
* Experiments on Atari with CQL as the base algorithm
* Additional baselines on RC D4RL HalfCheetah v2 datasets (BC on teacher and predictive)
* Additional analysis

I commend the authors on the hard work they did preparing this response. It is quite substantial and does improve the paper significantly. However, reviewers and I still have a number of concerns:
* The additional baselines are appreciated, however, the results are mixed. The additional baselines are a step in the right direction, but they need to be evaluated beyond a single dataset. It is hard to evaluate the results without reasonable baselines. I agree and think that even though the specific problem is novel, the idea of transfer learning is not, so it is reasonable to require that we have more extensive baselines. Furthermore, while the authors argue that their method has an edge on the more practical dataset, that is based on a very limited evaluation. Probing this further is important.
* The CQL modification is quite different than the TD3+BC modification. The performance of the modification for CQL is not significantly better than CQL. What should we make of this?
* For the Ads dataset, all hyperparameter settings except Transfer(0, 1) show the same performance. This seems surprising as even Transfer(0.1, 0.9) shows no difference. Finally, Transfer(0, 1) beating Transfer(1, 0) 7/10 times is not statistically significant.

At this time, the paper is not ready for publication, but the paper is moving in the right direction and I encourage the authors to submit a revised version to a future venue.
The paper considers the problem of learning both the physical design (morphology and parameters) of a robot together with the corresponding control policy to optimize performance at a target task. Unlike several contemporary methods that formulate this as two separate, but coupled, optimization problems, the paper unifies these decisions into a single decision making framework. More specifically, a conditional policy learns to first change an agent s physical design (i.e., the morphology/skeletal structure and its associated parameters), and then to control the design. The policy is formulated as a graph neural network, enabling a single policy to simultaneously control robots with different morphologies (and, in turn, different action spaces). Experimental results demonstrate that the approach outperforms recent baselines on a variety of simulated control tasks.

The paper considers an interesting and challenging problem, that of jointly optimizing an agent s physical design and its control policy, an area of research that has received renewed attention of late. As the reviewers note, the idea of treating design and control in the context of a single decision making process is novel. The approach is principled and the experimental results largely justify the significance of the contributions. The reviewers agree that the approach is described clearly and that the paper is well written. The reviewers initially raised a few concerns regarding the experimental evaluation, including the desire for more in depth evaluations and the need for more random seeds. They also questioned some of the claims made in the initial submission. The authors provided a detailed response to each of these points and made changes to the paper to resolve most of the concerns.

In summary, the paper proposes a novel approach to an interesting problem with convincing results.
The paper tackles the problem of detecting anomalies in multiple time series. All the reviewers agreed that the methodology is novel, sound and very interesting. Initially, there were some concerns regarding the experimental evaluation, however, the rebuttal and subsequent discussion cleared up these concerns to some extent and all reviewers are eventually supporting or strongly supporting acceptance.
This paper studies the pruning problem of graph neural networks, i.e. finding lottery tickets for GNN. In particular, it generalizes UGS by Chen et al. (2021) from transductive setting to inductive setting where prediction on unseen graphs is possible. The main idea is: 1) learn a mask network to assign importance scores for edges using the embedding features of the nodes connected, that avoids the double parameter memory costs in UGS; 2) prune the edges according to the importance score and weights of GCN according to their magnitudes. Main concerns from reviewers are about the novelty, evaluation, and scalability. Despite that generalization to unseen graphs using the mask functions on embedding features is a new aspect, the evaluation is compared with relatively weak baselines and inference time scalability of is still an issue.
All reviewers found that the proposed LM with Brownian motion is interesting and novel. Several reviewers raised (minor) concerns about experiments, but have been generally resolved by the authors.
Thanks for your submission to ICLR.

This paper considers binary hashing schemes, and makes two related contributions.  First, it analyzes a simple extension to SQ RFF; second, it introduces and analyzes a novel metric for ranking called ranking efficiency.  Some experiments are also performed on standard data sets.

This is very much a borderline paper, and could go either way.  I took a close look at the paper to offer my opinions in addition to the reviewers.  The paper itself is well written and seems to be correct.  I do like the simplicity of the proposed SignRFF method as well as the ranking efficiency measure.  However, the contributions are somewhat limited, and it s in an area that hasn t seen much work in the last several years (this paper mainly builds off of methods from 10+ years ago).  Further, it doesn t seem that methods such as KLSH and SignRFF are used much in practice, so I don t know if this will have substantial impact.  So while it s a reasonably interesting paper with some nice insights, I think it falls just below the acceptance threshold for me.
This paper proposes a new multi agent RL algorithm, based on the PPO algorithm, that uses a mean field approximation, which results in a a permutation  invariant actor critic neural architecture. The paper includes a detailed theoretical analysis that shows that the algorithm finds a globally optimal policy at a sub linear rate of convergence, and that its sample complexity is independent of the number of agents. The paper include some experiments that validate the proposed algorithm.

The reviews of this paper are mixed. Most of the reviewers appreciate the theoretical analysis, but one reviewer does not find the theoretical justification of the mean field approximation clear. The reviewer also points out to the absence of comparisons to relevant competing algorithms. These concerns are addressed by the authors in their rebuttal. A key issue with this work is the weakness of the empirical evaluation. The proposed method is tested on only two simple tasks, and the results on the second task do not show a considerable advantage of the proposed algorithm. This paper can be strengthened by adding experiments that clearly indicate the advantage of the proposed technique.
This paper studies the offline multi agent RL problem. The finding is that the dataset collected by one agent could be very different for other agents. The authors provide two solutions to this problem. Although being interesting, the reviewers found that the there are many imprecise math statements, and some of the methods are not well motivated. Hence, the overall recommendation is a reject.
This manuscript describes a method that turns sentences into reward functions by recognizing objects, parsing sentences into a simple formalism, and then grounding the parse in the recognized objects to form a reward for an agent.

1. The title and much of the manuscript are written in a way that reviewers found confusing. It would seem from the title and most of the text that the method integrates language models, CLIP specifically, into RL in a novel way to provide zero shot rewards. But this is not the case. CLIP is used purely as an object detector. Yes, the method requires a good object detector and CLIP provides that, but any good object detector that can handle arbitrary phrases would have done.

2. The overall setup of the work: extract the state of the world and then parse sentences to formulate rewards by grounding parts of the parse into parts of the world state has been explored widely in robotics. Reviewers provided citations going back several years, but many others exist.

I would encourage the authors to rewrite the manuscript around their central contributions and downgrade their use of CLIP and language models in general to a minor technical footnote. Similarly refocusing related work on the robotics literature and demonstrating how this approach differs and improves on the state of the art there could result in a strong contribution.
This paper presents a graph neural network (GNN) architecture that adopts locally permutation equivariant constructs, which has better scalability compared to globally permutation equivariant GNNs, and the paper claims this change also does not lose expressivity of the network.  All reviewers unanimously recommended rejection, and the main issues are the clarity and writing, to the point where it becomes hard for a reader to follow the precise implementation of the proposed approach and how that compares to prior work.  Therefore in its current form this paper is not yet ready for publication at ICLR.  When the authors work toward the next revision I’d suggest clarifying a little more about the precise algorithmic implementation of the proposed ideas, with a bit of additional intuition from a higher level, rather than staying at the current level of technicality.
In this paper, the authors consider linear quadratic network games (also known as graphical games) and they discuss a number of conditions and procedures to learn the underlying graph of the game from observations of best response trajectories (or possibly infinite sets thereof) in the game.

The reviewers  initial assessment was overall negative, with two reviewers recommending rejection and one giving a borderline positive recommendation. The authors  rebuttal did not address the concerns of the reviewers recommending rejection, and the authors did not provide a revised paper for the reviewers to see how the authors would implement the suggested changes, so the overall negative assessment remained.

After my own reading of the paper, I concur with the majority view that the paper has several weaknesses that do not make it a good fit for ICLR (especially regarding the lack of precision in the theorems and the statement of the relevant assumptions), so I am recommending rejection.
Overall, this paper receives positive reviews. The reviewers find the technical novelty and contributions are significant enough for acceptance at this conference. The authors  rebuttal helps address some issues. The area chair agrees with the reviewers and recommend it be accepted at this conference.
The authors develop an approach to improve upon methods for training certifiably robust models. They propose an input dependent margin based weighting and an automatically generated curriculum schedule and demonstrate improvements on training certifiably robust models on MNIST and CIFAR 10.

Reviewers agree that the paper makes interesting and novel contributions. However, the lack of novelty in the approach combined with the limited empirical gains make it difficult to justify acceptance. In particular, reviewers raise valid concerns on the quality of experiments comparing to prior work (in particular Crown IBP (Zhang et al 2020) and COLT (Balunovic & Vechev 2020)) (in particular hyperparameter tuning, inability to recreate baseline results and unjustified claims that the prior art cannot run on GPU hardware). Further, even the gains demonstrated are marginal. 

Hence, I recommend rejection, but encourage the authors to revise the paper based on the feedback received.
The paper proposes to learn disentangled trends and seasonal representations of time series for forecasting tasks. It shows separating the representation learning and downstream forecasting task to be a more promising paradigm than the standard end to end supervised training approach for time series forecasting. 

During the post rebuttal phase, there were interactions from all the reviewers, and reviewer KrXv raised the score. The reviewers think the contrastive learning method is novel and the added experiments have strengthened the paper.  The authors are encouraged to include more standard datasets (M5) in the final version.

Based on the above reasons, I am recommending accepting this paper.
In general, the reviewers recognized the importance of the question and the innovation in the proposed algorithm, but do not seem to be super excited about the overall contribution of the paper. (One or two reviewers did not seem to respond authors  response after the AC s reminder.) The AC read the reviews and responses and observed that the main concern appears to be the empirical performance   the improvements are not as strong for larger models or if more computational time is allowed. Modern models are indeed typically large, and it would be good to discuss this point more thoroughly. If the work s focus is limited resource setting, the paper might want to state that upfront. Indeed, one reviewer is still concerned post rebuttal about a clock time comparison. Given these considerations, the AC will recommend reject for the paper but encourage the authors to resubmit to a top venue conference after revising the paper.
This paper presents several variants and extensions (including stochastic and proximal) of the error feedback method EF21 and provides convergence rates for each of them and shows that they improve upon previous state of the arts. Despite the much broadened application scenarios and SOTA  in convergence rates/complexity, the main and common concern from the reviewers is the novelty of the paper beyond the original EF21 work. There are also concerns on the empirical evaluations that do not fully support the theoretical promises. I agree with the reviewers and regrettably have to recommend rejection for ICLR.
The paper aims at developing mechanisms for adversarial attack and defense towards combinatorial optimization solvers, where the solver is treated as a black box function and the original problem’s underlying graph structure is attacked under a given budget. While the reviewers found the problem novel and interesting, they are not convinced by the problem formulation and the proposed solutions, as well as the experimental setup. Some of the points that the reviewers brought up during the discussion include: (i) the attack to the TSP does not follow the main paper s attack principle of adding and deleting edges, (ii), in general, it has not been explained why all these modification are really "relaxations", (iii) the notations are very confusing, and (iv) while authors  response on loosening the constraints makes sense, but the experiments (i.e., the TSP problem setting) in this work are not consistent with such clarification. Addressing the above points will significantly improve the manuscript.
In this paper, the authors present an investigation of the impact of demographics on the peer review outcomes of ICLR. This is an important topic, as the demographics of ICLR and similar conferences are seriously skewed and may cause some people to feel excluded. The authors look into this complex problem with extensive manual annotations and analyses. 

The main weakness of this paper is that it is observational, and while the results are interesting, it is difficult to take away a clear and convincing message for the future. Part of the reason is that the whole problem is quite complex, and the hypotheses that are presented and tested in this paper reveal relatively shallow findings. Compared to the NeurIPS experiments which are carefully designed, these are not causal (see one of the reviewers  comments), so it is difficult to draw conclusions beyond correlations.

In summary, the results are interesting, and despite some of the reviewers  concerns, I would not exclude this paper because of the topic being irrelevant to the cfp, but I think the paper needs a more clear and convincing message.
The authors rely on the recent convex formulation of neural network training to establish an interesting correspondence between whitening of input data and batch normalization (after and before relu). While convex formulation is not scalable to realistic architectures, it is a great insight that would add value to our understanding of the batch normalization tool.
All reviewers agreed that analysis of PPO is interesting. 
During the discussion, however, there was an agreement that the current work is too thin in novelty and contribution: it provides only convergence analysis under very strong assumptions, and heavily builds on techniques from prior works. Meanwhile, for conventional policy gradient, recent works provided convergence rates.
As one reviewer pointed out   this work does not further our theoretical understanding on why PPO is better than vanilla policy gradient, as all the established results hold for policy gradient, even with less assumptions.
I encourage the authors to strengthen their paper by relaxing Assumption 4 (perhaps based on the robust classification idea raised in the discussion), and by further providing rate results.
This paper makes an important evaluation study to further the understanding of the representation capacity of DNNs. The novelty is in using the multi order interaction proposed in Zhang et al. (2020) to understand the complexity of interactions in DNNs. The authors discovered an interesting representation bottleneck phenomenon, i.e., in a normally trained DNN, low order and high order interaction patterns are easy to be learned, while middle order interaction patterns are difficult to be learned. They also propose two novel loss functions, which allow the model to encode interactions of specific orders, including middle order interaction. All reviews are positive.
This paper proposes a new variant of a stochastic gradient Langevin dynamics sampler that relies on two key ideas: approximation of the target density with a simpler function (as in [Deng, 2020]) and the parallel simulation of many chains. The authors also prove that their approach can be theoretically more efficient than a single chain algorithm.

The reviewers see the contribution as significant although they did raise some concerns regarding the clarity of the paper. Since these concerns do not appear to be major, I recommend acceptance but I advise the authors to address the comments of the reviewers to maximize the impact of the paper.
This paper presents a method for conditional generations for GANs.
The reviewers note the lack of novelty, or the lack of a theoretical or empirical motivation for the novel bits. They point out flaws in the correctness of the paper, and limited experimental evaluation. 
The reviewers agree to reject the paper. Unfortunately the authors did not answer the reviewers. I therefore recommend to reject the paper for this conference, and I strongly suggest that the authors address the reviewers concerns if they are to submit this paper again in a future venue.
This paper addresses an important issue of AutoML systems, specifically their ability to "cold start" on a new problem. Some of the reviewers initially had concerns about the experimental validation and the theoretical foundations of the method, but during the discussion phase the authors addressed concerns extremely well. The authors already included most of the feedback of reviewers, further strengthening the paper.
This paper shows the possibility to design a relatively shallow architecture, ParNet, based on parallel subnetworks, instead of traditionally deeply stacked blocks. During discussions, the reviewers pointed out two important concerns: (1) the current design heavily hinges on the recently proposed RepVGG block, whose comparison was even missed in the original submission (later added in rebuttal); (2) comparing ParNet with RepVGG, there seems no performance advantage. Although RepVGG is 2.5 times deeper than ParNet, it is still faster due to highly optimized layers.

The authors mainly argued that their contribution is to answer the scientific question “is it possible to build high performing non deep neural networks?” While this is indeed an interesting question, AC feels: (1) it is perhaps unfair for this paper to claim as the first work proving the feasibility. WideResNet provided similar insight much earlier, among others; (2) the presented results, with tools being not novel, are pre mature as they display no real appeal of using ParNet, in any aspect. Probing a new question is of course valuable, but presenting an immature and novelty lacking answer shouldn t automatically grant publication. 

In sum, the reviewers were unanimously UN convinced by this paper s value, nor was the AC. The authors are suggested to very seriously take into account reviewers  suggestions to make improvements, before submitting their work to the next venue.
This paper proposes to improve offline RL by a data augmentation technique that exploits the symmetry of the dynamics using Koopman operator. The idea is interesting but the draft at its current form has several weaknesses as pointed out by the reviewers. The scores are borderlines at this point. I read the paper and find myself agree with reviewer ohJ3 in both the lack of  clarity and the gap in theory and empirical results. The math presentation still  a careful check and improvement. Eq(1) (4) are already fairly confusing (should $Q_i$ and $\pi_i$ be replaced by $Q$ and $\pi$ in Eq(1) (4), and $\hat Q$ by $\hat Q_i$ in Eq (2) (3)?). I would like to suggest the authors to add a self contained algorithm box for the practical algorithm procedure. Do the readers really need to understand the full Koopman theory (section 3.1) before understanding the algorithm? The authors could think about if it is better to present the practical algorithm first with minimum math, and then analyze the property of the algorithm using the math tools (and in this case, make it clear what theoretical guarantees we get exactly). I think making the paper more accessible can help the paper gain more popularity in ML readers.
Four reviewers have evaluated this submission with one score 6 and three scores 8. Overall, reviewers like the work and note that *a rigorous and principled approach is taken by this work*. AC agrees and advocates an accept.
This paper proposes the framework CAGE (causal probing of deep generative models) for estimating counterfactuals and unit level causal effects in deep generative models. CAGE employs geometrical manipulations within the latent space of a generative model to estimate the counterfactual quantities. The estimator is written in potential outcome language and assumes unconfoundedness, positivity, stable unit treatment value assumption (SUTVA), and linear separability in semantic attributes of the latent space. Furthermore, the framework considers only the case of binary treatments.  

One major concern raised by reviewers TgM5 and xP5d is that the method is based on a trained generative model, which may not be the true data generating model. In this case, the paper appears to address statistical dependencies instead of the actual causal relationships in the real world. The authors claim to empirically show that their framework can probe unit level (individual) causal effects. However, the reviewers are concerned that no theoretical support for the correctness of the method is provided. In other words, the problem is assumed away once a probabilistic model is assumed to be equal to the true generative model, which is almost never the case in practice and is well known in the field. We want to encourage the authors to provide a more detailed theoretical justification, perhaps with proofs and/or references, that the proposed method can infer causal and counterfactual relationships given the underlying assumptions. 

After all, reviewers were interested but somewhat skeptical about the method s ability to learn causal and counterfactual relationships. Unfortunately, the paper is not ready for publication yet. Still, we would like to encourage the authors to take the reviews seriously and try to improve the manuscript accordingly.
The paper is interesting, and its focus is timely and important, given the continuing rapid rise of transformers (and their dependence of tokenization of images). All three reviewers recommend acceptance, to varying degree. The paper will be a valuable contribution to the program at ICLR.
The paper presents several interesting generalization results for Uniform LGI loss functions (a generalization of PL functions). Some of these bounds seem useful, but the overall connection with the optimization length remains unclear. This concern and other points of criticism remain present after the rebuttal phase. Other minor concerns seem fixable, but in a larger timeline compared to the camera ready one. The paper should be revised for a future venue.
Although the reviewers acknowledge that the paper is well written and easy to follow, they found that the contributions of the paper are not enough to be accepted at ICLR. Some concerns from the reviewers are as follows: 

1. Assumption 3 is very strong and uncommon. It is not easy to verified even for over parameterized setting. 
2. Both the theoretical and experimental results are not sufficient. No improvement in theoretical results compared to the previous work. Moreover, the performance of the method is no better than the baselines, which are themselves much weaker than state of the art results.
3. Motivation for small batch training, advantages over K FAC, the practicality of SLIM QN, and novelty compared to L BFGS are questionable.
4. The method is essentially LBFGS with momentum and damping of the hessian, hence its novelty is questionable. 
5. The authors emphasize that "we are trying to design a practical QN method with light compute/memory cost, especially when applied to large scale NNs". Any method that has 20 40 times as much memory requirement as SGD cannot be said to have light memory cost. 

Based on the above concerns, the paper is not ready for the publication at this moment. The authors should consider to improve the paper by addressing the reviewers  comments and implementing their suggestions and resubmit this paper in the future venues.
This paper trains an expert style DNN that routes input examples to appropriate expert modules resulting in high accuracy on ImageNet with less compute. Reviewers have been positive about the strong empirical results. However the paper itself is not written well and reviewers had hard time figuring out actual architecture and training methodology. For example reviewers couldn t easily figure out the differences between LGM, WGM and SRM. 

The paper itself is sparse on why some of the choices have been made, their relation to existing methods and how do they affect the final performance. For example   In eq2, TCP objective has been normalized for each expert separately with a vague No Superiority Assumption. What motivates this assumption? Why is it reasonable? Eq 4 is quite similar to the load balancing loss in Switch Transformer paper. However there has been no discussion about the similarities and differences.

I think the paper needs to rewritten with clear explanation of the actual architecture, in what aspects it is similar/differs to existing expert models. What key components are the reason for the superior performance?

While I appreciate the authors for the ablations studies they presented during response phase, I think the paper requires major rewriting and cannot recommend acceptance at this stage.
This paper proposes environment fields, a representation that models reaching distances within a scene. Dense environment fields are learnt using a neural network, and the effectiveness of this representation is shown on 2D maze environments and 3D indoor environments. This paper received hugely contrasting reviews, with two reviewers being very supportive and one reviewer providing the lowest score of 1. In light of this, I ll start with providing my takeaways on the review and discussion with reviewer z3Y4 (rating of 1) and then proceed to the remaining discussion.

Reviewer z3Y4 has provided the score of 1 and has made strong remarks that include: "what is proposed in this paper is simply not comprehensible", "description of the method itself is simply devoid of all required detail", "The main claims of the paper are incorrect or not at all supported by theory or empirical results." and " what is being proposed in this paper is simply too unclear and vague to be assessed". **Such dismissive remarks, in my opinion, are completely unnecessary and create a toxic discussion and review environment.**

Reviewer z3Y4 has many criticisms of the submission, but the primary ones include: (a) the lack of details throughout the paper (b) the positioning of the paper in the abstract and introduction, and (c) the lack of experiments in continuous environments. Re (a): It is well understood in our research community that providing every last detail in the main submission is nearly impossible due to the restriction on the number of pages. Providing excess details in the main paper also often reduces the readability of the paper. Such details are better addressed in the appendix and crucially, the code. The authors have provided some details in the appendix and have indicated that they will release a code base.  I also agree with the authors that justifying every last detail in the network architecture such as choice of an activation function is not necessary for this submission. The same goes with describing methods in past works in detail vs referring the reader to the appropriate citation. As a result, I believe that the authors have addressed (a) well. Re (b): This has also been addressed by the authors, by pointing out relevant parts of the paper that had the necessary details. Re (c): In this regard, the paper clearly contains a well laid out experiment in 3D indoor scenes, so as far as I am concerned, this has been addressed in the main submission.

Reviewers AhgQ and fAEP have supported this submission but also laid out some concerns that include:
(1) Are the gradients suboptimal ?
(2) Positioning the paper with regards to past works
(3) Motivation behind using the VAE
(4) Qualitative analysis and failures
The authors have addressed these 4 concerns well using the rebuttal as well as via a revision of the appendix. The reviewers, post discussion have indicated their satisfaction with the revised submission.

I think this paper is interesting and proposes a novel scene representation which can be useful for others in the Embodied AI community. I am in agreement with reviewers AhgQ and fAEP, and in spite of the strong reject score by z3Y4, I recommend accepting this paper.
This work considers the problem of how to predict on sensitive user points while preserving their privacy. It proposes a fairly straightforward way to create a local randomizer that optimizes loss for a given model subject to preserving LDP. The work also gives theoretical analysis of the randomizer for least squares linear regression. 
The problem formulation is different from the standard LDP framework where privacy of training data points needs to be preserved. The submission does not motivate this setting and I don t see a good motivation for this problem either. More importantly, it does not sufficiently emphasize that the problem is entirely different from prior work. Indeed all reviewers were confused about various aspects of comparison with previous work. Therefore, in my opinion, the submission is not sufficiently well motivated and clearly presented to be accepted.
The paper presented an empirical study of pre trained models on the Out of distribution Generalization problem. 
Authors evaluated various factors (such as model sizes, datasets, learning rate, etc) and claim some major findings:  1) larger models have better OOD generalization, and combining both larger models and larger datasets is critical; 2) smaller learning rate during fine tuning is critical; 3) strategies improving in distribution accuracy may hurt OOD. Overall, this paper is a well written empirical study with some useful insights, but the new findings from the empirical studies are generally not surprising and the overall contribution is not significant enough for acceptance.
This paper proposes a method to solve the inverse problem of identifying parameters of a dynamic physical system from image observations. The main idea is to train a rendering invariant state prediction (RISP), which estimates the inverse mapping from the pixel to the state domain. The authors introduce a new loss to this end, and an efficient gradient computation of the loss.

The paper received three clear accept recommendations. The reviewers discussed the potential improvement of RISP when combined to disentanglement methods, and also raise several concerns regarding experiments, e.g. rendering conditions during training and testing, or evaluation on real data. The rebuttal did a good job in answering reviewers  concerns, and the reviewers especially appreciated the new results on real videos. Eventually, all reviewers recommended a clear acceptance of the paper.

The AC s own readings confirmed the reviewers  recommendations. The paper is introduces very solid contributions for solving the complex task of physical parameter identification in the unobservable setting. The paper is also clear and well written, and validated with convincing experimental results. Therefore, the AC recommends acceptance.
This paper presents a new reinforcement learning algorithm for POMDPs that specifically deals with the credit assignment problem. The proposed algorithm consists in using at each time step t of a training trajectory the subsequent future trajectory that starts at time t+1 as additional inputs to the policy and value networks. Instead of using the trajectories directly, two RNNs are used to encode the trajectories into latent two variables that are then given as inputs to the policy and value networks. A key novel contribution of this work is the use of "Z forcing" to help the RNNs learn the relevant information. Since future trajectories are not available during testing, a "prior" network is trained to predict the latent variable given a state. During testing, the latent variable is sampled from the network. Empirical experiments on simple simulated environments show that the proposed algorithm outperforms several baselines.

Key issues raised by the reviewers include the complexity of the proposed algorithm, the fact that several interesting results are in the appendix rather than the main paper, and the weakness of certain baselines. The authors responses helped clarify these issues, and additional experiments (such as a comparison to a DQN with n step value updates) were performed and added to the paper. The reviews are updated accordingly.

In summary, the paper contains several novel ideas in the context of learning in partially observable environments. It is not entirely clear similar effects of the proposed algorithm can be obtained by using simpler tricks, but the evidence provided by the authors supports the claim that the algorithm outperforms several SOTA techniques in the context of POMDPS.
This paper concerns ensemble methods in deep reinforcement learning, examining several such methods, and proposes to address an important issue wherein ensemble members converge on a representation of approximately the same function, either by their parameters converging to an identical point or equivalent points that give rise to the same function. The authors propose a set of regularization methods aimed at improving diversity, and benchmark these augmentations on five ensemble methods and a dozen environments.

3 of 4 reviewers generally praised the method s simplicity and generality, and found the experiments convincing. Reviewer a9sA describes it as "clearly written and easy to follow", although others found clarity lacking in parts. There was agreement among these 3 reviewers that this was an interesting problem to tackle. Reviewer TfGq notes that this method lacks theoretical justification or guarantees, but that as a largely empirical paper this is perhaps of secondary importance. Reviewers 6miY and a9sA had questions about the precise choice of metrics, hyperparameters and seeds; the resulting discussion cleared up many of these concerns.

The most critical reviewer, i4M1, disputes the existence of the phenomenon at all, saying that "Neural networks converge to different solutions given the initialization is different and multiple local minima." The remainder of i4M1 s criticisms seem centered on the choice of environments and the number of seeds (also raised by other reviewers). The issue of seeds has been addressed partially and the authors have committed to strengthening their results in this regard.

Reviewer i4M1 s statement on the convergence of neural networks to different minima matches a bit of dated folk wisdom about neural networks, but the AC disputes this. The authors have cited a study from before the DL era properly began that identifies this issue and Section 5 addresses these criticisms directly. In practice, modern neural networks, especially with non saturating activations, tend to be surprisingly consistent across random seeds when trained against the same data stream, and more recent work posits that the loss landscape is less riddled with local minima than with saddle points (see e.g. Dauphin et al, 2014). _Equivalent_ minima are of course common due to scaling and permutation symmetries but SGD has a well documented preference for low norm solutions in the former case, and the authors  have chosen methods that would at least conceivably overcome these issues, by focusing on summary statistics of the representations rather than their precise values (and indeed, CKA is designed with these concerns in mind).

Despite i4M1 s incredulity I am inclined to agree with the majority of reviewers and view the paper as a worthwhile contribution to the body of knowledge (purely empirical though it may be) on both NN ensemble methods and DRL ensembles in particular. The introduction of measures from economics is clever and original, and the results are promising. A more exhaustive study on the entire Atari57 benchmark but can appreciate the resource problem this poses, and find that the suite of considered environments, combined with the augmentation of 5 different DRL ensemble methods, strikes a good balance. I concur on the issue of seeds and would encourage authors to include as many as possible for the camera ready, but on balance would recommend acceptance.
The paper uses graph kernels to perform local convolutions and achieve better expressiveness than classical GNNs. The paper received three borderline reviews. The area chair found the feedback to be consistent and constructive and agrees with most statements made by the reviewers. Overall, the idea has some interest (even though there are other works who also propose hybrid approaches between graph kernels and GNNs, as noted in the paper). Nevertheless, there is a lot of room for improvement regarding the experimental validation and the results are not very convincing (yet?). The datasets used in the paper have been traditionally used for evaluating GNNs but they have strong limitations due to their small size and it is often hard to draw conclusions from them. If the method does not suffer from scalablity issues, it is likely that more interesting results could be obtained by using ZINC or MOLHIV datasets, which are larger and often provide statistically significant results.

Overall, these issues may require a major revision and unfortunately, the area chair believes that the paper is not ready for publication.
Exploration can happen at various levels of granularity and at different times during an episode,  and this work performs a study of the problem of exploration (when to explore/when to switch between exploring and exploitation, at what time scale to do so, and what signals would be good triggers to switch). The study is performed on atari games.

Strenghts:
 
The study is well motivated and the manuscript is overall well written
Studies a new problem area, and proposes an initial novel method for this problem
extensive study on atari problems

Weaknesses
 
some clarity issues as pointed out by the reviewers
no illustrative task is given to give a more intuitive exposition of the "when to explore" problem
comparison to some extra baselines like GoExplore would have been insightful

Rebuttal:
 
Most clarity issues have been addressed satisfactorily. It has been explained why some requests for extra baselines would be challenging/or not relevant enough. While the authors agree that GoExplore would be an interesting baseline, they seem to have not added it. An illustrative task was not provided.

Summary:
 
All reviewers agree that this manuscript opens up and tackles a novel direction in exploration, and provides an extensive empirical study on atari games (a standard benchmark for such problem settings). While I agree with the reviewers that point out that this paper could have been made stronger by adding an illustrative task and additional baselines like GoExplore, there is a general consensus that the provided empirical study on this novel problem setting is a good contribution in itself. Because of this I recommend accept.
This submission proposes a new encoding mechanism, i.e. a new quantum data loader for images with a reduced number of qubits, which is then used for image classification with off the shelf quantum neural networks (from TensorFlow Quantum). There are two major concerns raised by most reviewers. The first concern regards the novelty in the design of the quantum data loader and the use of off the shelf quantum neural networks (QNNs), the latter of which is neither novel nor close to state of art QNNs for the same purpose.  The quantum data loading procedure also assumes a binary representation of images that might not be enough for low contrast images. Although the number of qubits is reduced, the circuit tends to have a large depth which makes it hard for practical implementations.  The second concern regards the overall performance of the proposed solution for image classification, where a clear quantum benefit is missing, or in some cases, a quantum disadvantage shows up.  Based on these discussions, we believe that the submission requires substantial improvements before its publication.
The paper proposes a method to improve explanation by example by identifying important parts of the image when using nearest engihbor explanations by example. Towards this goal, the notion of Critical Classification Regions (CCR) is proposed. The method is tested both computationally and a user study.

The reviewers felt that the paper had interesting ideas, but overall the reviewers agreed that the paper needs more work before being ready for publication: this includes improving the soundness of the empirical evaluations and clarifying the contribution of the paper.
This paper studies structured pruning methods, called kernel pruning in the paper which is also known as channel pruning for convolutional kernels.  A simple method is proposed that primarily consists of three stages: (i) clusters the filters in a convolution layer into predefined number of groups, (ii) prune the unimportant kernels from each group, and (iii) permute remaining kernels to form a grouped convolution operation and then fine tune the network. Although the novelty of the method is not high, it is simple and effective in experiments after the supplementary sota results in the long rebuttal. Majority of reviewers increase their ratings after the rebuttal (though one reviewer promised this but forgot to act), while some reviewers have concerns on the fairness to other authors by adding lots of new results in unlimited rebuttal and refuse to check more. In terms of the top end of performance, a reviewer thinks that "the authors haven t quite exceeded the results from existing works ("Discrimination aware channel pruning for deep neural network" and "Learning compression” algorithms for neural net pruning" for CIFAR 10 and many others on ImageNet)". In all, this work indeed lies on the boundary. After a discussion with other committee members, we recommend the acceptation of this work, if the authors could incorporate all the new results in rebuttal and get the reproducible codes released in the final version.
This paper introduces a new technique for discovering closed form functional forms (ordinary differential equations) that explain noisy observed trajectories x(t) where the "label" x (t)   f(x(t), t) is not observed, but without trying to approximate it. The method first tries to approximate a smoother trajector x^hat(t), then relies on a variational formulation using a loss function over functionals {C_j}_j, defined in terms of an orthonormal basis {g_1, …, g_S} of sampling functions such that the sum of squares of all the C_j approximates the theoretical distance between f(x) and the solution f*(x). These sampling functions are typically chosen to be a basis of sine functions. The method is evaluated on several canonical ODEs (growth model, glycolitic oscillator, Lorenz chaotic attractor) and compared to gaussian processes based differentiation, to spline based differentiation, regularised differentiation, and applied to model the temporal effect of chemotherapy on tumor volume. 

Reviewers found that the paper was well motivated and easy to follow (EBvJ), well evaluated (EBvJ), offering new perspectives to symbolic regression (79Ft). Reviewer vaG3 had their concerns addressed. Reviewer ZddY had concerns about the running time (a misunderstanding that was clarified) and the lack of comparison to a simple baseline consisting in double optimisation over f and x^hat(0) using Neural ODEs (the authors have added a Neural ODE baseline but were in disagreement with ZddY and 79Ft about their limitations).

Reviewers engaged in a discussion with the authors, and the scores are 6, 6, 8, 8. I believe that the paper definitely meets the conference acceptance bar and would advocate for its inclusion as a spotlight in the conference.
This paper proposes a new two stage second order unsupervised feature selection method via knowledge contrastive distillation. In the first stage, a sparse attention matrix that represents second order statistics is learned. In the second stage, a relational graph based on the learned attention matrix is constructed to perform graph segmentation for feature selection. 

This proposed method contains some new and interesting ideas and is novel in the unsupervised feature selection setting, though some components such as the second order affinity matrix are not totally new. The proposed method is technically sound. The authors compared their method with 10 methods including several recent deep methods on 12 datasets and demonstrated consistent improvements.
 
However, there are some concerns from the reviewers, even after the discussion phase. 1) The computational efficiency of the proposed method seems to be low. Since one goal of feature selection is to speed up downstream tasks, the efficiency of feature selection itself should also be considered. I suggest the authors analyze the computational bottleneck of the proposed method and improve the efficiency. 2) More ablation studies can be added to illustrate how the proposed method removes the redundancy issues of the selected features. 3) Some metrics like supervised classification accuracy can be potentially used as a metric. Though supervised classification is impossible in the unsupervised learning setting, running the experiments on some datasets that have labels by pretending having on label is one way to evaluate the method.

Overall, the paper provides some new and interesting ideas. However, given the above concerns, the novelty and significance of the paper will degenerate. Although we think the paper is not ready for ICLR in this round, we believe that the paper would be a strong one if the concerns can be well addressed.
The paper proposes a new neural network, the aestheticNet, for a bias free facial beauty prediction.
All the reviewers agree that the work is not suitable for publication as it raised some serious ethic concerns:
* Prediction of beauty (aesthetic scores) is a potential harmful application. Well intended as it may be, a research along these lines might be harmful.
* non anonoymity issue: writing reveals/implies authors identity with reference to previous work
* Research integrity issues (e.g., plagiarism, dual submission), a figure is copied from previous work.

There is also a concern that the work is not novel and not interesting as such. 
The authors did not respond to the concerns.

I suggest rejection.
This paper presents a locally connected spiking neural network model trained to do classification of MNIST using spike timing dependent plasticity (STDP) and reward modulated STDP. The authors show that this model can learn to classify MNIST images (though not at a very high accuracy) and that it can engage in classical conditioning. The reviews were initially all in the reject range. The common theme in the reviews was concerns about the weak and limited nature of the results. After a good amount of author response and reviewer replies to the authors, one reviewer increased their score to a borderline accept, but the other reviewers did not change their scores, producing scores of 3,3, and 6. Given these scores, and the reviewers  remaining concerns, a reject decision was reached.
All reviewers agree that the proposed idea looks interesting but the paper is seriously lacking in the definition of its scope: there is no quantitative result, experiments are quite limited, and there is not enough discussion of the limitations. With more work this could become a very interesting paper.
The reviewers  evaluation of this paper are borderline/negative.  The AC considered the reviews, rebuttal, and the paper itself, and concurs with the reviewers.  The AC found that the paper is an extension of previous work DM GAN (DM GAN: Dynamic Memory Generative Adversarial Networks for Text to Image Synthesis, CVPR 2019, https://arxiv.org/pdf/1904.01310.pdf). This work uses the word features in addition to sentence features at the first stage of generation, while DM GAN and other previous work don’t use word features in the first stage, but use them in the later stages when the feature resolution is higher. The authors improve the dynamic memory in DM GAN into spatial dynamic memory, and also change the image refinement process in DM GAN into an iterative refinement. The proposed multi tailed word level initial generation, spatial dynamic memory, and iterative refinement are incremental changes to DM GAN. Moreover, the proposed structure almost doubles the parameter size of DM GAN (shown in Table 2), yet the evaluation results on COCO are similar to DM GAN with only minor improvements. It is not clear whether the performance improvement comes from  the increased number of parameters or the architecture design. Especially on the CUB dataset with limited number of images, the model can easily overfit with a larger number of parameters.  The proposed method shares the similar network structure and dynamic memory blocks as DM GAN, except for a few changes.  Overall, the AC finds this paper not suitable for acceptance at ICLR in present form.
The initial reviews for this paper were diverging. After the rebuttal all reviewers have reached the consensus of recommending the paper s acceptance. Some reviewers have concerns regarding the novelty of the paper, however they appreciate that the paper is ell written and the empirical results are interesting. Following the reviewers recommendation, the meta reviewer recommends acceptance. In the final version of the paper the authors are encouraged to strengthen the weaknesses discussion as requested by one of the reviewers.
This paper presents a method to handle class imbalance in federated learning, while accounting for data heterogeneity and privacy. The key idea is to solve a constrained optimization problem where the difference between the global and local objective values has to be less than some parameter $\epsilon$. The paper proposes a primal dual optimization algorithm called CLIMB to solve this constrained FL problem. The paper presents a theoretical analysis of the algorithm, as well as experimental results. 

All the reviewers found the formulation interesting and novel and gave a positive assessment of the paper. Reviewer obo5 had some concerns about whether the optimization problem is improving fairness and getting reduced class imbalance as a side benefit or whether it is directly addressing class imbalance. After discussion with the authors, their concerns were partially addressed. Reviewer 8nbc had concerns about the assumptions and theoretical analysis. Their concerns were also mostly addressed by the authors during the discussion phase. I suggest the authors to also address Reviewer u6Lr and Reviewer Mp3G s concerns about experimental results and citing related work respectively when they revise the paper.

Overall, I recommend acceptance of the paper, and strongly encourage the authors to take the reviewers  suggestions about 1) fairness connections, 2) privacy connections, 3) theoretical analysis, 4) experimental results, and 5) prior work into account when revising it.
This paper proposes a new loss agnostic PU learning method based on uncertainty aware pseudo label selection.
I would like to thank the authors for their feedback to the initial reviews, which clarified many uncertain issues and improved our understanding of the current paper.
Nevertheless, even if the pseudo labeling technique was applied to PU learning for the first time, given that it is a common practice in many weakly supervised learning tasks, the technical novelty is rather limited.

Therefore I cannot recommend acceptance of this paper.
The submitted paper considers a form of second order extension of successor features building on a second order representation of the reward function in terms of state features. The authors demonstrate that this approach can be useful for transfer learning and also show an application to exploration.
All reviewers gave borderline recommendations (2x weak accept, 2x weak reject). While most reviewers agree that the proposed approach can be sensible and that the paper is well written, there are concerns that experimental results do not fully support all claims and additional experiments are required to clearly demonstrate advantages over existing baselines. Also the proposed approach for exploration is rather incomplete and not well studied. The raised concerns were not fully refuted by the authors during the discussion period but rather made some reviewers more concerned about full validty of all claims. Thus, while I think the paper has potential and can be turned into a good paper, I am recommending rejection of the paper in its current form. I would like to encourage to authors to carefully address the reviewers  concerns in future versions of the paper.
The paper applies a reinforcement learning (RL) approach to a medical diagnosis dialog task. Motivated by a large action space, the authors utilize a hierarchical model where the higher level model triggers a lower level model comprising of symptom checkers and disease classifiers. They evaluate their approach on real world and synthetic data sets.

Pros
+ The application (societal relevance) and the hierarchical approach (large action space) are motivated well
+ The paper is presented relatively clearly (with caveats: see reviewer comments) and improves performance over reasonable baselines (with caveats over one metric: why longer dialog is better?)

Cons
  The novelty of the work was not entirely clear, other than the application to a new task
  Lack of examples make it difficult to gauge the complexity of the task
  Ablation studies would also have provided better insight into task and the proposed model

The reviewers have several concerns about the work described in the paper. But the authors did not provide any response unfortunately.
This paper takes on (in my view) one of the most important questions in the lottery ticket literature today: how small are the smallest lottery tickets that exist in our neural networks? Many methods have been proposed for finding weak lottery tickets (those that require training to reach full accuracy) and strong lottery tickets (those that do not), but we have no idea how close they come to finding the smallest lottery tickets. Moreover, in many cases, we only know how to find lottery ticket subnetworks early in training rather than at initialization. Is this a fundamental limitation on the existence of lottery tickets, or is this simply a limitation of our methods for finding them? I am personally very involved in lottery ticket conversations in the literature, and I believe I can speak with some authority when I say that these are vital questions where any progress is important.

Moreover, these are exceedingly difficult research questions, and (again, in my view) the authors should be commended for taking them on. A naive approach to these questions would involve brute force search over all possible subnetworks, which is infeasible even on the smallest of toy examples, let alone the meaningful computer vision tasks where lottery ticket work typically focuses.

I am sharing all of this information to provide background for my confident recommendation to accept this paper over the many legitimate concerns expressed by reviewers and those that I saw when reading the paper in detail. Those include that:
* This paper does not solve any of these research problems in their entirety.
* It focuses on toy networks smaller than those traditionally studied in the lottery ticket literature, and it is well known that lottery ticket behavior changes in character at larger scales.
* Planting good subnetworks may be an unrealistic proxy for the kinds of subnetworks that actually emerge naturally.
* There may be multiple good subnetworks in a network, not just the one that was planted.
* The graphs are a bit hard to read.
* I find the mix of pruning methods studied, which were designed with very different goals (pruning after training, pruning before training, finding strong lottery tickets), a bit confusing.

**The bottom line:** With all of that said, in my view, the paper asks good questions and provides an initial foothold that other researchers will be able to build on as we seek more general answers. This is similar to the contributions made by Zhou et al., which started the conversation on strong lottery tickets, and potentially even Frankle & Carbin, which kicked off the lottery ticket discussion but got many things wrong. Both papers were good first attempts at solving big problems, and both were highly influential despite their flaws. Similarly, even if this submission isn t perfect in every way, this is among the most important kinds of contributions that a paper can make. For that reason, I strongly recommend acceptance under the belief that this paper will help to foster a valuable conversation in the literature.

P.S. I really, truly, strongly beg the authors to redo their graphs following the style of some of the more user friendly lottery ticket or pruning papers they have cited (e.g., Frankle et al., 2021). The graphs in this paper were really hard to parse. Really really really hard to parse. They re too small, the y axis is often squished, gridlines would be helpful, the lines are overlapping in ways that are difficult to distinguish because the colors blend, etc. etc. This is quite possibly the biggest impediment I see to this paper s ability to have broader influence.
This paper studies the problem of training tiny networks, by proposing a new training method called Network Augmentation (NetAug). The main challenge for training tiny networks lies in underfitting, which data augmentation and dropout etc. regularizations may suffer from for tiny networks. To overcome this hurdle, the proposed method first embeds or augments the tiny network as a subnet into a larger network, mostly by enlarging the width; then the gradients from the larger network are used as additional or auxiliary supervision. With this training strategy, the tiny model can perform better than the conventional training scheme on ImageNet and several downstream tasks. The proposed method is simple to implement and complementary with other techniques such as knowledge distillation and pruning. While there are lots of works studying how to improve the accuracy of large models, there are relatively fewer works focusing on the tiny network training. Despite that there are existing works sharing a similar idea of NetAug for large model training, which slightly hurts the novelty of this work, the majority of reviewers still like the idea and suggest to accept the paper.
The paper provides a method for with tuning continuous hyperparameters (HPs). It is closely related to a previous work (Lorraine, 2019) that was limited to certain HPs, and in particular could not be applied to HPs controlling the learning such as learning rate, momentum, and are known to be influential to the convergence and overall performance (for non convex objectives). 
The reviews indicate a uniform opinion that the paper tackles an important problem, that its methods provide a non trivial improvement over previous techniques and in particular those of (Lorrain, 2019), and that the provided experiments are extensive and convincing. The initial reviews had several concerns about technical details in the paper such as the analysis or how the meta hyperparameters are tuned. However, in the discussions the authors provided adequate responses, resolving these concerns. I believe that with minor edits that are possible to get done by the camera ready deadline the authors can incorporate their responses into the paper making it a welcome addition to ICLR.
This manuscript studies the problem of continual learning and introduces a reinforcement learning agent to select hyperparameters for replay/training. Ordinarily, replay based mechanisms for continual learning use settings and hyperparameters that are chosen and fixed through training. If it was possible to adjust replay dynamics online (in this case by looking at performance on a held aside test set), performance might be improved. This is the approach taken by this manuscript. 
Reviewers were generally happy with the writing of the paper and presentation of the material. At the same time, more than one reviewer worried about the novelty of the approach. In essence, the proposal amounts to using a black box optimizer (in this case RL) to adjust online the hyperparameters (e.g. the replay ratio) for continual learning (of the shelf ER and SCR). Viewed through this lens, and given that the optimizer in this case was a straightforward application of DQN, this concern is potentially well founded. The primary novelty then is the construction of the reward function to be optimized: in this case defined as the decrease of the CL loss measured on a held aside test set that is constructed online. Nevertheless, novelty is only part of the equation and strong empirical results can easily be a deciding factor in readiness for publication. On this front, reviewer GhFg points out that the empirical results and comparisons with baseline methods are not as clear as they need to be. Several issues are raise in discussion: the primary one is around the question of how the authors have allowed task specific information for the Q functions used by RL, and what the implications of this might be. The baselines compared against do not use any task specific information, which muddies the waters when trying to understand the comparisons. I agree with the reviewer that the manuscript needs to do a better job of making the empirical setting and comparisons as transparent and fair as possible. Given this, and the fact (raised by several reviewers) that some empirical evidence presented in the manuscript actually points to RL selecting near static parameters over time, I recommend that the manuscript be rejected. At the same time, I want to encourage the authors to focus on a streamlined version of the manuscript that addresses the issues raised by GhFg, as I believe that if the concerns can be addressed the work is close to making a compelling contribution for the field.
This paper proposes a method for self training in an open world setting where a significant portion of unlabeled data might include examples that are not task related. The proposed method (ODST) uses a more accurate OOD detection technique which allows an improved sample selection leading to higher accuracy.

Strong Points:
  This paper studies a very important and impactful problem.
  The paper is well written.
  The empirical results show that the proposed method improves over prior work.
  To better understand the iterative scheme, authors provide theoretical analysis using Bayesian decision theory.

Weak Points:
  Novelty: Given prior work on different variants of noisy students, this work has limited novelty.
  Dataset diversity: The main results are provided for CIFAR 10 and CIFAR 100 datasets which are very similar to each other. During the discussion period, authors added results on SVHN datasets but the accuracy gap between the proposed method and FixMatch is insignificant (FPR gap is higher but since the main goal is improving performance, I think showing accuracy is a more important measure here).
  Connecting theoretical results to the rest of the paper: The paper can be improved significantly if the theoretical results are more connected to the rest of the paper and in particular with the proposed algorithm.

While 4 out of 5 reviewers are recommending rejection, I think this was a very close decision. Most reviewers were concerned with novelty which I think is a valid point. Given that and the fact that the theoretical results are very limited, showing strong empirical results are required to accept this paper. Even though the provided results on CIFAR datasets are strong, the result on SVHN does not show a significant improvement. I understand that running experiments on ImageNet might not be budget friendly. However, it is possible to run similar experiments or other datasets to show the robustness of the proposed method to the choice of the dataset. Consequently, I recommend rejecting the paper and propose authors to resubmit after adding more datasets as part of their evaluation.
Paper presents an approach and evaluation setting for few shot learning in histology images. The approach leverages contrastive learning pretraining, and latent augmentation (LA) for data augmentation. The evaluation examines in domain few shot learning, mixed domain few shot learning, and out of domain few shot learning.  

Latent augmentation is an approach to learn how categories vary between samples within unsupervised clusters in a base dataset, and transfer that variation to the few shot sampled classes. 

Pros:
  A couple reviewers have claimed as a strength the novelty of the proposed latent augmentation method, but as other reviewers point out, there is much work in this field, some of which wasn t cited (i.e. Delta Encoder, NeurIPS 2018).
  The latent augmentation method is simple to implement, and outperforms standard input augmentation approaches.
  The paper is rich in content and details of experiments.
  Examining learning over a variety of domain shift settings is interesting.
  Shows contrastive learning can outperform supervised pretraining for this application domain.

Cons:
  Multiple reviewers raise concerns about technical novelty. This work applies mostly previously proposed methods, or variations thereof, to the domain of medical imaging. May be more suited to a medical imaging venue. 
  Some of the results are consistent with prior reports, such as finding that self supervised learning can outperform supervised pretraining. In that regard the results are not surprising.
  One reviewer raised issues about lack of comparison to other relevant few shot works. Authors argue that fine tuning is a competitive baseline. Authors did add comparison to one other variation augmentation approach, distribution calibration. But as mentioned, delta encoder is a very related work, which has not been cited nor compared against. Biggest difference is that delta encoder uses labels, but the unsupervised clusters can trivially be supplied as labels in this setting. AC feels authors should have done a more comprehensive comparison to related learned augmentation works. 
  Authors initially did not address how latent augmentation is affected by random seeds, but authors have replied to reviewers with additional data.

Reviewer consensus, excluding 1 reviewer, favors accept, though significant concerns regarding technical novelty and comparisons to other relevant works persist (especially in regards to works that learn how to augment as the proposed LA method does).
The manuscript develops new insights into how catastrophic forgetting takes place in the context of continual learning. The authors develop a new method based on this insight and demonstrate that it performs better than or as well as previously developed baselines, as well as showing that it is more widely applicable than close competitors (e.g. to cases where task boundary are unknown).  
The manuscript starts by pointing to evidence that catastrophic forgetting at task boundaries is due at least in large part to abrupt representation drift (e.g. in the penultimate layer of a network) caused by gradients coming from new class examples. Most reviewers found this novel and interesting. Reviewers also tended to be happy with the writing, motivation, and experimental results supporting the conclusions.
One of the reviewers recommends against publishing (3   Reject): mCT3 cites positives in the novel explanation of forgetting and the development of new metrics (Averaged Anytime Accuracy) and Total FLOPs, which they say help make the analysis more rigorous.
However, fundamentally, they believe that the relationship between the insights and the proposed methods are not strong enough and that the methods do not provide more than marginal improvements empirically. They point to work such as SS IL as a baseline which, in their opinion, is not improved upon significantly enough to adjust their recommendation.
The authors provide multiple effective rebuttals to the concerns, as well as detailed experimental analysis of SS IL: 0. The new method is shown to be as good or better than SS IL, 1. That their method are more computationally and memory efficient than SS IL, 2. SS IL requires task ids, whereas they do not, 3. Detail analysis (Appendix B) shows that SS IL mostly fails to learn the current task in the online setting in the miniImageNet case that the reviewer worries about, a fact that is obscured by the simple Acc metric. Reviewer mCT3 does not respond to the rebuttals in any substantive or compelling fashion, and leaves their score at 3/Reject.
While I believe that the reviewers concerns should be thoroughly addressed in a final version of the manuscript, I am in agreement with the 3 of 4 reviewers who recommend publication.
Strengths:
* Well written paper
* Strong empirical results on three benchmarks
* Interesting approach of producing semantically augmented LMs using dependency parses to extract svo triples, and finding coreferences between them across multiple sentences

Weaknesses:
* None of the reviewers seem particularly excited about the paper
* Stronger baseline comparisons would have improved the paper
* Authors re define a lot of terminology, but the novelty of the method is more from the type of graph used to initialize their method, which seems to be a function of OpenIE triplets
The paper presents a simple and effective solution to tune the receptive field of CNNs for 1D time series classification. The reviewers think the idea is original and elegant but would appreciate more theoretical insights into the solution.
This paper introduces a model, named Crystal Diffusion Variational Autoencoder (CDVAE), that can learn to sample valid material structures. It accounts for known symmetries (SE(3), permutation) of the structure via SE(3) equivariant GNNs.

The proposed model is a complicated combination of many existing models / modeling techniques (VAEs, NCSNs, diffusion models) but it is not entirely ad hoc; the revised paper does a reasonable job in justifying the many different modeling choices made. 
Existing model components, often designed in the context of molecule generation, do not account for the periodicity of the crystal s lattice structure; so this paper introduces modifications to account for this periodicity.

The paper evaluates the model on several datasets and also introduces new benchmarks that can be used for further research. The experimental results look promising but there are a few remaining clarity issues with the metrics used (cf. reviews).
This paper addresses features that are missing at random in deep supervised learning, especially regression and classification. A deep latent generative model is trained in conjunction with a discriminative model, so that the distribution of the covariates is properly modeled and allows efficient variational inference for imputation.  Superior performance is achieved in low capacity domain or when strong inductive bias is present in the discriminative model.

The paper is well written, with solid empirical support.  The approach is also generic.  Although there is some concern on novelty, overall this paper appears a solid contribution and is a good addition to the proceedings.
All reviewers agreed that this work on OOD and pseudo labeling presents interesting and strong results. The authors’ rebuttal has addressed some of reviewers’ concerns. Based on the current review and discussion, there are still several major concerns towards the expensive computational cost introduced by the clustering method, the lack of discussion around how the proposed work can be incorporated into SSL methods, and the sensitivity towards the selection of K.
The manuscript describes a method for improving the computational efficiency of randomized ensemble double Q learning for continuous action RL, by using a small ensemble of Q functions equipped with dropout and layer normalization, achieving matched sample efficiency at considerably less computational cost.

Reviewers praised the method s simplicity and achievement of its stated objective of reducing the computational cost of deploying ensemble Q functions. In general, the paper was found to be easy to understand and well written. Several expressed concern about the lack of interrogation of why this combination of dropout and layer norm worked so well and an overall lack of novelty. Other miscellaneous criticisms were well addressed in rebuttal and extensive new analyses in the Appendix were noted by several reviewers as adding much to the work.

In the AC s opinion, this is an example of a simple but non obvious combination of well known ideas that works very well. The review process has improved the level of empirical rigor that has gone into understanding the properties and trade offs of this method. I m happy  to recommend acceptance, though would echo reviewers concerns that dubbing the method "Dr.Q" will lead to confusion and would strongly urge adopting another name for the camera ready.
Description of paper content:

The paper describes a technique to learn option policies using behavioral cloning and then recombine them using a high level controller trained by RL. The underlying options are frozen. The method is tested in two published environments: a discrete grid world environment and a continuous action space robot. It is compared to three baselines.

Summary of paper discussion:

All reviewers moved to reject based on a lack of novelty and a lack of significant empirical results. No rebuttals were provided.
This paper tackles a problem at the intersection of AutoML and trustworthiness that has not been studied much before, and provides a first solution, leaving much space for a lot of interesting future research.
All reviewers agree that this is a strong paper and clearly recommend acceptance.
I recommend acceptance as an oral since the paper opens the door for a lot of interesting follow ups.
This paper does as it’s title suggests, it introduces an algorithm for constraining a CRF’s output space to correspond to a pre specified regular language. The authors build upon a wealth of prior work aiming to enable CRFs to capture particular non local dependencies and output constraints and present a coherent general algorithm to specify such constraints with a regular language. This is a clearly presented and well motivated contribution.

The reviewers predominantly agree that this work is clearly and rigorously presented and that the formalisation of constraints for CRFs through regular languages is a useful contribution for practitioners. One reviewer questioned the utility of constraining the output distribution at training time. In response the authors convincingly argue that unconstrained models will fail to learn the data generating distribution when non local constraints exist in the data and have included a clear synthetic example of this in the paper.

The most significant weakness identified of this paper is the limited experimentation, consisting of one synthetic experiment and an application to semantic role labelling. The key motivation for formalising constraints on CRFs with regular languages is the argument that this allows model builders to use a familiar formalism across disparate tasks rather than producing bespoke solutions for each. As such it would be informative when assessing the contribution of this work to see a number of practical examples of task output spaces formalised as regular languages such that we can form an intuition for how natural this representation is for more than one task, while also shedding light on the ease, or otherwise, of the crucial processing of minimising the representation to maximise efficiency.

While the application to a broader range of tasks would definitely strengthen this paper, in its current form it provides a useful formalism that will be of interest to those working in structured learning and as such is a contribution worthy of publication.
This paper presents the application of the hierarchical latent variable model, CW VAE which is originally developed in the vision community, to the speech domain with meaningful modifications, and provide empirical analysis of the likelihood as well as discussions on the likelihood metrics. The reviewers tend to agree that it is a promising direction to study hierarchically structured LVMs for speech, and the introduction/adaptation of CW VAE is useful. There were some discussion on the suitability of the likelihood evaluation, and it appears a fair comparison with wavenet shall take place at s 1 (single sample), a resolution level the proposed method does not yet scale up to. On the other hand, an important potential use case of the model is representation learning for speech, as it is a common belief that at suitable resolution the features shall discover units like phoneme. But I find the current evaluation of latent representations by LDA and KNN to be somewhat limited, and in fact there is no comparison with suitable baselines in Sec 3.2 in terms of feature quality. A task closer to modern speech recognition (e.g., with end to end models) would be preferred.
The reviews are of good quality. The responses by the authors are commendable, but reviewers remain of the opinion that the scientific contribution of the paper is limited, no matter how strong the software engineering contribution may be.
The paper presents a novel architecture, ModeRNN, for unsupervised video prediction by learning spatiotemporal attention in the latent subspace (slots).  ModeRNN effectively learns modular features using a set of mode slots and adaptively aggregates
the slot features with learnable importance weights. The paper has promising results on several benchmark video prediction datasets. 

During the post rebuttal discussion, the reviewer Wt6k and VMMf responded to the authors  rebuttal, but there was no discussion among them. The consensus is that even though the paper is a very strong engineering effort, it was not clear how the proposed architecture addresses the spatiotemporal mode collapse problem.  T SNE in Fig. 3/10/13 is insufficient to show disentangled feature space. In fact, PhyDnet was designed to disentangle different factors (physical vs unknown), hence not a good baseline.  [Hsieh et al 2018] is a better fit. In addition, synthetic data examples would be helpful to explain the underlying mechanism of the model and provide more insights for the video prediction community.

Based on this reason, I recommend rejecting this paper as it is now and encourage the authors to revise the draft and submit to future venues.

Hsieh, J. T., Liu, B., Huang, D. A., Li, F. F., & Niebles, J. C. (2018, January). Learning to Decompose and Disentangle Representations for Video Prediction. In NeurIPS.
This paper presents a method for ensembling light fine tuning methods and full fine tuning methods to achieve better performance both in domain and out of domain distributions. As authors agree, similar idea has been explored in the computer vision literature. The reviewers like the overall idea of the paper, but they all had some concerns regarding the experiments. The reviewers provide valuable feedback on how to improve the experiments, potentially running the same idea on more datasets and tasks, provide more analyses and discussions on how to understand the results.
This paper presents a through study of generalization in visual representation learning. It compares in distribution generalization to out of distribution generalization using a comprehensive benchmark. The paper received very positive reviews from all reviewers. Reviewers agreed that the paper has several strengths: It is very well written, the presented benchmark is very useful and the analysis is thorough. One concern that was brought up by the reviewers was that a majority of the presented findings are expected and in a sense, known to the community. The authors have addressed this concern by pointing out that their findings are more fine grained than past works and that their proposed benchmark is a stepping stone towards measuring general robustness. I must note that in spite of this concern, all reviewers have maintained their strong acceptance scores. I agree with the reviewers. This paper makes a strong contribution to this important problem via its benchmark and analysis, which future works can build off of, and hence I recommend acceptance.
The authors propose the OPT in Pareto algorithm that considers multi objective optimization, and includes an extra "non informative" reference metric for choosing between different Pareto optimal solutions.

The reviewers generally agreed that the work was compelling. However, one reviewer (6MZF) brought up the fact that the proposal is extremely similar to one proposed by a different arXiv paper, and convincingly argued that the authors of this paper were aware of the other before submission.

This is a difficult situation. On the one hand, for the purposes of establishing priority, an arXiv paper "doesn t count". On the other hand, I believe that authors are obligated to appropriately credit all relevant work of which they are aware, in *any* form: this includes journals, conference proceedings, preprints, emails, personal conversations, stackoverflow posts, tweets, etc. In this case, it seems that the authors did not adhere to this second condition, and while they have updated their manuscript, two reviewers said that they were unsatisfied by the changes on this point.

I want to emphasize that this isn t a question of priority: the first to publish "wins", and nobody has published this work, yet. However, other researchers working on the same problem, and proposing similar solutions, *must* be appropriately credited, even by the eventual winners (if they are aware of them).
This paper introduces Noisy Feature Mixup: an extension of input mixup and manifold mixup to all layers of a neural net, for the purpose of improving robustness and generalization in supervised learning. Experimental validation supports the increased robustness to attacks on the input data. The reviewers find the paper well written and they appreciate the theoretical analysis as well as the empirical results. The reviewers did not identify any big problems, and their minor concerns were sufficiently addressed in the author reponse. I m therefore happy to recommend accepting this paper.
The paper proposed a sharp attention mechanism in the context of image to sequence modeling. It seeks to build a “clear” alignment from the attention in order to improve the performance of the task. I don’t think there is a general consensus in the research community that the “clear” or “hard” attention performs better than the vanilla “soft” attention. Therefore, experiments become the key in justifying such motivation (and the model). However, as all the reviewers point out, the experiments in this paper are not satisfying. The numbers are far from the current mainstream results (Reviewer mESb). The experiments are done on relatively small datasets/tasks (Reviewer 1PRC) and the comparisons aren’t strictly speaking fair (Reviewer 5Hay). I think this alone is enough reason for the rejection of this paper. 

Additionally, on the algorithmic side, the novelty of this mechanism is not that high, given the existence of work such as the hard attention (Xu et al, 2015) and variational attention (Deng et al, 2018). It is also an unanswered question how such a mechanism can be introduced into the modern architectures that use attention (e.g. the multi head attention). The authors did not respond to the questions of the reviewers.
The paper considers a relevant and interesting problem of protecting the intellectual property of data. The goal of the proposed method is to prevent unauthorized usage of the data, and the protection is attained when a model trained on the perturbed dataset will predict poorly and thus cannot be considered as a realistic inference model by the unauthorized attacker. 

Technically, the paper tackles the problem of "unlearnable examples": to perturb the images of a labeled dataset to obtain perturbed dataset such that models trained on perturbed dataset have significantly lower performance, the perturbations are small, and one can approximately recover the original labeled dataset with the correct "secret key" (learnable parameters).

The authors propose two invertible transformations to craft adversarial perturbations: linear pixel wise transformation and convolutional functional transformation based on invertible ResNet. Numerous experiments demonstrate the effectiveness of the proposed transformations in both securing the data (making the data unlearnable when transformation is applied) and unlocking the transformation (making the data learnable when the transformation is inverted).

The paper is well motivated and exhibits competitive results. Although there are some concerns about the similarity of the work compared with [1], we believe the additional constraint of this work, that one can approximately recover the original labeled dataset with the correct "secret key",  justifies a significant contribution. 

[1] "Unlearnable Examples: Making Personal Data Unexploitable" Huang et al., ICLR  21
There was some disagreement between reviewers regarding the quality of the paper. Reading the paper, I had difficulty understanding what you were trying to achieve and, similarly to reviewer VgPP, felt the experimental section to be weak. While I can appreciate that compute is expensive, it would have been relevant to design more controllable continuous environments to get cleaner results in addition to those on MuJoCo. As it is, there is a lot of noise (and Table 1 does not contain confidence intervals) which, added to the general brittleness of RL algorithms, makes the experiments lack convincing power.

I encourage the authors to take all the feedback from the authors into account and resubmit an improved version of their work to another conference.
PAPER: This paper introduces a new method to learn joint representations from multimodal data, with potentially missing data. The primary novelty builds from the idea of semi supervised VAE, introducing the concept of bi directional information flow, which is termed “mutual supervision”. This approach brings the same advantages of semi supervised VAE to the multimodal setting, allowing the cross modal interactions to be modeled in the latent space. 
DISCUSSION: The discussion brought many important issues, addressed by both reviewers and authors. In general, it seems that most reviewers appreciate the technical novelty of the paper, related to the mutual supervision. While some concerns were expressed about the similarity with semi supervised VAE (Joy et al., 2021), I would agree with other reviewers and the authors that the extension is not straightforward. Bi directional information flow is a worthwhile novelty in itself. One reviewer also mentioned a concern about previous work on multimodal generative models; previous work on the same topic should not preclude new papers, as long new technical ideas are proposed. The final observation is about modeling more than 3 modalities. This is effectively a challenge with the proposed idea and should be acknowledged in the paper, but it is also an issue for many other approaches. New research will be needed to study 3+ modalities, but it should be seen as a future work direction.
SUMMARY: Based on the reviews, discussion and personal reading of the paper, I lean towards acceptance of this paper. The paper introduces a new technical idea (bi directional information flow, aka mutual supervision) which enables multimodal representation learning with missing data. The authors should revise their paper to acknowledge potential limitations of the approach (e.g., complexity challenges with 3+ modalities), but the idea is very interesting and worth publication.
This manuscript proposes an extension of semi supervised learning to the federated setting. The contributions include a thorough evaluation of performance and some method extensions. 

There are four reviewers. One reviewer points out a name leakage issue in the code that was missed and suggests deks rejection. The area chair has chosen not to desk reject the paper. Three other reviews agree that the manuscript addresses an interesting and timely issue   indeed, label acquisition is a significant issue in federated learning. Three reviewers agree to reject the paper   raising concerns about novelty compared to existing methods, some details of the evaluation, and some lack of clarity. The authors provide a good rebuttal addressing many of these issues. However, the reviewers are unconvinced that the method is sufficiently novel after reviews and discussion. Authors are encouraged to address the highlighted concerns for future submission of this work.
This paper proposes a bottom up multi person pose estimation method using a Transformer model. There is consensus among the reviewers that this paper is not ready for acceptance/publication. Although some reviewers find the proposed idea interesting (some find it lacking novelty though), all the reviewers agree that the quantitative experimental results are not promising. Some reviewers explicitly criticized lacking empirical accuracy compared to state of the arts. The authors provided additional details and results in the rebuttal, but they were not sufficient to change the opinions of the reviewers.

We recommend rejecting the paper.
This paper proposes a simple change to Transformer architecture to improve efficiency. While the reviewers appreciate the writing, all the reviewers agree that the novelty and contributions of the paper are limited both in the problem being solved by the paper and the level of experiments in it. Authors did not respond to reviewer s comments. Hence I recommend rejection.
### Description 
The paper develops a new automatic scheduler to schedule the learning rate during the training. The scheduler has access to the current training state summarized by certain statistics of weights and gradients in all layers and the loss history. It is trained by reinforcement learning with the reward derived from the progress with respect to the performance measure, such as validation accuracy. The key innovations are the design of the state vector using graph convolutional neural networks and empirical improvements to the reward function. The main claim is that GCNs allow to take into account the architecture of the network to be trained and the state of all layers, which, authors hypothesize and demonstrate experimentally, improves performance and transferability of the scheduler across networks and tasks.

### Decision
The reviewers recommendations after the rebuttal settled on 4 x "marginally above" and one "accept". Respectively, I recommend to accept. I recommend a poster based on the reception by reviewers: the novelty was assessed as limited because the idea of an automatic schedulers based on RL with a similar learning strategy belongs to the prior work. On the strong side, the paper satisfied all requests by reviewers for the experiments, regarding alternative methods, large datasets and ablation studies demonstrating that it is indeed the new architecture that allows to achieve a significant improvement, making it a solid improvement step. Amongst alternative methods the paper considers all viable alternatives: a function based schedulers, a hyper gradient method, and a the RL based scheduler, optimized in hyperparameters.

### Discussion
There was no significant non public discussion. As an additional feedback, let me just share my observations.

What is somewhat unclear in that the paper starts by discussing the directed graph of a feed forward network, then it proposes to run GCN on it, which is undirected. Then the hierarchical method is proposed, which runs GCN on each block sequentially while taking the aggregated input from the preceding block. This makes it a directed processing method on the level of block. I wonder whether the directed processing is desired or not desired here? Can a sequential processing summarize the network state efficiently on its own, similar to feedforward propagation, without the global averaging proposed?

It was not very clear to me from the paper what is the meaning of the batch size for training the GSN, and respectively what the batch normalization is doing there.

From some 100 mile perspective, it seems to me that whatever efficient optimization can be performed on the validation set, it helps. It does not matter so much what is varied: the learning rate schedule, other hyperparameters or even the network architecture. So in a sense it is not surprising that one can improve. What is more interesting is that the learned schedulers are generalizable / transferable, as demonstrated in Section 5.4 (changing the architecture or going from CIFAR to ImageNet while keeping the scheduler).

The work has done quite a lot on the experimental side with the baseline GCN model that they proposed. It seems to have still lots of potential via different possible enhancements. For example, what authors mentioned, including exponentially weighted running averages of gradients and squared gradients into the features. They already tried GAT instead of GCN as proposed by reviewers. There may be hyperparameters other than those controlling the learning rate. Some such hyperparameters, e.g. momentum, are apparently tightly coupled with the learning rate. The paper does not discuss how to tune them together with GNS. It could be a difficulty. On the other hand, they can be potentially scheduled with the same GNS. 

When considering SGD with momentum (which is not used in the paper), please note that the common use of a momentum parameter $\mu$ actually mixes the learning rate together with the smoothing parameter controlling the exponentially weighted averaging. So if one wants to control the learning rate alone, it is better to implement the gradient smoothing is done in .e.g. Adam, with its hyperparameter $\beta_1$.
This paper develops an approach to learning hierarchical representations from sequential data. The reviewers were very positive about the overall approach, finding it well motivated and interesting with strong potential, and thought that the paper was extremely well written with clear examples throughout. There was a good back and forth between the reviewers and the authors, discussing several aspects of the paper and providing constructive suggestions for improvement. In particular, the reviewers suggested improvements in terms of independence testing, comparison to further baselines, further experiments, and other improvements as detailed in the reviews. The authors were extremely receptive of these suggestions, which is to be commended and is very much appreciated, and in a response state that they are planning to take the time needed to revise this paper before publication.
This paper studies the use of natural language explanations during the training of an agent for odd one out tasks. Experiment results show that using quality explanation as abstract information about object properties helps with the agent performance, as compared with the vanilla method.

Strengths:
  Experiment results are conducted thoroughly to support the major claims made by the paper
  The problem is well motivated and has an important implication

Weakness:
  There has been extensive discussion about whether the paper lacks a more formal and rigorous definition of "explanation" as considered in the scope of this paper. 
  Concerns are raised regarding the gaps between the broad claims in the paper and the restricted experiment settings
The paper introduces a way of making Ratio Matching (RM) scale better to high dimensional data when training energy based models (EBMs). The main idea is to estimate the sum over the datapoint dimensions in the RM objective with importance sampling (IS), achieving computational savings by using fewer samples than dimensions. A key part of the method is a proposal that uses gradient information w.r.t. discrete variables to efficiently approximate the optimal (minimum variance) proposal, resulting in much better performance compared to uniform sampling. The authors also introduce a biased version of the estimator that samples from the same proposal but drops the importance weights when averaging over the samples, which, somewhat surprisingly, outperforms the unbiased version.

The idea of using Monte Carlo estimation based on importance sampling to speed up Ratio Matching is novel and sound. The use of gradient information to approximate the optimal IS proposal is also novel in this context, though the idea of using gradients this way to reduce the number of EBM energy function evaluations comes from Grathwohl et al. (2021), where it was used to speed up Gibbs sampling.

While the method is well described, the paper is insufficiently rigorous in several places, most importantly in claiming that Eq. 4 corresponds to Ratio Matching, which is not true. Eq. 4 is instead equivalent to the objective for Generalized Score Matching (GSM) given by Eq. 17 in (Lyu, 2009). Crucially, while both GSM and RM recover the true model if the model class is nonparametric/unconstrained, as is stated in (Lyu, 2009) (and thus agree with each other as well as with maximum likelihood estimation) ), they do not yield the same solution for constrained model classes such as neural networks. This means that the method in the paper implements GSM and not RM. Unlike RM, which has been used widely in the literature, GSM is essentially empirically unproven and thus is a less interesting choice. The main difference between the GSM and RM objectives is the presence of the squashing function g(u)   1/(1+u) around the probability ratios in RM to avoid division by zero, when the probability in the denominator is vanishingly small (as is explained above Eq. 12 in (Hyvarinen, 2007)). This means GSM is likely to be prone to stability issues due to using probability ratios directly. This is one possible explanation for the puzzling empirical results in the paper, where the proposed sampling based methods outperform the exact method they are supposed to approximate, with the biased method clearly performing best. The intuition based arguments made in the paper to explain these results are not convincing and need to be improved upon. While, as the authors pointed out in their response, it is possible to apply the strategy in the paper to RM by applying IS to Eq. 3 instead of Eq. 4, that would be essentially a different paper.

One example of puzzling experimental results is Figure 3, which shows that the base method ("Ratio Matching") does not find the correct solution while the proposed approximate methods do. This suggests that there is something wrong either with the method (e.g. with the objective, as mentioned above) or with the experimental setup. In either case, the cause needs to be thoroughly investigated.

Currently the empirical evaluation is primarily MMD based, relying on sampling from the model using MCMC. Ensuring that MCMC chains mix sufficiently well to sample from the true distribution by visiting all of its modes is difficult, and it is important to provide some evidence that this was done. As suggested by a reviewer, the results would be substantially strengthened by reporting the log likelihoods for the models, estimated e.g. using AIS, even if that requires including scaled down versions of some of the experiments.

The title of the paper is misleading and should be changed because the proposed method is specific to EBMs for binary data, even if the intent is to extend it to other types of discrete data in the future.

The clarification and additional results provided by the authors to the reviewers and the AC were appreciated, but unfortunately the outstanding issues with the paper are too major to allow acceptance at this point. The main idea of the paper has substantial promise however, and the authors are encouraged to develop it to its full potential by addressing the points from this meta review as well as the additional ones from the reviewers.

Bibliography correction: Hyvarinen is the solo author of "Estimation of Non Normalized Statistical Models by Score Matching". Peter Dayan was the editor of that paper and not a co author. Please correct your bibliography.
This paper proposed a transformer based routing network which removes the constraints in the original routing network such as the depth of a network. Multi Task learning (MTL) based on routing has been an interesting topic in the deep learning research community.  Our reviewers have serious concerns on the experiments. The presented empirical results do not seem to be able to sufficiently support the claims in this paper. Comparing with SOTA MTL methods is needed to make the proposed method convincing.
The paper proposes a novel post processing method technique that can mitigate the model bias, called the Ethical Module. It transforms the deep embeddings of a given model to give more representation power to the disadvantaged subgroups. 

The idea of ​​resolving discrimination against a specific group through effective post processing is promising, and proposing new metrics for fairness is also a very important and relevant issue.

However, the connection between the technique proposed in this paper and the newly proposed fairness metric is not clear, so the focus of the paper is somewhat lowered. Moreover, several design choices are somewhat unclear and ad hoc. In particular, although there was a lot of improvement through the rebuttal period, it is difficult to verify the superiority of the proposed method via the experiments in the paper; Direct comparisons with existing methods for fairness is essential, and it seems necessary to consider a hyperparameter selection strategy that can be taken in a practical scenario rather than simply choosing the best performing hyperparameter for the test set.
The paper analyzes a variant of the Q Learning algorithm with two modifications: Online Target Learning (OTL), and Reverse Experience Replay (REP). OTL is essentially the same as using the target network. REP is a new modification of ER, which instead of randomly selecting samples from the buffer, replays them in the reverse order.

Most reviewers are positive about this paper, so I am going to recommend acceptance. There are, however, several concerns that have been raised by the reviewers. As the authors have not revised the paper during the discussion period, my acceptance recommendation is under the good faith expectation that the authors make a serious effort in improving their work based on the reviews. Some of the concerns are:

  The intuition of why REP breaks the correlation is not clear enough. This has been brought up several times by the reviewers.
  What are the technical differences in the analysis compared to previous work such as Zou et al., 2019?
  The kappa appearing in Assumption 4, and showing up in the error bounds, can be dimension dependent. Please clarify this and its effect on the results.
  Much of the paper is in the appendix. It helps if the authors can include more about the proof technique in the main body of the paper.
  Describe the relation between the error in the value function vs. the performance of its greedy policy.
This paper focuses on investigating the relations between the heterophily and over smoothness problem. However, the relationship is not clear.

The over smoothness problem considers the features and the adjacency matrix, while the heterophily incorporates the adjacency matrix and the labels. They have different views on the graph. It may not be treated as the same coin. Besides, the stacked aggregations lead to indistinguishable node representations and poor performance in the over smoothing problem. The same phenomenon appears in the heterophily problem because the features in different classes are falsely mixed, leading to indistinguishable nodes [2]. They have the same phenomenon but different origins. It may be not a necessity to combine these two problems.

Besides, MADGap[1] is proposed to evaluate the over smoothness problem. It is unreliable to use the accuracy and the degree to measure this problem. Therefore, in section 3, the relations between node degrees and the homophily ratio cannot infer the relations between the heterophily and over smoothness problem.

As a result, the authors should carefully re organize their paper and results. 

A suggestion is to pack the submission as a new method to learn from heterophily instead of trying to make such a close relationship with over smoothing.

  [1] Measuring and Relieving the Over smoothing Problem for Graph Neural Networks from the Topological View. AAAI 2020
  [2] Beyond homophily in graph neural networks: Current limitations and effective designs. NeurIPS 2020
Motivated by empirical observations that SGD performed on deep networks converge to regions of flatter loss curvature relative to large or full batch GD, the authors perform a theoretical analysis of trajectories of SGD with the presence of heavy tailed noise. The primary observation of the theory is that heavy tailed noise has a higher probability of "kicking" the current parameters to a new region of the input space, which has some probability of lying in a sharper region. However, it s important to note that in this analysis SGD with heavy tailed noise doesn t stay in the sharp regions, but will eventually be kicked back out of it back to other regions. In a sense, this defines a transition graph which predicts that the steady state distribution should spend some fraction of time in different regions of the input space (and different sharpness) while never "converging" anywhere. This is shown most clearly in Figure 1 top center where the heavy tailed SGD randomly jumps between different regions of the input space throughout the entire training trajectory. Experiments are then run on deep networks showing that heavy tailed SGD with gradient clipping converges to regions of flatter curvature. 

Reviews of the work were generally positive, the theory is well presented and Figure 1 does a solid job demonstrating the main idea. The primary criticism was raised by reviewer HGyL, arguing that the results should be largely irrelevant to deep learning. Most of the debate between this reviewer and the authors centered around whether or not ReLU networks have minima which extend off to infinity. The AC will not dig into the details of the argument. It seems clear, however, that if there were a deep learning workload with heavy tailed noise that the authors results will have some relevancy, though the exact nature of the resulting transition graph may have a complicated dependence on the loss surface. Unfortunately the authors were unable to find a such a workload in image classification (there is some prior work suggesting the NLP models with rare tokens may be a better fit) and so needed to artificially induce heavy tailed noise to test their theory. This is a bit of a limitation, but given the clear writing and interesting experiments as noted by reviewers the work seems worth accepting. The AC strongly urges the authors though to include a more lengthy discussed of Wu. et. al. as that work seems to agree with experiment of the sharpness of stable regions selected by SGD when run on deep models without heavy tailed noise.
This paper addresses the performance of normalizing flows in the tail of the distribution. It does this by controlling tail properties in the marginals of the high dimensional distribution. The paper is well motivated, and the key theoretical insight has merit. However, the general perspective and methodology appears to be incremental relative to past results. Furthermore, some concerns over correctness remain after discussion with authors. Also, clear baselines and more realistic settings are lacking in the experimental results. Thus, while the paper generally has promising ideas on a pertinent topic, it appears to be not developed enough to merit dissemination.
This paper proposes a new wavelet based model to represent textures. The model incorporates a wide range of statistics, by computing covariances between rectified wavelets coefficients, at different scales, phases and positions. The model can synthesize textures that have a similar quality to state of the art texture models using CNN structure. Qualitative results are shown to demonstrate the effectiveness of the model.  The paper studies an important problem in computer vision and neuroscience, which is texture modeling. However, many important related works are missing. After rebuttal, three of four reviewers champion accepting the work because the proposed wavelet based texture model, which produces competitive synthesis with much less parameters than the CNN based model, will be beneficial to the fields of computer vision and neuroscience. One reviewer has critical comments on this paper because the paper lacks a comparison again more recent works both quantitatively and qualitatively. However, during rebuttal, the authors expressed their disagreement with it and pointed out that the goal of the paper is to bridge the gap between the classical work of Portilla and Simoncelli (2000), and the CNN based models and to find what statistics are needed to describe the geometric structures in natural textures. Their discussion didn t reach an agreement after rebuttal. After an internal discussion, AC recommends accepting the paper but urges the authors to improve their paper by taking into account all the suggestions from reviewers, especially include the discussion or comparison with those related works mentioned in the rebuttal.
The submission proposes triangular dropout training to provide adaptive capacity of the network at inference time. The proposed approach is simple and sound. However, the experiments are lacking in terms of complexity of the task and up to date architectures (e.g., transformers or convolutional layers) to demonstrate the effectiveness of the method.
Therefore I recommend this paper for rejection.
This paper explores the hypothesis that bloat can be prevented in Genetic Programming by identifying "winning subtrees" from simplified solutions, and use these to seed new GP runs. This idea is connected with the lottery ticket hypothesis in deep learning.

Reviewer are unanimous that the paper as it stands is not ready to publish. One big issue is that the empirical results are not particularly good. Another is that the conceptual foundations of the paper, in particular the parallell to the lottery ticket hypothesis, might be flawed. Nevertheless, there is much interesting research to do in this direction.
The paper presents the Language complete Abstraction and Reasoning Corpus (LARC): a collection of natural language descriptions by a group of human participants who instruct each other on how to solve tasks in the Abstraction and Reasoning Corpus (ARC).

Overall, the reviewers found the LARC benchmarks to be well motivated. However, there were concerns about whether the value of the dataset to downstream tasks. Results from additional program synthesis systems (like Codex and GPT Neo) would also make the paper stronger. I agree with these objections and am recommending rejection this time around. However, I encourage the authors to continue pursuing this line of work and resubmit after incorporating the feedback from this round.
This paper presents a study of the over parametrization of linear representations in the context of recursive value estimation.

The reviewers could not reach a consensus over the quality of the paper, with a fairly wide range of scores even after the rebuttal.

After considering the paper, the rebuttal, and the discussion, I lean towards accepting the paper. Despite the concerns voiced by some of the reviewers, the topic and analysis of the manuscript are novel and interesting, and it is my expectation that this manuscript will prove a valuable source of inspiration for future work.

I invite the authors to carefully consider the feedback received by all the reviewers (and in particular Reviewers xq3y and gT5o and) and to revisit the manuscript accordingly.
Though some concepts discussed in the submission are interesting, there are many major concerns: there is a lack of literature review, comparison experiments with the state of the art methods are missing, the technical novelty of the proposed method is very limited. 
In the rebuttal, the authors agreed with reviewers  comments and did not provide responses to address reviewers  concerns.

Therefore, based on its current form, this submission does not meet the standard of publication at ICLR.
The paper changes the metric in self paced reinforcement learning to be a Wasserstein distance and shows that this outperforms other metrics in simple toy like experiments.

Even after discussions with the authors, two major concerns were identified with this submission: First, the proposed modification of the metric appears to be rather incremental with regards to the original paper. Second, the proposed method is only evaluated on relatively simple environments. The approach should be evaluated on more difficult tasks.

Given that there was no strong champion for acceptance among reviewers of this paper and the above mentioned limitations, I recommend rejecting this paper.
The paper studies multi label classification problem. Particularly, they introduce multi label box model, which uses probabilistic semantics of box embeddings, representing labels as boxes instead of vectors. Their model is evaluated extensively on 12 datasets, and reviewers agreed the paper was well written and well motivated. While it is pretty straightforward application of box embeddings to multi label problem, it is well motivated and the paper adds to the existing literature on box embeddings. 

Reviewer Eo7g had a concern with experimental setting, including missing a baseline Abboud et al. (2020). Even after the baseline was added, the reviewer was not convinced about the model’s performance, as the baseline was not extensively tuned. The authors responded with the heavy computational costs of associated with tuning the margin. Reviewer X5cP also pointed the problem with HMC dataset, for which the experimental results should be updated. Given the issues, the paper could benefit from another round of revisions.
This work provides a formal framework for discussing membership inference attacks (MIA). It then examines existing attacks and proposes some new ones. The attacks are evaluated on several datasets.  The framework mostly formalizes the types of information and error types that an attack may use and is presented as the main contribution of this work. However the presented formalizations do not appear to contribute significantly beyond the existing work on MIAs. The new attacks may be of interest and, according to the presented experiments, (mildly) improve on some of the existing MIAs. At the same time, as presented, the discussion of the the benefits of the new attacks is relatively short and reviewers did not find the results to be sufficiently convincing. Therefore I cannot recommend acceptance for this work in its current form.
This is an interesting paper on improving score based conditional sampling and its use in solving inverse problems. The current method of sampling from NCSNv2 is somewhat inefficient and the authors propose a different SDE that seems to work better for conditional generation. 

The paper is applied to Computational imaging and MRI and shows very good results and reasonable comparisons with the recent state of the art. One limitation is that the measurement process is artificial and ignores specifics of MRI (real measurements and multi coils would strengthen the paper). In any case since this is a fundamental methods paper with a solid technical innovation on score based sampling, I recommend acceptance.
This paper proposes a new learning procedure for quantizing neural networks. Basically, DQA method proposed in this paper uses attention to obtain a linear combination of the existing network quantization techniques and uses it to pursue more efficient quantization.

Overall, it seems the submission was written in haste, so there are many typos and errors. Above all, the motivation that it can be applied to various existing techniques could not be proved experimentally at all since it only covers one somehow obsolete work. In addition, as in [1], it seems necessary to quantize not only weights but also activations, or to verify in lightweight networks such as MobileNetV2 rather than ResNet.

[1] Cluster Promoting Quantization with Bit Drop for Minimizing Network Quantization Loss, ICCV 2021
The paper considers the problem of accelerated magnetic resonance imaging where the goal is to reconstruct an image from undersampled measurements. The paper proposes a zero shot self supervised learning approach for accelerated deep learning based magnetic resonance imaging. The approach partitions the measurements from a single scan into two disjoint sets, one set is used for self supervised learning, and one set is used to perform validation, specifically to select a regularization parameter. The set that is used for self supervised learning is then again split into two different sets, and a network is trained to predict the frequencies from one set based on the frequencies in the other set. This enables accelerated MRI without any training data. 
The paper evaluates on the FastMRI dataset, a standard dataset for deep learning based MRI research, and the paper compares to a trained baseline and an un trained baseline (DIP). The paper finds their self supervised method to perform very well compared to both and shows images that indicate excellent performance. It would have been even better to compare the method on the test set of the FastMRI competition to have a proper benchmark comparison.

Here is how the discussion went:
  Reviewer pt6r is supportive of acceptance, but notes a few potential irregularities, such as the method pre trained on brain and tested on knees performing better than the method pre trained on knees and tested on knees, and not providing a comparison of the computational cost. The authors added a table to the appendix revealing that the computational costs are very high, much higher than for DIP even. The reviewer was content with the response and raised the score.  

  Reviewer mBMk argues that the contribution is too incremental compared to prior work, in particular relative to the results of [Yaman et al., 2020], and also argues that the idea of partitioning the measurements is not new. The authors argue in response that their approach of partitioning the measurements is new, and the reviewer was inclined to raise the score slightly, but still thinks that the novelty on the technical ML side remains limited, and doesn t want to back the submission too much, and did not raise the score at the end in the system.

  Reviewer 19v3 has the concern that the all elements used (transfer learning, plug and play, etc) are well known techniques and have been applied before to MRI, and therefore thinks that the paper does not clear the bar for acceptance. The paper points out that while those ideas might be applied for the first time to MRI, they have been used before in other image reconstruction problems, in particular denoising. 

I ve read the paper in detail too, and am somewhat on the fence: I think it s very valuable to see that a clever application of self supervised learning works so well for MRI. I agree with the reviewers that the technical novelty is relatively small, but on the other hand this is the first time that I see self supervised learning being applied that successfully to MRI. I don t share the concern about novelty   yes, the paper s approach builds on prior work, but it s not clear from the literature how well such a well tuned self supervised learning approach would work. 
What I would have liked to see in addition to the experimental results is a proper evaluation on the FastMRI dataset: An advantage of the FastMRI dataset is that it provides a benchmark and if researchers evaluate on that benchmark (on the testset/validation set) we can compare different methods well. The paper under review doesn t do that, it only evaluates on 30 test slices, and thus it s hard to benchmark the method. Also, the paper would benefit from more ablation studies.

In conclusion, I would be happy to discuss this paper at the conference, and think that other researchers in the intersection of deep learning and inverse problems would be too, and therefore recommend acceptance.
The authors provide an analysis of soft winner take all (WTA) networks with Hebbian local learning as a generative probabilistic mixture model. They then present experiments on comparably simple data sets, MNIST and F MNIST. Results are compared to hard WTA networks and an MLP of the same size (single hidden layer) trained with backprop. Besides accuracy, the learning speed and adversarial robustness of the networks are compared.

This paper is borderline, and I was discussing it quite a bit with the reviewers.
The reviewers agree that the manuscript has some merits, but they also point to a number of weak points.

Besides the objective evaluation, I would like to comment on the review dynamics of this paper. The paper had initially rather low ratings. The authors commented extensively on the reviews, in several waves, and with suggestive text such as "All reviewer s points addressed" (as a comment title) or "Based on the Reviewer s earlier comment, the revised paper is now a clear  accept ." to name just a few. I and the reviewers had the impression that the authors strongly urged the reviewers to increase their scores.

Due to the borderline ratings, I decided to read the paper carefully. My impression is in line with the main criticisms of the reviewers, and summarized in the following:
On the positive side:
  The manuscript tackles an interesting problem. WTA architectures are biologically highly relevant structures and it is relevant to study learning in them.
  The authors provide a nice theoretical analysis.
  The observation that WTA architectures improve adversarial robustness is very interesting.
  Learning is local.
  The manuscript is well written.

On the negative side:
  Theory: Similar analyses have been performed before. While there are differences, the main ideas are rather similar, in particular with respect to (Nessler et al., 2009). The authors argue that in contrast to their work, Nessler et al. 2009 deals with spiking neurons. But since the authors argue with biological plausibility, I would see that as an advantage of Nessler et al.
  Performance: The performance of their model is comparable to the standard hard WTA network, often showing only a very slight advantage. This raises the question why the soft WTA should be preferred over the hard WTA. The performance of the single hidden layer ANN is clearly better. This raises the question of the scalability of the approach.
  The analysis of adversarial robustness is interesting, but there is no comparison to other defense methods (e.g. adversarial training). The authors argued in their comments that it is not an adv. defense paper, so this comparison is out of scope. This reasoning is understandable, but since this is maybe the most interesting point of the paper, it would be a nice to have.
  Scalability: It is true that the learning is local, but the question is whether it scales to larger problems and deeper networks. After the first reviews, the authors added experiments on CIFAR 10 and a convolutional version of the model. However, the results were clearly below the state of the art and the convolutional model is barely described (5 lines in the appendix).

Conclusion: The manuscript has some interesting points. Given the the strong competition within ICLR however, I cannot propose acceptance.
This paper studies the important problem of time series anomaly detection using deep neural networks (DNNs). Unlike many other DNN models, it focuses on incorporating in its model architecture interpretable components that are inspired by previous studies based on both conventional statistical methods and more recent DNN models.

While the paper has merits as pointed out by the reviewers (esp. TtBt), a number of concerns have also been raised, including the choice of datasets (e.g., by reviewers rnBY and zX4p). We appreciate the authors’ effort by adding some preliminary results of further experiments, but addressing all the concerns thoroughly will need a lot more work to get a scholarly paper that is more ready for publication. We believe this work has potential to be accepted for publication in a reputable venue if the concerns are thoroughly addressed after substantial revision.
This paper presents a probabilistic framework that explains why models trained adversarially are robust generators. It received fairly high initial scores. The reviewers thought the work was novel and interesting. They liked that the analysis provided a way to derive a novel training method and sampling algorithms. Reviewers confirmed their support of acceptance and I think this paper is clearly above the bar. Respectfully, I’d prefer that the authors don’t ask the reviewers to “raise your score”. It is up to the reviewers to make that decision.
This paper presents a hierarchical memory for cross domain and few shot classification problems. The paper is well written, tackles an important topic, and the proposed approach which is an extension of VSM is interesting. Reviewer YEXZ has some concerns regarding comparison to a more proper baseline. I believe that the authors have adequately addressed this. Reviewer 2Ajk and g1Bf also have suggestions that the authors have incorporated in the revision. I recommend accepting this paper.
The paper considers the natural class of algorithms, namely Aggregators with Gaussian noise for distributed SGD with differential privacy (DP) and Byzantine resilience (BR). Previous results shows VN >BR > convergence of SGD. The authors first show that aggregators with Gaussian noise algorithms satisfy DP but violates VN necessarily, so approximate VN is proposed. Theorem 2 shows approximate VN >convergence. Proposition 2 shows the above algorithms satisfies approximate VN with certain parameters. With the combined bound Corollary 1, the authors observe (and then verify by experiments) that larger batch size is beneficial and in particular more beneficial than when DP or BR is enforced alone. In the formulation, an important baseline of robust mean aggregation [Diakonikolas,Kamath,Kane,Li,Moitra,,Stewart 2016] and even more relevant baseline of robust and DP mean aggregation[Liu,kong,Kakade,Oh, 21] are somehow missing. One would assume that directly applying these well known techniques might give the desired DP and robust SGD. The field at the intersection of differential privacy and robustness has evolved quite a bit recently and tremendous technical innovations are happening. Given the relveance of the proposed problem to this line of work, one should make the connections precise and explain the differences.
The paper studies an important problem of quantifying uncertainty (as measure by calibration) of predictions made by an ML algorithm in the presence of distribution drift. However, all reviewers point out a slew of concerns that went un rebutted by the authors. The reviewers concurred that the paper deserved to be rejected at the current stage, and I concur. I recommend that the authors take the critical and constructive feedback into account to improve the paper and perhaps resubmit to a different venue in 2022.
This paper proposes a simple meta algorithm to speed up data thinning algorithms with good theoretical guarantees. The method is both theoretically interesting and useful for practical applications.
This manuscript presents a novel approach to learning a shared language between multiple agents.

In general, reviewers had difficulty understanding the symbolic mapping component. For such a critical part of the manuscript, questions by multiple reviewers were extremely basic, asking what symbolic mapping even is. Authors did clarify this in the discussion and updated the manuscript, but further improvements to the manuscript are warranted.

Reviewers had concerns about the novelty of the approach. Including being confused about whether this is just an application of curriculum learning. Reviewers were also concerned about the lack of ablations.

Reviewers also had concerns about the fact that this is a toy domain. Symbolic mapping as defined in the manuscript appears to be possible only for such toy domains. It fundamentally wouldn t scale to simple language games with real images. This significantly limits the scope of the work. More broadly, reviewers wanted to see symbolic mapping exercised much more. If this is a useful idea, they wanted to see the authors apply it to other domains.

Reviewers were confused about many other details in the manuscript. For example, about the fact that refdis is later discarded as a metric, which the authors answered is due to redundant symbols ("the symbolic mapping is not a highly compositional representation here because of the redundant symbols"). Why redundant symbols lead to less compositional representations seem unclear.

With significant additional improvements to the clarity of the manuscript, a demonstration of how symbolic mapping is useful in another domain, and additional experiments suggested by multiple reviewers this could be a strong submission in the future.
Although this paper is on an interesting topic, there is a consensus that this paper is below the bar for acceptance. My advice is to take take criticisms of the reviewers seriously, add the extra experiments, rewrite the paper and then submit it to a different conference. If the authors feel that the reviewers misunderstood their paper, please remember that the level to which they were able to understand it is also a function of how the paper is written.
This paper introduces an architecture that uses pooling regions and
eye movements to sequentially build up an object representation.  A 
confidence threshold is used to allow recognition in less time for
easier images.

There was a lot of disagreement on this paper.  Those in favor argued 
that it is a worthy endeavor to explore new biologically motivated
architectures and foveated eye movements are an important aspect of  
human vision that is worth exploring for computer vision.  Another pro
was the improved robustness to some adversarial attacks.  Those
arguing for not accepting the paper, argued that classification
performance is not improved over SOTA and that more ablation studies 
should be done to better understand the role and importance of the
various aspects of the model and how they differ from other
architectural designs with dilated convolutions instead of the 
foveation module.
 
I agree that more ablation studies would be useful to better
understand the role of the different model components. While I 
feel that this novel sequential processing algorithm is worth publishing to
increase activity in this area, I feel it would be best received after further 
studies help clarify the importance of different aspects of the model.
I recommend resubmission after further analysis.
The paper presents a new method for detection of backdoor attacks under strong limitations such as the lack of access to training data and the reference benign model. Its main idea is to utilize a new expected transferability statistic that can be used for detection in broad range of application domains. The effectiveness of the proposed approach is demonstrated experimentally.
The paper considers matrix and tensor factorization, and provides a bound on the excess risk which is an improved bound over the bounds for ordinary matrix factorization. The authors also show how to solve the model with standard gradient based optimization algorithms, and present results showing good accuracy. The method can be a bit slow but this depends a bit on the number of iterations, and in general it achieves better accuracy in a similar amount of time to other baseline algorithms.

The reviewers raised a few points, such as jdoi noting the tensor experiments were for small tensors and should include the method Costco as well; other reviewers mentioned more methods as well.  The authors seemed to address most of these concerns in the rebuttal, adding more experiments and more details on timing.  26KD mentioned the optimization procedure was unclear, but the revision includes pseudocode in the appendix that clarifies.

Overall, the paper has both a theoretical and algorithmic contribution, and would be of interest to many ICLR readers.
The authors propose a neural network model to preserve the sub class similarity. The key of the model is to add a prototype layer to a multi scale deep nearest neighbor network. The prototype layer stores the representative prototypes of some fine grained sub classes. The use of the prototype layer preserves intepretability and computational efficiency. Experimental results demonstrate that the proposed approach reaches state of the art prototype learning performance.

The reviewers generally find the paper clear and with sufficient contributions. The empirical validation is sufficiently thorough to back the claims in the paper. The main concern prior to the rebuttal among some of the reviewers was about the novelty of the paper (e.g. with respect to DkNN), but the authors convinced most of the reviewers in the rebuttal about the key differences. The authors are encouraged to highlight the novelty aspect more clearly in the revision. Another suggestion was to add an ablation study to justify the importance of the r1/r2 parameters, and the authors have done a successful job addressing the suggestion. Several other comments, such as explanations of the hyperparameters, have been taken into account in the revision. The reviewers thus reach the consensus to recommend acceptance.
The paper propose a universal technique that enables weak supervision over any label type while still offering desirable properties, including practical flexibility, computational efficiency, and theoretical guarantees.

Over the course of the rebuttal, the authors have made a substantial overhaul on writing and experimentation. The universality claims are now better supported by bounds, and experiments cover comparison to snorkel, majority vote and supervised learning, on multiple applications. The authors are encouraged to move the related work section to the main body of the paper. The authors should also clarify to what extent the contributions they make pertain to Snorkel as opposed to weak supervision more generally. This may require revisiting both the introduction as well as perhaps the title.
This paper proposes an adaptive tree search algorithm for NMT models with non decomposable metrics and shows its efficacy against strong baselines. This is an interesting contribution towards overcoming the performance caps introduced by the uncontrolled for biases of beam search, and it speaks to a growing community interested in decoding beyond greedy surprisal minimisation.

The initial reviews brought to light a number of concerns that in my view are well addressed in the rebuttal and in the current version of the manuscript. One of the key issues was a confusion caused by the use of the term  non autoregressive  to refer to the intractability of the metric / objective function of certain models. This use clashed with the more standard use in MT, which refers to a tractable factorisation of a joint probability by means of strong conditional independence assumptions. 

The confusion is easy to address and in no way compromises the thoroughness of the empirical section. The authors are aware of the confusion and how to resolve it, and they have acknowledged the need to pick a less ambiguous term. 

I d like to recommend this for acceptance, but I urge that the authors do not ignore the confusion caused by  auto/non auto regressive  and the missing literature that came up in the discussion with reviewer i2pz (I understand the discussion happened too late for the manuscript to be updated, but I trust this can be done for the final version).
The paper focuses on the strong adversarial attack, i.e., an attack that can generate strong adversarial examples and thus can better evaluate the adversarial robustness of given deep learning models. One review gave a score of 8 while the other 3 reviewers gave negative scores. The main issue lies in the limited experiments, as a potential substitute for AA, the proposed MM should be widely tested against different defenses, just as done in the AA paper. The writing of the paper is somehow is not rigorous including many incorrect statements and unsupported claims which should be well addressed in the revision. Thus, it cannot be accepted to ICLR for its current version.
Initially, some reviewers have raised several points of criticism regarding certain aspects of the model whose novelty/significance was a bit unclear. After the rebuttal and the discussion phase, however, everyone agreed that most of these concerns could be addressed in a convincing way, and finally all reviewers were in favor of this paper. After carefully going over all the reviews, the rebuttal and the discussions, I fully agree with the reviewers and came to the conclusion that this paper indeed contains some interesting, novel and relevant contributions.
The paper derives a new parameter initialization for deep spiking neural networks to overcome the vanishing gradient problem.

During the review, concerns were expressed about how well the method would scale to larger neural networks. It was also questioned how this parameter initialization technique compares with a recently proposed batch normalization technique, especially when training larger neural network on more challenging datasets. There were also concerns raised about the readability of the paper.

I commend the authors for improving the readability of their paper in their revision. I also commend them for taking the time to implement the comparisons requested by the reviewers. These new comparisons revealed that batch normalization and its recently proposed variant were superior to the initialization method on its own, and that the initialization proposed in the paper did not significantly improve performance when paired with batch norm [[1](https://openreview.net/forum?id T8BnDXDTcFZ&noteId yIAPcSbUAQ0)]. The authors also acknowledged based on the new results, that their proposed parameter initialization scheme appears to fail to scale to more complex datasets and networks, especially relative to competing methods, which invalidates a key claim that their approach can "accelerate training and get better accuracy compared with existing methods" [[2](https://openreview.net/forum?id T8BnDXDTcFZ&noteId j12fwayWEb)].

The recommendation is to reject the paper in its current form.
The idea of having two policies with opposing strategies, one aiming to maximize a notion of surprise whereas the other tries to minimize it, is an interesting one. However, even after the author rebuttal, all reviewers have lingering concerns about the evaluation protocol. In addition, there are remaining questions about the bonuses used; there are concerns that these only work for very specific domains. For these reasons, I m recommending rejection. I encourage the authors to carefully read the concerns of the reviewers about evaluation and consider using a different evaluation protocol for a future version of this work.
This paper addresses a meta learning method which involves bilevel optimization. It is claimed that two limitations (myopia of MG and restricted consideration of geometry of search space) that most of existing methods have can be resolved by the MBG with a properly chosen pseudo metric. The algorithm first bootstraps a target from the meta  learner, then optimizes the meta learner by minimizing the distance to that target under a chosen pseudo metric. The authors also establish conditions that guarantee performance improvements and show that metric can be sued to control meta optimization. All the reviewers agree that the idea is interesting and experiments well support it. Authors did a good job in the rebuttal phase, resolving most of concerns raised by reviewers, leading that two of reviewers raised their score. While the current theoretical results are limited to a simple case where L 1$, the method is attractive for meta learning community. All reviewers agree to champion this paper. Congratulations on a nice work.
This paper presents an empirical study which shows that pruning FBNets with larger capacity results in a model with higher accuracy than one searched via neural architecture search. The below are pros and cons of the paper mentioned by the reviewers:

Pros
  The observation that optimized architectures such as FBNets can benefit from pruning is interesting.
  The paper is well written and easy to follow.

Cons
  It is trivially known that training larger model and then pruning it will yield a better performing model, than training a smaller model from scratch.  
  The authors do not propose a novel pruning technique for optimized CNN architectures, and use existing pruning techniques for all experiments. 
  The experimental validation is only done with FBNets on ImageNet, and it does not show when pruning starts to break down. 

All reviewers unanimously voted for rejection, especially since the main “findings” of this paper that compact architectures can be further pruned down for improved accuracy/efficiency tradeoff, and that pruning a larger compact model results in models that outperform smaller models trained from scratch, have been already shown in many of the previous works on neural pruning. In fact, compact networks such as MobileNets and EfficientNets are the standard architectures for measuring the effectiveness of pruning techniques, and thus the contribution of this work reduces down to showing that the same results can be obtained with FBNets. This could be of interest to some practitioners, but is definitely not sufficient to warrant publication.
All reviewers consistently agree on the high quality of the research presented in this paper, such that it the paper clearly is significantly above the acceptance threshold of ICLR.
The paper considers the problem of controlling the dynamics of a networked dynamical system, under partial observations, considering a reduced order system from coarse data, and providing approximation bounds and an empirical evaluation.  Reviewers agree it is a borderline paper.  Technical results are nontrivial, and it introduces new questions, but the main contribution is rather narrow and it could be better written.
I thank the authors for their submission and active participation in the discussions. The majority of reviewers have concers with this paper, in particular, regarding the motivation of the method [dgHr], clarity [Mgm9], and theorethical support [4ENc]. I side with reviewers 4ENc, dgHr and fFaW, and recommend rejection of this paper. I want to encourage the authors to use the feedback by the reviewers to improve their paper.
In this paper, the authors leverage information gain in conjunction with Bayesian Neural Networks in order to to improve the robustness of Bayesian Neural Networks. However, as pointed out by reviwers, there are several mistakes in theier derivations and evaluations. Moreover, the authors failed to crrectly refer to the exisiting work proposing similar methods.
This paper proposed algorithms (based on natural actor critic methods) to solve two player zero sum Markov games. The authors established theoretical support for the convergence properties and hence sample complexity of the proposed methods. The authors claimed, based on their theoretical results, that the proposed methods are sample efficient. 

As the reviewers pointed out, the original submission focused on the dependency on epsilon without explicit dependency on other important parameters like S, A, B, etc. The revised version has made explicit the dependencies on all these problem parameters, which I appreciated. However, the sample complexity presented in the new version scales as either S^3 max{A,B} or S^4 * \max{A,B}^6 on the sizes of state space and action spaces, which are all huge. What is more, the sample complexities also rely on additional parameters like rho, x, y, which could all depend on S,A,B, etc. As a result, the resulting sample complexity bounds do not seem to imply sample efficiency. In addition, Assumptions 1 and 2 are somewhat unnatural to make.
The paper investigates weighted empirical risk minimization where the weights on an example in the training set is given by a polynomial function evaluated on the loss on the given example. Authors show that the choice of the weighting function induces a data dependent variance penalization in the training objective. Authors present an algorithm for weighted ERM and empirical results to support their claims. While the problem setting is broadly relevant and the approach the authors take in this paper is interesting, several questions remain unanswered. First, the authors argue that variance penalization helps but do not compare with other regularized ERM approaches. Second, it is not clear if the proposed algorithm is indeed gradient descent on the weighted ERM objective as pointed out by one of the reviewers. Finally, the writing can be improved with more emphasis on the novelty and significance of the contributions. I believe the initial comments from the reviewers has already helped improve the quality of the paper. I encourage the authors to further incorporate the feedback and work towards a stronger submission.
The authors develop a memory based method for continual learning that stores gradient information from past tasks. This memory is then used by a proposed task aware optimizer that, based on the task relatedness, aims at preserving knowledge learned in previous tasks.

The initial reviews were reasonable but indicated that this paper was not yet ready to be published. In particular, the reviewers seemed to agree on the somewhat limited methodological novelty of the paper given prior work (such as LA MAML and OGD in terms of method and GEM in terms of task similarity comparison).

In their response, the authors do seem to agree to a certain extent with some of the criticisms, but also point to clear differences with respect to previous work (and other distinguishing aspects such as a smaller memory footprint than OGD). The authors also carefully responded to reviewer comments and provided additional results when possible.

In the end, the main criticism from the reviewers remained (Reviewer 95tf also suggests that the authors should compare their method to others in terms of memory consumption (which the authors partly did) and compare to replay based methods) and this paper was a borderline one. Three, out of the four, reviewers suggest that it is not ready to be published. One reviewer did give it a high score (8) but also understood the limitations raised by the other reviewers. As a result, my recommendation is that this paper falls below the acceptance threshold. 

I am sorry that for this recommendation and I strongly suggest the authors consider the reviewer s suggestions in preparing the next version of this work. In particular, it seems like providing a full study of the memory usage of your approach vs. others as well as providing more insights about the "trajectory" (see the comment from ZR5n) might go a long way toward improving the paper.
This paper presents a new method for clustering multiple graphs, without vertex correspondence, by combing existing approaches on graphon estimation and spectral clustering. All reviewers agree that this is a neat paper with new theoretical and empirical results. The main concerns were also properly addressed during rebutal. Overall, it is a good paper.
This paper explores replacing the Gaussian noise typically used in diffusion based generative models with noise from other distributions, specifically the Gamma distribution. The effect of this change is studied empirically for both image and speech generation.

Reviewers welcomed the exploration of the design space of diffusion models, and several reviewers consider the study of alternative noise distributions in particular an important contribution. They also raised several issues with precision and clarity (several mistakes in the manuscript were pointed out), the quality of the experiments, and, especially, a lack of convincing motivation for this exploration / sufficient demonstration of its impact.

While the authors have made a significant effort to address the reviewers  comments and suggestions, which includes running additional experiments, all reviewers have nevertheless chosen borderline ratings, with half erring on the side of rejection, and the other half tentatively recommending acceptance.

I am inclined to agree that, as it stands, the benefit of the proposed change of noise distribution is not convincingly shown to outweigh the additional complexity this introduces, so I am also recommending rejection.
This paper proposes a hardware aware pruning method which structurally prunes the given deep neural networks to retain their accuracy while satisfying the latency constraints. Specifically, the authors formulate the latency constrained pruning problem as a combinatorial optimization problem to find the optimal combination of neurons to maximize the sum of the importance scores, and propose an augmented knapsack solver to solve it, as well as a neuron grouping technique to speed up the training. The proposed method is validated for its classification tasks on two devices, namely Titan V and Jetson TX2, and for object detection performance on Titan V, and is shown to achieve superior accuracy/latency tradeoff compared to existing pruning methods, including latency aware ones.

The paper received split reviews initially, and the following is the summary of the pros and cons mentioned by the reviewers.

Pros
  The proposed formulation of the latency constrained pruning problem as a constrained knapsack problem is novel. 
  The method achieves competitive performance against existing latency constrained pruning methods. 
  The paper is written well, with clear motivation and descriptions of the proposed method.

Cons
   The idea is not very exciting since posing pruning as a combinatorial optimization problem, or a knapsack problem is not new, and the proposed method only adds in additional latency constraints.
  The title “hardware aware” is vague and misleading since what the authors do are latency constrained pruning.
  The experimental validation is only done on two devices, which makes the method less convincing as a “hardware aware” method and how it generalizes to other devices (e.g. CPU, FPGA)
  Use of lookup tables to obtain the latency constraints is not novel, has a limited scalability, and is inefficient.  
  Missing discussion of design choices. 

During the discussion period, the authors cleared away some of the concerns, which resulted in two of the reviewers increasing their scores. However, one reviewer maintained the negative rating of 5, and the positive reviewers were still concerned with limited novelty.

I believe that this is a good paper that proposes a neat solution for latency pruning, which may have some practical impact. However, the novelty of the idea is limited, as pointed out by the reviewers. The use of lookup tables also does not seem to be an efficient solution for adapting to edge devices for which the collection of latency measurements could be slow. The experimental validation on only two devices of the same type (GPU) also seems insufficient, as how the method generalizes to diverse devices is uncertain. It would be worthwhile to consider using a latency predictor (e.g. BRP NAS [Dudziak et al. 20]), and perform experimental validation on diverse hardware platforms (e.g. CPU and FPGA). Comparing against recently proposed hardware aware NAS methods could be also interesting, as there has been a rapid progress on the topic recently.

Thus, despite the overall practicality and the quality of the paper, the paper may benefit from another round of revision, since both the method and the experimental validation part could be improved. 

[Dudziak et al. 20] BRP NAS: Prediction based NAS using GCNs, NeurIPS 2020
This work provides an empirical investigation on the adversarial attacking problem in deep neural networks. While it contains some interesting ideas, the work is still in the preliminary stage, lacking substantial support for the main points. Many of the ideas discussed in the paper have been explored in the past and hence more discussions on previous works would be needed. We encourage the authors to keep improving the work for future submission.
The paper addresses hierarchical kernels and provides an analysis of their RKHS along with generalization bounds and cases where improved generalization can be obtained. The reviewers appreciated the analysis and its implications. There were multiple concerns regarding presentation clarity, which the authors should address in the camera ready version.
I recommend acceptance. This paper presents an interesting "in between" of work on lottery tickets and work on supermasks, and I think it is sufficiently novel to merit acceptance even if the significance of the results will need to be left to the judgment of future researchers. The reviewers seem broadly in favor of acceptance, and I defer to their judgment as a proxy for that signal.

For a quick bit of context, work on "supermasks" (Zhou et al., 2019) has shown that randomly initialized networks contain subnetworks that can reach high accuracy without training the weights themselves. That is to say, within randomly initialized networks are high accuracy subnetworks. This work is interesting in its own right and has had a number of interesting implications for the theoretical community. This work derives from work on the lottery ticket hypothesis (LTH; Frankle & Carbin 2019), which shows that randomly initialized networks contain subnetworks that can train to full accuracy on their own. The key distinctions between these two kinds of work are (1) the LTH trains the subnetworks, while supermask work does not and (2) the LTH work requires that the subnetworks train to full accuracy, while work on supermasks obtain high (but not full) accuracy in many cases. No one approach is "better" than the other; they simply showcase different properties of neural networks.

As far as I understand, this paper creates space for an "in between:" high accuracy subnetworks are created by finding subnetworks at random initialization and flipping the signs of some of the weights to improve accuracy. This is a limited modification to the subnetworks that falls short of actually training them (LTH work) but is more than leaving them at their random initializations (supermask work). Doing so appears to produce subnetworks that perform better than in supermask work but with a lighter weight procedure than LTH work. The procedure for accomplishing this feat is different than either approach (using SynFlow to find the subnetwork and a binary neural network training scheme to find the signs), and there is probably significant room for improvement in this new algorithmic space (just as there was for both LTH and supermasks).

This is novel and interesting, and I defer to the reviewers who find it worthy of acceptance. I have reservations about the eventual significance of the work, but that determination will be made by future researchers.
In recent years, artificially trained RNNs have been used for studying systems and behavioral neuroscience in terms of their learned representations, dynamics, computation, and the learning process itself. This paper contributes to further identify learning principles that may be revealed by curricula. The proposed approach for finding signatures of different loss functions is a novel and interesting idea which is very well fit for the neuro oriented ICLR audience and has potential impact in other fields.
The paper proposes a new pipeline parallel training method called WPipe. WPipe works (on a very high level) by replacing the two buffer structure of PipeDream 2BW with a two partition group structure, allowing resources to be shared in a similar way to PipeDream 2BW but with less memory use and less delays in weight update propagation across stages. The 1.4x speedup it achieves over PipeDream 2BW is impressive.

In discussion, the reviewers agreed that the problem WPipe tries to tackle is important and that the approach is novel and interesting. But there was significant disagreement among the reviewers as to score. A reviewer expressed concern about the work being incremental and difficult to follow. And while these were valid concerns, and the authors should take note of them when revising their paper, I do not think they should present a bar to publication, both based on my own read of the work and also in light of the fact that other reviewers with higher confidence scores did not find novelty to be a disqualifying concern. As a result, I plan to follow the majority reviewer opinion and recommend acceptance here.
This paper presents a variant of SARAH, which employs the stochastic recursive gradient and adjustable step size based on local geometry. The main concerns about this paper include (1) the empirical comparison with other algorithms might not be fair (which is arguable); and (2)  the theorem proved in the paper is for a simplified algorithm rather than the algorithm used in the experiments. Even after author response and reviewer discussion, this paper does not gather sufficient support from the reviewers. Thus I recommend rejection.
This paper studies generalized label smoothing (GLS), which unifies positive label smoothing (PLS) and negative label smoothing (NLS), and studies its connections to existing loss functions. It also shows the benefit of NLS in the high noise regime.

Although the reviewers acknowledge that the idea of NLS in this paper is interesting, they also expressed the concerns that: the practicality of GLS is not thoroughly evaluated against prior works; the empirically best setting of parameter r is only verified in a limited number of datasets; the theoretical results  difference with prior works is limited. We encourage the authors to take the reviewers  feedback to strengthen the paper in the next iteration.
This paper proposes a model selection technique for classification problems, called mutated validation (MV), based on randomizing the labels of the training set. The idea is interesting but not well served by the presentation of the objectives and the experimental results provided in the paper. The discourse is confusing because it is not clear what is the purpose of model selection here. Usually, model selection aims at finding a model with lowest generalization error. This is a well defined goal, and the performance regarding this goal can be measured by the expected test error that is usually estimated from a test set. In this paper, the goal of model selection is not precisely defined, but it cannot be as usual, since some models that minimize test error are said to overfit. Most experimental results show that models selected by MV differ from the ones selected by cross validation (CV), without providing an objective measure of the relative merits of MV and CV (see details below). Although the authors do not make this clear in the paper, they argue in the discussion that the merit of their approach is to select a "simple" model that avoids overfitting. Their message should be clearly stated in the paper, and it should be supported by experiments displaying simplicity *and* test error, or by any experimental result showing the objective benefits of the proposed MV, possibly combined with CV as some of the reviewers suggested. I therefore recommend rejection, but with encouragement to pursue this work, by defining precisely the formal objectives pursued in this work with model selection, and by measuring the benefits of MV on these formalized objectives.

Details:
The arguments in Figure 2 and Table 2 are not substantiated and are clearly not applicable to a model selection mechanism that would aim to select the model with the lowest expected error (or complexity). The "obviously ill fitting rectangle shaped decision boundaries" are perfect with respect to expected test error and simply described in Occam s razor terms. 
Similarly, most results are subjective:
   Figure 3 shows that MV and CV are different (on the left), and that CV is a better estimate of the test error (on the right).
   Figure 4 shows some differences between CV and MV, with no clear way to judge which would be better.
   Table 4: variance says nothing about relevance; choosing an arbitrary value before seeing the data gives a zero variance.
   Figure 6, 7: again, subjective result
   Figure 8: no comparison
   Figure 10: label swapping or random label replacement is the same in binary classification, there is just a difference in the parameterization (swapping 20% of the labels is the same as randomizing 40% of the labels).
   Figure 12: I am not sure what is drawn here, but it seems to be related to the training error only.
   
Regarding objective test results:
   Figure 5 is an experimental result that shows objective differences between CV and MV, and appear to be marginally in favor of MV for selecting the model with lowest test error (in 3 out of 8 graphs). However, there is no joint optimization on the 2 hyper parameters: there is not a single data set for which the best value of the test error is identical in the two graphs (wrt dropout, wrt learning rate). In other words, the graphs for dropout rate and/or learning rate are provided for a suboptimal choice of the other hyperparameter.
  Figure 9: CV is more closely related to the test error than MV.
  Figure 11: MV chooses models that do not achieve the highest test accuracy.

As a side note, I think that the proposition could be also positioned with respect to papers that presented similar ideas, where model selection is based on stability, either with unlabeled examples [1] or by modifying the training set [2].
 
[1] Dale Schuurmans, Finnegan Southey: Metric Based Methods for Adaptive Model Selection and Regularization. Mach. Learn. 48(1 3): 51 84 (2002)

[2] Olivier Bousquet, André Elisseeff: Stability and Generalization. J. Mach. Learn. Res. 2: 499 526 (2002)
The paper aims at characterizing conditions for optimal representations required for the domain generalization problem under covariate shift. Under the Idealized Domain Generalization (IDG), the paper provides a variational characterization of the optimal representation and shows a number of intriguing results: (i) optimal representation should remain discriminative  across domains, (ii)  the representation’s marginal support needs to be the same across source and target. (ii) It is also shown that without any target information, no representation can do uniformly well over constant representation, thus supporting the necessity of target knowledge. Finally, the paper provides practical objectives of the proposed variational characterization by self supervised learning using data augmentation with experimental results. 

The reviewers had raised a number of concerns, many of which were alleviated through the responses provided by the authors. All in all, in the discussion period, the reviewers indicated the novelty of the results and their importance in learning good representations for the domain generalization problem. The reviewers still had reservations about the following points, and I strongly recommend to the authors to address these points in the revised version (please see the reviews for more details): 

(i) For the experiments, it is clear that there is a noticeable gap between the derived theory and the experimental results. It was argued that alt text augmentation of CLIP is one of the practical choices for (approximate) domain agnostic augmentation, but it is difficult to verify this.  

(ii) The empirical gain for the proposed method over CLIP is not very significant. As shown in Table 1 (or the complete results in Table 4), the performance of the proposed method is tightly related to the original CLIP method. In the case where the pre trained representations of CLIP fail to cover the target set (see the TerraIncognita column in the table), the proposed method can be much worse than other alternatives. Thus I m worried about the usefulness of the proposed method in practice.

(iii) One of the reviewers had asked about the importance of the necessity condition and its implications (e.g. in practice). Please, make sure to address this in the final version. 

Also, there has been several recent works on learning disentangled representations for the domain generalization problem, using e.g. weakly supervised approaches (Matsuura et al,  20), or model based approaches (Robey et al,  21). It would be interesting to see how the results/ideas in this paper would connect to/improve those settings.
This paper shows that (under some parameter range) graph convolutional networks learns communities in the stochastic block model. The result is clean, the proof techniques rely on partitioning neurons of three types and seems applicable to more general settings. The reviewers agree that the main theorems are interesting. There are some concerns among reviewers about the presentation of the paper, but many of them seem to be already addressed in the revised version, and I would recommend the authors to continue to improve the writing. There are also some concern about experiments, but the experiments are mostly used to validate the theorems, so clarifying how they are related would suffice. Overall the paper seems to have an interesting theoretical result on GCN.
The reviewers acknowledge that the paper is well written and contains interesting ideas to combine adaptive control and learning. However, they identified issues regarding the claims about transient tracking and STL formula. Moreover, the significance of the presented learning rule was unclear regarding one reviewer. While the authors could respond well to the identified transient tracking issue, they also needed to weaken their claims, limiting the contribution of the paper. The reviewers therefore stayed with a a reject rating.
The manuscript introduces a taxonomy for organizing continual learning research settings and a software framework that realizes this taxonomy. Each continual learning setting is represented by as a set of shared assumptions (e.g., are task IDs observed or not) represented in a hierarchy, and the software is introduced with the hopes of unifying continual learning research.

The manuscript identifies a clear issue in the field: settings and methods for continual learning have proliferated so that there is little coherence in benchmarks, making progress difficult to judge. Reviewers generally agreed that the motivation of building software to help unify continual learning research was a positive.

However, reviewers also pointed to many concerns with the manuscript and software package (Sequoia) that comprises its main contribution. In particular, there is concern that the software is at an early stage of development and makes heavy use of existing libraries to function (e.g. Avalanche and Continuum). This makes it unclear what Sequioa offers over using its dependencies directly. As well, there is concern that multiple standard benchmark tasks and common methods are missing from the implementation — particularly for large scale experiments with, e.g. ImageNet 1k. In theory, the library allows extension and these might be implemented by others in the community. However, this would require that the original manuscript+software are strong enough to draw buy in from other researchers. 
In sum, the manuscript+software does not yet offer a convincing starting point for researchers looking for a starting point to begin their continual learning research.
This paper deals with segmentation of time series. The paper has received quite detailed reviews and the approach seems to have several interesting aspects (interesting architecture choice, stepwise classification approach, ability of capturing long range dependencies). However, there is a consensus that the paper would definitely benefit from a further iteration before publication in ICLR or in any other similar venue. The authors in their final response have already identified the improvement points raised by the reviewers. In addition to these, I believe it would be helpful to put the contributions better into perspective with existing literature. I think all these this would require a major rewrite and I encourage the authors to make a fresh submission in a future venue.
This paper presents a batch active learning approach (where in each active learning round, instead of a single input, we wish to select several inputs to be labeled). The paper attempts to solve this problem by posing it as a sparse approximation problem and shows that their approach performs favorably as compared to some of the existing methods such as BALD and Bayesian Coresets for batch active learning.

While the reviewers appreciated the basic idea and the general framework, there were several concerns from the reviewers (as well as myself upon reading the manuscript). Firstly, the idea of batch active learning as a sparse subset selection problem is not new (Pinsler et al, 2019). While previous methods such as (Pinsler et al, 2019) have used ideas such as Coresets, this paper uses sparse optimization techniques such as Greedy and IHT. Moreover, there were concerns about experimental settings relying on various heuristics, and lack of a more extensive and thorough comparison with important baselines, such as BatchBALD and others, which the authors acknowledged.

The reviewers have read the authors  response and engaged in discussion but their assessment remained unchanged. Based on their assessment and my own reading of the manuscript, the paper does not seem to be ready for publication. The authors are advised to consider the points raised in the reviews which I hope will help strengthen the paper for a future submission.
In this paper, the problem of estimating the average of a moment function that depends on an unknown regression function.
It heavily relies on prior papers by e.g. Chernozhukov et al. and the actual novel material consists of making these theoretical 
results more practical. Experiments for two practical approaches based on neural networks respectively random forests are also reported.

Initially, the presentation of the paper was heavily criticized by the reviewers, but during the rebuttal phase at least some of the issues were removed. Together with some other improvements this lead to an increased average score. However, it seems fair to say that reading the other papers first, is still kind of necessary.

Despite the still unclear novelty the paper has some merits, which in principle make it acceptable. Compared to the other good papers in my batch, however, it is more incremental and the overall contribution is not as strong. For this reason I vote for rejection, but a comparison to other papers outside my batch is probably a good idea.
This paper explores the idea that fixational drift of a sensor over an image (something that primate eyes do) could be used to achieve visual hyperacuity, i.e. image recognition with low resolution images equivalent to what would be achieved with high resolution images. The authors construct networks where the bottom of a deep convnet is replaced by recurrent networks and the network is then trained on low resolution versions of high resolution images that are sampled with fixational drift across the image. The authors show that this approach allows their system (dynamical recurrent classifier, or DRC) to get much better classification performance on CIFAR images than can be achieved without the early recurrence and drift. The authors also show that the most robust classification mandates drift trajectories with higher curvature, and they show that this matches some of the properties of visual drift trajectories in humans. 

The reviews on this paper were highly divergent (ranging from 3 to 10). Three of the reviewers felt this paper should be rejected, but one felt very strongly it should be accepted. The primary concerns from the negative reviewers were lack of appropriate controls, lack of insight into why the system works, lack of appropriate references to past work, and lack of connection to biology. The authors made a very concerted effort to attend to all of the reviewers  comments. They ran all of the requested control experiments, updated the text to better reflect past literature, and included some comparison to psychophysics data. In the end, only one reviewer increased their score, though, leading to final scores of 3, 10, 5, and 3. Discussion did not lead to any more consensus. 

Thus, this paper was still very much in the borderline zone, and required AC consideration. After reading through the paper, reviews, and rebuttals, the AC felt that the authors really had addressed the primary concerns as best as could be hoped for in the time frame for ICLR, and that the paper was sufficiently interesting and informative for ML and neuroscience to be worthy of publication. Some of the negative review points stand, e.g. there are still some mysteries as to why this works and there is certainly a lot more that could be done to make this paper informative for neuroscience. Nonetheless, in total, the AC felt that this paper deserved to be accepted, given that the authors did most of what the reviewers requested of them.
The paper addresses safe multi agent reinforcement learning and makes two key contributions. First is a safety concerned multi agent benchmark, which is an extension of MAMuJoCo. Second, is the formulation and two solution to safety MARL problem. The authors pose safe MARL, and MARL problem with safety constraints, as a constrained Markov game.

The safety constrained MARL is an important, difficult, and understudied problem. The problem is more difficult that the single agent safe RL because of the non stationarity in the MARL setting, which renders any theoretical guaranties conditioned on the assumptions of the behaviors of other agents. The authors are right to point out the lack of the benchmarks in the space. 

That said, reflecting on the reviewers  feedback and my own reading of the paper, this paper is attempting to do too much (benchmark, problem formulation, and two methods), in too little space, and is falling short. For example, the benchmark is an important contribution, but it is barely mentioned in the main text of the paper. If this was fully safety benchmark paper, there is an opportunity to go beyond MAMuJoCo, which feels like a forced multi agent problem, and construct a safety benchmark with energy constraints, cooperative and competitive tasks etc... If this was fully methods paper, there would be an opportunity for more in depth analysis of the results that the reviewers  pointed out. In it s current form, the paper feels like proposing a benchmark not grounded in a real world problem, and then a method to solve the problem.

I would suggest the authors to either:
  submit the paper to a journal where a space constraint would not be in a way, or
  split it into two papers, a more comprehensive benchmark, and methods paper evaluated on more difficult problems. 

Minor:
  Please update the literature. Some of the papers have been published, and they are cited as Arxiv papers.
Meta Review for Neural Circuit Architectural Priors for Embodied Control

The motivation of this work is to address an important challenge: To understand innate contributions to neural circuits for motor control. This paper proposes both a set of reusable architectural components and design principles, and also interesting principles for producing biologically inspired neural networks for embodied control. This work aims to be at the intersection between neuroscience and machine learning for improving the design of artificial neural networks and improving our understanding of observed biological networks. In their model, various components of biological networks are replicated (such as the balance between excitation and inhibition, sparsity, and oscillation). They show that a resulting model, inspired by C.elegans, can learn to swim more efficiently (when evaluated on the Swimmer RL environment) and requires fewer parameters while achieving similar accuracy as an MLP.

Most reviewers, including myself, recognize (and appreciate) the ambition of this work, and are excited at the goal of looking at problems from the perspectives of both system neuroscience and machine learning. The motivations of this paper are clearly explained, and the paper is well written (also the diagrams are great). I m very excited about this work, and hope to see it succeed, but in the paper s current state (even with the revisions), I don t think it addresses the reviewers  main concerns.

After discussions and examining the paper and the reviews in detail, I feel reviewer GaKc best summarizes the main issues with the work at its current state:

  This paper is interesting but does not proposes a significant improvement to the literature as the gap between the promises made in the motivation and actually delivered work is too wide.

  From a neuroscience point of view, this work does not provide substantial evidence of the importance of the model at either modeling or simulating biological neural systems.

  From a "theoretical" point of view, the model does not provide much advancement to the machine learning community either.

So while the current work (especially in the revised state) I would consider to be an outstanding workshop paper, I cannot recommend it for acceptance at ICLR 2022. An advice I would give to the authors (as someone who publishes to ML conferences, and Sys Neuro/Bio venues) is that for these ML conferences, it might be easier to make the narrative of the work narrower, and well defined. If the method is supposed to demonstrate significant advantages of biologically inspired network architecture over current RL methods, the results should clearly demonstrate convincing experimental results that can persuade the (non neuro) RL community to have interest in the method. If the method does not achieve SOTA results, then try to present the method capable of something really useful that existing RL methods simply fail at (and emphasize that as a core contribution). Conversely, if the narrative is to use a bio inspired network to emulate biological behaviors, the method must have something important to offer for the community of people working on simulating biological neural systems. 

I look forward to seeing this work improved and eventually published at a journal or presented at a conference in the future, good luck!
Reviewers were in agreement but borderline.  The paper has a nice hypothesis and develops the work using two realistic datasets, Wikipedia and Code.  One reviewer was initially more negative but changed their views based on the authors improvements to the paper.
The idea is fairly simple, but does require modellers come up with the structural features.  There was discussion that more down stream tasks are needed to highlight the approach.  Moreover, more datasets should be experimented with.  In all, experiments are good but improvement is easily done.
The submission introduces the sparse hierarchical table ensemble (S HTE), based on oblivious decision trees for tabular data. The reviewers acknowledged the clarity of the presentation and the importance of the computational complexity analysis. However, they also raised concerns regarding the novelty of the proposed method and the significance of the results compared to competing methods (e.g., CatBoost). Given the consensus that the submission is not ready for publication at ICLR, I recommend rejection at this point.
This paper presents a decentralized cooperative approach in multi agents using Markov games theory. After reviewing the paper and reading the comments from the reviewers, here are my comments:
 
  The paper is well written, quite difficult to follow, but very informative.
  The contribution is clearly stated and the results support it.
  Theoretical results are interesting for the RL community.
  The main concern is about learning the epsilon approximate Nash equilibrium policy which is a fundamental part of the paper.
The authors’ present a precise definition of deployment efficient RL, where each new update of the policy may be costly, and theoretically analyze this for finite horizon linear MDPs. The authors include an information theoretic lower bound for the number of deployments required. The reviewers found this an important setting of interest and appreciated the theoretical contributions. The authors’ carefully addressed the raised points and also addressed questions about deployment complexity and sample complexity in their revised work. One weakness of the paper is that it does not provide empirical results and the linear MDP assumption, while quite popular in theoretical RL over the last few years, is quite restrictive. However,the paper still provides a very interesting theoretical contribution for an important topic and I recommend acceptance.
The title of the paper nicely summarizes the main goal of the paper and the abstract does the same for the achieved results. For this reason I abstain from providing another summary. 

The initial reviews were somewhat mixed but during the discussion phase, a lot of questions have been resolved so that actually three reviewers updated (upgraded) their score. Remark 14 certainly needs to be updated according to the discussion in the final few days of the rebuttal phase. In addition, one reviewer pointed to a naive application of Mercer s theorem. This should be addressed as well, either by restricting to compact domains and continuous kernels as suggested by the reviewer, or by considering generalizations as done by e.g. the cited Fischer and Steinwart. Finally, the cited survey by Kanagawa et al also contains some information on learning curves and thus it should be cited more prominently, e.g. around Remark 14.

In any case, this paper is above the acceptance threshold.
3 reviewers recommend accept, 1 rates the paper marginally above acceptance. The authors provided satisfactory answers to criticism   all in all this is a paper worth accepting at ICLR. Please make sure that criticism in the reviews is adequately addressed in the final version, e.g. include various experimental results in the rebuttal, add the symbols in sec 3.2 & 3.3 to fig. 2, add a related discussion on ablations when the model is fully trained, etc.
The paper proposes a conditional generative adversarial network with an auxiliary discriminative classifier for conditional generative modeling. The auxiliary discriminative classifier can provide the discrepancy between the joint distribution of the real data and labels and that of the generated data and labels to the generator by discriminatively predicting the label of the real and generated data. Experiment results are provided to demonstrate the effectiveness of the proposed idea.  The current paper receives mixed ratings after rebuttal (5, 6, 5, 8). Except that one reviewer (the Reviewer uPwH) will champion the paper with a score of 8, the concerns of the other three reviewers remain. To be specific, even though Reviewer ebJs assigns a score of 6, he/she doesn’t champion the paper because additional experiments requested are not provided by the authors, including (i) training on more datasets or higher resolutions, (ii) visualizing feature norm and grad norm as done in ReACGAN, (iii) experiments on ADC GAN without unconditional GAN loss. The Reviewer DPgR pointed out that the paper might have a novelty issue because it bears some similarities with other works but it lacks a discussion in the revision. Additionally, Reviewer mZT7 pointed out that the authors didn’t provide a revised paper during the rebuttal, thus leading to a difficulty to assess the quality of the final paper. As a result, AC thinks that the paper is not ready to publish at the current stage and recommends a rejection.  The AC urges the authors to revise their paper according to the comments provided by the reviewers, and resubmit their work in a future venue.
The authors build an encoding model of whole brain brain activity by integrating incomplete functional data with anatomical/connectomics data. This work is significant from a computational neuroscience perspective because it constitutes a proof of concept regarding how  whole brain calcium imaging data can be used to constrain the missing parameters of a connectome constrained, biophysically detailed model of the C. elegans nervous system. There were issues related to clarity in the initial submission which all appeared to have been addressed in the final revision. This paper received 3 accepts (including one marginal accept) and 1 reject. The paper was discussed and the reviewers (including the negative reviewer) were unanimous that the current submission should be accepted.
The paper investigates the interesting problem of the local intrinsic dimension (LID) of graphs, and interpreted the GNN learning from Feature LID (FLID), Structure LID (SLID), and Representation LID (RLID). The concepts are novel but the paper needs better insights on how LID can improve graph learning and stronger empirical evidence to support their claims.
This paper builds on the success of the FermiNet neural wave function framework by pairing it with a graph neural network which predicts the parameters of neural wave function from the geometry. The resulting PESNet trains significantly faster, with no loss of accuracy. This method constitutes an important advance in ML powered quantum mechanical calculations.

The reviewers unanimously recommend acceptance.
This paper links OOD generalization with adversarial training and argues that adversarial training can help address the problem of OOD generalization. Based on all responses and reviews, there still are novelty concerns in this paper. In the meantime, this paper lacks theoretical justifications. More importantly, DAT only considers very limited situations regarding DG, which also reflects on its experimental results. In the following, I summarize the drawbacks of this paper for the possible revision in the future.

1. It seems not novel to link AT with OOD generalization since two reviewers show some references related to using AT to address DG.

2. From Eq. (9), DAT is based on perturbations rather than transformations. This means that DAT only considers very limited situations regarding DG. In the ordinary DG, source and target domains are from the same meta distribution, which is clearly a more general case compared to the case considered in this paper. DAT based AT might mislead the research direction of DG. It would be better to consider smart ways to generate adversarial examples, such as "Pixeldefend: Leveraging generative models to understand and defend against adversarial examples" (ICLR2018).

3. Eqs. (7), (8) and (10) are not rigours. It is not convincing to propose a method based on these formulas.

4. The method doesn t show substantial improvements compared to ERM in most tasks (CMNIST dominates the average), which implies the limitations of DAT (see 3).

5. There are no theoretical contributions regarding DG. This paper does not mention the key assumption behind the DAT. For example, DG can be a well defined problem if source and target domains are from the same meta distribution. However, this paper does not clarify what assumptions it assumes and does not show how DAT can address DG in theory.

Based on the above drawbacks, I recommend rejection for this paper.
The paper tackles the problem of missing data in centralized training multi agent RL approaches. The authors propose 1) using generative adversarial imputation networks for imputing missing data and 2) discarding training data where data from multiple consecutive timesteps is missing.

Reviewers agreed that the problem of missing data in multi agent RL is interesting. At the same time, several reviewers shared two main concerns about the experimental evaluation:
* The lack of comparisons to baselines other than MADDPG, especially decentralized critic approaches.
* The lack of experiments on non toy domains such as SMAC.

The author response did not sufficiently address these concerns leaving the reviewers in agreement that the paper should not be accepted without these additional experiments.
This paper claims a practical improvement over one of earlier meta BO methods. Warm starting BO or HPO by making use of data from past experiments or tasks seems to be interesting and useful for some applications. In fact, there are a large amount of work on this topic, but a lot of relevant prior work is ignored in this paper unfortunately. I appreciate the authors for making efforts in responding to reviewers’ comments. However, after the discussion period, most of reviewers had serious concerns in this work, pointing out that the proposed method is rather trivial and the comparison is made only against a simple baseline. It was also suggested to improve the experiments. While the idea is interesting, the paper is not ready for publication at the current stage.
This work tries to extend mixup to graph structured data, where graphs can differ in the number of nodes, and the space is not Euclidean.  This is achieved by G Mixup, which interpolates the generator (graphon) of different classes of graphs through the latent Euclidean space.  Experimental results show some promise.

Several concerns have been raised by the reviewers, and although the rebuttal helped, some concerns remain.  For example, how to confirm that the graphon can be accurate estimated.  Several weakness in experiment is also raised, and a revision is needed before the paper can be published.
This paper proposes a method to train autoregressive model that takes advantage of a well designed energy based learning objective model. With the importance sampling, the model can be trained efficiently without requiring an MCMC sampling. Experiments are conducted to verify the effectiveness of the proposed method.  The idea is interesting and well motivated, but the experiments need to be improved. Reviewer FnWE’s major concerns include limited novelty, lack of discussion with closely related works, and insufficient experiments, and recommend rejecting the paper by assigning a rating of 3. Rebuttal doesn’t address his/her concerns. Reviewer in11 is concerned with the computational cost and training instability due to the extra EBM module and has a few unclear technical details that need to be clarified. The author’s reply along with additional experiments during rebuttal partially addresses the concerns of Reviewer in11, who eventually increases the rating to 6. Reviewer AQxn’s major concern is also about the lack of sufficient comparison with other relevant energy based models.  Reviewer DZsJ pointed out that the more insightful analysis about the model is missing in the experiments. Even though the authors provide additional experiments for Reviewer DZsJ, they are not satisfied with the feedback because the additional results are not supportive of the claims made in the paper, and end up with a rating of 6. Reviewer SjXn’s concerns include the lack of comparison with relevant works and the unclear motivation of the design of the joint distribution. After the rebuttal, Reviewer SjXn’s concerns remain and assign a rating of 5 to the paper. The overall rating of the paper after rebuttal is marginally below the acceptance rate. Even though this paper proposes an interesting idea, the reviewers’ comments are not well addressed. As a result, AC cannot recommend accepting the paper.  The AC urges the authors to revise their paper according to the comments from the reviewers, and resubmit their work in a future venue.
This paper suggests a new technique to utilize generative replay for continual learning. Specifically, the authors claim that even though the generated samples are imperfect (thus cannot be used as positive samples for old classes), they can still be used as negative samples for the current class. 3 reviewers are negative and 1 reviewer is positive. The main concerns of negative reviewers are (a) non ablated effects of baseline and proposed components, (b) insufficient analysis of negative replay, and (c) no assessment of generated data quality. The rebuttal provides an additional experiment to address the issue (a), but the reviewers and AC think the experiments should be better polished. Also, AC believes the issues (b) and (c) should be better analyzed. The rebuttal claims that issue (c) is not applicable as they generate samples on the latent space. However, the main motivation of the paper is the low quality of generated samples, and the paper should provide a quality measure to support their claim. For example, an update of the feature extractor may move the latent space generative replay to the wrong class (i.e., low quality), and thus one should not use it as positive but only as negative, as suggested in this paper. Here, the negative replay would increase the margin of current and old classes, enhancing the accuracy of the current class. To analyze the source of benefits (old vs. current classes), the authors could report the task wise accuracy trends, not only the overall accuracy. It would be a nice addition to the issue (b). Due to these unresolved concerns, AC tends to recommend rejection.
This paper presents a new formulation for the infinitely wide limiting case of deep networks as Gaussian processes, i.e. NNGPs.  The authors extend the existing case to incorporate a scale term at the penultimate layer of the network, which results in a scale mixture of NNGPs or a Student t process in a specific case.  This formulation allows for a more heavy tailed output distribution which e.g. can be more robust to outliers.  The four reviews averaged just above borderline, with a 5, 8, 6, 6.  The reviewers found the approach to be sensible, technically correct and timely given the recent literature.  They found the experiments to be compelling for the most part, demonstrating the added robustness of this approach over the baseline NNGP.  The main concern raised by the reviewers is that the work is incremental, given that both NNGPs and Student t processes are already established.
The paper considers the question of identifying bad data so that models can be trained on the subset of data that is good. This question is formulated as a utility optimization problem. The paper shows that some popular heuristics are quite bad in the framework they propose. They also propose a new algorithmic framework called DataSifter. There is empirical evaluation provided for this. Questions have been raised in the reviews about the size of the models that have been used in the empirical evaluation. The authors have responded to this by suggesting the use of proxy model techniques. There are also questions about learnability of data utility for which some responses are provided in the rebuttal.
This work proposed a nested evolutionary algorithm to choose image filters and filter parameters for back box attacks, with the emphasize of high transferability. 

After reading the manuscript, the comments of reviewers and the authors  responses, I think the main issues of this work include: 
1. The limited novelty of the main idea, since there have been many filter based attacks, and this work is very close to an existing work;
2. The solution is not new, since the evolutionary method is also well adopted in adversarial attacks; 
3. Many many black box attack methods are not cited and compared, though the authors argued that their perturbation upper bound are different such that they cannot be compared, which is not convincing; 
4. The claimed high transferability is not well explained, maybe due to the model ensemble (as indicated by reviewer eN8o). Besides, many existing works that studied transferability are not cited and compared. 
5. Experiments are inadequate. The authors added some results in the revised version, but the current shape is still not ready for publication. 

Thus, my recommendation is reject. Hope the reviews can help to improve this work in future.
This paper proposes a new distributional assumption and a new algorithm for learning convolutional neural networks. However, the reviewers reach a consensus that this paper s assumptions are not natural and may not be satisfied in real world domains. The meta reviewer agrees and thus decides to reject the paper.
The paper applies proximal iteration to Q learning, which significantly improves the performance of DQN. Reviewers agreed the paper is not ready for publication, for a couple reasons. DQN is quite far from current state of the art. Improvements therefore need to be well founded to be of broad interest. If the algorithm that is being improved is not competitive, there should be more general lessons that can be extracted from how and why the improvement works. Unfortunately, the reviewers felt that there was insufficient understanding of why proximal iteration helps.
This submission proposes "Mako", which enables continual learning when only a limited amount of labeled data is available (along with a good deal of unlabeled data). Reviewers shared concerns about difficulty in understanding which components of the proposed system were novel, especially given that the most important components seemed to be proposed in past work. Reviewers also had difficulty getting insight on which parts of the system were most useful, and further requested additional experiments on harder benchmarks. There consensus was therefore to reject the paper.
This paper studies the following hypothesis that gradient based explanations are more meaningful the more they are aligned with the tangent space of the data manifold. The reviews are negative overall. The general feeling is that the paper reads like a set of subjective observations about the meaningfulness of explanation and relationship with data manifold + tangential theory. There isn’t a coherent story.
This paper received six reviews, consisting of three 8s two 6s and one 3.
The reviewers generally felt that the proposed Electra like pretraining provided fairly significant downstream improvements.
Additional ablations were provided to during the author response period and other author responses were sufficient to cause scores to rise during the discussion period.
The vast majority of reviewers recommended accepting this paper and the AC also recommends acceptance.
This paper adapts the idea of progressive growing of GANs to time series synthesis. The reviewers thought that the idea was well motivated. DRP7 initially expressed concern w.r.t. novelty. They were also concerned with the lack of certain baselines. The authors responded, highlighting its contributions w.r.t. Evaluation (Context FID score) and extensiveness of the evaluation. The authors also added missing references but pushed back on the additional baselines. DRP7 raised their score.  Reviewer pbaT was also positive about the work though had some questions and suggestions for improving clarity. They had initially given a low score for “correctness” but raised this, indicating they were satisfied their clarity concerns were addressed. Reviewer RLDM (whose code name happens to match a ML conference) thought the work was novel and appreciated the introduction of a new metric for evaluating the quality of generated time series data. They remarked on the thoroughness of the experiments and the quality of the presentation. They asked some clarifying questions to which the authors provided a response. Reviewer 4v5L also had a concern with novelty, felt the loss function was “heuristic” and didn’t see the utility of the FID based score. They also presented several clarifying questions. The authors provided a lengthy response to that reviewer’s concerns, having run additional analysis, and the reviewer upgraded their score in response. With all reviewers on the accept side of the fence I am inclined to recommend acceptance. Please note 4v5L’s comment that “the paper still needs significant edits to reflect the points in the reviewer responses”.
The paper introduces some interesting ideas on how use causal random forests for conditional average treatment effects (CATE), with respect to some baseline treatment level ("0"), when the treatment variable is continuous. Figure 1 summarises the scope of the paper neatly. Scalability issues are also considered.

I think this *is* a paper "nearly there" in terms of a impactful contribution. The main issues are some presentation kinks and extra steps in the theory. I think the very low scores from the reviewers are not quite representative of the overall quality (I would be more generous). However, I m afraid I m also inclined towards a reject. The paper neglects some other developments on ML for CATE with continuous treatment e.g. Bica et al. s "Estimating the Effects of Continuous valued Interventions using Generative Adversarial Networks" (NeurIPS 2020) and the references within. A focus on the theory would help to differentiate it, but I m not that confident that the results are currently mature enough to claim them.

Although I suggest a rejection, let me make clear I strongly encourage the authors to further pursue their ideas. You are doing good work, and the next iteration might nail it. As you found out in the discussion, emphasise the continuous aspect of it. I d also emphasise the fact that you have a clear setup of the problem in terms of the contrast wrt to a baseline treatment effect instead of some generic contrast function. People in ML tend to be oblivious to such a setup, but I m not convinced you are properly exploiting it.
The reviewers recommended rejection. There was no reply from the authors. The main weaknesses are:
  No experiment on real life dataset (only simulated)
  Unsubstantiated claims about the literature 
  No discussion on the time complexity
  Incremental contribution
The weaknesses of the paper can briefly be summarised as follows: i) the suggested motivation is not so clear, and in addition the experimental results (by themselves questionable in the way they are obtained) do not support the main claim of the paper that "...edges are generated by aggregating the node interactions over multiple overlapping node communities, each of which represents a particular type of relation that contributes to the edges via a logical OR mechanism." In fact, the observed separation among components is not proven to be of the predicted nature. ii) empirical results are obtained using a deprecated experimental protocol. For the field to make real progress, experimental assessments should follow statistically sound protocols. Already published papers that were not following a sound protocol should not be taken as reference for future empirical assessments.
The last point alone is a strong motivation for rejecting the paper.
The authors make an experimental case that dropout aids generalization by
promoting "flatter minima".

The reviewers felt that the work reported in this paper makes a useful step
forward on a question of central interest.  The consensus view was that the
total weight of evidence presented was not sufficient for publication in
ICLR.  The paper could be strengthened was more extensive and varied experiments
and/or theoretical analysis.
All reviewers agree that this paper does not meet the bar for ICLR. The reviewers provide detailed feedback to the authors on how to improve the writing as well as the overall content of the paper.
This paper tackled the reward shaping problem under the framework of Markov games. The authors proposed reward shaping algorithms for RL with mild theoretical guarantees. The AC agrees with the reviewers that the empirical performance is ambiguous. The paper should be substantially improved before being accepted.
This is a strong empirical paper that studies scaling laws for NMT in terms of several new aspects, such as the model quality as a function of the encoder and decoder sizes, and how the composition of data affects scaling, etc. The extensive empricial results offer new insights to the questions and provide valuable guidance for future research on deep NMT. The datasets used in the study are non public, which may make it hard to reproduce the evaluation.
This paper provides a novel path auxiliary algorithm for more efficiently exploring discrete state spaces within a Metropolis Hastings sampler for energy based models. In particular, it essentially replaces the "single site update" by instead proposing an entire path using local information, thus enabling the chain to take larger steps, which can improve acceptance/mixing significantly as they demonstrate. The work is a timely contribution that improves upon exciting recent work. After much discussion among several knowledgeable reviewers and clarifications regarding some details of the main theorem from the authors, there is consensus that the contributions are correct, novel, and likely of impact to the machine learning community. Since the revision period, the empirical evaluations have also been improved and the contributions have methodological novelty as well as promising practical performance.
Reviewers raised several valid concerns about novelty of quantization idea and lack of discussions related to prior art (AISTATS 2020 paper). The rebuttal did not convince the reviewers to raise their score. We hope the authors will benefit from the feedback and improve the paper for future submission.
The paper develops an instance of physics informed neural network inspired from multigrid methods for solving PDEs. The proposed framework describes the solution of a PDE problem as the sum of terms operating at different resolutions. Training is performed by an iterative optimization algorithm that alternates between the different resolution models. Experiments are performed on 1D and 2D problems.

All the reviewers agree on the originality and the potential of the proposed method. They however all consider that the current version of the work is too preliminary both in the form and in the content. The experimental contribution should be developed further with tests performed on more complex problems and complementary analyses. Some of the claims should be given more evidence or moderated. It also appeared during the discussion that the models are not well tuned, making the results inconclusive. The authors are encouraged to develop and strengthen their work.
This paper proposes a method for detecting two types of distributional shifts: covariate shifts in the input space  (due to input corruption) and semantic shifts (due to test data falling outside the support set of ID classes). The idea is based on the decomposition of KL divergence between softmax prediction and a uniform vector. Furthermore, the authors propose Geometric ODIN to improve OOD detection and calibration, outperforming strong baseline on CIFAR10, CIFAR 100, and SVHN datasets. The paper aims to solve a very important problem in ML and the approach is thought provoking.

However, there were several questions and confusions raised by the reviewers, such as the applicability of the model, justification of use of feature norm, discussion on sensitivity vs robustness, framing of the novelty, clear definition of OOD detection, definition of parameters, etc. (please see reviews for a comprehensive list). I invite authors to incorporate these points in the next version of the paper which will significantly improve the paper.
This paper proposes a multi task version of Gradient Boosted Machines (GBMs). The paper proposes a learning algorithm that adaptively adjusts the learning rate per task. Empirical evaluation is carried out on two datasets with the method implemented in the LightGBM framework.

The reviewers thought that the paper is not very clear. They were not ready to accept the paper claims based on the current version. In particular, the algorithms are hard to follow, the empirical evaluation is not easy to follow and there are missing comparisons to related work. The authors did not offer a response to the reviews.
This paper presents a novel method for identifying simuli induced
patterns in MEG and EEG signals.  The authors develop a novel statistical
point process model and a fast EM algorithm to learn the parameters.

Discussion of this paper centered around: how to fit hyperparameters,
and similarity and comparison with other algorithms, especially ICA, as
well as the small number of subjects

Comparison to other methods would make the work stronger, as would
adding more datasets but this novel algorithm seems worth publishing.
I recommend acceptance as a poster.
While the reviewers agree that the paper contains interesting ideas and the method is elegant, it unfortunately does not meet the bar for acceptance. I strongly encourage the authors to revise their paper, in particular using the numerous comments made throughout the discussion phase; for example:

* It is important that the authors polish their work, in particular for the updates provided (e.g. Figure 3, see EFwa)

* Reviewers pointed the lack of updates on important claims by the authors (in particular the claim regarding clustering vs decision trees, see EFwa, the comments on the lack of diverse datasets, see meXP, )

* Some answers might have gained in clarity, such as the reply to EFwa on the application and conclusions following Wilcoxon sign test.
It appears that the reviewers have reached a consensus that the paper is not ready for publication at ICLR.
This paper proposed a flow based approach FCause to Bayesian causal discovery that is scalable, flexible, and adaptive to missing data. Reviewers were split on this paper and could not reach a consensus during the discussion, and no reviewer pushed for acceptance. After taking a closer look myself, I agree with several of the reviewers that while the core ideas here are interesting and novel, there remain too many unresolved issues that require another round of revision.

I encourage the authors to carefully take in account the reviewers  comments and re submit this promising work to another ML venue.
It is important to have good stable and trustworthy algorithms.  Though I am unconvinved that the C DQN algorithm proposed here is the final word (and I suppose this is not controversial, and the authors might agree), the ideas presented here are sufficiently interesting to be disseminated and discussed more widely.  All reviewers recommended accepting the paper, and I ll follow their lead.

That said, the paper can still be improved, and the authors are encouarged to carefully consider the feedback provided by the reviewers.  In particular, it is good to be clear about which parts are principled, and which parts are somewhat heuristic or arbitrary, and could therefore presumably be improved in future work.  In fact, doing so clearly could make the paper _more_ rather than less impactful.

In any case, it seems good to include this paper at the conference, to highlight the questions and partial answers given here, and to inspire more discussion.
Most of the reviewers have concerns that the experimental results don’t show stronger enough improvements over baselines and that the theoretical contribution of the paper is not completely clear. These concerns make the paper a borderline paper for NeurIPS. Some reviewers have pointed out problematic or unsupported claims in the paper. With these in mind, I encourage the authors to revise the paper with more clarity and address the reviewers  comments on the exposition of the paper.
The paper proposes a new method to train ensembles of classifiers that are robust to adversarial attacks, in particular black box transfer based ones. This is achieved by enforcing the output of early layers of different members of the ensemble to have, on average, gradients with low cosine similarity, which should in turn create different decision boundaries. For this, the authors design a specific loss function, PARL, to be minimized at training time. Two reviewers gave the score of 6 while two reviewers gave the score of 3. The main concerns are: 1) unclear meaning of taking the sum of the gradients of different neurons, and why the similarity of that across models is a proxy for similarity of the decision boundaries; 2) lack of experiments, that is, omitting a simpler baseline like individual robust classifiers. Positive score reviewers also did not champion the paper, thus, the paper should be well addressed these main concerns in the revision and cannot accept to ICLR for now.
For pairs of pieces of text, the central idea of this paper is to combine the approaches of using bi encoders (where a vector is formed from each text then compared), which are easily trained in an unsupervised manner, with cross encoders (where the two texts are related at the token level), which are normally trained in a supervised manner. The chief contribution of this paper is to train a combined model (as a "trans encoder") by doing co training of both model types in an alternating cyclic self distillation framework. The paper suggests that this allows unsupervised training of a cross encoder. This claim met some pushback from the reviewers, since the method does require good quality aligned text pairs (much like a traditional MT system does), and so the result is a  task specific sentence pair modeling approach rather than a generic unsupervised learning approach. 

In the discussion, downsides included the claims of "unsupervised" being overstated, the genuine remaining need for related sentence pairs, the lack of a more theoretical understanding of why this works, and the feeling that the paper is not yet fully mature. Upsides include solid work building from existing models, big performance improvements over SimCSE, novelty in combining previous ideas in a new way for a new problem, and good experiments. To my mind, while the requirement of related sentence pairs does mean the model is task specific and less than fully unsupervised, this is still a common and useful scenario, the performance of the model is strong, and, while the proposed model is built from existing components and ideas, they are combined in an interesting new way to achieve an intriguing and strong new way of training models, and the discussion here (and now in Appendix A.2) of what the authors had to do to get the model to work in terms of choosing different losses, etc., convincingly demonstrated that the authors had thought significantly and deeply about the nature of their proposal and how to get it to work well. Moreover the authors were able to work expeditiously during the reviewing period to address other weaknesses, such as now providing results with other methods than SimCSE (DeCLUTR and Contrastive Tension) and on other language models (RoBERTa).

As such, although this paper is clearly somewhat borderline rather than an unambiguous accept, I find myself quite convinced by the novelty, thoroughness, and intriguing nature of this work, and so my vote is to accept it.
The submission receives mixed ratings initially. Three reviewers are on the borderline and one reviewer EG97 leans negatively. The raised issues mainly reside on the technical contribution, technical correctness, and experimental validation. In the rebuttal, the authors have tried to address the raised issues and discussed them in depth with reviewers. However, the discussion does not change the reviewer s mind. After checking all the reviews, rebuttals, and discussions. The AC stands for the reviewer side that the technical contribution is a major issue that ought to be solved. The proposed TPN comes from the summarization of the existing FPN based structure and there are not sufficient insights to make significant improvements. Besides, there are still unsolved issues regarding the technical presentation and experimental validations. The authors are suggested to improve the current manuscript based on these reviews and welcome to submit for the next venue.
This paper studies the problem of generative modeling by convolving an unknown complex density with a factorial kernel called multi measurement noise model (MNM) to obtain a smoother density that is easier to sample from. Poisson and Gaussian MNMs are proposed for the convolution. Experiment regarding image synthesis are conducted to demonstrate the effectiveness of the proposed framework. The paper studies a problem that is of great interest to the machine learning community, and the results are impressive and promising. However, the paper in the current form lack a comparative discussion and a quantitative comparison with some related works. After the rebuttal, all three reviewers tend to accept the paper. After several rounds of internal discussion among AC, reviewers and authors, the AC agrees with the reviewers, and recommends accepting the paper, given the changes the authors promised to make.  

In summary, the AC recommends an acceptance and urges the authors to further revise their paper by adding a comparative discussion with those closely related works regarding generative models using MCMC sampling.
This paper aims to improve the efficiency of adversarial training. Specifically, by analyzing the differences between the adversarial perturbations generated by FGSM RS and the adversarial perturbations generated by PGD, this paper proposes a new single step attacker I PGD (which imitates PGD by creating diverse adversarial perturbations) to accelerate adversarial training. Empirical results are provided on CIFAR 10 and Tiny ImageNet to support the effectiveness of the proposed method.

Overall, the reviewers think it is an interesting paper, but are severely concerned about some statements. During the discussion period, the authors actively clarify these points. However, the Reviewer bLbt is not fully convinced and believes 1) the approximation in Eq. (6) is incorrect and 2) the proposed method is loosely motivated by imitating the behavior of PGD. The authors fail to further follow up on this discussion. The Reviewer bLbt and the Reviewer Dz2K are also concerned that the proposed I PGD AT only yields margin improvements over Fast Adversarial Training. In addition, as suggested by the reviewer AAHj, given this work focuses on developing efficient adversarial training, it is important to include results on larger scale datasets like ImageNet.

I encourage the authors to incorporate all the reviewers  comments and make a stronger submission next time.
The paper provides a new way of weighting data to build weighted estimators of causal effects (which themselves can be used in other contexts, e.g. doubly robust estimators). It s novel in the sense that it optimizes the choice of weighting based on information about the response function space. The approach is simple to implement, and opens up other possibilities for different classes of estimators.

I liked it. I think the paper is nearly there in terms of a well rounded contribution. But I have to say that I did share the concern about the choice of random response functions. It s not only a matter of function space (everybody wants the most flexible one), but also of the random measure that goes in it   so the more flexible the random space, the least understood (to me at least) is the influence of the random measure. Surely that are choices of function space distributions that can do worse than uniform weights for some classes of problems? It s not that it s a implausible starting point (Bayesians do it all the time in terms of prior distribution, on top of a full blown likelihood function that is more often than not just a big nuisance parameter), but I think the paper covers this aspect just too lightly. I think it s of benefit to the authors to release a published version of this paper once they have some more formal guidance or a more complex experimental setup providing a more thorough insight of it. I do think the contribution is really promising, but it feels unfinished, and I d be curious to see where it could go following this direction.
One of the four reviewers failed to engage in discussion, two acknowledged the author s response and paper revision without changing their scores, and one reviewer engaged in considerable discussion resulting in a score increase to a weak accept.  No reviewer gave the paper a strong endorsement.  I do appreciate the large effort that the authors put into revising their paper and addressing reviewers concerns.  However, major post submission revision puts an inappropriate burden on reviewers.  In any case, there is not strong support for this paper even from the one heavily engaged reviewer.
The authors propose a simple method to estimate the accuracy of a classifier on an unlabeled dataset given an in distribution validation set. In extensive experiments the authors show that the proposed method is significantly more accurate than previous methods and other baselines. 

The reviewers are quite consistent in their judgement, just the weighting of the different aspects is different.
After the rebuttal four out of five reviewers recommend acceptance.

Strong points:
  simplicity of the method
  strong experimental results for various tasks and domain shift problems

Weak points:
  there is no clear theoretical statement when the method is supposed to work
  the discussion in Section 3.1 is pretty obvious and seems a bit like a waste of space whereas the motivation for the actual method is very short

While I agree with the reviewers that there is little theoretical justification for the method, the strong experimental results on various datasets, tasks and different domain shifts make this paper interesting for a large audience. Thus this paper is a nice contribution to ICLR and I recommend acceptance.
However, I strongly recommend to the authors to add more motivation in Section 4 and add a limitation section where the cases are discussed where the method is definitely not working. Section 3 is pretty obvious and could be significantly shortened or integrated into the limitations section. 

One case which is highly relevant for this limitations section is the provable asymptotic overconfidence of neural networks which is discussed in

Hein et al, Why ReLU networks yield high confidence predictions far away from the training data and how to mitigate the problem, CVPR 2019

This would definitely lead to a failure of the presented method as all predictions would get a score above the threshold. I would also assume that the method would predict high accuracy values for out of distribution tasks which are semantically similar e.g. training on CIFAR10 and then using CIFAR100 as unlabeled dataset. In that context it would be interesting to evaluate OOD aware classifiers using ATC such as discussed in

Hendrycks et al, Deep Anomaly Detection with Outlier Exposure, ICLR 2019

Also it would be helpful to understand better the influence of the classifier performance on the original task on the performance of ATC on unlabeled data.
This paper proposes a method for parameterizing orthogonal convolutional layers that derives from paraunitary systems in the spectral domain and performs a comparison with other state of the art orthogonalization methods. The paper argues that the approach is more computationally efficient than most previous methods and that the exact orthogonality is important to ensure robustness in some applications.

The reviewers had diverging opinions about the paper, with most reviewers appreciating the theoretical grounding and empirical analysis, but with some reviewers finding weakness in the clarity, reproducibility, and discussion of prior work. The revisions addressed many, but not all, of the reviewers  criticisms.

One point that was highlighted in the discussion is that the method is restricted to separable convolutions. The authors acknowledged this limitation, justifying the expressivity of the method with a comparison to CayleyConv (Trockman & Kolter) and a suggestion that more expressive parameterizations are not necessarily available in 2D. I am not sure this is entirely accurate. In the discussion of related work, the paper briefly mentions dynamical isometry and the prior work of Xiao et al. 2018, who develop a method for initializing orthogonal convolutional layers. What the current paper fails to recognize is that Algorithm 1 of Xiao et al. 2018 actually provides a method for parameterizing non separable 2D convolutions: simply represent every orthogonal matrix in that algorithm in a standard way, e.g. via the exponential map. While I think there is certainly value in the connection to paraunitarity systems, it seems to me that the above approach would yield a simpler and more expressive representation, and is at minimum worth discussing.

Overall, between the mixed reviewer opinions and their lingering concerns and the existence of relevant prior art that was not discussed in sufficient depth, I believe this paper is not quite suitable for publication at this time.
The paper made a solid theoretical contribution on the adversarial  generalization bounds of multi layer neural networks.

However, the paper, at the current form, has many issues in the claim that "the product of the norm can explain the generalization gap":

(1). Weight decay. The authors uses the weight norm as the proxy for generalization gap, however, it is unclear to me that "adversarial trained networks have a larger generalization gap" can be explained by the product of weight norms. To carefully verify this, the authors have to at least carefully tune the weight decay, to the largest possible extend so the generalization error is not hurt, and compare the product of the weight norms in this scenario.  Without weight decay, the neural networks might learn a lot of redundancies in the weights (especially with adversarial training)  which makes the product of the norm to be too large. 

The authors do perform experiments showing that with weight decay, the generalization gap becomes smaller and the norms become smaller, however, it is totally unclear to me that the weight decay considered in the experiments are actually optimal   It could still be the case that with proper weight decay, the product of the norms in adversarial training is actually smaller comparing to that of the clean training. 


Moreover, the authors should also clarify that **the product of the norms, according to the experiments, are simply too large and they can not be used in the theoretical result to get any meaningful generalization bounds**.



(2). The product of the norm in Rademacher complexity  is tight: This  claim only holds for neural networks with 1 neurons per layer. Once there are more than one neurons, there can simply be one neuron that learns f(x) and the other learns  f(x) and they completely cancels each other. So the product of the norm is obviously NOT TIGHT for any neural network with MORE than ONE NEURON per layer. In fact, the gap can be INFINITELY large. 


Unfortunately, I like the paper very much and I hope this paper could be published, however, the claims  "the product of the norm can explain the generalization gap" is simply too misleading and ill supported. I encourage the authors to completely remove this claim and submit the paper to COLT.
After much back and forth about prior work, 3 reviewers score this paper as an 8 and one scores it as a 3.
Other reviewers have written to the 3 and told them they believe that their review is now too harsh, in light of clarifications w.r.t. related work. I tend to agree, though I must admit that I am not an expert on this topic. 
Given that there is almost unanimous support for accepting and it s possible that the one hold out has not seen some of the extra information, I recommend acceptance. 
Given the praise from the other three reviewers, I moreover recommend a spotlight.
This paper proposes a diagonal approximation to the Hessian in a quasi Newton method for non convex stochastic optimization problems. They combine several good existing ideas and show empirically that the method performs well on several learning tasks, but reviewers found that the comparisons were limited in that as an (approximate) second order method, it would be more fair to compare to other second order methods rather than largely focusing on SGD and some variants of ADAM. Overall, reviewers found the novelty limited and had some concerns about the strength of assumptions, parameter wise updates, and some more minor comments on gaps in the presentation. The author response did not fully convince the borderline/negative reviewers, though the paper includes good ideas that would potentially be well received in a future revision.
This paper proposed a compositional approach to (conditionally) steer pre trained music transformers to the direction intended by the user.  Overall the scores are mostly negative. The reviewers pointed out some interesting aspects of the paper (e.g., using hard binary constraints as opposed to the soft ones, the contrastive approach). However, one common issue shared by all the reviewers is the clarity of the presentation, which led to many reviewers being confused about various aspects of the paper especially the empirical evaluation. The authors did provide a detailed response to address some of the concerns, but to fully address all the points I anticipate it would require quite substantial change to the paper. A couple reviewers also raised the concerns regarding the limited contribution of the paper. Finally, there appears to be some disagreement between the authors and reviewers regarding how to interpret the listening test results. I hope the authors can take the comments into consideration to further improve this paper for the next submission.
The paper proposes an FL framework that optimizes the performance of a subset of clients. Reviewers did appreciate the value of several contributions, but unfortunately consensus is that it remains below the bar, even after the discussion phase. Concerns remained on correctness issues, motivation of assumptions, and distinction of personalized vs selfish FL, even after the author feedback.

We hope the detailed feedback helps to strengthen the paper for a future occasion.
The paper addresses coordination improvement in the MARL setting by learning intristic rewards that motivate the exploration and coordination. The  paper is theoretically founded and the empirical evaluations back up the claims.

During the rebuttal the carried out an impressive amount of work. They provided several additional studies and substantially improved the presentation, addressing all of the reviewers  requests. Although not all the reviewers responded to the authors, the authors  response was taken into the account when recommending the decision.

Minor:
  The authors should comment on the learning intristic rewards with evolution (Faust et al, 2019): https://arxiv.org/abs/1905.07628
The paper presents an empirical analysis of Vision Transformers   and in particular multi headed self attention   and ConvNets, with a focus on optimization related properties (loss landscape, Hessian eigenvalues). The paper shows that both classes of models have their strengths and weaknesses and proposes a hybrid model that takes the best of both worlds and demonstrates good empirical performance.

Reviewers are mostly very positive about the paper. Main pro is that analysis is important and this paper does a thorough job at it and draws some useful insights. There are several smaller issues with the presentation and the details of the content, but the authors did a good job addressing these in their responses.

Overall, it s a good paper on an important topic and I recommend acceptance.
This work proposes an approach to improve non ML based methods of text generation. It reformulates the problem with the soft Q learning approach from RL instead of standard hard RL formulations from previous text generation work. By doing this, the work allows application of path consistency learning. This is an elegant formulation. However, this reformulation into soft Q learning appears quite straightforward and so the application of path consistency learning does not require much change to be used for text generation. This limits the novelty of the work. The experiments are also relatively small scale and consists of some non standard tasks such as prompt generation (which is typically evaluated indirectly, the response to the prompts rather than the prompt itself). As the reviewers mention, evaluating on more large scale standard tasks such as summarisation or dialog would be more convincing. Finally the work lacks references to recent works in the field, such as LeakGAN.
This paper observes the similarity between the universality in renormalization group and the lottery ticket hypothesis and proposes that the iterative magnitude pruning, which is used to find the winning tickets, could be a renormalization group scheme. The authors also provide some evidence on their theory on vision model of ResNet families. While it is interesting to try a theoretical explanation of the transferability of lottery ticket used in similar tasks using the theory from statistical physics, the paper does not provide enough experimental results to show how to use such an explanation to improve iterative magnitude pruning or determine the best architecture that can be transferred for different tasks. Therefore the work is more like working in the progress report and not ready for publication yet.
This paper examines the implicit bias of gradient descent of linear group equivalent convolutional neural networks with a single channel and full dimensional kernel when trained on separable data with exponential loss. The main result is that the linear predictor converges in direction to the first order stationary point of the minimum 2/L Schatten matrix norm max margin problem, under some assumptions. This generalizes previous results on linear convolutional neural networks.

I appreciated this paper states the theorems in terms of general group operations; if done correctly and written well, this can be a good reference for future papers. But I think this paper needs a little more work before getting there, as I explain below.

The reviewers were borderline (6,6,6,5) and did not have high confidence. Some stated clarity issues, other criticized the model being used: either that (1) only the case of single channel and full dimensional kernel we examined, or that (2) the full model is not actually invariant. Given previous results (Jagadeesan et al.) I am OK with (1). I find (2) problematic, but not enough to be a reason to reject the paper. 

So I took a closer look.

First, I felt that indeed the paper writing could be improved. Specifically, the notation could be better explained (e.g., the h and g functions in eq. 3 are not defined: what are their range and domain?), and more discussions and examples can be added throughout the paper to clarify the significance of the results.

Second, the experimental results in the non Abelian case (figure 4a) and non linear case (figure 5) seemed somewhat weak (not so sparse) after I noticed the y axis does not start from zero, as in Figure 3a.

Most importantly, looking at the proofs, I felt they were rather incremental, as I explain next. The authors claim their main result, Theorem 5.4, does not follow from Yun et al. s paper. But Gunsekar et al. 2018b already had KKT condition results for max margin in parameter space, and even stronger results are in [1] (which the authors should cite and discuss clearly). These already give a stronger version of Theorem A.6. So the main extra contribution here is to extend it to a guarantee on function space (in the space of linear functions beta). 

But for L>2, I unless I am missing something, I feel this is straightforward, by using results like in [2], where they relate subdifferentials of unitarily invariant matrix functions to the corresponding vector subdifferentials on the singular values (and in the vector case, the subdifferential is trivial). 

The L 2 is not trivial as we need to show the condition in Assumption A.7, which is the most technical part of Gunsekar et al. / Yun et al. papers, and is often non trivial. The authors in this paper, however, did not show this and rather leave to future work in the last paragraph of Appendix A.  I think that this is a concrete opportunity to make the paper better, perhaps following the same methodology in Gunsekar et al. / Yun et al. papers.

Minor comments: 
1) The informal Theorem 1 should state we converge to the f.s.p. of eq. 1, not a solution of eq. 1.  
2) The main paper is non searchable, which makes it harder to read.
3) Many hype refs in the appendix do not work well (they get me to some random page).

[1] K. Lyu, and J. Li. "Gradient descent maximizes the margin of homogeneous neural networks." 2019.
[2] A. S. Lewis  The Convex Analysis of Unitarily Invariant Matrix Functions, 1995
This paper proposes a tree based method for interpretable policy learning, for fully offline and partially observable clinical decision environments. The models are trained incrementally, as patient information becomes available. 

The method was overall deemed novel by the reviewers, and the interpretability of the model well validated by clinicians.

Numerous points of clarification were brought up by reviewers, related to the notation, learning process and result reporting. All of the concerns were responded to by the authors in great detail and the manuscript was appropriately revised. All the reviewers have raised their scores as a result of the updates.

Thus, the paper is ready for acceptance.
This paper empirically studies various design choices in offline model based RL algorithms, with a focus on MOPO (Model based Offline Policy Optimization). Among the key design choices is the uncertainty measure used in MOPO that provides an (approximate) lower bound on the performance, the horizon rollout length, and the number of model used in ensemble.

The reviewers are positive about the paper, found the experiments thorough, and the results filling a gap in the current literature. They have raised several issues in their reviews, many of which are addressed in the rebuttal and the revised paper. I would like to recommend acceptance of the paper. Also since the results of this work might be of interest to many researchers working on model based RL, I also recommend a spotlight presentation for this work.

I have some additional comments:

(1) The paper studies the correlation of uncertainty measures with the next state MSE, with the aim of showing which one has a higher correlation. The underlying assumption is that the next state MSE is the gold standard that we should aim for.

If we go back to the MOPO paper, we see that to define an uncertainty penalized reward, we need an upper bound on the absolute value of G(s, a), which is the difference between the expected value of the value function at the next state according to the true model and the estimated model.

If we assume that the value function belongs to the Lipschitz function class w.r.t. a metric d, this upper bound is proportional to the 1 Wasserstein distance between the true next state distributions and the model s distribution.
If the dynamics is deterministic, 1 Wasserstein distance becomes the $d( T(s, a), \hat{T}(s,a) )$. If the distance d is the Euclidean distance, this becomes the squared error.

Therefore, the squared error makes sense for deterministic dynamics, and it only provides an upper bound of $|G(s, a)|$.
If the environment is not deterministic, the squared error may not be a reasonable gold standard anymore to compare the correlation of various uncertainty measures with.

The paper introduces a generic MDP framework, but does not mention anything about its focus on MBRL for deterministic environments until the last sentence of its conclusion. Please clarify this in your camera ready paper.

(2) The experiments are conducted using 3 or 4 seeds. Although this is the common practice in the deep RL community, it is too small. Standard deviations in Tables 1, 2, ... are computed with 3 seeds, which would be cringeworthy to statisticians and empirical scientists. I encourage the authors to increase the number of independent random experiments to make their results more powerful.
The paper addresses a few very important points on sequential latent variable models, and introduce a different view on meta RL.  Even  though the methods that this paper poses are incremental, it is such a hot debated topic that I would prefer to see this published now.
The paper shows a causal perspective to the adversarial robustness problem. Based on a causal graph of the adversarial data creation process, which describes the perceived data as a function of content and style variables, the authors identified that the spurious correlation between style and label is the main reason for adversarial examples. Based on this observation, they propose a method to learn models for which the conditional distribution of label given style and image does not vary much when attacked. Experiments on MNIST, CIFAR 10, and CIFAR 100 datasets show that the proposed method is better than two baselines, Madry and TRADES.

Overall, the paper contains interesting ideas and tackles an important problem. Due to some concerns regarding the clarity and motivation of the paper, we strongly recommend the authors take the reviewers  comments to heart and incorporate their thoughts in preparing the camera ready version of their manuscript.
This paper adapts the mixup data augmentation strategy to the case of metric learning. The main challenge addressed is the fact that in metric learning, the loss function does not treat each example as an IID sample. The paper takes the view of metric learning as learning over positive and negative pairs (those belonging to the same/different classes) and uses this to develop a fairly general metric mixup formulation. To measure the effectiveness of the approach for metric learning, the paper introduces a new measure called utilization that looks at the distance of a query point to its nearest training point in embedding space.

The reviewers (5 of them) all favour acceptance on the grounds of novelty, and the performance of the method. During the discussion, some issues were raised around whether utilization is a useful measure, improvements to the paper clarity, whether the clean loss in eq. 10 is necessary, and potential limitations on the generality of the approach. However, additional experiments and clarification during the discussion period has resolved these issues to their satisfaction.
This is a clearly written paper about integration of entity abstraction to the transformer based language modeling methods for language processing tasks that require reasoning (this is clarified by the authors later as tasks that require linger chains of reasoning) and have shown results on CLUTTR, HotpotQA, and CoQA. The reviewers seem to agree on two issues: First, it is not clear why the proposed idea does not result in a lot of improvement, except the synthetic CLUTTR. Authors provided additional experimental results on yet another dataset. Second, the paper would benefit from a detailed analysis of the experimental results, for example, why don t abstractions help on all datasets.
This manuscript proposes a quantization approach to improve adversarial robustness. Reviewers agree that the problem studied is timely and the approach is interesting. However, note concerns about the novelty compared to closely related work, the quality of the presentation, the strength of the evaluated attacks compared to the state of the art, among other concerns. There is no rebuttal.
The paper introduces a pipeline to discover PDEs from scarce and noisy data. Reviewers engaged in a very thoughtful discussion with the authors. I read the extensive rebuttal, and I believe the authors have addressed the major concerns claimed by the reviewers. I ask the authors to make sure to include all the changes and additional experiments in the camera ready version.
The authors propose a memory based continual learning method that decomposes the models  parameters and that shares a large number of the decomposed parameters across tasks. In other words, only a small number of parameters are task specific and the memory usage of storing models from previous tasks is hence a fraction of the memory usage of previous approaches. The authors take advantage of their method to propose specific ensembling approaches and demonstrate the strong performance of their methods using several datasets.

In the rebuttal, the authors were very reactive and provided many useful additional results during including a comparison of the computational cost of their method vs. others, results using two new datasets (CUBS & Flowers), and additional results on mini ImageNet. They also answered, through additional experiments, several reviewer questions including the robustness to different first tasks in the sequence.

Overall, the reviewers after the rebuttal/discussion period agree that this is a strong contribution: novel and fairly simple method with some theoretical justification, thorough empirical evaluation, well written and easy to follow manuscript. It also opens a few interesting avenues some of which the authors have already explored in their paper (e.g., ensembling).
This paper proposes a new approach, which combines offline reinforcement learning with learning in simulation. There were different views on the paper among the reviewers and we had quite a lot of discussions. As a consequence, there were still serious concerns remaining, e.g., whether the results are significant enough, whether there are clear advantages of the proposed mehtod over directly using offline RL methods. It is not justified whether the proposed framework can use offline data more efficiently or better reduce the gap between mismatched simulators and offline data. The reviewer who gave the highest score decided not to champion the paper. Considering all the discussions, we believe the paper is not ready for publication at ICIR yet.
The paper presents several related results. The initial main result consists in relating GPCA to GCN, showing that GPCA can be understood as a first order approximation of some specific instance of GCN where the W matrix is directly defined on data. This result is then exploited to define a supervised version of GPCA. As a follow up the authors propose a novel GPCA based network (GPCANet) and a GPCANet initialisation for GNNs. The paper is well written and easy to read. Empirical results are reported to verify the above mentioned connection between  GPCA and GCN, as well as the performances of  GPCANet  and the proposed initialisation for GNNs. Overall, while the mentioned connection was never explicitly reported in the literature, its existence is not surprising and thus its significance seems to be limited. Also the performances of GPCANet do not seem to be significant from a statistical point of view. The novel initialisation procedure for GNNs seems to be interesting and promising, although the used datasets may not make evident its full power. Authors rebuttal and discussion did not change the reviewers  initial assessment.
The paper demonstrates a case of federated learning with unlabelled but systematically partitioned data between clients. A title along terms like "FL with unlabelled data" would be much better   the considered setting here is not fully unsupervised but relies on the key assumption that while not the labels, at lease the precise label frequencies have to be known on each client, which is a strong assumption (also iid up to the class shift). Semi supervised FL approaches should also be discusses.

Overall, reviewers all agreed that the paper is interesting, well motivated and deserves acceptance.
We hope the authors will incorporate the open points as mentioned by the reviewers.
The reviewers agree that this paper studies an important problem, provides theoretically analysis to understand graph injection attack.
The authors propose a new regularizer to improve the attack success. Extensive experimental results also show the effectiveness of the proposed method.
Reviewers are positive overall   the is a general consensus towards acceptance. Reviewers viewed the simplicity, novelty, and effectiveness of the propose pre training approach as strengths. Further, reviewers praised the draft as very clearly written, and viewed experimental ablations as relatively in depth   e.g. two reviewers found the additional analysis of impact of data size to be valuable. A few concerns about additional ablations and claims were brought up, but all were adequately addressed in author response.
This paper provides a near optimal analysis of the unadjusted Langevin Monte Carlo (LMC) algorithm with respect to the W2 distance. The main statement is that the mixing time is ~ d^{1/2}/eps under standard assumptions. The authors also give a nearly matching lower bound under these assumptions. The reviewers agreed that this is an interesting contribution obtained via non trivial techniques. The consensus recommendation is to accept the paper.
The paper describes a new testbed to evaluate Bayesian techniques in the context of joint predictive distribution.  Since this is not the first paper that considers marginal vs joint distribution evaluation, the paper should include a thorough discussion of the differences with prior work.  The paper simply states that it refutes Wang et al. s previous observation that joint distributions do not distinguish techniques much more than marginals.  However, the paper does not really explain why their observation is correct and Wang s observation should be discarded.  Since this is the core contribution of the paper and it is doubtful, this is problematic.  The discussion of epistemic/aleatoric uncertainty also seems superfluous and therefore distract the reader.
This paper extends the Neural Collapse (NC) phenomenon discovered by Papyan, Han and Donoho (2020) on deep learning image classifications with Cross Entropy (CE) loss, to the scenario with Mean Squared Error (MSE), that achieves similar performance to CE and favors deeper analysis. In particular, the paper shows that the least square loss can be decomposed orthogonally into a  central  path as the optimal least square loss, and its perpendicular loss. Moreover, the paper shows by experiments that after the zero training error (Terminal Phase of Training, or TPT) the perpendicular loss is typically much smaller than the optimal least square loss, and the optimal least square loss is further decomposed into the NC1 loss which is the dominance and NC2/3 loss (even smaller than the perpendicular loss). Such a discovery with loss decomposition is very thought provoking to understand the training dynamics of deep neural networks.

Reviewers unanimously accept the paper, so is the final recommendation.
Most reviewers came to the conclusion, that this work lacks novelty and theoretical depth. Further severe concerns about the validity of some statements and about the experimental setup have been raised. The rebuttal was not perceived as being fully convincing, and nobody wanted to champion this paper. 
I share most of these points of criticism. Although there is certainly some potential in this work, I think it is not ready for publication and would (at least) need a major revision.
The paper presents a reinforcement learning based approach for object localization given an exemplary set of images.  The paper shows that test time policy adaptation to new environments is possible with object detection experiments. All four reviewers find the proposed approach interesting. Most of the reviewers feel their initial concerns (including the clarity of the paper and the approach details) addressed after the discussion with the authors and the revision. One reviewer still finds the experiments limited even after the author rebuttal, but the reviewers all agree that there is value in the paper.

We recommend accepting the paper.
This paper introduces a benchmark for experimental design algorithms
for an important cellular biological question, causal discovery of
effective genetic knock out interventions. It uses existing datasets.

The paper was discussed by the reviewers after the authors correctly
pointed out that methodological machine learning novelty is not a
necessary condition for accepting papers. Two reviewers increased
their scores and all are slightly positive. The benchmark was seen as
valuable, and one reviewer even commented they might use it in their
own research. However, the paper is still on the borderline as this
benchmark is only a first step. It has not been shown yet that machine
learning insights can be produced with it, as the authors have not 
actually used it for benchmarking yet. In other words, the benchmark
can be considered a potentially excellent idea which has not been
tested empirically yet.

This seems a highly promising research direction and the authors are
strongly encouraged to continue to providing the benchmarks and
releasing the method to the community so that others can help them in that.
This paper proposes that ML models might be better expressed in a way closer to their mathematical representation than to Python code.  This is an attractive proposition, but the paper s development of this proposition is that models might best be expressed in LaTeX, which is not a hypothesis that the reviewers consider proven.

Ultimately, this paper proposes a new language in which to express ML models, and compares that language against one baseline: PyTorch.  However, this is far from a reasonable baseline.  Even within Python, systems such as JAX (which the paper dismisses as a "Program translator") are much closer to the pure functional style; and going further afield, comparisons should be to DEX and Julia, to name but two.

The reviewers appreciate the approach to autobroadcasting, but again note that this does not require a new language, and again, systems like JAX, DEX, Julia all have approaches to broadcasting which are not compared.

Reviewer 8MK2 is concerned that we "still need to write other modules .. in Python", but the authors rebut this well: Kokoyi is not expected to be applied to an entire program, just to the model components.

Even if Kokoyi were to be successful, there is a question of its wider applicability.  A major strength of PyTorch/JAX is that they are used by a much larger community than just ML paper authors.  It is because the authors write in these tools that their work is usable by other practitioners.  The paper explicitly says it is targeted not even at ML paper authors, but at a subset of that community.

The usability analyses are very much lacking.  Lines of code is a notoriously coarse tool to assess programming paradigms.  I would also caution against trying to do any small group user study   the best initial study is to release Kokoyi into the wild, get feedback from users, and then if it proves popular, prepare a paper or monograph.  This is the path of PyTorch and other frameworks.  

Until then, the paper may be of interest to a workshop very focused on programming models for ML, but is not currently suitable for the wider ICLR audience.
This paper introduces the Stiffness aware neural network (SANN) for improving numerical stability in Hamiltonian neural networks. To this end, the authors introduce the stiffness aware index (SAI) to classify time intervals into stiff and non stiff portions, and propose to adapt the integration scheme accordingly.

The paper initially received three weak accept and one weak reject recommendations. The main limitations pointed out by reviewers relate to missing references from the literature, assumptions behind the proposed approach (e.g. structure of the mass matrix, separable Hamiltonian), and clarifications on experiments including additional baselines and hyper parameter settings. 
The rebuttal did a good job in answering reviewers  concerns: RiTTU increased his rating to a clear accept, and RMYXe increased his rating to weak accept.
Eventually, there is a consensus among reviewers to accept the paper. 

The AC s own readings confirmed the reviewers  recommendation. The method is straightforward yet effective, and the paper is well written. The effectiveness of the proposed approach is shown in different contexts. Since several complex systems exhibit chaotic characteristics, the paper brings a meaningful contribution to the community.
### Description
The paper investigates the choice of a fixed quantization grid for weights. Namly, the paper observes that symmetric uniform quantization levels such as { 1.5, 0.5,0.5,1.5} lead to better results than non symmetric ones, e.g. { 2, 1,0,1}. While it is a small thing, it can be appreciated that it is investigated systematically and pedantically, proposing an explanation and showing experimentally that the effect is constantly present in favour of symmetric quantization. While the improvement is small, it comes almost at no cost. A part of the contribution proposes an efficient implementation. 

### Decision
Reviewers and AC came to a consensus that the contribution of the paper is marginal. Symmetric quantization schemes themseleves were already employed by many models, albeit without analysis or even a discussion of such choice. The analysis presented in the paper was found unconvincing by the reviewers (see below). The efficient implementation follows from basic linear algebra (see below). The potential impact of the work was considered as limited due to a rather marginal observed improvement. The average rating of the paper was 4.5. Therefore must reject.

### Details
Regarding the proposed analysis of CSQ, it is not clear, why the number of quantization levels of an elementary product matters, given that these numbers are then summed over all corresponding input channels and spatial dimensions of a convolution kernel applied at a single location. It is questionable whether the number of these quantization levels indeed corresponds to the representation capacity. Finally, the paper misses to demonstrate the effect on binary (1 bit) networks. In this case the standard approach is to use { 1,1} weights and { 1,1} activatinos. The paper could investigate the case of {0,1} activations, where there would be 50% more unique possible outputs from the product, namely { 1,0,1} to validate their hypothesis. If the hypothesis holds, an improvement in the binary case would be observed. This is important since the binary case is know to be the hardest and since the respective recommendation of representations would be non standard. It could be further questioned why the distribution of real valued weights has any relevance (such as in the arguments in appendix E) if the model is trained from scratch? A training method need not keep any real valued latent weights in the first place.

The technical part in section 5 "efficient realization" adds very little, if anything, to the paper s contribution. A simple linear algebra suffices to see that 

$(W 0.5) \ast x   W* x   0.5 I \ast x,$

where $I$ is the kernel of ones of the same shape as $W$. It is clear that the convolution $I \ast x$ can be implemented efficiently (e.g. it is just a sum over channels followed by a separable spatial only convolution) and is not a bottleneck and. The final detail such as whether to slice by bits and use popcount for it or to use 8 bit addition, depend very much on the choice of the bit packed representation and the hardware available. It would be known to engineers in the field how to implement it efficiently.
This paper proposes a modification of the training objective of non autoregressive MT which claims most of the improvements that other approaches obtain only through knowledge distillation (KD) from an autoregressive teacher. 

The strategy has been largely appreciated as simple and the results suggest that it s rather effective. One of us was not ready to accept certain aspects of the comparisons in the paper, and challenged the paper also from a speed of generation point of view. While I see that NAT MT is very much concerned with speed, removing the dependency on an autoregressive teacher is an important step in the NAT MT agenda (as KD has various drawbacks, e.g., it corrupts the statistics of the training data), and, in my view, disentangling the two desiderata (e.g., faster models, and no KD) is okay at this stage. I hope the authors will not take this recommendation as a reason to ignore the comments in that review, rather, that they take it as an opportunity to address those comments as well as possible (for example, by positioning the work more carefully wrt speed and the possibly negative impact of KD).

On style: Table 1 should fit within the margins of the paper, please fix it. Also, avoid boldfacing model names and avoid vertical bars (please check this nice guide on making [tables look nice](https://people.inf.ethz.ch/markusp/teaching/guides/guide tables.pdf)).
The paper proposed an approach to search image augmentation policies. The paper formulates this problem as a cooperative multi agent decision making problem, which is interesting. The paper received 3 borderline accept and 1 borderline reject ratings. The reviewers originally had multiple concerns regarding the necessity of RL based approach, lacking references, and additional experiments, and the authors responded to some of the concerns of the reviewers reasonably. However, none of the reviewers ended up strongly supporting the paper, staying with their ratings.

The RL formulation of the problem is interesting, but it requires multiple rounds of the target network training due to its nature (i.e., it is not an end to end approach). The paper misses some details on how exactly the patch wise RL based augmentation works and it requires additional hyperparameters for the selection of patch size and shape. It is also unclear how this RL based method is conceptually superior to previous augmentation approaches and the empirical results are not strong enough, as some of the reviewers also pointed out.

Although the paper has interesting ideas and the AC also think the paper has some merit, the senior AC finds the technical contribution of the paper weaker than the others. We unfortunately need to recommend the rejection of the paper.
This paper presents a second order optimization algorithm for neural nets which extends LeCun s classic Lagrangian framework. The paper derives a method for computing the exact Newton step for a single training example for a multilayer perceptron. It then describes approximations that can be used to extend the method to more examples.

The authors claim to have spotted factual errors in the reviews. However, I ve looked into the issues, and I find myself agreeing with the reviewers on each of those points (or, if there are misunderstandings, they result from a lack of clarity in the paper rather than insufficient scientific computing background on the part of the reviewers).

The authors claim to have solved a longstanding problem by giving an efficient method for calculating the stochastic Newton step (for a single training example). However, it s not clear this is very useful; as a reviewer points out, estimating the curvature with a single example can t give a very accurate estimate. Once the method is extended to batches, more approximations are required. I also agree with the reviewers that the later parts of the methods section appear a bit rushed.

As the reviewers point out, in the experimental comparisons, the proposed method seems to underperform SGD with momentum even in terms of epochs, which is the setting where second order methods usually shine. Other second order optimizers (e.g. K FAC) have been shown to outperform first order methods in terms of both wall clock time and epochs, so epochwise improvement seems like the minimum bar for a second order optimization paper.
The reviewers are largely in agreement that this proposal would benefit from more clarity and comparison to key papers/findings in this space. While one reviewer is leaning towards acceptance, and their points were considered by the other reviewers, there wasn t a consensus towards aligning towardsa an acceptance. Thus, I recommend that the authors take advantage of the reviewers  comments to further improve their manuscript.
This paper analyzes local SGD under the random reshuffling data selection setting. As is the case for standard random reshuffling, better rates are shown for local SGD when random reshuffling is used. This would already be a nice contribution to a line of work on random shuffling methods—but the paper goes beyond that by showing a matching lower bound and designing a (theoretically) better variant algorithm. The reviewers were all in agreement that this paper should be accepted (as a result not much further discussion happened after the original reviews), and I agree with this consensus. The modification seems to improve the paper, although I did not look through it in detail.
As evident by the title the paper focuses on understanding sharpness aware minimization which is a contemporary training procedure  based on minimizing the worse case perturbation of the weights in ball. It has been observed that SAM improves the generalization and this paper aims to demystify this success. They also provide a convergence proof of SAM for non convex objectives in a simplified setting and also discuss benefits of SAM in the noisy label setting.The reviewers thought this paper was an interesting first step The reviewers raised concerns about (1) novelty of the proof technique, (2) interpretation of the analysis. The response mitigated some the concerns but did not resolve them. I concur with the reviewers. The paper has some nice insights and good potential. However, there are a few things that need to be clarified and the paper has to be substantially rewritten to reflect this and thus I do not recommend acceptance at this time.
This submission addressed offline imitation learning problem with non optimal demonstrations. The AC went through the draft, reviews, and replies. The AC agrees with all reviewers that the mathematical analysis, empirical evaluation, and general quality of writing haven t reached the bar of ICLR papers.
This paper proposes to leverage topological structure between domains, expressed as a graph, towards solving the domain adaptation problem.

Reviewer n4Lk thought the ideas were interesting, appreciated the theoretical analysis and indicated that the experiments were “well thought out”. The reviewer asked for more detail on Lemma 4.1 and suggested that a proof be provided for Proposition 4.1. They asked for more justification on why the change of task for the discriminator from classification to generation would improve performance. The authors responded to these comments, clarifying the proof of Lemma 4.1 in the appendix. They clarified that proposition 4.1 can be derived from Corollary 4.3 or Corollary 4.4. On the point of classical vs. enhanced discriminator the authors provided additional experiments. 

Reviewer rNQp commented that the method was easy to follow and noted the theoretical and empirical analysis. They expressed some concern that previous work on graph based domain adaptation was inadequately addressed. Like reviewer n4Lk they seemed unconvinced that the proposed graph discriminator was an improvement over past SOTA and questioned its novelty. In terms of claims about novelty and competitiveness relative to previous works I would have liked to see the reviewer make specific references rather than criticize in general terms. The authors’ responded to the reviewer, adding a recent entropy based method (SENTRY) to the experiments and showed that their method outperformed this ICCV 2021 work by a large margin. They responded to the reviewer’s remarks about the original discriminator and variants, pointing out that this was already established in the paper. They used the other reviews to dispute the claim of lack of novelty. 

Reviewer uDYW felt that the work was novel and interesting. Like rNQp they thought the paper was clear. They questioned the practical advantage over baselines. The authors responded to the reviewer’s question about using a data graph. They responded to the question about parameter tuning and computational cost. They addressed the question about limited improvements in real world datasets.

I had some difficulty motivating the reviewers to engage in the discussion and acknowledge the authors’ response. The authors also politely attempted to nudge the reviewers to consider their updated results. In my opinion, the author responses have addressed most of the reviewer concerns  and I don’t see any critical issues remaining. Therefore I think that this paper should be accepted as a poster.
The paper formally studies the problem of partial identifiability when inferring a reward function from a given data source (e.g., expert demonstrations or trajectory preferences). To formally characterize this ambiguity in a data source, the paper proposes considering the infinite limit data regime, which bounds the reward information recoverable from a source. Furthermore, this ambiguity is then studied in the context of different downstream tasks, as recovering an exact reward function may not be necessary for a given task. The paper is primarily theoretical, and the results provide a unified view of the problem of partial identifiability in reward learning for different sources and downstream tasks.

Overall, the reviewers acknowledged the importance of the problem setting and found the results promising. There is quite a bit of spread in the reviewers  final assessment of the paper with ratings 8, 8, 3, 3 (note: one of the reviewers with rating 3 has a low confidence). The authors  responses did help in discussions; however, a few of the concerns, as raised by reviewers, still remained. The key issues are related to the general accessibility of the paper and the lack of concrete examples to highlight the proposed theoretical framework. At the end of the discussions,  several reviewers (including those with an overall positive rating) shared concerns about the paper s accessibility.  With this, unfortunately, the paper stands as borderline. Nevertheless, this is exciting and potentially impactful work, and we encourage the authors to incorporate the reviewers  feedback when preparing a future revision of the paper.
The paper analyzes the flow of information in convolutional neural networks with parallel pathways by using logistic regression probes and similarity indices. 
Following the analysis, the authors concluded that:
  pathways of similar size have similar contributions to learning and have high redundancy
  shorter pathways directly improve solution quality to a greater extent than longer pathways
  pathways of different lengths also lead to grater variety amid features in the  downstream  layers of the network

The novelty in this type of analysis and its thoroughness was appreciated by the reviewers. The insight about the benefits of pathways of different length is also valuable; although there is a sense in the community that long pathways without skip connections bring diminishing returns, as pointed by reviewer ac9X, there is still some benefit in quantifying it through such an analysis and establishing the correct mix of long/short pathways. [note: This paper is not there yet, but has the potential.]

On the other hand, the experiments were performed on a single network, a network selected such that this instrumentation is possible, so there is the issue of whether these conclusions generalize, as pointed out by reviewer Hchc.

There were other comments, in terms of structure, errors and typos, as pointed out by reviewer PRea. 

The authors have not responded to the comments, nor updated their manuscript.
In its current form, the paper is not ready for acceptance.
This paper explores the contrast in performance between easy and hard tasks (episodes) in few shot image classification and propose mitigating strategies to avoid large performance gaps.

None of the reviewers support the acceptance of this work, despite the authors  detailed rebuttals, with all reviewers confirming their preference for rejection following the author response. Issues raised included lack of clarity of writing and lack of sufficiently convincing experimental results.

I unfortunately could not find a good reason to dissent from the reviewers majority opinion, and therefore also recommend rejection at this time.
This article presents novel distillation based methods for neural network training and uncertainty estimation. While the idea is interesting, there is a general agreement amongst reviewers that the paper lacks clarity, adequate discussion of the relevant literature  and comparisons to existing work. Although the revision uploaded by the authors goes in the right direction by adding some experiments and clarifying some of the issues raised by the reviewers, further work is needed to make the submission stronger.
The main detractor of this paper feels that the paper makes a relatively small technical and empirical contribution given existing results on HER (Andrychowicz et al., NeurIPS 2017).  However, several other reviewers, who had more engagement in the discussion, were strong supporters. Having looked at the paper myself I thought the selection of experimental problems undermined the results.  Experiments are most compelling when many unaffiliated groups compete on the same benchmarks.  But the basic idea of integrating HER with AlphaZero, and a reasonable attempt at this, seems to be interesting enough to warrant a poster.
The submission proposes to learn a causal transformer model over a pretrained VQ GAN representation to generate videos. While the paper is well written and clear, proposing a simple idea, the novelty of this method is not well explained compared to pre existing publications see ([reviews JjT4](https://openreview.net/forum?id K hiHQXEQog&noteId 22EyPvpodh), [3gJ8](https://openreview.net/forum?id K hiHQXEQog&noteId 9ikn_nBC_Sf), [6x6m](https://openreview.net/forum?id K hiHQXEQog&noteId zCX9VP8I5uL), and [pKCo](https://openreview.net/forum?id K hiHQXEQog&noteId uRgHWX4C5yX)) especially since it s lacking [ablation](https://openreview.net/forum?id K hiHQXEQog&noteId uRgHWX4C5yX) or comparative (see [reviews 6x6m](https://openreview.net/forum?id K hiHQXEQog&noteId zCX9VP8I5uL), and[pKCo](https://openreview.net/forum?id K hiHQXEQog&noteId uRgHWX4C5yX)) experiments. 
The authors have expressed their consideration of reviewers requests but will not satisfy them in time for this conference. Therefore I am currently recommending this submission for rejection.
This paper studies the influence of recommender systems on users  preferences. The authors propose a method for estimating preference shifts, evaluating their desirability, and avoiding such shifts (when needed).

After the initial review and discussion period, a fourth reviewer with significant recsys experience and a very good knowledge of this sub area was invited to provide an additional review of the paper. This is reviewer vNt7. Their review was positive overall but did highlight some limitations and potential ways to improve the paper s grounding in the recsys literature.

Overall, the main strengths of this paper were that it studies an interesting and practically motivated question. The reviewers also found the proposed solution reasonable. 

The main limitations are twofold. One, the results use a single set of simulation assumptions. Showing similar results under different simulation assumptions would be helpful to better understand the robustness and potential limitations of the approach. Two, there is a certain disconnect with the simulation literature. See comments from reviewers vNt7 and kWQ2 (although I found your reply to Virtual Taobao convincing).

Overall and given the final reviewer recommendations (three marginally above and one marginally below), this is a very borderline paper. However, the consensus view of the committee is that it would benefit from additional work before publication.

I am sorry that I cannot recommend acceptance at this stage. I do believe that some of the suggestions from the reviewers highlighted above (more diverse simulation, better grounding in current recsys simulation literature and in the field) will be useful in preparing the next version of this work.
This work performs a mean field analysis of a certain class of fully connected networks with and without layer normalization. Theory is provided which successfully predicts when some networks will exhibit either exploding gradients, or "representation shrinkage" which is similar to the extreme ordered phase discussed in prior works on signal propagation. The primary concerns raised by reviewers included, large overlap with prior works on signal propagation, a bug in the proof of the main theorem, lack of clarity, and many assumptions made in the theory which significantly limit the space of architectures for which the theory can be applied. Some of these concerns were addressed in the rebuttal period, notably major flaw in the main theorem was resolved and some concerns on clarity were addressed. However, with the remaining issues (notably overlap with prior work, and overly restrictive assumptions made) a majority of reviewers did not recommend acceptance in the end. The AC agrees with this final decision and recommends the authors look to further expand upon the contributions relative to prior work.
This paper studies the problem of inverse reinforcement learning by relying on only demonstrations and no interaction (like imitation learning). The reviewers liked the premise but had major concerns with evaluation and baselines. The paper initially received reviews tending to reject. One of the questions was about missing behavior cloning baseline which the authors added in rebuttal. But the BC baseline seems to be really competitive (in fact, better in 3 out of 4 envs) as compared to the proposed approach. In conclusion, all reviewers still believed that their concerns regarding insufficient evidence for justifying approach and missing comparisons to other prior work still stand. AC agrees with the reviewers  consensus that the paper is not yet ready for acceptance.
The paper considers the important problem of performance degradation under distribution shift and proposes a simple yet effective method to alleviate this problem. They do so by considering feature statistic to be non deterministic and rather a multivariate Gaussian distribution.  The model can be integrated into networks without additional parameters and experiments show that it works better than BN as well as if the assumed distribution was uniform. The latter was added during rebuttal period.

There were two main concerns regarding distinguishing the work from AdaIN and baseline that were addressed during rebuttal and some parts of the paper were re written to address repetition.
This paper casts entity linking in a retrieve then read framework by first retrieving entity candidates and then finding their mentions via reading comprehension. All reviewers agree that the proposed approach is novel, well motivated, and simple yet performant. The authors have done a good job of addressing all the concerns raised, and the reviewers are unanimous in their recommendation for accepting the paper. I hope the authors will also incorporate the feedback and their responses in the final version.
The authors presents an alternative view of Neural ODEs, offering a novel understanding of depth in neural networks. The reviewers were overall impressed by the novelty and potential for insights this work brings. There was some disappointment that the empirical results were not stronger (both in terms of pure performance and computational cost) and that it wasn t clear how the theoretical insights into depth actually translated into a practical insight. Nevertheless, I agree with the reviewers that this is a good submission and would I think make for an interesting addition to the conference programme.
This paper proposes a message passing neural network to solve PDEs. The paper has sound motivation, clear methodology, and extensive empirical study. However, on the other hand, some reviewers also raised their concerns, especially regarding the lack of clear notations and sufficient discussions on the difference between the proposed method and previous works. Furthermore, there is no ablation study and the generalization to multiple spatial resolution is not clearly explained. The authors did a very good job  during the rebuttal period: many concerns/doubts/questions from the reviewers were successfully addressed and additional experiments have been performed to support the authors  answers. As a result, several reviewers decided to raise their scores, and the overall assessment on the paper turned to be quite positive.
This paper proposes an image tesselation scheme to improve the robustness of image classifiers.  The reviewers agree that the method is simple and intuitive, and view this as a positive attribute.  At the same time, the reviewers want to see if the method works on higher resolution images.  It was also not clear to reviewers how the attacks on the method were constructed, whether they were white box, and whether they were adaptive.  Without a rebuttal, these questions remain unanswered.
The paper sets up a complex algorithm for out of distribution generalization. The algorithm requires first, a generalization of identification results for variational autoencoders, the followed by second, a causal discovery subroutine, and third, learning an invariant predictor using the discovered causes. The procedure reads sound, and the results on common benchmarks look good, though I do not know how practical the approach would be in general.
The paper develops optimization algorithms for fitting structured neural networks. It focuses on the manifold identification property, which guarantees after finitely many iterations, all iterates have the same sparsity structure as at convergence. The proposed method extends dual averaging to include momentum. The paper’s analysis shows that if the proposed method converges, it converges to a stationary point, and identifies the sparsity pattern of the limit in finitely many iterations. Experiments show improvements in sparsification compared to existing two step sparsifiers, without a degradation in accuracy. 

The initial review raised concerns about clarity, as well as some of the claimed significance of the results: the paper does not prove that an optimal, or even good sparsity structure is obtained — rather, it proves that the sparsity structure at convergence is obtained after finitely many iterations. The reviewers also raised a number of detailed concerns about the paper’s mathematical exposition. 

After considering the authors response, and a revision which significantly clarified both the paper’s notation and its main claims, the reviewers converged to a recommendation to accept. The paper provides a principled approach to sparsification, with supporting theory (albeit about finite identification, rather than optimality). The proposed algorithm appears quite practical and is supported by experiments demonstrating improvements over existing sparsification methods.
This paper argues several loosely related points about the evaluation of pretrained models on commonsense reasoning datasets in the Winograd style, and presents experiments with existing models on several datasets, including a novel 20 example benchmark. All four reviewers struggled to find a clear contribution or theme in this paper that is novel and thorough enough to meet the bar for publication at a selective general ML venue.

I d urge the authors to focus in on just one of these points and expand, and to consider submitting to a venue that more narrowly focuses on methods for commonsense reasoning in NLP.
The authors proposed a two stage algorithm for exploiting label smoothing and provided some analysis based on how label smoothing may have reduced the variance in the stochastic gradient. While the authors provided substantial experiments to justify their work (with additional ones during the response stage), none of the reviewers was very excited in the end, for obvious reasons perhaps: (a) the two stage algorithm is a straightforward combination of existing practices (first run with label smoothing and then run without label smoothing), without any new, interesting insight from the authors  side; (b) the analysis is a direct consequence of the authors  assumptions. Basically, if label smoothing reduces variance, SGD would converge faster and vice versa, which is nothing surprising or insightful. The key is to understand when and how any particular way to smooth the label would lead to significant reduction of the variance, which the authors did not provide any guidance or insight other than offering some empirical results. Overall, we do not believe this work, in its current form, adds significant value to our understanding of label smoothing.
The paper demonstrates that test error of image classification models can be accurately estimated using samples generated by a GAN. Surprisingly, this relatively simple proposed method outperforms existing approaches including ones from recent competitions. All reviewers agree this is a very interesting finding, even though theoretical analysis is lacking. Given the importance of the problem of predicting generalization, I recommend acceptance.
To improve the generative adversarial nets, the paper proposes to add an implicit transformation of the Gaussian latent variables before the top down generator. To further obtain better generations with respect to quality and diversity, this paper introduces targeted latent transforms into a bi level optimization of GAN.  Experiments are conducted to verify the effectiveness of the proposed method. The paper is highly motivated and well written, but the experiment part still needs to be strengthened because the goal of the paper is to improve the GAN training, comprehensive and thorough evaluation of the proposed method is necessary. 

After the first round of review, in addition to the clarification issue and missing reference issue, two reviewers point out that the method is only tested in small scale datasets, and suggest authors evaluate the performance of the proposed method in more complex datasets. Two reviewers point out that the experimental validation and comparison to prior approaches are insufficient. During the rebuttal, the authors provide extra experiment results to partially address some issues.  However, most of the major concerns from other reviewers, such as (i) how are the performance of the method in large scale datasets that have complex latent space manifolds, (ii) non convincing performance gain, and unclear problem setup, still remain. After an internal discussion, AC agrees with all reviewers that the current paper is not ready for publication, thus recommending rejecting the paper. AC urges the authors to improve their paper by taking into account all the suggestions provided by the reviewers, and then resubmit it to the next venue.
This paper addresses fair representation learning, with the aim of obstructing the recovery of sensitive features from the learned representation, hence enforcing the fairness of subsequent prediction tasks.  In the setting where probability density can be estimated for sensitive groups, Fair Normalizing Flows (FNF) tries to minimize the statistical distance between group wise latent representations, thereby providing theoretical fairness guarantees.  Experimental confirm the effectiveness of FNF in fairness, transferrability, and interpretability.

The paper received extensive and in depth discussion.  The rebuttal did an excellent job in clarification.  Although there are still some concerns on the theoretical properties of the optimal solution, overall the reviewers and myself find this paper interesting and worth publishing.
Dear Authors,

The response you have provided, based on the main concerns of reviewers, have answered most of the questions raised. 
As far as I understand from the added experiments you have provided, the proposed methodology shows resilience in being at least as good as state of the art approaches, while at the same time it is a mathematically interesting approach. 

Your response has covered concerns like comparison to other communication techniques (the comparison list is not complete, but yet your effort is appreciated), adding discussion on the rank parameter, add comments on expressiveness and the connection with low rank parameterization, etc. 

These efforts cannot be overlooked, and for that reason I suggest acceptance (poster).

Best

AC
In this paper, a novel machine learning based method for solving TSP is presented; this method uses guided local search in conjunction with a graph neural network, which is trained to predict regret. Reviewers disagree rather sharply on the merits of the paper. Three reviewers think that the paper is novel, interesting, and has good empirical results. Two reviewers think that the fact the results are not competitive with the best non learning based ("classic") solvers mean that the paper should be rejected.
This area chair believes that research is fundamentally not about beating benchmarks, but about new, interesting, and sound ideas. The conceptual novelty of this method, together with the good results compared with other learning based methods, is sufficient for accepting the paper.
The submission receives mixed ratings initially. Reviewer WK7k stays positive, edUG and qbmC are borderline, and MXU1 is negative. They raise several issues including limited technical contribution, insufficient sota experimental comparison, and detailed theoretical proof. The authors have responded to these issues in the rebuttal. The final ratings of these reviewers do not alter. 

After checking the revised manuscript, all the reviews and responses, the AC feels the review from MXU1 lacks sufficient details and the claim shall be better supported with evidence. On the other hand, the issue of the technical contribution raised by edUG still exists. The modification upon original CL is marginal. Also, the experimental validation, although tested in several configurations, only includes ResNet 50 as the encoder backbone. The sufficient validation upon different scales of CNNs are common strategies used in sota SSL methods (e.g., MoCo, BYOL, SimCLR). Without a thorough evaluation, the effectiveness of CACR on different scales of CNN backbones is in doubt. Meanwhile, the experimental comparison upon sota methods (e.g., BYOL, SwAV+multi crop) does not clearly show the performance advantages of CACR. The small scale dataset utilized for ablation study (i.e., CIFAR) is not as convincing as ImageNet. 

Overall, the AC feels that the marginal technical contribution is acceptable, but it shall be equipped with thorough experimental validation and thus serves as a fundamental baseline (i.e., potentially benefits the SSL community). Based on the current form, the authors are suggested to further improve the current submission and are welcome to submit for the next venue.
This paper proposes to address the problem of domain adaption using Knothe Rosenblatt transport withe the method denoted as KRDA . The main idea is to perform density estimation of the different distributions with mixture of Gaussians and then estimate a  an explicit mapping between the distribution using  Knothe Rosenblatt. Experiments show that the proposed method works well on toy and real life datasets.

 The paper had low score during the reviews (3,3,3,3). While the reviewers appreciated the idea, they felt that the originality of the method is not well justified compared to a number of existing UDA approaches using OT. Also the reviewers noted several important references missing and that should also be compared during the numerical experiments. A discussion about the limits of the method in high dimension would also be very interesting.

The authors did not provide a reply to the reviewers  comments so their opinion stayed t same during the discussion. The paper is then rejected and the AC strongly suggests that the authors take into account the numerous comments from the reviewers before re submitting ton a new venue.
This paper starts from the observation that a certain class of rescaled gradient flows   referred to in the paper as RGF and SGF   converge to a solution in finite time (Wibisono et al., 2016; Romero and Benosman, 2020). As a result, it is plausible to ask whether the Euler discretizations of these flows   viewed now as optimization algorithms   enjoy superior convergence properties or not. The authors  main results establish a linear convergence rate under a certain gradient dominance condition, as well as linear convergence to an $\epsilon$ neighborhood of a solution if the algorithms are run with minibatch gradients of size $O(1/\epsilon^\rho)$ for some positive exponent $\rho>0$.

The reviewers raised several concerns regarding the motivation of the authors  work and the comparison of the rates they obtain to other related papers in the literature. The reviewers that raised these concerns were not convinced by the authors  rebuttal and maintained their original assessment during the discussion phase.

From my own reading of the paper, I was perplexed by the fact that the authors did not compare the rates they obtained to existing results in the context of KL optimization, such as the cited paper by Attouch and Bolte and many follow up works in the area. Also, in the stochastic part, while the authors argue that "utilizing batches with size dependent on $1/\epsilon$ is absolutely reasonable and usual, in both theory and practice", it should be noted that a high accuracy requirement (small $\epsilon$) could lead to completely unreasonable batch sizes (effectively exceeding the size of the dataset, especially when $\psi$ is small). Thus, while it is possible to achieve convergence to arbitrarily high accuracy with a sufficiently small step size for a _fixed_ batch size, the rate of this convergence cannot be linear overall   in contrast to the way that the authors frame their result.

In view of the above, I concur that the paper does not clear the bar for ICLR, so I am recommending rejection at this stage (but I would encourage the authors to resubmit a suitably revised version of their paper at the next opportunity).
The paper studies the use of pretrained language models (LM) for training the policy in embodied environments. Specifically, a pretrained GPT 2 LM is used to initialize the policy. Environment observations, goals, and actions are encoded appropriately (e.g., converted into text strings) to apply the LM based policy. The experiments study the generalization effect of initializing with pretrained LMs. Reviewers have found the paper made limited contributions. In particular, prior works on text adventure games have explored the use of pretrained LMs for playing games and studied the generalization effect, such as [1]. It s also suggested that the paper should revise the claims made in the experiments, given the limited experimental scope and results. 

[1] Keep CALM and Explore: Language Models for Action Generation in Text based Games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan. EMNLP 2020.
This paper considers the question of whether recent concept based learning algorithms, as well disentangled representation learning algorithms, result in high quality representations. In particular, the authors consider what high quality should mean in terms of the relationship with ground truth concepts and the ability to make accurate predictions for a downstream task. To this end, they propose two main metrics for representations that are explicitly or implicitly encouraged to encode concepts. While the premise of this paper has been appreciated by the reviewers, some concerns about the details of the metrics proposed and experimental results which have been raised by the reviewers remain post rebuttal. Given this, we are unable to recommend the acceptance of the paper at this time. We hope the authors find the reviewer feedback useful.
The paper formulates ResNet like classifiers as the evolution of a base classifier through an operator corresponding to a PDE, up to a given final time. Using a set of assumptions on the desired properties of the flow operator, the authors show that it can be obtained as the solution of a convection diffusion equation. This generalizes ideas developed e.g. for Neural ODEs. The authors provide several examples showing that their formulation encompasses regularization methods proposed for deep NNs.  They further provide robustness guaranties for a classifier defined according to their framework. They introduce an algorithm based on a restricted version of their framework and propose different experiments showing the increased robustness of their model to a family of adversarial attacks compared to baseline ResNets.

The paper introduces an original idea, providing a very interesting connection between ResNets and PDEs. This allows the authors to exploit known properties of PDEs and opens the way to new theoretical insights on DNNs while allowing the development of DNN models with proved properties. As mentioned this generalizes the view of ResNets introduced in Neural ODEs. 
Besides, the paper presents weaknesses. First the form will make it accessible only to a very small audience in the ML community. No effort is made in the writing to introduce the required PDE concepts that would help a lot understanding and appreciating the contribution. This is a pity since given the current trend on this topic this could be of interest to a large community. Then the use cases in the experiments focus solely on robustness properties and one type of attacks. This illustrates only one aspect of the potential of the framework, and this does not provide a strong case in support of the ideas introduced before. The global message carried out by the paper then becomes unclear. Overall, the current version could be largely improved and this will certainly lead to a strong contribution.
In this paper  propose a novel approach for semi supervised domain adaptation based on the cyclic monotonicity property of optimal transport map. The main idea is to adapt (perturbed wrt a source classifier)  the labeled source samples  toward the target samples while preserving the known labels via the cyclical monotonicity. Then these perturbed samples can be used to perform classical OT domain adaptation. This pre processing of the data has been shown in the numerical experiments to lead to better performance in average.

The proposed method has been found intriguing by all reviewers but the writing of the paper has been found clearly lacking and several suggestions were proposed by the reviewers. The choice of the authors to call the perturbed samples adversaries for instance made the paper harder to understand and the (anti adversarial is also not a good choice of words). Another concern was that despite encouraging numerical results lack more baselines semi supervised Domain Adaptation methods discussed by the reviewers were not compared (with or without OT). 

The authors provided a short but clear response that was appreciated by the reviewers. But the clarifications promised by the authors were not done in the PDF during the editing period which means that the paper clearly needs a new round of reviews.  For this reason the consensus during the discussions was that this paper should be rejected. The AC believes that this is an interesting research direction that should be investigated but that the paper needs some more work before reaching the threshold for acceptance in selective ML venues. The authors are strongly encouraged to take into account the comments form the reviewers before resubmitting their work.
This paper tackles a bandit problem that incorporates three challenges motivated by common issues encountered in online recommender systems: delayed reward, incentivized exploration, and self reinforcing user preference. The authors propose an approach called UCB Filtering with Delayed Feedback (UCB FDF) for this problem and provide a theoretical analysis showing that UCB FDF achieves the optimal regret bounds. Their analysis also implies that logarithmic regret and incentive cost growth rates are achievable under this setting. These theoretical results are supported by empirical experiments, e.g. using Amazon review data. The main concern with this paper is that the considered challenges have all been tackled already in different bandit settings, so the novelty here is that they are being tackled altogether. It would be more convincing if experiments included baselines from these existing settings to motivate the need for a new strategy rather than simply relying on methods that have been proposed previously to address each of these problems independently; the experiments currently contain only a baseline for bandits with self reinforcing user preference, which has been added during the rebuttal phase.
This paper was close and also very polarizing with the reviewers. On the positive side, some reviewers found:
1. the results impressive
2. the proposed method to be novel, interesting, and produce good performance across several settings
3. the paper was well written

On the other hand, others found:
1. the motivation suspect
2. missing experiments to characterize the sensitivity to numerous hyper parameters 
3. the baselines compared with weak and not representative
4. significant performance drop comparing the results in the original submission and the new ones added during discussion period
5. low number of seeds initially

In the end, multiple reviewers raised serious issues regarding the motivation for the approach and the quality and ultimately credibility of the results presented. One of the high scoring reviewers agreed the paper was a bit misleading (limitations relegated to the appendix). Unfortunately, none of the high scoring reviewers provided counters to this points.
This paper presents an approach that uses ASR based scores to guide masking high confident blocks for speech representation learning. As most of the reviewers mentioned, it is an incremental improvement over baseline systems with limited novelty. About the use of confidence scores which is a key factor of the method, it lacks enough discussion on its quality and sensitivity.
The reviewers overall thought the problem was worth studying.  However, no reviewer was particularly excited about this work.  The main concern was that the new problem formulation is difficult to compare to prior work. Reviewers felt both more explanation and a deeper detailed comparison would make this a stronger paper.
All reviewers concur that the paper has promise, but fails to deliver on that promise.  The idea of learning potentials based on DNNs is appreciated, but the evaluation of the contribution is considered lacking by all reviewers.  In addition, reviewers note that the training is not differentiable, which the rebuttal acknowledges is future work.

I do not reject the paper simply for failing to beat a deep learning baseline, but for having chosen applications which do not even test the paper s hypotheses: reviewers note that the models are tree structured, so loopy BP is not tested, despite the revised paper s claim that "the inference strategy is compatible with graphs containing cycles".
The paper proposes to compute local representations on device, which are then shared between clients using an alignment mechanism. Reviewers did appreciate the value of the topic and several contributions, but unfortunately consensus is that it remains below the bar, even after the discussion phase. Concerns remained on privacy and motivational positioning with FL, and lack of simpler baselines, even after the author feedback.

We hope the detailed feedback helps to strengthen the paper for a future occasion.
This paper studies the problem of distilling the knowledge present in different GAN based image generation tasks. The paper received mixed reviews. The reviewers had difficulty understanding some details regarding the approach, and requests for ablations and clarifications on existing empirical evaluation. The authors provided a strong thoughtful rebuttal that addressed many of those concerns. The paper was discussed and two reviewers updated their reviews in the post rebuttal phase. Reviewers generally agree that the paper should be accepted but still have concerns regarding contribution and writing. AC agrees with the reviewers and suggests acceptance. However, the authors are urged to look at reviewers  feedback and incorporate their comments in the camera ready.
In this work the authors consider the automatic selection of time series forecasting model (and hyperparameters) based on historical data. It adopts a conventional feature based meta learning approach. Experimental results show an improved performance over the considered baselines.

The reviewers appreciated the clarifications provided by the authors, but a number of concerns were unresolved. For instance, questions remained regarding the dataset collection, the baselines against which the proposed method was compared to (which were considered too weak) and the large number of missing details in the presentation of the method. Based on this the reviewers concluded that the paper could not be accepted in its current form and would require a major revision.
This work aims to improve style transfer in the unsupervised non parallel case. It does this by proposing a style equalization approach to prevent content leakage and assuming that content information is time dependent whereas style information is time dependent. This is an important problem to solve and lots of prior work in the area exists. The work is well organised with good experimental results. However, there are strong claims in the paper and there is insufficient experimental comparison to similar related work such as Hsu et al. 2019 and Ma et al. 2018 to back that up. If there s no comparison with the current state of the art (e.g. due to a private implementation or dataset) then it s hard to justify calling a new work a new state of the art. Even though an implementation may be private, it can be worth spending time to reproduce a paper or asking the authors for an implementation. Finally task and metric selection could be improved to better highlight the performance of the approach. The reviewers thank the authors for the rebuttal but it was insufficient to change their decision.
This paper provides some novel ideas combining neural processes, active learning, and deep sequence models toward accelerating the computationally intensive task of stochastic simulations for epidemic models. The authors incorporate aspects such as spatiotemporal dependence and age structure in the models they consider, and propose an active learning framework to leverage a neural process that can serve as a proxy for direct simulation of the stochastic dynamics. Most of the reviewers agreed that some of these ideas are novel, but the assessments together present a borderline case, and one reviewer remains convinced that the paper needs refinement and a more focused exposition to make clear the contributions and relative efficacy compared to existing state of the art. I tend to agree with some of the concerns raised by that reviewer, who responded favorably to the author response but remained not fully convinced.I also agree that important parts of the paper feel rushed, if I am to interpret "feeling rushed" as a statement on the ideas being spread thin in exposition due to the amount of ground the authors try to cover (rather than a statement on the quality of the presentation). There are many moving parts that combine good intuitions toward accelerating simulation based methods for stochastic epidemic models. Several reviewers mentioned improving the empirical comparisons with recent existing methods, including likelihood based methods, and though I agree with the authors too that it is not possible to exhaustively explore outcomes in stochastic process approaches, the largely empirically supported contributions can be better motivated with a more complete comparison and streamlined exposition. I encourage the authors to continue to revise and refine this manuscript to maximize the potential behind the ideas they propose here.
This paper was particularly discussed between the reviewers, the AC and SAC. A last minute reviewer was also called to clarify some issues raised, as one of the reviews never got into the system.

The paper was overall perceived as well written and well presented, and that the software contribution of implicit differentiation techniques is a nice asset for the community, especially its modularity.
The stability guaranty constitutes a nice (though straightforward) result providing a theoretical ground for the proposed approach.
Yet, the paper is often loose on mathematical justifications, in particular on minimal validity assumptions. Details on when the proposed framework could fail would be of interest, both on theoretical and practical parts. A discussion on the minimal assumptions required for validity of the approach should be highlighted more in the text.

Furthermore, the paper lacks discussions and comparisons with concurrent works,
for instance how would the framework compare with existing estimates for implicit differentiation or for unrolling. This could be improved along with providing more analysis on the implementation efficiency.
On the practical part, a high level description the software details would also be much beneficial.
A core discussion focused around what should be expected of this type of paper (i.e., "implementation issues, parallelization, software platforms, hardware" papers as suggested by Q3Lr)

A point of concern was the novelty aspects in the discussion phase was the novelty of the proposed framework: even if the contribution is the framework introduced, this is not new per se (the literature on implicit differentiation now contains a considerable amount of results and implementation examples).
The relevance of the work, both on theoretical and computational aspects, beyond the development of a computational library was found difficult to assess by several reviewers.
Overall, the reviewers judged the novelty and the paper s contribution more on the software side. Hence, a core discussion could focus on aspects expected for code oriented papers (i.e., implementation issues, parallelization, hardware, etc.).

Following the long discussion phase (more than 30 posts on OpenReview) and the aforementioned comments, the paper was rejected.

We encourage the authors to submit a revised version in a future conference or possibly to a software oriented journal, such as JMLR MLOSS or JOSS for instance.
The paper studies an interesting question of whether neural networks can approximate the target function while keep the output in the constraint set. The constraint set is quite natural for  e.g. multi class classification, where the output has to stay on on the probability manifold. The challenge here is that traditional universal approximation theory only guarantees that $\hat{f}(x) \approx f(x)$, but can not guarantee that $\hat{f}(x)$ lies exactly in the same constraint set as $f(x)$.

The paper made a significant contribution in the theory of deep learning   It is shown that the neural network can indeed approximate any regular functions while keep the output stay in the regular constraint set. This gives a solid backup in terms of the representation power of neural networks in practice, to represent target functions whose outputs are in certain constraint set (e.g. probabilities).
I thank the authors for their submission and active participation in the discussion. The reviewers unanimously agree that this submission has significant issues, including comparison to baselines/ablations [BnLV,yX9d,PtA1], clarity [BnLV], justification of the method [nX4W]. Thus, I am recommending rejection of this paper.
Dear Authors,

The paper was received nicely and discussed during the rebuttal period. However, the current consensus suggests the paper requires another round of revisions before it gets accepted. 

In particular:

  it is not clear if the new method with randomization improves over the deterministic methods, either in theory and practice.
  it is not clear how the assumptions made in this work compare to the existing ones and what the implications are, in terms of applications.

Reviewers were not satisfied by the replies received during the rebuttal period. 
One reviewer stated that the argument "first coordinate method for this setting" is valid, but not sufficient to justify publication at this stage. 

Best
AC
This paper finally received divergent and borderline reviews with one positive (6) and two negative (5) rates. Based on the reviews, authors’ responses and updated manuscript, we would like to decide to reject this work at this time even though this submission has a lot of potentials such as simplicity and efficiency.
Positively, all the reviews agree that the proposed approach is simple but effective to improve the robustness of few shot classifiers. However, there is some room for improvement to be a stronger submission: (i) the technical novelty may need to be better presented, and (ii) the improved performance may need to be better justified (e.g., the effect of the pretrained stage).
A novel method is described that uses RL to search for a rule set which predicts multiple relations at once for KBC like problems.  The rules can include latent predicates, which reduces the complexity of individual rules, similar to Cropper & Muggleton s (2015) meta interpretive learning framework, which is usual for rule learning systems.  Another novel aspect is use of a cache memory for rules.

Pros
   the idea of using RL instead of carefully designed discrete search for symbolic learning systems is a very nice novel idea
   the experimental results are strong

Cons
   the benchmarks are synthetic (although GraphLog does at least include noise)
   although the Cropper and Muggleton work is cited the relationship between the search spaces of the two systems is not discussed   this should be corrected in the final version.
The paper presents a method for cooperative ad hoc collaboration by learning latent representations of the teammates. The method is evaluated in three domains. All the reviewers agree that the method is novel and adds an interesting contribution to the important and difficult problem of the ad hoc collaboration, making fewer assumptions about the team and the teammates.

The next version of the paper should comment:

  On the societal impact of the centralized training.
  Wang et al, CoRL 2020, https://arxiv.org/abs/2003.06906, which addresses the cooperative tasks in the ad hoc teams without privileged knowledge and assumptions about the teammates.
Dear authors,

I apologize to the authors for insufficient discussion in the discussion period. Thanks for carefully responding to reviewers. Nevertheless, I have read the paper as well, and the situation is clear to me (even without further discussion). I will not summarize what the paper is about, but will instead mention some of the key issues.

1) The proposed idea is simple, and in fact, it has been known to me for a number of years. I did not think it was worth publishing. This on its own is not a reason for rejection, but I wanted to mention this anyway to convey the idea that I consider this work very incremental. 
2) The idea is not supported by any convergence theory. Hence, it remains a heuristic, which the authors admit. In such a case, the paper should be judged by its practical performance, novelty and efficacy of ideas, and the strength of the empirical results, rather than on the theory. However, these parts of the paper remain lacking compared to the standard one would expect from an ICLR paper. 
3) Several elements of the ideas behind this work existed in the literature already (e.g., adaptive quantization, time varying quantization, ...). Reviewers have noticed this.
4) The authors compare to fixed / non adaptive quantization strategies which have already been surpassed in subsequent work. Indeed, QSGD was developed 4 years ago. The quantizers of Horvath et al in the natural compression/natural dithering family have exponentially better variance for any given number of levels. This baseline, which does not use any adaptivity, should be better, I believe, to what the author propose. If not, a comparison is needed. 
5) FedAvg is not the theoretical nor practical SOTA method for the problem the authors are solving. Faster and more communication efficient methods exist. For example, method based on error feedback (e.g., the works of Stich, Koloskova and others), MARINA method (Gorbunov et al), SCAFFOLD (Karimireddy et al) and so on. All can be combined with quantization. 
6) The reviewer who assigned this paper score 8 was least confident. I did not find any comments in the review of this reviewer that would sufficiently justify the high score. The review was brief and not very informative to me as the AC. All other reviewers were inclined to reject the paper. 
7) There are issues in the mathematics   although the mathematics is simple and not the key of the paper. This needs to be thoroughly revised. Some answers were given in author response.
8) Why should expected variance be a good measure? Did you try to break this measure? That is, did you try to construct problems for which this measure would work worse than the worst case variance? 

Because of the above, and additional reasons mentioned in the reviewers, I have no other option but to reject the paper.

Area Chair
This paper offers "natural attribute" methods for shift detection.  Several reviewers are positive, but reviewer czxP is the most authoritative in the eyes of the area chair.  In particular the AC is concerned that the task that this paper defines is artificial and not useful. Naturally occurring shifts are real, happen all the time and meaningfully affect model performance... Natural shifts should be defined over distributions not singular instances. The authors create artificial instantiations of natural shifts to illustrate a well known flaw in OOD detection algorithms.
To demonstrate the usefulness of this approach to "natural shifts" the authors should should show how this algorithm performs in settings like "WILDS"... where the shift are meaningful and not artificial, in the opinion of the AC.  The paper should cite and contrast WILDS (https://arxiv.org/abs/2012.07421) and Mandoline (http://proceedings.mlr.press/v139/chen21i.html) in a future revision.
The paper aims to improve image and video compression keeping in mind computation cost. In this regard, authors propose variants of Swin Transformer for image and video coding. The experimental results shows that Transformer based transforms can replace Conv based transforms in image and video compression, and simultaneously achieving better rate distortion performance at much faster decode times, i.e. resulting in a better rate distortion computation trade off. We thank the reviewers and authors for engaging in an active discussion. The reviewers found some results to be surprising (in a good way) and are in a consensus that the empirical results are strong across datasets for image and video compression. For completeness, the authors should provide FLOPs or CPU runtimes in the final version so that one can compare to methods like VTM even if CPU is not the desired hardware for proposed method.
The authors make a case for a phenomenon of deep network training
that they call the "silent alignment effect": that, while the training
error is still large, the NTK associated with the network aligns its eigenvectors
with key directions in "feature space".  They support this with non rigorous theoretical
analysis of linear networks, and extensive experiments with real networks on real data.
The consensus view was that this paper provides novel and useful insight into training
dynamics, in particular regarding feature learning.
The paper presents a continuous framework for GNNs based on neural diffusion PDE and is an evolution of a previous method (GRAND). The main novelty appears to be the additional source term, which the author show to be beneficial in reducing the oversmoothing effect typical in deep GNNs. While novelty is somewhat limited, the paper provided detailed theoretical and experimental assessment of the idea. Overall, the reviewers liked the approach and expressed some questions/concerns that were satisfactorily addressed in the rebuttal. We recommend acceptance.
Realizing the fact that cross entropy loss and focal loss are widely used for training deep learning models but mathematical understanding and exploration for such losses are lacking, the authors propose a simple framework named PolyLoss to express the loss function as a linear combination of polynomial functions. 

In this framework, the aforementioned cross entropy loss and focal loss are the special cases of PolyLoss by easily adjusting the importance of different polynomial bases depending on the targeting tasks and datasets. The final version of PolyLoss, Poly 1 formulation, is simple with one line of code and an extra hyperparameter but outperforms the cross entropy loss and focal loss on 2D image classification, instance segmentation, object detection, and 3D object detection tasks, sometimes by a large margin.

This paper is well motivated by a novel perspective of polynomial expansion. The proposed method is novel, simple to implement, and effective in practice. The authors have a deep and thorough discussion with reviewers, and most concerns were well addressed. After rebuttal and discussion, reviewers increased their scores, and all agreed with acceptance. AC checked the paper and all relevant information, and found sufficient ground for acceptance.
This paper proposes a federated averaging Langevin dynamics (FA LD) for numerical mean prediction with uncertainty quantification under the setting of federated learning. Convergence analysis for the proposed method under the smoothness and strong convex assumptions is also provided, and the results are summarized in Theorems 5.7 5.10, each of which bounds the Wasserstein 2 distance $W_2(\mu_k,\pi)$ between the model distribution $\mu_k$ and the target distribution $\pi$ under different settings.

This paper received 5 reviews in total, with scores 6, 5, 3, 5, and 3. Some reviewers evaluated positively the novelty of the idea of using the Langevin dynamics in the federated setting, which I would also like to acknowledge. Upon reading the paper by myself, however, I find that the mathematical formulations are in some places not correct. What I think problematic is the third equation in equation (3): The right hand side is a function of $N$ variables $\\\{\theta_k^c\\\}$, and they undergo different local updates at different clients when $k\not\equiv 0\mod K$ (i.e., the synchronization does not take place). Also $\nabla\tilde{f}^c$ is in general a nonlinear function of its argument. Therefore, the right hand side cannot be written in general as a function of a single variable $\theta_k$ which is defined as $\theta_k \sum_{c 1}^Np_c\theta_k^c$, making this equation incorrect. This problem would affect various parts of the arguments to follow in this paper, such as the first two equations in equation (16) on page 14, the two inline equations just after equation (16), equation (18), the second equality in the inline equation in page 15, line 1, and the third line in equation (25) on page 18, to mention a few. Thus I have to question the validity of the theoretical development in this paper.

Another point I would like to mention is that I did not understand the definition of Schemes I and II in Section 5.4. It is not stated at all that $\mathcal{S}_k$ is a random quantity here. Furthermore, the conditions "with/without replacement" are not described at all.  Still another point to mention is that I did not understand the claim in page 7, lines 30 31. Does it mean: If one knows the number $T_\epsilon$ of steps to achieve the precision $\epsilon$, then one should set the number $K$ of local steps per synchronization should be set of the order of $\sqrt{T_\epsilon}$. But $T_\epsilon$ depends on $K$, so that it would be unnatural to assume that one knows $T_\epsilon$ irrespective of $K$ in the first place.

Because of these, I would judge that this paper is not yet ready for presentation in its current form. I would therefore not be able to recommend acceptance of this paper.

Minor points:
  Citation style: The authors use throughout the paper what are called the *narrative citations* even though there are occasions where what are called the *parenthetical citations* (the author name and publication date are both enclosed in parentheses) should be used.
  page 3, line 7: is (the  > an) unbiased stochastic gradient; There are several unbiased estimators for the gradient, and what is mentioned here is only one instance of them.
  page 3, lines 23 24: The aggregation should take place not on each client but on the central server.
  page 3, line 36: a(n) energy function; a(n) unbiased estimate
  page 5, lines 17 20: The contents of Assumptions 5.1 and 5.2 are not assumptions but definitions.
  page 6, line 2: to obtain (the  > a)  lower bound
  page 6, line 18: $\mathcal{D}^2$ is undefined.
  page 8, line 39: (a  > the) probability $p_c$ if it is meant to be the one defined in page 3, line 8. Otherwise, use of the same symbol to represent different quantities should be avoided.
  page 14, line 25: mod ($E$  > $K$)  0
  page 15, line 30: $H_\rho^2$  > $H_\rho$
This paper introduces a first occupancy representation for reinforcement learning problems, with potential benefits on problems with non stationary rewards.  The representation is defined analogously to the successor representations, but captures the expected discounted time to first arrive at a state instead of measuring discounted visitations.  The paper develops the idea and illustrates some uses for exploration, unsupervised RL, and non stationary reward functions (for example when food rewards are consumed).

The reviews brought forward a number of related older ideas in the literature, where several aspects of the method have been previously developed.  These include dynamic goal learning, option conditional predictions, general value functions, dynamical distance learning, and temporal difference models.  However, from the author response and ensuing discussion, the exact form of the proposed representation has not been studied for the purposes presented in this paper.  The reviewers appreciated the utility of this representation for problems with non Markovian rewards, in particular that “the use of the first occupancy values as an exploration bonus results in much more efficient exploration”.  Multiple reviewers commented on the desire for a stronger empirical evaluation, but they were satisfied with the contribution of the paper.

The reviewers arrived at a consensus that the paper contributes a new representation for RL problems with non stationary rewards, with two reviewers strongly convinced and none opposed.  The paper is therefore accepted.
The AC and reviewers all agree that the paper proposes a very interesting framework to extend Granger Causality to DAG structured dynamical systems with important applications. 

The submission was the object of extensive discussion, and the AC and reviewers all agree that the author feedback satisfactorily addresses the vast majority of their concerns. We strongly urge the authors to incorporate all the points and revisions mentioned in their feedback. 

We certainly hope that the author will pursue this line of work and consider scaling their approach to tackle larger applications such as those related to social networks.
All reviewers recommended rejection, and I agree.
I encourage the authors to follow the reviewers  recommendation and resubmit.
This paper presents PiCO, a novel approach for partial label learning, which achieves very strong performance close to that of fully supervised learning and outperforms PPL baselines. The experiments are extensive with very impressive results and the analysis are thorough.
Three reviewers had a positive impression of this paper, two of them were willing to champion it. The main positive aspects mentioned by these reviewers were clarity, methodological strength, novelty and convincing experimental evaluation. On the other hand, the was one clearly negative vote, raising issues about the proposed concept of  entropy of entanglement  and about the use of tensor products. It seems that after the rebuttal, this reviewer was still not fully convinced. In my opinion, however, the rebuttal addressed most of these points of criticism in a clear and transparent way, so I recommend acceptance.
The reviewers were not convinced by the authors  responses to their concerns, and this paper generated little followup discussion. Some primary concerns include the privacy analysis, limited technical contribution and scope (e.g., only being applicable to iid data), and lacking comparison to suggested baselines. The authors are suggested to take the reviewer comments into account for further investigation.
The paper presents improvements to AlphaZero and MuZero for settings where one is restricted in the number of rollouts. The initial response from reviewers was generally favorable  but the reviewers wanted more details and clarifications of multiple parts of the paper, and further intuition about the Gumbel distribution.  The authors’ responses were detailed and convinced or maintained strong positive support of most reviewers. The authors also stated that they plan to provide a release of the code and also provided a policy improvement proof. Overall this is an interesting approach that is likely to be of significant interest to many.
The paper presents an RL approach to the problem of graph sparsification. The reviewers expressed concerns about novelty, presentation, the correctness of some claims, and experimental validation. While the authors provided rebuttal and addressed some questions (leading to the increase of the score), some reviewers thought the authors focused on a justification of why suggested experiments were not done rather than doing them. We believe the paper in its current state is below the bar and recommend rejection.
This paper presents an extension to the MAT model by using relative attention that considers graph level distance, geometric distance, and bond type between nodes during the attention computation.  This is shown to lead to improved performance on several benchmark datasets against MAT and GROVER.  The inclusion of additional information into the attention computation is a sensible and natural choice for transformer with the success of relative positional embedding in the NLP and vision domains. But it is somewhat a straightforward extension of the existing ideas from other domains to MAT, hence the novelty is somewhat limited.  It is also worth noting that the proposed method should be considered in the context of the larger body of research on 3D GNNs. The authors drew inspiration from DimeNet s design in the encoding of geometric distances but do not consider it in the empirical comparisons. Instead, it focused exclusively on transformer based models. This limits the scope of conclusions that we can draw from these experiments and makes it difficult to gauge the practical impact of RMAT in comparison to many other GNN methods that uses 3D geometries of the molecule.
The paper proposes few shot robust (FROB) model for classification and few shot OOD detection. While the paper has some interesting contributions, all the reviewers felt that the current version falls short of the ICLR acceptance threshold and the consensus decision was to reject. I encourage the authors to revise the paper based on the reviewers  feedback and resubmit to a different venue.

As Reviewer r838 pointed out, that this paper uses TinyImages dataset which has been since retracted. I appreciate that prior work used TinyImages, but please see "Why it is important to withdraw the dataset" https://groups.csail.mit.edu/vision/TinyImages/ and consider not using the TinyImages dataset for future revisions.
This paper tackles a small batch online unsupervised learning problem, specifically proposing an online unsupervised prototypical network architecture that leverages an online mixture based clustering algorithm and corresponding EM algorithm. Special features are added to deal specifically with the non stationary distributions that are induced. Results are shown on more realistic streams of data, namely from the RoamingRooms dataset, and compared to existing self supervised learning algorithms including ones based on clustering principles e.g. SWaV. 

Overall, the reviewers were positive about the problem setting and method, but had some concerns about hyper parameters (hYzM, cvrN, LjvY) and motivation for the specific setting where the method excels compared to other methods not designed for such a setting (hYzM, cvrN), i.e. small batch setting, where it is not clear where the line should be drawn in terms of batch size and memory requirements with respect to performance differences between the proposed approach and existing self supervised methods. Importantly, all reviewers had significant confusions about all aspects of the work ranging from low level details of the proposed method to the empirical setting and evaluation (including for competing methods). After a long discussion, the authors provided a large amount of details about their work, which the reviewers and AC highly appreciate. However, in the end incorporating all of the feedback requires a major revision of the entire paper. Even the reviewers that were more on the positive side (cvrN and LjvY) mentioned it would be extremely beneficial for this paper to be significantly revised and go through another review. Since so many aspects were confusing, it is not clear to the AC that the underlying method, technical contributions, and other aspects of the works had a sufficient chance to be evaluated fairly, given that much of the review period was spent on clearing up such confusion. 

In summary, while the paper is definitely promising and tackles an important area for the community, it requires a major revision and should go through the review process when it is more clearly presented. As a result, I recommend rejection at this point, since it is not ready for publication in its current form.
The paper proposes an improvement to graph based neural network, by improving their attention mechanism (introducing recursive attention and jumping knowledge attention) to flexibly attend to its neighborhood. The paper shows solid experimental results over competitive baselines, as acknowledged by reviewers. The reviewers agree that the paper is clearly written, but overall have issues with the novelty of the approach. The paper combines multiple components (last residual connection module, improved attention mechanism) to show gains, but none of the pieces are very new.
The paper presents a new approach for distinguishing synonyms and antonyms via an extension of a parasiamese neural network, called "the repelling parasiamese network".  The strengths of the paper, as identified by reviewers, are a novel architecture for antonymy detection, a new dataset, and solid empirical results. However, there are major drawbacks identified by reviewers w5dj and hoTU. Specifically, there are clarity issues in writing, lack of a proper justification to the proposed architecture, insufficient details about the quality of datasets, insufficient contextualization in prior work. The scores are borderline, but unfortunately, the authors did not use the rebuttal opportunity to sufficiently address these questions/concerns raised by the reviewers. I thus recommend to reject the paper.
This paper proposes a new algorithm to solve the discrete optimal transport problem, consisting of showing how the well known Douglas Rachford algorithm can be efficiently applied, and also providing a convergence rate. Secondly, the paper gives an efficient implementation suitable for GPUs.

The paper would be a bit weak on just the DR contribution alone, so the efficient implementation, and experiments, are significant. However, the reviewers were not consistently happy with the experiments.

Two big issues raised by reviewers were wanting more experiments (having comparison experiments with real world data), and wanting to compare with variants of the baseline algorithm (Sinkhorn), such as the log transformed version which is more stable.  Looking at the revision, I think the reviewers have done a good job adding experiments.  Overall, for a paper with strong theoretical components, I think the computational aspects are strong.

As for (not) comparing with the log transformed Sinkhorn and other more robust methods, the authors argue that this implementation would be slower.  I agree with reviewers that it would be nice to have these comparisons, but the authors  argument is plausible and I don t find it grounds to reject the paper.

Overall, it seems there is some evidence that this is a worthwhile method, and there are no theoretical concerns other than presentation issues. Thus I think it would be a benefit to the community to accept this paper.
This paper presents a hierarchical Bayesian approach to exploration in grid worlds.  The paper considers the hypothesis that humans maintain a hierarchical representation when exploring a space, where the distribution over unknown space can be modeled with a structured probabilistic program.  The paper compares the behavior of people during exploration tasks to the behavior of a Bayesian model under different distributional approximations.  The results indicate that people can behave similarly to a sophisticated Bayesian model on small grid world domains.

The reviews highlighted several concerns about the paper.  One initial concern was that the experimental domain is too simple and small compared to real world environments encountered by robots or humans.  However, this work is similar in scope to other exploration work in reinforcement learning and psychology studies, where tiny grid worlds are still commonly used to gather insight.  Thus, this concern does not reduce the potential contribution of the paper.  The reviewers raised several other concerns about the work that were largely addressed by the author response.  The remaining reviewer concerns centered on the limited strength of the evidence in the experiments, but the reviewers expect the paper will still be of interest to the broader research community.

Four reviewers indicate to accept this paper for its contribution of a study into the use of probabilistic program induction to infer possible completions of maps in small environments.  The paper is therefore accepted.
Strengths:
* Strong results across two benchmarks
* Ablation study demonstrates importance of components
* Provides improvements especially in low resource settings
* Well written paper

Weaknesses:
* Novelty of the method may be limited as previous works have explored structured outputs as intermediate plans
* Not clear method will extend to other domains as decent AMR parses are required to train the imagination module, which might work well on the datasets used (e.g., RocStories), but wouldn t work in settings with more complex language
This paper proposes a variant of stochastic gradient descent that parallelizes the algorithm for distributed training via delayed gradient averaging. While the algorithm (DaSGD) proposed is sensible and seems to work, it also seems to miss a lot of related work. As pointed out by one of the reviewers, the class of asynchronous decentralized methods already seem to cover the space of DaSGD, and it s not clear how DaSGD differs from the existing methods in this space. As a result of this lack of comparison to related work, the reviewers recommended that the paper not be accepted at this time, and this evaluation was not challenged by an author response. I agree with this consensus.
The initial reviews for this paper were 6,6,6, the authors have provided a rebuttal and after the rebuttal the recommendation stayed the same. The reviewers have reached the consensus that the paper is borderline but they have all recommended keeping it above the acceptance threshold. Following the recommendation of the reviewers, the meta reviewer recommends acceptance.
Reviewers have all agreed that this paper studied an important problem and made valuable contributions. The goal is to reduce the communication costs  of Federated learning where the data are stored in different parities based on subsets of features.  The paper developed the theory to show guaranteed convergence and provided empirical evaluations to validate the theory.  

On the other hand, compared with existing literature, Reviewers feel that the novelty of this submission appears limited and the improvements seem to be incremental. Reviewers appreciate the Authors  efforts in conducting the detailed rebuttals and providing an improved manuscript. We hope the authors would continue to improve the paper based on reviews, when they prepare for their future submission.
Despite some positive points, the criticisms (and overall scores) put this paper below the bar. The reviewers raise issues of novelty, as well as problems with the experiments and argue that some claims are unsupported.
This paper studies the embedding compression problem related to GNNs and graph representation. A two stage method is proposed to generate the compressed embeddings: firstly, it encodes each node into its composite code with hashing; secondly, it uses a MLP module to decode the embedding for the node. Experiments are performed to evaluate the compression effect with both pretrained graph embeddings and node classification tasks with GraphSage.

The paper considers hashing/compressing the rows/columns of adjacency matrices and using the compressed rows of the adjacency matrices as node features. The adjacency matrices are  intrinsically redundant. Therefore, it is unclear whether the achieved compression rate is significant, especially when applied to settings with known node features. 

Some reviewers pointed out existing methods on learning to hash methods, which train a GNN based encoder to compress the hidden representation/embedding, are relevant. Although the authors claim that in their scenario, the goal is to efficiently compress the input feature/embedding without any embedding/encoder pre training step, it is unclear how the proposed method compares with the learning to hash methods when considering the adjacency matrices as the auxiliary information. 

The dependence on the number of nodes is also a concern in terms of scalability, as we know the bottleneck of scalability in GNNs is the number of nodes. 

The authors use the adjacency matrix of the input graph as the auxiliary information in the paper, which only considers local structure information. The reviewers are curious whether this approach would work for tasks in which global graph structure information is required. 

On a minor note, the reviewers also think that the paper would be stronger if the authors provide more principled guidance on how to select the code cardinality c and the code length m.
This paper studies the problem of motion prediction for multiple agents in a scene using transformer based VAE like architecture. The paper received mixed reviews initially which generally tended towards borderline acceptance. All reviews appreciated extensive experiments but had some clarifications and requests for ablations. The authors provided a strong rebuttal that addressed many of the reviewers  concerns. The paper was discussed and all the reviewers updated their reviews in the post rebuttal phase. Reviewers unanimously agree that the paper should be accepted. AC agrees with the reviewers and suggests strong acceptance. The authors are urged to incorporate reviewers  comments in the camera ready.
This work presents a principled objective function for large margin learning. Specifically, it introduces class margin and sample margin, both of which it aims to promote. It also derives a generalized margin softmax loss which to draw general conclusions on the existing margin based losses. The effectiveness of the proposed theory is empirically verified in visual classification, imbalanced classification, person re identification, and face verification.

The reviewers initially raised some concerns, but most of them were well addressed in the rebuttal and convinced the reviewers. Specifically, pU1u was satisfied by authors  reply on Theorem 3.2 and the practical methods. pGzf appreciated clarifications around the evaluation metric used on IJB C and believes this work can improve our understanding of margin based face recognition. Finally, 3YiD had some reservations about number of parameters which got clarified by the authors. 

In sum, all post rebuttal ratings fall in the accept zone, and the reviewers find the paper interesting and insightful. In concordance with them, I recommend this paper for publication. Please make sure to include suggestions made by reviewers in the camera ready version.
Summary: The paper studies RL and bandits in the conservative setting where the performance of the new, learnt policy should never be significantly worse than that of a baseline. 

Discussions: The main concern of the reviewers was about novelty, and specifically what new techniques and ideas were brought in this work compared to (Wu et al. 2016) and (Garcelon et al 2020). The authors have addressed these concerns and updated their draft accordingly. The reviewers have now all reached a consensus and recommend to accept this work. 

Recommendation: Accept
The paper introduces a novel control based variational inference approach that learns latent dynamics in an *input driven* state space model. An optimal control solution (iLQR) is implicitly used as the recognition model which is fast and compact. Reviewers unanimously agree on the high quality writing and high significance of the work. This paper advances the horizon of nonlinear dynamical system models with unobserved input, an impactful contribution to the neuroscience and time series communities.
The paper considers representation learning of 3D molecular graphs.
The authors propose a message passing scheme using spherical coordinates. It is
tested on three datasets of 3D moleclular graphs. The authors offer an in depth
analysis of different aspects, with an extensive experimentation of the method.

Strengths:

  The SMP introduces an interesting method to alleviate the computation cost issue in SCS from O(nk^3) to O(nk^2). This method is important and can be generalized to more broad types of tasks.
  This is an empirical work, and the experimental results support the effectiveness of SMP.
  The proposed MP approach can better distinguish certain structures than some existing models.
  Incorporating torsion information when representing 3D molecules is novel and helpful
  While message passing methods on graphs exploit only the connectivity, this work shows an interesting method to include the embedding information in the case of geometrical graphs.

Weaknesses:

  The proposed SMP scheme in Eq. (1) lacks novelty since it basically enriches the GN framework in [1] with geometry features
  the architecture of the proposed SphereNet is similar to DimeNet
  Why SMP is better than Cartesian coordinate system (CCS) is not well explained.

Overall, a majority of reviewers are in favor of acceptance and a third reviewer is happy with either acceptance or rejection and does not give strong reasons for rejecting the paper. My recommendation is, therefore, acceptance. I recommend the authors use the reviewers comments to improve the paper for its camera ready version.
The authors develop a framework for improving robustness certificates obtained by randomly smoothed classifiers in settings with multiple outputs (segmentation or node classification), by combining local robustness certificates obtained for individual classifiers. They validate their results empirically and demonstrate gains from their approach.

The reviewers were mostly in agreement that the authors make a novel and interesting contribution. However, there were a lot of technical concerns raised by reviewers that, while addressed during the discussion phase, would require a substantial revision of the paper to address adequately. Overall, I feel the paper is borderline but recommend rejection and encourage the authors to incorporate feedback from the reviewers and submit to a future venue.
The authors extend the result of Ongie et al. (2019)  and derive sparseneural network approximation bounds that refine previous results. The reuslts are quite ineteresting and relevant to ICLR. All the reviewers were positive about this paper.
This work addresses the problem of learning representations from noisy expert demonstrations in in adversarial imitation learning. The authors build on top of GAIL, which utilizes a discriminator to model a "pseudo" reward from demonstrations. In this work, the discriminator is replaced with an auto encoder. The authors hypothesis is that using an auto encoder helps in 2 ways: 1) denoising expert trajectories for more "robust" learning; 2) using the reconstruction error (instead of binary classification loss) to distinguis experts from samples provides more informative signal for reward learning. 

**Strengths**
on a global perspective this work is well motivated
a novel algorithmic variant of GAIL is proposed 
thorough experimental evaluation

**weaknesses**
The manuscript doesn t clearly distinguish between adversarial imitation learning algorithms (like GAIL) and "true" inverse reinforcement learning algorithms. This makes it unclear what the real goal of the proposed method is. The ultimate goal of adversarial IL is to learn a policy (by inferring a pseudo reward at "train" time which is then never used again), while the primary goal of IRL is to learn a reward function at train time, which can then be used at test time. The manuscript motivates the algorithm by saying it will have a more informative signal for learning reward functions, but the algorithm itself is an adversarial IL algorithm which primary goal is to learn a policy from demonstrations. Overall, makes the evaluation and analysis confusing. Ideally, the authors would have focussed on the question "Does the reconstruction error lead to better policies?" (through better pseudo reward modeling)   or would have extended an IRL method.

Second, the motivation is that the autoencoder helps with more "robust" learning, but it s unclear to me that the evaluation really shows that learning is more robust (also because "robustness" is not clearly defined)

The experimental evaluation is a bit of a mixed bag, and it s a unclear why the new algorithm performs better on non noisy data (when compared to baselines), but not less so on the noisy data.

**Summary**
Overall, this work provides a promising direction, however in it s current form the manuscript is not yet ready for publication.
The paper proposes an empirical study on the effect of various types of output layers of deep neural networks in different scenarios of continual learning. The authors draw several insights, such as ways of selecting the best output layer depending on type of scenario and a description of the different sources of performance drop (forgetting, interference, and projection drifts). The paper proposes different ways of mitigating catastrophic forgetting: a weight normalization layer, two masking strategies, and a variant of NMC using median vectors.

The paper presented a detailed experimental setup covering a large number of scenarios of continual learning: incremental, Lifelong Learning and Mixed Scenario. This was highlighted by Reviewer BTLN, and the AC agrees. 

The main point of criticism for the work is the lack of novelty and the low significance of the findings. These were highlighted by all four reviewers.

Perhaps the aspect limiting significance is the fact that the feature extractors are assumed to be fixed, which is unlike most interesting settings in continual learning. This was mentioned by reviewers BTLN, uN9P, e1ZF. It is unclear whether the findings provided in this work would generalize to that setting. On that note, Reviewer e1ZF points out that not adapting the feature extractor could be the source of some inconsistencies observed. Studying this further would improve the work.

Overall, all four reviewers recommend rejecting the paper. The AC agrees with this decision and encourages the authors to consider extending the analysis to situations where the feature extractor is not fixed.
This work proposed a method for encouraging an agent showing altruistic behaviour towards another agent (leader) without having access to the leader s reward function. The basic idea is based on the hypothesis that having the ability to reach many future states (i.e., called choice) is useful for the leader agent, no matter what it reward function is. The altruistic agent learns a policy that maximizes the choice of the leader agent. The paper defines three notions of choice, and evaluates them on four environments.

The reviewers believe that this work attempts to solve an important problem, proposes a novel approach, and performs reasonably good experiments. The reviewers are all on the positive side at the end of the discussion phase. Therefore, I recommend acceptance of the paper. I also suggest a spotlight presentation for this work because of the novelty of the problem, which might be of interest to other researchers.

The authors have already done some revisions to their paper (including adding a new environment). I encourage them to consider any remaining comments from reviewers in their final version.
This manuscript introduces a theoretical framework to analyze the sim2real transfer gap of policies learned via domain randomization algorithms. This work focusses on understanding the success of existing domain randomization algorithms through providing a theoretical analysis. The theoretical sim2real gap analysis requires two critical components: *uniform sampling* and *use of memory*

**Strengths**
All reviewers agree that this manuscript provides a strong theoretical analysis for an important problem (understanding sim2real gap)
well written manuscript, and well motivated
Intuitive understanding for theoretical analysis is provided


**Weaknesses**
analysis is limited to sim2real transfer without fine tuning in the real world
the manuscript doesn t provide a novel experimental evaluation
lack of take aways

**Rebuttal**
The authors acknowledge the limitation of not addressing fine tuning, but also point out that several papers have performed sim2real transfer without fine tuning. 
The authors address the lack of novel experimental evaluation by arguing that the theoretical analysis can be directly linked to existing algorithms for which empirical evaluations have already been performed. I agree with the authors that in that context it seems of little value to redo those experiments. However, I also believe that those links could be made even clearer in the manuscript and I would encourage the authors to do so. Furthermore, while the authors do provide intuitive take aways for domain randomization algorithms, it would be helpful if those take aways were more clearly linked to existing algorithms as well (given that there is no experimental evaluation of this). 

**Summary**
This manuscript provides a theoretical framework for analyzing the sim2real gap and using that framework provides bounds on the sim2real gap. All reviewers agree this is a strong theoretical analysis. Some take aways on what makes domain randomization algorithms successful are provided by the provided sim2real gap analysis (memory use, uniform sampling). Thus I recommend accept.
The paper considers the effect of permutations in SGD   exploring the question of can we go beyond random permutations (which themselves have shown to be better than with replacement sampling)? The paper studies these questions from multiple viewpoints   showing that there is a one dimensional function for which the optimal permutation can be exponentially better in terms of rate than random. Further they show that for the general high dimensional the gap between random and optimal is non existent. Further they study a Flip Flop algorithm which flips the permutation every alternate epoch and for convex quadratics they show that this technique can lead to improved convergence rates for multiple base permutation schemes. 

Overall the results of the paper was found to be interesting across the reviewers. The reviewers agree that the paper is well written. The paper initiates the analysis in a new direction for optimization, i.e. how can we leverage permutations to further improve the convergence of GD and how much further can we go beyond random permutations. 

The only weakness highlighted by the reviewers is the limitation of scope to quadratic functions for the FlipFlop algorithm   while this is a significant restriction, given the new line of enquiry opened by the paper this can be discounted.
This article proposes a novel uncertainty quantification method, formulating the problem as a Bayesian inference problem. Instead of training multiple ensemble models through MAP optimisation, as in ensemble methods, the proposed approach tries to learn a mapping function between the prior distribution and the posterior distribution of model parameters. This avoids the complex training of ensemble models and achieves better efficiency. 

The approach is novel, and the problem of importance. The paper however suffers from a number of weaknesses:
* Some theoretical results would need to be made mathematically more rigorous
* The presentation is unclear and confusing in some places
* Empirical results are not reproducible due to the lack of details
Although the authors clarified some of the points raised by reviewers in their response, the paper in its current form is not ready for publication, and I recommend rejection.
This manuscript proposes and analyses can approach to address the centralized and personalized tasks in federated learning jointly. Existing work has tackled this issue by developing separate tasks. Instead, this manuscript proposes a shared architecture that aims to optimize centralized and personalized models. One observation motivating this work is that local models trained during federated learning effectively optimize local task performance. The resulting approach results well when label shifts primarily drive the client variability. Here, the centralized components are trained to optimize a balanced risk, while the local components are trained to optimize the standard empirical risk. 

Reviewers agree that the manuscript is well written and appropriately addresses the timely issue posed. The main concerns are the clarity of the technical contributions and technical statements during the review. The authors respond to these concerns and have satisfied the reviewers. After discussion, most reviewers are generally strongly positive about the strength of the manuscript contributions.
The authors propose an alteration to Dreamer that incorporates a swav like objective. The reviewers raised a number of issues with the paper, overall arguing for rejection. In particular, the reviewers felt that the work was not well motivated, weak performance, that a number of baselines were missing, and a lack of analysis of the results, a lack of novelty. While the authors addressed many of these concerns during the rebuttal, the majority of reviewers still felt this was not enough and that the paper did not meet the bar for acceptance. Therefore, I recommend rejection at this stage so that these concerns can be addressed.
The reviewers all seemed to agree that the investigation of other losses is an interesting direction of study, and acknowledged there was some empirical performance improvement for standard computer vision tasks. However, they felt the justification of the specific form of loss was a bit shaky and heuristic, and were furthermore unconvinced by results exclusively for image classification (one reviewer was unmoved by the magnitude of improvement). This was a borderline decision, but we hope the authors refine and resubmit their work as this is an interesting but underexplored direction within DPML. 

As one recent related work which investigates the effect of other architecture differences in the DP setting, the authors may be interested in https://arxiv.org/abs/2110.08557.
The authors propose a method for associative learning as an alternative to back propagation based learning. The idea is to interesting. The coupling between layers are broken down into local loss functions that can be updated independently. The targets are projected to previous layers and the information is preserved using an auto encoder loss function. The projections from the target side are then compared with the projections from input side using a bridge function and a metric loss. The method is evaluated on text and image classification tasks. The results suggest that this is a promising alternative to back propagation based learning.

Pros
+ A novel idea that seems promising
+ Evaluated on text and image classification tasks and demonstrated utility

Cons
  The impact of the number of additional parameters and the computation is not clarified (even though epoch s are lower)

The authors utilized the discussion period very well, running additional experiments that were suggested (especially ablation studies). They  also clarified all the questions that were raised. In all, the paper has improved substantially from the robust discussion.
Thanks for your submission to ICLR.

This paper considers a variational inference hierarchical model called Variational Predictive Routing.

Prior to discussion, several reviewers were on the fence about the paper, most notably having concerns about some of the experimental results as well as various clarity issues throughout the paper.  However, the authors did a really nice job addressing many of these concerns.  Ultimately, several of the reviewers updated their scores, leading to a clear consensus view that this paper is ready for publication.  We really appreciate your effort in providing additional details and results.

Please do keep in mind the concerns of the reviewers when preparing a final version of the manuscript.
This paper experiments with a combination of Sparse MoEs and Ensembles on the Vision Transformer (ViT), showing improved performance. To efficiently combine Sparse MoEs and Ensembles, the paper presents Partitioned Batch Ensembles (PBE), where the parameters of the self attention layers are shared, and an ensemble of Sparse MOEs are used for the MLP layers of the Transformer blocks.

While reviewers agree that the proposed approach is interesting, they also point out several weaknesses, such as the limited novelty of the proposed method (a simple combination of existing techniques) and small experimental gains. They also pointed out several weakness related to the experimental part. While the authors responded in a very detailed manner to several of these points and presented several additional experiments, I feel this paper will benefit from consolidating all these new results and going through another round of reviews.
Reviews for this paper were mixed (6,6,6,5) with one review (Z6sN) being somewhat uninformative. During the rebuttal, some reviewers raised their scores to 6 but overall there was not strong excitement among the reviewers, AC, and SAC. From fresh readings (by SAC and researchers with relevant expertise), this paper’s technical approach looks reasonable but feels quite incremental (novelty is not high) and the experimental results are conducted on a small scale problem where up to 100% success rate is achievable by baseline methods. Therefore, the practical significance of this approach for real world problems with complex and noisy environments is quite unclear. Overall, the paper looks below the ICLR acceptance threshold. For improvement, we suggest providing evidence/demonstration that this method can successfully tackle more challenging real world problems.
Inspired by the observtion that the poisoned samples tend to cluster together in the feature space of the attacked DNN model, which is mostly due to the end to end supervised training paradigm, the authors propose a novel defense method based on contrastive learning and decouple end to end training to defend against backdoor attacks.
The issues, including training time, difference from certain previous studies, ablation study, and so on, raised by the reviewers have been properly addressed and the reviewers are satisfied with the responses from the authors.
According to the consistent positive opinions from the reviewrs, this manuscript is recommended to accept.
This paper a framework of learning with noisy labels named PARS that combines three types of approaches, i.e., sample selection, noise robust loss, and label correction.  The framework leverages both original noisy labels and estimated pseudo labels of all samples for improving the training performance, and the empirical studies demonstrated competitive results on CIFAR datasets especially in high noise and low resource settings. 

Reviewers raised some major concerns about the weaknesses. For example, empirical gain in small noise regime are small or negligible, and no empirical gain against SOTA in large dataset with real world noise (Clothing1M).  While large gains in large noise regime (more than 80%), such setting may not be very realistic and there also  lack of in depth analysis on the sources of the gain (e.g., it is unknown if the gain is mainly because of using a better SSL or other factors since LNL becomes more similar to SSL when noise is very high). For technical novelty perspective, while the proposed approach is new, the overall novelty may not be very significant as this paper mainly combines existing techniques, e.g., negative learning and FixMatch (a semi supervised learning method) in the proposed learning approach. 

Authors have made great efforts for addressing the reviewers’ concerns partly, but some major concerns on the technical novelty and empirical studies remain.  Therefore, the paper is not recommended for acceptance in its current form. I hope authors found the review comments and discussions useful and constructive, and like to see it accepted in the near future after these issues are fully addressed.
The work proposes a method to learn graph representations based on subgraphs that are invariant to spurious subgraphs. The reviewers found the paper easy to read and the theory interesting, well explained and justified. The reviewers seem happy with the existing and new experiments that came during the rebuttal phase. I too found the paper interesting and mostly well written. 

Besides the corrections done during the rebuttal, in further discussion with the authors, I raised a concern that the work must make additional assumptions about the support of the induced subgraph distributions that were not clearly stated in the paper: The work makes the assumption that there is enough training data such that all spurious induced subgraph patterns $S$ that are smaller than the truly correlated induced subgraph $C$ can be identified as spurious. The authors promised to make this into a clearly demarcated assumption since it a key requirement for the method to work.
This paper proposes a new approach to solve mixed discrete continuous action RL problems, based on embedding actions into a latent space so that standard continuous control algorithms (like TD3) can be applied. Experiments over standard discrete continuous benchmarks demonstrate the superiority of the proposed approach vs. existing baselines.

There is overall a strong consensus of all reviewers towards acceptance, especially after the discussion period where the authors were able to submit several revisions addressing most of the questions and concerns raised in the original reviews, in particular w.r.t. the quality and relevance of the results.

I believe this submission could be improved along two axes though:

1. As pointed out by some reviewers, the current environments are somewhat simple. A more realistic robotic task for instance could be a good fit for such an algorithm. That being said, as the authors pointed out, this may require custom development due to the lack of existing public environment with the proper setup.

2. As a reviewer mentioned, there is no "Related Work" section, and although previous work is discussed in the Introduction, I consider that it remains limited, and more previous work should have been discussed. Here are some pointers regarding relevant work I am aware of:
  Hierarchical Approaches for Reinforcement Learning in Parameterized Action Space (https://arxiv.org/abs/1810.09656)
  Neural Ordinary Differential Equation Value Networks for Parametrized Action Spaces (https://openreview.net/forum?id 8WKd467B8H)
  Improving Action Branching for Deep Reinforcement Learning with A Multi dimensional Hybrid Action Space (https://ipsj.ixsq.nii.ac.jp/ej/index.php?action pages_view_main&active_action repository_action_common_download&item_id 199976&item_no 1&attribute_id 1&file_no 1&page_id 13&block_id 8)
  Distributed Reinforcement Learning with Self Play in Parameterized Action Space (https://cgdsss.github.io/pdf/SMC21_0324_MS.pdf)
  Discrete and Continuous Action Representation for Practical RL in Video Games (https://arxiv.org/abs/1912.11077)
  Multi Pass Q Networks for Deep Reinforcement Learning with Parameterised Action Spaces (https://arxiv.org/abs/1905.04388)

In particular, I believe the last one (MP DQN) should have been one of the baselines, since it is supposed to be an improvement over the P DQN algorithm (that is one of the baselines used here). I encourage the authors to try and incorporate it for the final version (at the very least, it should be cited).

In spite of these concerns, I still recommend acceptance since the combination of action space embedding with mixed discrete continuous actions is novel and non trivial, and the empirical validation is convincing enough in its current state.
The paper focused on deep regression problems and proposed a label encoding technique which can be thought as a sibling of the famous error correcting output codes but designed for regression problems. The main idea is well illustrated in Figure 1 at the top of page 3, where the encoder and decoder are the main objects of the proposal (and a quantizer is also needed for using the encoder/decoder which is a uniform quantizer in the paper). The idea/proposal is supported by solid theoretical arguments and convincing empirical evidences (not only the paper but also the rebuttal). While there were some concerns in the beginning, the authors have successfully clarified all the concerns and then the average score has been increased from 5.5 to 7.5. As a result, the paper is clearly above the bar of acceptance. What is more, an advantage is that the proposed method is task agnostic and can be combined with different task specific feature extractors borrowed from very complex regression problems (e.g., head pose estimation, facial landmark detection, age estimation, and end to end autonomous driving), making its significance and potential impact high. Given these facts, I think the paper might be selected as a spotlight presentation at the conference.
This paper introduces a novel method for learning distributional robust machine learning models when only partial group labels are available to improve performance of learning algorithms on minority groups. 

Pros: The paper is well motivated and written.  The ideas are interesting.  Most work on distributional robust optimization (DRO) are in unsupervised settings where group information is not available.  They provide an approach for the semi supervised setting through a constraint set.

Cons: 
The empirical results do not show better performance over unsupervised baselines as pointed out by reviewers.  

The authors claim one of the benefits of their proposed approach is a one stage approach, in contrast to competing models that require a two stage approach; hence, allowing their approach to reduce compute time.  It’ll be helpful to strengthen this point by showing time comparisons.

Missing labels in this case due to participants withholding sensitive information is not an MCAR case, but the proposed work makes an MCAR assumption.  It’ll help to add a discussion and point out such limitations of the approach.

Summary:  This paper has novel and interesting ideas, but still has several issues as pointed out by the reviewers before it is ready for publication.
This paper extends recent and very active literature on analyzing learning algorithms in the simplified setting of Gaussian data and model weights, with the main generalization being to allow for non isotropic covariance matrices.  The main technical results seem to be correct and slightly novel, though reviewers feel they are not innovative or unexpected enough to stand on their own.  However, the main contributions of the paper are then to interpret these results to give phenomenological results (regarding double descent, etc.), and reviewers were unanimously happy with these. In the end, all reviewers were positive about the paper.

The largest reviewer criticisms of the paper were technical issues (ot31) and lack of context of recent literature (3RfG). Both of these concerns were mostly addressed by the revision/rebuttal. The reviewers still had specific comments on improvement, but found no major faults.
This work proposes to train large scale graph neural networks by replacing the moving averages used in the stochastic compositional optimization (SCO) framework with sparse moving averages. This reduces the memory required for SCO, allowing their algorithm to scale  to larger graphs. 

The consensus is that the approach is reasonable, but incremental both in the change over SCGD and the change in the analysis. More importantly, the reviewers identified several sampling based methods for scaling up training of GNNs that are important baselines for the proposed algorithm; the relative merits of the method against these approaches should be established with further experiments.
This paper introduces a prompting technique for eliciting factual knowledge from frozen pretained transformer LMs. The key idea is to modify the embeddings produced by the embedding layer before they are passed to the first attention layer and the paper investigates several different design choices. The Reviewers all agree that the paper tackles an important problem with interesting methods, that it is well written and has strong results. The main concerns, raised by Reviewer jddf, were about clarifying the connections to the robust optimization literature and evaluating on OOD relations. The former has been addressed in the revised version. While the latter point remains valid, I find that the paper in its current state has enough useful experiments and analysis to warrant publication. The authors have clarified most of the other points raised by the reviewers in their rebuttal.
All reviewers agree that this is a reasonable contribution but that it is also extremely limited in scope. The authors suggest in one of their response that their technique could apply to "any data mixing method with “batched k sum” structure". Such a larger level of generality might make the paper more interesting, but at the moment it is an extremely niche result.
This paper develops a mechanism for learning modular state representations in RL that organize recurring patterns into composable schemas. The approach combines modular RNNs as in RIMs (Goyal et al., 2020) with a dynamic feature attention mechanism. There were a variety of concerns in the initial reviews that were addressed by the authors through a set of clarifications and improved empirical analysis, substantially improving the paper. However, there still remain some issues in clarity of presentation and inconsistent empirical results, especially in the form of clear take aways from the empirical analysis and broader insights from the paper, as detailed in the individual reviews. The authors are encouraged to take these aspects into consideration in revising their manuscript.
In this paper, authors introduce and study provably robust adversarial examples. Reviewers had mixed thoughts on the work. One reviewer mentioned that the "provable" robustness is somehow overstated in the work: looking at the title and abstract, it sounds like the paper develops a new algorithm that is guaranteed to be robust, but in reality the robustness hinges on the black box verifiers (which is acknowledged by the authors during discussion). I agree with this. This should be more clearly stated in the work. I strongly suggest authors to calibrate exaggerated statements of contributions in the revised draft. Having said this, reviewers liked the the experimental study of the paper and found it to be comprehensive and convincing.
A heterogeneous federated learning framework is proposed which does
not require auiliary public data sets, and does not reveal the private
data to the server or answering parties if they operate as
honest but curious entities. It builds a new protocol for private
inference, which can run on GPUs, and proposes a dataset expansion
method to not need an auxiliary data set. The paper presents extensive
empirical experiments on the method.

The paper was extensively discussed with the authors. The concerns
included both technical issues and more general issues on missing DP
guarantees and realisticness of the threat model. Many of the issues
were resolved by the clarifications provided by the authors, and as a
result two reviewers increased their scores. However, all reviewers still
place the paper to the borderline.

While the paper contains solid work, and improves efficiency compared to
previous models, this is a borderline paper where the final judgement
needs to be based on importance of the presented new contributions in
advancing the field. The paper may not yet quite reach the bar, but
I believe the reviewer comments have enabled the authors to improve the
paper for further work.
Meta Review for Variational Neural Cellular Automata

This paper proposes a generative model, a VAE whose decoder is implemented via neural cellular automata (NCA). The authors show that this model performs well for reconstruction, but they also show that the architecture has some robustness properties against damage during generation. 

Experiments were conducted on 3 datasets: MNIST, Noto Emoji, and CelebA, and while experimental results were great on MNIST, the method was less performant so on the other two datasets, although there is clear evidence that the model can learn to generate meaningful images. For the robustness experiments, the authors show that VNCA is robust to perturbations (occlusions) and show that the model has a reasonable degree of robustness even without ever seeing any perturbations at training time.

All authors agree that this model is an improvement over neural cellular automata, and that the approach is interesting and the results are sound (and even useful). Initially, there were concerns that NCA s were simply convolutional neural networks (the connection is already known, and not the point of the paper), and issues with comparison with baselines for damage reconstruction tasks, but these were addressed by the authors (which the reviewers have acknowledged, and have improved their scores). The authors have also responded to the concerns of reviewer cp9d, and due to the lack of response from cp9d, I assessed the authors  response myself and find that they do address the concerns (in particular, they removed claims of super resolution, and improved the clarity of the work). With that in mind, the score of 5 is viewed as a score of 6 from my perspective (giving this work effectively an average score of 6).

After my assessment of the paper and reviews, I agree with reviewer kwgv, as they have summarized the work in their original review:
  The authors propose a variational neural cellular automaton, which learns to generate images by iterating the transition rule.
  The paper is interesting, with good results, and a good fit for ICLR.
  The paper solves an interesting problem on the topic of neural cellular automata.
  There are some doubts/limitations that I have asked the authors to address (mainly concerning the architecture of the model).
  There are some missing references and details that would help the readers to get a better sense of the subject.

Crucially, kwgv have acknowledged that the *authors have improved the paper significantly after the reviews, and they have addressed all questions and comments that [they] raised* (especially with regards to the last 2 points), and kwgv has subsequently championed the work with a score of 8. With the increased scores from kwgv and AnwX in mind, and also with what I view as an increased score of 6 from cp9d (in the lack of response from the reviewer, the authors have addressed the concerns in my judgement), my conclusion is that this is a nice work that bridges NCAs with generative models, and I think the work will be a useful addition to the growing literature in this space. I will recommend it for acceptance at ICLR 2022 as a poster.
The paper proposes to use the recently introduced "Barlow twins" contrastive learning objective, to the case of graph networks. The main concern raised by reviewers was the limited novelty of this work, which they argued mostly combines existing lines of work, and does not introduce sufficiently new concepts. This was also discussed between the authors and the reviewers.
Having read the paper and the reviews, I tend to agree with the reviewers that this paper is more of a combination of existing works, and their relatively straightforward application to the graph network domain. Thus, although the empirical results are encouraging, I agree the paper has limited novelty, and falls below the ICLR acceptance bar.
Mixup is very helpful when the training sample is scarce or has weak supervision. The paper studies how to adapt mixup to positive and unlabeled (PU) learning, a representative weakly supervised learning problem. By studying the specific properties of PU learning, the authors propose the concept of marginal pseudo negative instances, which are more likely to be positive but actually annotated by negative. A novel mixup variant has been proposed for PU learning by mixuping the marginal pseudo negative instances with the positive instance around the classification boundary. The effectiveness has been empirically shown.
Most reviewers agree that the paper addresses a relevant problem. However, they also  believe that
the paper lacks in several points : not well supported claim, sometimes clarity, incremental in term of novelty.
This paper proposes speeding up certain optimization problems common in physics by reparameterizing their parameters as the output of a graph neural network. The reviewers appreciate the idea, but are not convinced enough to recommend the paper for acceptance. They point out the following weaknesses:
* The method amounts to linear preconditioning, and hence it s reasonable to expect a fairly complete comparison to the many linear preconditioning approaches that have been proposed previousl. The reviewers are not satisfied with the currently provided comparison. 
* The main idea is not presented clearly enough. In particular, it s not obvious the proposed method is best described as neural reparameterization, since it seems to amount to linear preconditioning.
* The experiments are not persuasive enough: The presented problems may not be relevant to all of the target audience of ICLR, and the experimental evaluation does not seem sufficiently exhaustive.

The suggested areas of improvement provided by the reviewers seem reasonable to me: I therefore recommend not accepting the paper in its current form. To make the paper more accessible and appealing, the authors may consider rewriting the paper to more closely match the perspective taken by the reviewers, and to provide a more thorough comparison to the previous approaches and the existing literature.
Dear Authors,

The paper was received nicely and discussed during the rebuttal period. The current consensus suggests the paper be accepted, but could have another round of revisions before it gets published

  Definition of sparsity within theoretical results + clarity of results. This seems to be the main concern by one of the reviewers. 
  The reviewers acknowledged that some of the concerns raised could be found somewhere in the appendix, which raises further the concern of the presentation of the results: reviewers suggest a more focused and proper dissemination of the results (main theorems in main text + explanation of the results obtained, etc), which requires another round of revisions and reviewing. 

Best AC
This paper provides an interesting study on the adversarial robustness comparisons between ViTs and CNNs, and successfully challenges the previous belief that ViTs are always more robust than CNNs on defending against adversarial attacks. Specifically, as revealed in this paper, when the attacker considers the attention mechanisms, the resulting patch attack can hurt ViTs more. 

Overall, all the reviewers enjoy reading this paper and appreciate the comprehensive robustness comparisons between ViTs and CNNs. The reviewers were concerned about the missing experiments about adversarial training, vague statements about the inspiration for future defenses, visualization of adversarial examples, etc. All these concerns are well addressed during the discussion period, and all reviewers reach a consensus on accepting this paper.

The final version should include the experiments, visualizations, and clarifications provided in the rebuttal. In addition, please release the code as promised.
This paper proposes to apply a piece wise polynomial filter on the spectral corresponding to the graph convolution to enhance the model expressivity of graph neural networks. The effectiveness of the proposed model is investigated through numerical experiments and it was shown that the method achieves fairly nice performances.  

This paper gives a natural extension to the usual adaptive Generalized PageRank approaches to more expressive piece wise polynomial filters. However, the reviewers are not enthusiastic on this paper. This is mainly because of the following concerns: (1) Since it requires diagonalization of the aggregation operator, it requires much more computational burden than the usual polynomial filters, which prevents the method from being applied to data with much more large size. (2) The choice of the filter could be more investigated, in particular, the complexity expressivity trade off (in other words, bias variance trade off) could be discussed more, for example, by theoretical work.  

In summary, the paper seems not to be well matured for being published in ICLR conference.
This paper has been reviewed by four experts. Their independent evaluations were all below the acceptance threshold citing various issues ranging from disconnection between stated goals of the presented work and the means in which the approach was evaluated, to doubts about the scalability of the proposed approach, to the lack of clarity regarding the actual novelty of the approach given some key missed references, to name a few items of criticism. Most reviewers were impressed with the empirical performance achieved in the conducted experiments, and one of the reviewers raised their mark in response to the author s rebuttal. Yet, the overall evaluation places this work as it stands now below the threshold for ICLR acceptance. I would like to encourage the authors to continue pushing their promising endeavor and systematically incorporating the feedback received here to improve the overall quality of this work.
The submission aims to improve the quality of the bootstrap when the number of samples is small.  It does so by gradient descent on the to approximate the ideal bootstrap in Wasserstein distance.  The submission combines a nice set of methodologies, and aims to address an interesting statistical problem in a principled way.  The reviewers were unanimous in their opinion that the submission falls below the threshold for acceptance to ICLR.  It was revealed in post rebuttal discussion with reviewer y4AP that they wish to retain a reject recommendation due to a lack of clarity in the methodology even after author comments.  The review details specific issues that can eventually be clarified in a revision for submission to another venue.
The work demonstrates that adversarially perturbing inputs can change the output of concept based explainability tools. Reviewers generally agreed that the writing was clear and the experiments were easily understood. Regarding novelty, reviewers noted that there are several existing works which study the adversarial robustness of explainability tools (one even has experiments specifically on concept based explainability tools). As a result, there is not much novelty in the finding that concept based explainability tools are sensitive to adversarial perturbation. Regarding the technical contribution of the algorithm, it is expected that standard optimization approaches (e.g. PGD) would be sufficient to break concept based explainability tools so there is not a clear technical challenge being solved in the work.

The work could be improved by refocusing the robustness analysis to derive new insights regarding the behavior of concept based explainability tools. In doing so, it would be beneficial to deemphasize the claims regarding novel security concerns these methods don t even work reliably in non adversarial settings, as evidenced by poor out of distribution robustness. It is expected that performance will be even worse under adversarial settings.
All the reviewers think that the work is significant and new. Therefore, they support the paper to be published at ICLR 2022. Given the strong results and the “accept” consensus from the reviewers, I accept the paper as “spotlight”. The authors should implement all the reviewers’ suggestions into the final version.
## A Brief Summary
Recent works in deep learning have shown that it is possible to solve [[combinatorial optimization]] problems (COP) with neural networks.  However, generalization beyond the examples seen in the training set is still challenging, e.g., generalizing to TSP with more cities than the ones seen in the training set. This paper proposes the GANCO approach, where a separate generative neural network based on GAN generates new hard to solve training instances for the optimizer. The optimizer and the generative network are trained in an alternating fashion. The authors have run experiments with the attention model (AM) and POMO with their GAN based data augmentation approach. The authors provide experimental results on several well known COPs, such as the traveling salesman problem.

## Reviewers  Feedback

Below, I will summarize some reviewers  feedback and would like the authors to address the cons noted below.
### Reviewer sEuD

*Pros:*
  Paper is well written.
  Task is important and well motivated.
  Good experimental results.
*Cons:*
  The paper s core contribution on the necessity of adversarial entities is not well motivated.
  Missing baselines:
  RL agent trained on all target distributions. To figure out how far GANCO is from the optimal policy.
  The performance of an agent trained on a curriculum.
  Figure 2 is unnecessary/redundant in the paper.



### Reviewer tjCH
*Cons:*
  The paper is reasonably written. However, it would be much easier to follow with a few changes. For example, section 3.1 explains the architecture and, in related works, a more comprehensive overview of the methods to improve the robustness of RL methods.
  It is widely known that data augmentation helps in deep learning. The paper s claims would be more convincing if it provided some crucial baselines, such as comparing different data augmentations methods and carefully ablating them.
  
### Reviewer N945
*Pros:*
  Well written
  Good evaluation
  Simple model with good results
*Cons:*
  Missing citation to the PAIRED paper.
  How important are the adversarial entities generated? Is it possible to achieve similar results by just training on more samples?
  Missing baselines: Instead of training in stages, alternate optimizer and generator network per step basis.

### Reviewer mumN
*Pros:*
  The proposed approach is novel.
  Comprehensive and extensive experiments.
  Figure 1 provides a good summary of the approach.

*Cons:*
  Motivation is for the GANCO is not very convincing.
  Concerns about the capacity of the neural nets used in the paper.
  Concerns on forgetting the original distribution.
  Concerns about experimental evaluation protocol.
  Including experiments on routing problems to show the generality of the proposed approach.
  Request for improvements in the writing and the formatting of the paper.

## Key Takeaways and Thoughts
I think this paper attacks an interesting problem. As far as I am aware of the approach is novel. However, generative adversarial networks have been used in the machine learning literature for data augmentation and RL for augmenting the environment (see the PAIRED paper.) GAN type of approaches hasn t been used to improve the generalization of the deep learning approaches for COP. The results look promising. However, as pointed out by Reviewer mumN and tjCH, this paper would benefit more from further ablations, particularly the necessity of adversarial generation part to make the arguments more convincing. As it stands now, it is not clear where exactly the improvements are coming from. Reviewer mumN also raised some concerns about the poorly configured LHK3 baseline in the discussion period. Furthermore, I agree with the reviewer mumN and tjcH that this paper would benefit from restructuring to make it flow better. I do think that this paper needs another round of reviews. I would recommend the authors go over the feedback provided here and address them for future submission.## References
The paper proposes a pruning approach that regularizes the gram matrix of convolutional kernels to encourage kernel orthogonality among the important filters meanwhile driving the unimportant weights towards zero. While the reviewers found the proposed method well motivated and intuitive, they believe that the proposed claims are of limited novelty and are not supported well by the experiments. Analyzing and explaining the effect of different parts of the proposed method, i.e., orthogonalization and regularization of batch normalization parameters, on the accuracy of the pruned models would significantly improve the manuscript.
This work defines the new problem of lifelong few shot language learning where the goal is to continually learn new few shot tasks and use those to benefit future tasks while not forgetting previous tasks. With larger models, this is an important goal due to the cost of updating and retraining these models. The work also shows superiority to existing approaches like EWC and MAS. After the author s rebuttal, the experimental section is also thorough with evaluation on a good range of tasks and approaches such as adapters showing good results. While this setting appears simpler than the full lifelong learning setting and the approach combines existing ideas, this work s contribution to the definition and thinking about this problem is valuable. However, the authors should more clearly state the advantages of their approach vs standard prompt tuning (with an emphasis of benefiting future tasks) since two reviewers seem caught up on this point. The other two reviewers comments were addressed by the rebuttal as they stated in their comments.
An interesting paper on combining NerFs with StyleGAN to get high quality and high resolution 3d aware generative models. The results are very good visually and also allow interactive speeds.  The technique is natural and concurrent papers are proposing variations

The reviewers identified a few limitations including that the nerf does not have a viewing direction and also seems limited to aligned objects with a common structure, like faces. Still the results are very interesting and suitable for publication.
The paper proposes an interesting and well motivated improvement of Sharpness Aware Minimization.  Overall the AC and reviewers are satisfied by the author feedback in improving the solidity and rigor of the theoretical results. 

The points made by the authors in response to the reviewers initial concerns are essential, especially those regarding interpretation of Corollary 5.2.1, making the proofs rigorous, and fixing the potential for crude convergence bounds. It is therefore critical that the authors incorporate them into their manuscript.
The reviewers unanimously think the paper has lack of novelty, its contributions are quite limited, and is not ready for publication.
This paper provides actor critic method for fully decentralized MARL. The results remove some of the restrictions from existing results and have also obtained a sample bound that matches with the bound in single agent RL. The authors also give detailed responses to the reviewers  concerns. The overall opinions from the reviewers are positive.
This manuscript proposes an adversarial method to address non IID heterogeneity on federated learning. Distinct from existing methods, the mitigation is implemented by training and local communicating learned representations, so the metric of success is indistinguishability of representations across devices. 

There are four reviewers, all of whom agree that the method addresses an interesting and timely issue. However, reviewers are mixed on the paper score, and many raised concerns about communication overhead, apparent privacy costs, and convergence concerns with the adversarial methods. There is also some limited concern of novelty compared to existing methods. The authors provide a good rebuttal addressing these issues   either based on experimental evidence (adding differential privacy), comparing communication overhead tradeoffs as it varies with model and sample size, and some existing convergence analysis. However, after reviews and discussion, the reviewers are unconvinced that the method is sufficiently robust to the concerns outlined. Authors are encouraged to address the highlighted technical concerns in any future submission of this work.
The submitted paper considers the very interesting problem of imitation learning from observations under transition model disparity. The reviewers recommended 2x weak accept and 1x weak reject for the paper. Main concerns about the paper regarded clarity of the presentation, complicatedness of the proposed method, and experimental validation. During the discussion phase, the authors addressed some of the comments and provided an update of the paper providing additional details. While some of the reviewers  concerns still stand, I think the addressed problem is very relevant and the proposed method can be (with clarifications and improvements of the presentation) be interesting to parts of the community. Hence I am recommending acceptance of the paper. Nevertheless, I strongly urge the authors to carefully revise their paper, and taking the reviewers  concerns carefully into account when preparing the camera ready version of the paper.
The paper investigates how the geometrical compactness of in distribution examples affects OOD detection performance and proposes architectural modifications to enable compact in distribution embeddings. All the reviewers agreed that the paper has several interesting contributions. I agree with the authors that simplicity is a strength, not a weakness. 

My main concern is that the paper s contributions feel a bit scattered. For instance, the paper does a detailed evaluation of normalization and compactness, but makes a few other minor contributions (as detailed by 
the authors at https://openreview.net/forum?id 7VH_ZMpwZXa&noteId m 1y5byLbwS​). However, the latter contributions feel a bit narrow to specific methods and are not as comprehensively tested as the claims around normalization.

Overall, the reviewers and I think that the current version falls below the acceptance threshold. I encourage the authors to revise the draft and resubmit to a different venue.
The authors propose to combine ideas from SDEs and time series modeling with stochastic optimal control to present a framework for modeling continuous time stochastic dynamics. The reviewers are in agreement that there are several good ideas presented here and that the interface of the perspectives the authors combine toward their proposed framework is an interesting one to explore. One referee mentions valid concerns in confusing points of the details in the presentation, and the positive reviewers echoed these concerns. In particular, more details and clearer exposition are needed for the decomposition into the subproblems and the problem of many hyper parameters. Nonetheless, my overall impression after a careful read of the paper and discussion is that these concerns are addressable and are to a degree ameliorated by the author response, and that they may be viewed as limitations outweighed by the merits of the novel ideas presented here. I emphasize that all reviewers were surprisingly consistent in their assessment of the shortcomings, and I encourage the authors to take these constructive criticisms seriously in preparing a final version of this paper.
This paper empirically evaluates the performance (in time and accuracy) of randomized signatures for time series, an idea that was developed theoretically in a series of recent paper. While reviewers acknowledge that implementing and testing this idea is relevant, they also consider that the lack of methodological and theoretical novelty, combined with the fact that the experimental results do not convincingly show that randomized signatures outperform existing methods on a variety of tasks, puts the paper below the acceptance bar.
The paper eventually got 5 "marginally above the threshold" after rebuttal. Such scores testify to that the paper is a borderline one. By reading the post rebuttal comments, it is evident that most of the reviewers still deemed that the novelty is incremental. One of the reviewer (vUb9) raised the score simply to "encourage the authors to think more important problems", rather than acknowledging the merits of the paper. The AC also read through the paper and had the following opinions:
1. The paper is actually about DNN compression, based on the "new finding" that the weights across layers are low rank. However, the authors would not write the paper in the way of DNN compression, but put more emphasis on the "new finding", which has no theoretical support at all (only some heuristic reasoning). The AC would deem that the "new finding" is only an assumption.
2. Actually the "new finding" is not new at all. For example,

[*] Zhong et al., ADA Tucker: Compressing Deep Neural Networks via Adaptive Dimension Adjustment Tucker Decomposition, Neural Networks, 2019, 

used a shared core tensor (which could be regarded as the common dictionary) across all layers for higher compression rates. More recent references that use tensors and consider shared information across layers for compression can be easily found as well.

So the AC thanked the authors for preparing the rebuttals carefully, but regretfully the paper is not good enough for ICLR.
This paper presents a new method to decrease the supervision cost for learning spurious attributes using worst group loss minimization. Their method uses samples both with and without spurious attribute annotations to train a model to predict the spurious attribute, then use the pseudo attribute predicted by the trained model as supervision on the spurious attribute to train a new robust model having minimal worst group loss. The experiments show promising results in this domain for reducing annotation cost. 

The reviewers vote to accept the paper, and some of them increased their scores during the discussions since the authors have addressed their concerns.
This paper proposes a novel method called CCE for explaining mistakes by DNNs on image classification. It is built on top of two prior ideas: counterfactual explanations and concept activation vectors. CCE explains a mistake by assigning scores to a shot list of concepts, where a large positive score means that adding that concept to the image will increase the probability of correctly classifying the image, as will removing or reducing a concept with a large negative score.
The strengths of the paper include novel combination of previous work, clear presentation, interesting experiments, convincing results on controlled settings.  The weaknesses include the lack of results on less controlled settings, the lack of more meaningful spurious correlations in the medical examples, and the lack of user studies.
Although the reviewers have shown interests in this paper, they clearly do not support the paper strongly. In addition, the authors have missed the following paper that also combines counterfactual explanations and concept activation vectors:
Akula, Arjun, Shuai Wang, and Song Chun Zhu. “Cocox: Generating conceptual and counterfactual explanations via fault lines.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 03. 2020.
The paper proposes  a min/max reformulation for JKO gradient flows appealing the variational formulation of f divergences. This would alleviate the need of an explicit density. All reviewers pointed out the limited novelty in the work and the limited experimentation. 

We encourage authors to add a theoretical analysis to their work and further strengthening of the experimental section with high dimensional experiments, and to resubmit the work on an upcoming venue.
This paper proposes Perceiver IO, a general neural architecture that handles general purpose inputs and outputs. It operates directly in the raw input domains, and thus does away with modality specific architecture components. The paper contains extensive experiments showing the capabilities of this architecture in different domains. The paper received very positive reviews from all reviewers. Some concerns included a need for additional details such as a missing task from GLUE, FLOPs comparisons to past works, nomenclature for the versions of Perceiver IO, etc. These concerns were well addressed by the authors. Others concerns by reviewers were the lack of experiments in a multi task setting. However, it was acknowledged by the authors and reviewers that this is an open area of research and is a good fit for future work. Given this high quality submission, strong reviews and a very positive discussion amongst authors and reviewers, I recommend accepting this paper.
This paper tackles a very timely problem. 
Scores of 5,6,6,8 put it in the borderline region, but in the private discussion the more negative reviewer noted that they would also be OK with the paper being accepted. I therefore recommend acceptance. 
Going through the paper I missed any mention of available source code. I strongly recommend that the authors make code available; this would greatly increase the paper s impact.
Summary: The given work studies one shot object detection, and demonstrates an array of experiments that show that by increasing the number of categories in training data, the model can get better at one shot detection. 

Pros:
  Well written
  Presents many experiments that are interesting
  Improves SOTA one shot performance on COCO by using more data.

Cons:
  Authors did not test their claims on a variety of model architectures
  Similar conclusions have been made in prior art.
  Conclusions are intuitive. As more categories are added, the likelihood of reducing semantic dissimilarity to a new novel category is quite high.
  Overall contribution is currently limited.

Reviewers are unanimous in their decision. Authors did not alleviate concerns of reviewer. AC recommends authors take all feedback into consideration and submit to another venue or workshop.
Metareview:

This paper proposes a transformer based automated program debugger, called DeepDebug. All reviewers agree that the addressed problem is interesting. However, there was a consensus among reviewers regarding concerns in terms of the novelty and the lack of comparisons with other. 
In general, all reviewers consistently gave a score that is below the acceptance threshold. This paper is of interest to the ICLR audience, but current form is not ready for acceptance.  

Summary Of Reasons To Publish:

 Good Result on QuixBug

 Synthetic bug generation

Summary Of Suggested Revisions

 Comparisons in/with other datasets/tools
Overall the reviewers like the ideas in this paper.  It calls out some of the issues with the current line of thinking in the ML/AI community.  There were some concerns, but overall this paper offers a new way to think about, present, and question efficiency results.  This could be quite infulential.  I think this is interesting enough to warrent publicaiton.
This paper proposes a novel variational autoencoder to utilize functional connectivity (FC) features from resting state fMRI (rs fMRI) scans in order to uncover latent nosological relationships between diverse yet related neuropsychiatric disorders. The methodology and main technical contributions are clearly articulated and explained, and the experimental results seem convincing. On the other hand, the proposed framework is somewhat limited in scope and clinical applicability, and the writing in the paper needs improvement (as pointed out by two reviewers).
This work performs an analysis of the generalization ability of pre trained models under the condition of vocabulary scrambling. The paper is well written and easy to understand. However, a full story and investigation into the cause of the observed transfer under word scrambling is lacking. For example, do more powerful models transfer less because pre training is more effective?

While the effect described in the paper is interesting, it lacks a solid connection to important areas such as adversarial attacks, cross lingual domain shifts and doesn t seem to have any effect on the application of fine tuning to pre trained models. The experimental section could also be improved with comparisons to other more recent models such as RoBERTa and GPT 2. Even though these more recent models are still Transformer based models it can help answer the question if more powerful models transfer less under word scrambling as raised by reviewer 6a8W. The results on LSTMs seem to imply this case. We thank the authors for including additional results on DeBERTa but this was insufficient to change the authors  opinion of the value of the paper.
This paper proposes a theoretically sound and practically effective method to compress quantized gradients and reduce communication in distributed optimization. The method is interesting and worth publication.
I thank the authors for their submission and active participation in the discussions. This papers is borderline with reviewers WXXr and eK4b leaning towards acceptance and reviewers f6jT and FV5x leaning towards rejection. On the positive side, reviewers remarked that the paper is interesting [FV5x] and novel [FV5x,f6jT,eK4b,WXXr]. However, there all reviewers found some flaws with respect to the execution and empirical validation [FV5x], specifically around lacking baselines [FV5x,WXXr] and some ablations [f6jT,WXXr]. I side with the comment made by reviewers FV5x as well as WXXr that a comparison to stronger baselines (UCB DrAC) is warranted. Therefore, I recommend that this paper is not ready for publication at this point and that it will benefit greatly from another iteration with stronger empirical results. I want to very strongly encourage the authors to further improve their paper based on the reviewer feedback.
The authors propose a multi resolution pyramidal attention mechanism to capture long range dependencies in time series forecasting, achieving linear time and space complexity. The authors conduced an extensive set of experiments and ablation studies demonstrating that  the proposed method consistently outperforms the state of the art and provided evidence for the various components of the architecture. They also provided a proof guarantee the linear complexity of long sequence encoding and adequately addressed the concerns raised by the reviewers. The additional additional benchmarks conducted by the author further demonstrated the strong performance of the method. All reviewers agreed that this work makes a solid contribution to the field.
This paper presents a new graph neural network layer that is sensitive to topological structure in the graph. Reviewers all believe the work is technically sound, and the experiments (particularly after author revisions) show clear benefits in cases where topological structure is important. The main questions are about whether the experimental evaluation is sufficient. While there are always more experiments that could be run, I tend to agree with the authors that the chosen experiments support the key claims in the paper, so it seems ok. The other question about the experiments is if they sufficiently convince the reader that topological structure is useful in practice. This seems more mixed. The paper would certainly be improved if there was a motivating application where there was a clear win. For example, molecular structures are used as motivation in the intro, but the best performing method on proteins doesn’t use the topological layer. All in all, though, there does appear to be clear improvements on carefully constructed cases, and there appear to be some benefits in real world datasets.
This paper generated a large amount of discussion.  Three reviewers were marginally above and one marginally below.  The paper presents an intriguing relationship between self supervised learning and topic model inference that extends earlier work of Tosh.  The result seems to be subtle because there was considerable discussion with the authors wrapping up with a reminder of what the main goal is:  SSL can achieve the state of the art performance for topic inference problem, moreover (main goal) SSL can be oblivious to the specific topic model.  This is indeed intriguing.  But with all the discussion, and one persistent negative reviewer, I feel the paper needs to be polished.  Given the theorem gives a testable statement, I don t see why experimental results cannot be done for 4 different real data sets, to give us more confidence.
The paper develops kernel functions in Banach spaces. However the results seem to be preliminary and further development is needed before
the manuscript can be published. Reviewers point out several errors and also author/authors have graciuously agree with the suggestion 
that they will incorporate all the feedback in future submissions.
This submission tackles the problem of model explainability from the perspective of masking based saliency methods. 
Several metrics are proposed for evaluating saliency methods including  a new « soundness »  concept.
Experiments using a consistency score to simultaneously evaluate completeness and soundness are provided.  

Most of the reviewers were not convinced by the approach and have raised several issues. 

After rebuttal and despite the interest in the introduction of the concept of « soundness »  to better explain model decision, the current proposition needs to be improved. In particular, the interest of this soundness concept does not bring out, many details of the method are not clear enough and the effectiveness of the proposed measure is still questionable. It would be interesting that the authors consider the R’s comments as the ones for additional  experiments to demonstrate the relevancy of their contribution.
Overall, the paper proposes an interesting idea to share parameters across words and reduce the size of the embedding which hasn t been explored in the past with promising results on XNLI task. However, all the reviewers agree that the novelty of this paper is not enough. In addition, the clarity and experiments are not sufficient enough too.
The paper compares MAML and NAL for meta learning, and provides theoretical explanations on some very simple models when MAML can be significantly better than NAL, related to a definition of task hardness. The findings are also supported by experimental results.
While the results are plausible and can mark the starting point of a useful analysis, the models analyzed in the paper are too simplistic to warrant publication at ICLR. The authors are encouraged to extend their methodology to more complicated task models, as well as to, e.g., multi step versions of MAML (since the considered version of MAML makes a single step, the proposed problem hardness may not be applicable in more general situations). It is also not clear how the derived insights can guide the practical applications of MAML.
This paper modifies the AlphaZero algorithm to generate proof tree size heuristics and shows empirical improvements over standard search algorithms. This is an interesting distinction that might lead to algorithms with distinct play styles and a deeper understanding of the games that we apply our agents to.

The two positive reviewers felt that it was a solid contribution, worthy of publication. There were some questions regarding the clarity of the writing that were addressed in the discussion phase. The two reviewers that gave lower scores felt that the paper did not do a sufficient job motivating the work and distinguishing itself from the literature. Ultimately, I agree with the positive reviewers, and it is my opinion that the revised version is acceptable for publication.
The paper develops steerable partial differential operator and show how it can be used to build equivariant network. Experimentation on rotated MNIST and STL10 show the merits of the proposed method. Reviewers agreed on the significance of the work and that it brings new perspective on equivariance that would be interesting to the ICLR community. Accept
Thanks for your submission to ICLR!

This paper presents a novel way to combine domain adaptation with semi supervised learning.  The reviewers were, on the whole, quite happy with the paper.  On the positive side, the results are very extensive and impressive, it s a clever way to combine domain adaptation and semi supervised learning, and it s a fairly general approach in that it works in several settings (e.g., unsupervised vs semi supervised domain adaptation).  On the negative side, the approach itself is somewhat limited technically.

After discussion, the one somewhat negative reviewer agreed that the paper has sufficient merit and should be accepted; thus, everyone was ultimately in agreement.  I also read this paper carefully and personally find it very interesting and promising, so I am happy to recommend acceptance.  It seems to give state of the art performance in several cases, and could possibly lead to more research down the road on methods to combine adaptation techniques with SSL.
This paper presents a very interesting study of using an artificial language (generated using a specific algorithm via a transformer model) and training SOTA transformer and LSTM language models on that language;  the authors show that these LMs underestimate the probability of sequences from this language and overestimate the probability of ill formed sentences, among other observations.  This is a very interesting study that captures the behavior of recent LMs.  All reviewers are supportive of accepting this paper and it is good to see the engagement between reviewers and authors of this paper.
Four experts reviewed the paper. All but Reviewer HSTU recommended acceptance. The authors clearly did a great job with the rebuttal, which convinced two reviewers to raise their scores above the acceptance threshold. Notably, the reviewers found the newly added experiments impressively strong. The rebuttal also addressed some clarification questions. Based on the reviewers  feedback, the decision is to recommend the paper for acceptance. As mentioned by the reviewers, some experiments and discussions in the rebuttal should be included in the paper. The authors are encouraged to make the necessary changes to the best of their ability. We congratulate the authors on the acceptance of their paper!
This paper focuses on answering complex logical queries over an incomplete KG and use neural networks to do so flexibly handling multiple operations from FOL. Overall reviews agree that empirical performance is impressive. One reviewer gave a strong accept, one leaning to accept and two leaning to reject. Overall, the reviewers who are leaning to reject had mostly clarity issues which seem to have been addressed by the authors (without response from reviewers). 
Given this I recomment acceptance.
The paper studies an interesting problem, but as pointed out by reviewers, the presentation of the problem statement and contributions need to be improved.
The paper proposes a new loss function for the training of spiking neural networks leading to significant improvements in generalization performance across a variety of datasets and network architectures. While conceptually simple, the approach leads to substantial performance gains, and some intuition is provided to explain its success.

The reviewers are split on the issue of significance of the paper, in part due to the simplicity of the proposed loss function. Still, good results speak for themselves, and the effectiveness of the technique has been demonstrated thoroughly.
In this work, authors use proxy distributions learned by advanced generative models to improve adversarial robustness. In the discussion period, authors did a good job in addressing reviewers  questions and comments. All reviewers think the paper is above the accept threshold, so do I.
All reviewers are very consistent with their evaluation of the paper. The discussion phase did not change their initial evaluation. Therefore, I also recommend to reject the paper.
This paper investigates Bayesian optimization where a prior distribution over the optimal is available. The authors conducted a systematic study on a very intuitive prior augmented acquisition function that multiplication the prior probability with the EI heuristic   including an asymptotic analysis on the regret, comprehensive (controlled) synthetic experiments, and moderate empirical support on several real world case studies.

All reviewers find the paper well written and appreciate the rigor of the empirical evaluation. The theoretical analysis is also helpful to provide additional justification for the proposed approaches. I would like to add that the paper also included a brief but comprehensive survey on prior work related to leveraging prior in BO, which I find useful for the general audience.

Reviewers noted that such a bound could become trivial with a bad prior in practice, and further suggest that one may leverage these theoretical insights as general guidelines to practitioners in designing the prior. I think this is a valuable message to convey and suggest authors take it into account in the revision.

There were initial confusions pertaining to the experimental details, mainly concerning the effect of the quality of the prior on the performance of the proposed algorithm. The authors provided an effective rebuttal with much concrete empirical support, and after a few rounds of interaction during the discussion phase, the reviewers are convinced about the empirical significance of the proposed work. Overall, this makes a solid work.
The work studied the problem of inserting backdoor into a deployed model through bit flip.

Some important concerns have been proposed by reviewers, including: the incorrect claim of the treat model, the potential defenses are only discussed but not validated, experimental setups and analysis (e.g., the sensitivity test of hyper parameters). Although the authors provided some responses, but all reviewers are not well convinced. 

After reading the manuscript, reviews and discussions between reviewers and authors, I think this work is not ready for publication. The reviewers  comments are supposed to be helpful to improve this work.
All three reviewers recommend borderline rejection based on limited novelty, missing comparisons with other methods, and runtime inefficiency. The authors’ response helped clarify other questions but did not eliminate the main concerns about the paper. The AC agrees with the reviewers that, in its current form, the paper does not pass the acceptance bar of ICLR. The reviews have detailed comments and suggestions that should help the authors to improve the work for another conference.
In this paper, the authors enhance the adversarial transferability of vision transformers by introducing two novel strategies specific to the architecture of ViT models: Self Ensemble and Token Refinement method. Comprehensive experiments on various models (including CNN s and ViT s variants) and tasks (classification, detection, and segmentation) successfully verify the effectiveness of the proposed method.

In general, the problem studied is relevant and important. The paper is well written and well motivated with empirical findings. The proposed two strategies are novel, simple to implement, and effective in practice. Following the author s response and discussion, the average score increases from 6 to 7.5, with most concerns well addressed. AC believes that the paper should be highlighted at the ICLR conference.
The paper considers neural importance sampling (that is, importance sampling with a trained flow proposal) and its application to high energy physics. The two contributions of the paper are: (a) a methodological improvement in the training of the proposal; (b) a description of a software library that implements the framework.

All reviewers were critical of the paper and recommended rejection. The main issue raised was that the methodological contribution was not novel or significant enough, and not sufficiently evaluated. The authors disagreed with the reviewers that the methodological contribution was not significant enough, but they acknowledged that the first version of the paper did not present the contribution clearly; consequently, they submitted a heavily revised second version following the reviewers  feedback.

Although it seems that the second version is an improvement over the first one, it s clear that the paper requires a second round of reviewing to ascertain whether it satisfies the requirements for acceptance. At this stage, the consensus among reviewers remains that the paper should be rejected. For that reason, I cannot recommend acceptance to ICLR. I sincerely hope the reviewers  feedback will be useful to the authors for a future submission to a different venue.
This paper presents new insights for training on random subspaces of low dimension, with several theoretical and experimental contributions. This is a paper that would be interesting to many people doing research in deep learning, both from the theoretical and practical side.
This paper proposes a federated learning (FL) scheme that is suitable for clients/devices with heterogeneous resources. The scheme Split Mix trains multiple models of different sizes and adversarial robustness levels, which are tailored to the budgets of the individual device. Empirical results show encouraging results.

It is clear that FL will have to work with clients with diverse resources, a point that is appreciated. Indeed, it is anticipated that widely dispersed inference will have to deal with a highly heterogeneous mix of clients. The study is quite thorough. One aspect that is not convincing in the experiments is the budgets being exponentially distributed: having a strong concentration around a mean (with something like a Gaussian tail), or a power law distribution, would be more suitable.
This paper offers flow based alignment methods for alignment of distributions in a domain adaptation setting.  While there are many positive aspects of the submission, the experimental results only weakly support the results.  The AC agrees with the critical comments mentioned by reviewer sZ2C, and in particular observes that the experimentation is not state of the art with regard to current domain adaptation literature. Unfortunately the submission is not acceptable in present form.
While the reviewers appreciated the new methodology and presentation of the paper the reviewers were concerned about the experimental section. Specifically they wanted to see optimization outside of penalized logP and QED, which are now viewed by the community as toy molecule optimization tasks (e.g., Penalized logP can always be improved by just adding a longer chain of carbon atoms). The authors responded that this would have taken too long to run Guacamol tasks in the rebuttal phase as all methods would need to be rerun for all tasks, but this is not true: many methods e.g., Ahn et al., 2020, already have reported these results and could be directly compared against (as this paper is near state of the art this would have been a convincing comparison). Another odd thing about the experimental setup is that the authors compare with Ahn et al., 2020 only for constrained property prediction. However Ahn et al., 2020 achieves a Penalized logP of 31.40 whereas the proposed method only achieves 13.95. It s suspicious that this result is missing in Table 2 of the current paper. If the authors are able to improve their work beyond Ahn et al., 2020 and related recent work on Guacamol and othe real world tasks, the paper will make a much stronger submission.
The reviewers all acknowledge the importance of the paper as it addressed the challenge of the insufficient data problem in conditional contrastive learning, feeling that the idea was novel, the experiments verified the effectiveness of the model well, and the paper is well written. Reviewers also raised some good questions, such as the computational complexity, comparison with Fair_InfoNCE in the experiments, and kernel ablations. These questions are well addressed in the rebuttal and the revised version. One reviewer raised the issue of similarity to [1]. After taking a close look at this paper and [1], the AC felt that the motivation and focus of this paper are quite different from [1]. The authors should incorporate all the rebuttal info into the final version.

[1] Jean Francois Ton et al. 2021.
The paper considers the empirical distribution of layer/channel in CNN ,and proposes to use global null tests with Simes and Fisher statistics to aggregate the p values. This method is competitive while computationally efficient. The underlying theoretical insights are discussed in detail.

The paper received mixed ratings, and the discussions weren t active. So, AC carefully read the paper and inspected all reviews. Reviewer a8KZ comments were factually inaccurate in listing references, and lack substantial feedback on the actual content of the paper. Hence, the review was down weighted. 

The other negative reviewer Ni17, as an OoD expert, unfortunately did not offer more feedback to author rebuttals. From what AC comprehends, the authors should have clarified their The theoretical guarantee and compared properly with Liu et. al. 2020 energy score (ES).

Considering the above, AC feels that the study deserves to be published.
This paper interprets pre trained masked language models (MLMs) as energy based sequence models and designs a tractable MCMC sampling algorithm based on Metropolis Hastings with proposals derived from MLMs themselves. 

The strategy is simple, reasonably elegant, and fixes technical mistakes of prior work. The proposed algorithm addresses intractabilities of some naive MCMC schemes, does not require modifications to MLM training, and makes good use of MLMs themselves as proposals thus being crucially economical about resources. 

We had some concerns about speed of generation and the paper s positioning with regards to existing strategies for sampling from energy based models (already during parameter estimation). While I understand that for many applications speed of generation is crucial, I think that on its own should not keep this line of research outside our best venues. And I hope steps like this one will lead to faster algorithms in the near future. I do relate to the issue of positioning, and I am glad the authors did not take it lightly. In the rebuttal phase the related work and positioning have been improved, but the authors remarked that the limited space for the camera ready was preventing them from expanding the discussion. A note to authors: it s not a bad idea to have an expanded related work section in appendix.
The major concern with this paper is the unfair comparison between global and local clipping (at least from the theoretical point of view). The assumption that the norms of the gradients are bounded in Theorem 2 is too strict for the following reasons. Clipping has been introduced exactly because we cannot assume the norm of the gradient to be bounded by a fixed constant  in the first place. Accordingly, comparing two clipping methods under the bounded gradient assumption does not seem relevant. Further, the two methods are not studied under the same set of assumptions (In Theorem 1, the norm of the gradient is not assumed to be bounded, but in Theorem 2 it is). 
A fair comparison needs to be presented to make the case for the proposed method.
The authors give an effective framework PRIME to tackle the challenges of automating hardware design optimization.  This problem is of importance to the community.  Overall, the reviewers thought the paper gave a nice clean approach to the problem and that the community would be interested with these results.
The work AdaFocal proposes an approach to tune Focal Loss  $\gamma$ hyperparameter to improve the model s calibration, particularly to avoid the occasional underconfidence when using focal loss. This tuning is done not as a learned constant hyperparameter across training but as one that evolves over training.

The work is both well motivated and well written. However, multiple reviewers share the concern (which I agree with) that the method fails on ImageNet experiments, and the method often fails to beat even temperature scaling. The experimental comparison arguing that the approach improves upon many methods pre temperature scaling is unfair as no other method leverages the validation set. This makes for a fairly deceiving slight of hand if not read carefully. When compared to temperature scaling, which does use the validation set, the performance improvement gap is diminished altogether.

I recommend the authors use the reviewers  feedback to enhance their preprint should they aim to submit to a later venue. In particular, improve the clarity around the experimental validation.
This paper focuses on leveraging static and dynamic sparsity in efficient robust training. The proposed methods can significantly mitigate the robust generalization gap while retaining competitive performance (standard/robust accuracy) with substantially reduced computation budgets. The philosophy behind sounds quite interesting to me, namely, sparsity allevating overfitting and improving training efficiency simultaneously. This philosophy leads to two novel algorithms design, i.e., static Robust Bird training, and dynamic Flying Bird training.

The clarity and novelty are clearly above the bar of ICLR. While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. Thus, most of us have agreed to accept this paper for publication! Please include the additional experimental results in the next version.
This paper studies the problem of choosing the best cloud provider for a task. The problem is formulated as a bandit and solved using algorithm CloudBandit. The algorithm is compared to several baselines, such as SMAC, and performs well. The evaluation is done on 60 different multi cloud configuration tasks across 3 public cloud providers, which the authors want to share with the public.

This paper has four borderline reject reviews. All reviewers agree that it studies an important problem and that the promised multi cloud optimization dataset could spark more research in the area of cloud optimization. The weaknesses of the paper are that it is not technically strong and that the quality of the new dataset is not clear from its description. At the end, the scores of this paper are not good enough for acceptance. Therefore, it is rejected.
This paper received the initial scores with large variance. During the intensive discussion (Number of Forum replies is up to 60), the opinions reached the consensus. I have read all the materials of this paper including manuscript, appendix, comments and response. Based on collected information from all reviewers and my personal judgement, I can make the recommendation on this paper, *rejection*. Here are the comments that I summarized, which include my opinion and evidence.

**Research Problem and Motivation**

(1) It seems that the authors aimed to address the question that “are negative examples necessary for deep clustering?” This research problem has been proposed and addressed in BYOL and SimSiam (If I remembered correctly, some reviewer pointed this out). What the authors actually did is to add two more components, positive sampling strategy and prototypical contrastive loss, on the top of BYOL. In my eyes, it is like putting two patches on BYOL, where one of them does not work (I will explain later). 

(2) Moreover, the authors failed to clearly illustrate the drawback of BYOL. In the last sentence of the third paragraph in the Introduction part, the authors mentioned that “BYOL only optimize the alignment term, leading to unstable training and suffering from the representation collapse.” This sentence is too general, which lacks strong motivation. 

Therefore, the research problem addressed here is an incremental problem over BYOL, rather than brings new insights into the contrastive learning community.

**Philosophy**

Without a clear motivation, it is difficult to catch the philosophy of this paper, i.e., how their proposed components tackle BYOL’s drawbacks. Moreover, the relationship between two components is also unclear. 

**Novelty**

I believe Reviewer Na7g has a thorough analysis of the novelty of this paper. I will not go into details here. The difference does not mean novelty. 

**Technique**

Positive sampling strategy does not work. If we take a closer look at Table 4, the rows of BYOL and NCC with PS, there is no significant performance gain. The p values of t test on ACC results on CIFAR 10 and CIFAR 20 are 0.92 and 0.32, respectively. Actually, the prototypical contrastive loss is the key element to boost performance over BYOL.

**Misleading Title**

Based on the above point, the title is misleading. Although no negative data sample pairs are used in the training, the contractiveness on the cluster level should also belong to the scope of contrastive learning.

**Experiments**

(1) In the Introduction part, the authors mentioned that SimSiam is in the same non contrastive category with this paper. However, this paper is not included in the comparison.

(2) The competitive methods in Table 2 and 3 are not consistent. The authors even did not report the performance of BYOL on ImageNet 1K.

(3) Positive sampling strategy does not work. See the above Technique point.

(4) The authors only reported the running time on CIFAR 10 and CIFAR 20.

Therefore, the experimental results are not very convincing and solid to me.

**Presentation**

I believe the presentation also needs many efforts to smooth the logic. For example, “Even though Grill et al. (2020); Richemond et al. (2020) have proposed to use some tricks such as SyncBN (Ioffe & Szegedy, 2015) and weight normalization (Qiao et al., 2019) to alleviate this issue, the additional computation cost is significant.” Actually, Grill et al. (2020) is BYOL, where the authors added their components on. The computational cost of the proposed method should be heavier than BYOL. 

Based on the above points, this paper suffers from several severe issues, which makes it not self standing.
This paper shows how constraining the representation to be invariant to augmentation shrinks the hypothesis space to improve generalization more than just introducing additional samples through augmentation. I agree with the reviewers that this is a novel, intuitive, and interesting finding. However, there were many technical and clarity issues with the original submission. These were partially addressed by the authors in the rebuttal. The reviewers appreciated the authors  efforts and commitment in the rebuttal, but my conclusion from our discussion that this paper requires another round of revisions. I hope the authors would follow the reviewer s comments, improve the paper, and re submit.
The paper considers the convergence of the Monte Carlo Exploring Start (MCES) algorithm, a basic method in RL. Although the method is very simple and known for a long time, the condition for its convergence is not completely understood.

One of the latest results is from almost 20 years ago (Tsitsiklis, "On the Convergence of Optimistic Policy Iteration," 2002). That paper shows the convergence of the method under some restrictive assumptions on how the algorithm operates. In particular, that result requires that only the action value function of the initial state action pair of an episode be updated, as opposed to all visited state action pairs. What is not known is whether we can update the action values of all state action pairs observed in a trajectory.

It is notable that one of Tsitsiklis (2002) result requires a uniform selection of the initial state action. But he also describes a modification of the algorithm that allows convergence with a non uniform initial distribution (see page 66 of that paper, close to the end of Section 3   Optimistic Policy Iteration Using Monte Carlo for Policy Evaluation).

On the other hand, this paper establishes the convergence of MCES with no assumption on how the algorithm works. In particular, the algorithm updates the action value function for all state action pairs observed in an episode. As long as all state action pairs are visited infinitely often (and not necessarily from a uniform distribution), the convergence is established.

There is a catch, however. The paper requires some assumptions on the class of MDPs. In particular, it requires the environment to be either Stochastic Feed Forward (SFF) (more restrictive) or Optimal Policy Feed Forward (OPFF) (less restrictive). The OPFF assumption states that under any optimal policy, a state is never revisited within an episode.

The proof technique of this paper is different from the usual stochastic approximation method, and may be considered simpler.

We have two positive reviewers (with score of 8) and two slightly negative ones (with score of 5). The concern of negative reviewers is that the OPFF assumption is very restrictive. And given such an assumption on the class of MDPs, the proof becomes very simple. Moreover, it is not clear that the proof techniques developed in this work is a step toward analyzing MCES for more general MDPs.

My related concern is whether OPFF vs. Non OPFF is a good way to characterize MDPs for which MCES with every state update is convergent. Figure 3 in the paper shows the current state of knowledge in the analysis of variants of MCES. We know a problem with "No convergence" within the fourth quadrant (which is Non OPFF).
Is the class of Non OPFF a tight superset of the non convergent ones?  Or is it a much larger superset? If it is tight, then OPFF is a good characterization of when MCES works or not. If it is not tight, OPFF may not be the right way to characterize the convergence of MCES.

All being said, I believe this paper positively contributes to our knowledge of a basic and fundamental RL algorithm. It does not fully resolve the convergence question, but it is indeed a progress. Whether OPFF characterization is going to be the right one or not remains to be seen in the future. I would act optimistically here and recommend acceptance of the paper.

I encourage the authors to incorporate the comments by the reviewers in updating their papers, including:
  fixing all the typos
  providing more examples of problems that are OPFF
  resolving the claim about alphaZero
  providing empirical comparison with Q Learning
In this paper, a new learning scheme for minimizing the confidence set by conformal prediction is proposed. Most of the reviewers agree that the idea is interesting and novel. This is an important contribution to trustworthy ML, with theoretically sound considerations and thorough experimental validation.
This paper investigates an important problem, i.e., the fairness of the learned representation in deep metric learning, which is relatively under explored by the research community. Observing that the existing metric learning approaches become less fair when trained on an imbalanced dataset, the authors propose finDML to benchmark previous methods on multiple imbalanced datasets with three newly proposed metrics. 
Further, a PARADE module is adapted into this problem to tackle the fairness issue. 

The paper is meticulously written of good structure, and well motivated by experimental findings. The authors have a deep and thorough discussion with reviewers, through which the mixed preliminary ratings became all positive, with most concerns well addressed. AC found no ground for rejection and thus recommended acceptance. Authors shall integrate all response material into the next revision.
The authors propose a normalization method for cross lingual text representations. The goal is to normalize the monolingual embeddings based on spectral normalization. The study shows that produced text representations keep their meaning and improve performance on downstream tasks.

There is a disagreement among the reviewers. The main concern is whether the main contribution is an empirical study or a novel idea.  I think the authors well addressed the concerns of most reviewers. The idea and empirical study are enough for publication for ICLR 2022.
The paper studies attacks on the self supervised training pipeline of multi modal models, e.g., CLIP and related models.  The reviewers agree that the poisoning results are impressive in that they achieve good poisoning success with a fairly small number of samples.  The threat model is fairly specific to one (high profile) type of self supervised training, but the concepts presented are likely portable to the study of other related training pipelines.
The authors propose bringing lexicase selection from evolutionary computation and applying it to the optimisation of gradient descent. This is done by training a set of p networks and using their performance to select this set of p networks as training progresses on random subsets of the training data.
The reviewers felt, and I agree, that the paper was well written and its method now well described. Concerns raised during review include: additional computational cost, novelty, and marginal performance improvements. Nonetheless after discussion, while the computational cost is indeed higher, it is a novel application, and the reviewers were all in agreement with acceptance after further discussion around experiments.
The submission received split reviews: two reviewers recommended accepts, and the other two rejects. The AC went through the reviews, responses, and discussions carefully. The AC appreciates the authors  effort during the response period and agreed that the revision has addressed some of the concerns of the reviewers. However, a few key issues are not fully addressed. This includes results on additional, more complex object categories; discussion on why the performance of the proposed method is not even as comparable as BSP Net (Table 4); and others.  Further, while the authors have significantly refactored the paper to address the concern on presentation and clarity, the changes are too major for the reviewers to review during the response period (the reviewers are expected to check minor updates, but not review a new paper during the response period).  

Considering all pros and cons, the AC recommends rejection. The authors are encouraged to revise the paper for the next venue.
The reviewers are split about this paper and did not come to a consensus: on one hand they agreed that the paper has valuable theoretical contributions and addresses an important problem in current ML literature, on the other hand they would have liked to see empirical results on a real world problem setting. After going through the paper and the discussion I have decided to vote to reject for the following reason: I believe the reviewers  concerns about empirical results is not just a request for applying this to more datasets (which is easy to satisfy and I don t think is grounds for rejection), but is actually for a clearer connection for how this work would be used in the machine learning problems described in the introduction and related work sections. What would really help this paper is a real world running example, in place of the blue plus example, in Figure 1 (I think the blue plus problem is still a useful experimental tool and should be evaluated, but it doesn t clarify the real world use cases of this work. This led the reviewers to look to the experimental section for clarification on this, but this wasn t clarified there either. The authors  response to these concerns was an out of scope argument: the goal of this paper is to derive/test theoretical results, and there are a number of possible use cases we could apply this to. The authors argue that the current work sends  a strong signal to the ICLR community that the Prover Verifier Game is interesting and promising . I m sorry but I disagree here: the authors need to do more to convince the ICLR community that this is a framework that will solve outstanding problems in ML. This is solved if the authors (a) run their approach on a real world dataset in a paper they cite in the related work, (b) they include baselines in this experiment, and (c) if they add this as a running example throughout with a figure that explains this real world example. With these additions the paper will be a much stronger submission.
This paper proposes a self supervised auto encoder latent image animator that animates images via latent space navigation. The task of transferring motion from driving videos to source images is formulated as learning linear transformations in the latent space. Experiments conducted on real world videos demonstrate that the proposed framework can successfully animate still images. The proposed framework is novel, the experimental results are supportive and promising. However, some related works are still missing and might need to be added to the current paper for discussion and comparison. 

The rebuttal has addressed all major concerns raised by all 5 reviewers. The revised paper also included some feedback from the reviewers, except those discussions and comparisons with some missing related works pointed out by reviewers. After the rebuttal, all reviewers tend to accept the paper. AC agrees with the reviewers and recommends accepting the paper as a poster. Lastly, AC urges the authors to further improve their paper by incorporating the discussion on other missing related works suggested by the reviewers.
Somewhat borderline paper given the scores, but leaning on the side of accepting mostly because the positive (and weak positive) reviews are a little more persuasive. The negative review is a bit of an outlier; the main issues raised in the negative review are that the novelty is on the lower side or otherwise that the work is incremental. These complaints are largely not shared by the other reviewers, and furthermore seem not like deal breakers. Still a borderline paper, but fairly safe to accept.
The reviewers in general agree that the proposed complex valued DP method is interesting and novel. However, there are two key concerns due to which the paper might not be ready for publication at ICLR: a. the key technical contribution of the work is not clear, as the methods seem relatively straightforward extension of real valued DP methods to complex valued domains. 
b. More importantly, the experimental results (and hence the motivating applications) are not convincing and do not strongly support the claims of i) complex data provides more flexibility and hence provide better model, ii) proposed method is accurate. 
For example, the accuracy numbers for SpeechCommands even without DP seem quite low. For example, standard methods like matchboxnet for keyword detection have accuracy numbers in the range of 97%. While the work considers a subset of keywords, but it would be important to show how the standard methods work on this dataset. If the gap is this large, then the case for using complex valued datasets itself is weak. 
Similarly, on CIFAR10 it seems like that the considered architecture is quite poor as the accuracy is just ~80% while most standard architectures get >93% on the dataset. So the experiment claims of the paper might not hold for practically relevant architectures.
This paper proposes a pre training technique for improving the logical abilities of pre trained language models.
Reviewers point to many issues with clarity and experimental evaluation. No response was given by authors.
The paper explores surprise minimization in multi agent learning by using free energy across all agents in a multi agent system. A temporal EBM represents an estimate of surprise which is minimized over the joint agent distribution. Empirical studies on the proposed method are conducted. This paper builds in an interesting direction around surprise minimization in multi agent learning by using the energy based framework, but the presentation of the method seems to need more efforts to be improved to avoid confusion. 

The discussion between authors and reviewers is summarized below: The major concerns of Reviewer doix include that: (i) the empirical results are not compelling, (ii) qualitative results are missing, and (iii) the motivation of surprise minimization in multi agent RL is unclear. After the rebuttal, the authors addressed the concerns of Reviewer doix, who changed his/her score from 5 to 6. The major concern of Reviewer zFND comes from the understanding and justification of the paper. After the rebuttal, the concerns of Reviewer zFND  have been partially addressed by clarification on what measures of surprise can be used and how these would be estimated. Reviewer zFND eventually changed his/her score from 5 to 6.  Also, most of the concerns about theory and experiments from Reviewer 8Wiu have been addressed after the rebuttal. Reviewer 8Wiu accordingly changed the rating from 5 to 6. Reviewer g9cM is still not satisfied with the authors  answers, and his/her concerns regarding some technical issues remain and points out that the current paper has many inconsistencies across the writing that make it hard to evaluate the soundness and correctness of the results.  

After the rebuttal, the author successfully addressed most of the concerns from 3 of 4 reviewers, but the overall rating of the paper is on a borderline level. Given the fact that the paper still has some unaddressed concerns from Reviewer g9cM, and other reviewers actually do not champion the paper. The AC tends to recommend rejecting the paper at the current stage. AC urges the authors to improve their paper by including all the suggestions provided by the reviewers, and then resubmit it to a future venue.
The paper studies dyna style MBRL in a resource limited setting. It is evaluated on an acrobat task where it shows very promising results.

The reviewers appreciated the extensive replies, but they did not fundamentally change their opinion. In particular:
  Lack of formal problem statement and definitions
  The experiment on a single task (and that being a non standard version) isn t sufficient to demonstrate the general merits of the method

While the ideas are very promising, the paper cannot be published in its current form. We d hence like to highly encourage the authors to revise the paper and to re submit at a different venue.
This paper explored pre training for deep offline reinforcement learning, developing a method that first pre trained decision transformers on trajectories without rewards, and then fine tuned on limited data with rewards. The reviewers were pleased with the overall research questions and directions, but found that they were substantial shortcomings in the experimental setup and results that make this paper not yet suitable for inclusion. The approach is relatively simple and straightforward, which is actually a good thing, but that means that it must be correspondingly investigated and developed with convincing empirical results. Unfortunately, there are a number of open questions about the experimental set up, and the results are not convincing that the method is effective against alternatives, as detailed in the reviews. There was no author rebuttal.
The reviewers agreed that this is a technically novel and interesting paper with results for a very natural problem and all voted for acceptance. The paper gives more evidence for the wide ranging compatibility between the goals of sketching and of privacy.
All reviewers concur on the fact that the paper contains solid ideas. The discussion helped clarify the case of class imbalance and no major concerns remained after discussion phase. I thank the authors for the additional details on execution time / complexity.

On a separate note and perhaps to dig further in the paper s ideas,

1  the validity of the Gaussian assumption carried in the paper was raised (e.g. ercK), but I would like to point out that Theorem 2 can also be derived for general exponential families given the objective in (2), with perhaps a reformulation of the trace constraint (still, this would imply the knowledge of the exponential family for the KL divergence to simplify).

2  when it comes to protecting labels, the authors might want to have a look at the rich literature on learning from label proportions, which shows that the knowledge of the class is not necessary to learn a supervised model (see for example Patrini et al, NeurIPS / NIPS 2014). Thus, protecting the class could in fact be more achievable than by just considering that learning “needs observed classes”.
The authors propose a new way of addressing the ML as a service problem through using garbled circuits. As the reviewers point out the novelty is limited and comparison to existing work is not complete. The authors have also not responded to the reviews.
The reviewers agree that the paper introduces an interesting approach for estimating Shaley values in real run time. The effectiveness of the method is well demonstrated across different tasks/datasets.
This paper introduces a meta learning approach to "amalgamate" optimizers. The reviewers all found the idea interesting and unanimously found it to be acceptable for publication. In particular, I appreciate that the authors expanded their results to include more larger problems. One of the outstanding questions that would be interesting to address in future work is the use of tuned learning rate schedules.
This paper presents a SLAM based approach for the ALFRED benchmark. The presented method, Affordance aware Multimodal Neural SLAM has two key advantages over past works: It uses a multimodal exploration strategy and it predicts an affordance aware semantic map. It also obtains a very large performance improvement over the ALFRED benchmark. The reviewers for this paper were quite impressed by the large improvements obtained by this technique. However, there were two major concerns across the reviews: (1) Are the design choices made in this paper heavily engineered towards ALFRED ? (2) Does the work make too many assumptions about the setting (unrealistic assumptions that may not really hold in more realistic environments or the real world) ? The authors have provided a detailed response and answered many questions posed to them, but the reviewers continue to have concerns about the generalizability of the proposed method. Another point of concern pointed out by a reviewer is whether it is reasonable in a realistic setting to perform exploration with a knowledge of the downstream task. This point has not really been answered satisfactorily by the authors. My takeaway is that the method presented by the authors clearly works on ALFRED. But it contains several design choices that are largely ALFRED specific and in some cases unrealistic. This provides fewer benefits to readers looking for more general insights that can be valuable across a suite of tasks. As a result of this, and in spite of the large gains, I recommend rejecting this paper.
Dear Authors,

The paper was received nicely and discussed during the rebuttal period. The current discussions mostly lie on the acceptance side. 

Some prons of the paper include:

  Timely topic: This paper deals with the problem of distributed training of GNNs. 
  New algorithm: this method captures the idea of transmitting only local averages but adds a centralized step on the server to account for global structural information lost in the subgraph partition.
  Theory: The authors further provide theoretical convergence guarantees. 
  Clarity: The paper is fairly well written and the proposed result is simple and powerful. 

The current consensus is that the paper deserves publication.

Best AC
### Summary

This work demonstrates that it is possible to identify lottery tickets with some manner of structural sparsity. The work finds success through refilling (perhaps better termed infilling) and regrouping, two techniques that have found previous homes in other parts of the literature. 

### Discussion

#### Strengths

  Tackles an interesting problem. 

  At face value, I believe the paper achieves its claimed goal, though not with full clarity as written.

#### Weaknesses

  There is room to clarify the atypical form of structure here for readers. The suggested title change would appropriately set expectations. However, refinements in the text as well would be welcomed.

  As I discuss below, the claims should be settled with respect to the strongest baselines.

  Random reinitialization should play a primary role in the presentation of the text. As the authors note, the original lottery ticket paper did not require besting the performance of random reinitialization. However, the random reinitialization results are central to the main figures of the original paper and demonstrate that the result is not merely happenstance. This paper should follow that practice.

### Recommendation

I recommend Reject and I do not do so lightly, given the scores. The work here is promising because finding a path to better performing lottery tickets remains an open challenge. However, Reviewer YWUh has voiced reasonable concerns about the evaluation methodology. 

I ve read the detailed authors  responses and agree with the authors that the presented results may depend on choices in the hyperparameters and training strategy. Having said that, it is critically important for the paper to include in the primary text the strongest baselines available, despite this dependence. On such baselines, the results are worse than originally reported and hence, the primary claims must either be revised to be these results or revised to include these results, with the primary claims providing a range of results. Though the authors offer to make revisions for the camera ready, the required revisions here are substantial enough to require additional review, which is out of the scope of the current process.

An additional oversight in this methodology   at least as reported   is that the primary target of the lottery ticket hypothesis is sparse training, not sparse inference. Evaluating solely the inference performance   rather than training performance, which includes both the forward and backward pass   is, therefore, inconsistent with the purpose of lottery tickets. This methodological error will too need to be repaired in the final version of this work.
The paper aims to scale transformers to large graphs. In this regard, authors propose to first obtain a "coarse" version of the large graph using existing algorithms. With reduced number of nodes in the coarse graph, we can employ the transformer efficiently to capture the global information. To capture the local information, GNNs are employed. Finally, authors carry out extensive experiments on a range of graph datasets. Also, reviewers do appreciate reporting the confidence intervals. We thank the reviewers and authors for engaging in an active discussion. Unfortunately, the reviewers are in a consensus that novelty of the proposed method is limited: it is combination of existing techniques and similar ideas have been widely used in the literature. Also, the empirical results are not very significant. Thus, unfortunately I cannot recommend an acceptance of the paper in its current form.
Thank you for your submission to ICLR.  This paper presents a straightforward but reasonable approach to (slightly) improving the performance of large batch training via adversarial training.  The basic approach is to apply (small epsilon) adversarial training, shown to help performance in small batch settings, but accelerate the method using stale parameters to allow for parallel computation of the perturbations.  This speeds up adversarial training while improving performance, enough to enable it to be more effective than existing techniques for this large batch setting.

The reviewers are not entirely in agreement about this paper, but I personally found the objections of the reviewer to be fairly generic, and not really addressing the core contributions of the paper.  However, I also felt that the overall contribution of this work seems somewhat incremental, using a not particularly unexpected result (that we can use stale gradients for this form of adversarial training) to achieve moderate speedup in what ultimately seems like one point in hyperparameter space.

That all being said, though, clearly the authors are working within standard benchmark frameworks, and "simple" algorithms here are indeed a positive rather than a negative.  So I am inclined to slightly recommend the paper for acceptance.
The paper proposes an approach for learning a decomposition of a scene into 3D objects using single images without pose annotations as training data. The model is based on Slot Attention and NeRF. Results are demonstrated on CLEVR and its variants. 

The reviewers point out that the method is reasonable and the paper is quite good, but even after considering the authors  feedback agree that the paper is not ready for acceptance. In particular, the key concern is around experimental evaluation   that it is performed on one dataset (and variants thereof) and that the evaluation of the 3D properties of the model is not sufficiently convincing: it does not outperform 2D object learning methods on segmentation and is not compared to those on "snitch localization".

Overall, this is a reasonable paper, and the results are promising but somewhat inconclusive, so I recommend rejection at this point, but encourage the authors to improve the paper and resubmit to a different venue.

(One remark. The paper makes a point of not using any annotation. It is technically true, but in practice on CLEVR unsupervised segmentation works so well that it s basically as if segmentation masks were provided. If the authors could demonstrate that their method   possibly with provided coarse segmentation masks   works on more complex datasets, it would be a nice additional experiment)
All reviewers agree that the proposed SphereFace2 approach   training face recognition models by using multiple binary classification losses   is interesting and innovative. The reviewers agree that the paper is well written and are satisfied with the presented experimental study. The rebuttal addressed all additionally raised questions. I believe that the paper will be of interest to the audience attending ICLR and would recommend a presentation of the work as a spotlight.
The work presented in this submission is focused on a new approach for learning a model that can perform well at any point in time, and called Anytime Learning at Macroscale (ALMA). The algorithm processes data through a series of training batches, each of these processing steps being followed to a model evaluation. The total loss is the average (or sum) of the losses computed at each step.

Reviewers agreed that the paper is not ready for acceptance at ICLR 2022 as the presentation of the work lacks of clarity, especially w.r.t. to the similarities with online learning and the learning of streams of data, and the fundamental difference between small or moderate batch sizes and very large batches.
Note: This meta review is written by the SAC, but it s synced with the AC.

Summary (adopted from Reviewer wCmR): This paper presents a modification of monotone deep equilibrium layers that allows to compute the bounds on the output via the IBP algorithm. This also allows to train a certifiably robust DEQ model with a competitive performance.

Initial reviews were mixed, but post rebuttal the opinions generally improved. Reviewer wCmR intend to increase their score slightly (6 to 7) and Reviewer KViU also mentioned that their opinion improved. Reviewer 7ZJs maintained their score and, during discussion phase, made many arguments against acceptance. One of those was about Tarski s theorem which was deemed not so important by the AC and also KViU. Another concern was about experimental results to which KViU agreed, and this remains the main concern for now.

Most reviewers agree that the work is interesting and is a good step, but then utility of the new modification and significance of the results remains a question. It is likely that the work may be useful in the future, and as there is an overall increase in the opinion, I believe that it is okay to accept the paper.

I encourage the authors to take the comments of the reviewers into account, and clearly mention the issues raised in the paper.

SAC
The paper propose a Fully Online Meta Learning (FOML) method which extend MAML for continual learning in a fully online learning  without requiring the knowledge of the task boundaries. Experiments show that FOML was able to learn new tasks faster than several existing online learning methods on Rainbow MNIST, and CIFAR100 datasets. 

There are a few major concerns from reviewers. One concern is about the lack of clarity on the problem statement: The authors cast the problem as meta learning that must be done in a fully online setting, but it requires to store all the training data in a buffer storing all the training data seen so far, which contradicts to the principle of “online learning”. Another major weakness is the poorly written literature survey, which missed to cite a large body of related work in continual learning and online meta learning (such as Online Continual Learning, task free continual learning, continual learning without task boundaries, etc). These should at least be discussed carefully if not fully compared in the empirical studies. Also experiments are quite weak in both settings, datasets and rather out of date baselines. Finally, there also lacks of theoretical justification or analysis. 

Therefore, the paper is not recommended for acceptance in its current form. I hope authors found the review comments informative and can improve their paper by addressing these review comments carefully in future submissions.
This paper proposes the algorithm which they call DEO* SGD, which is a combination of the ideas of the generalized DEO scheme, denoted by DEO*, to facilitate exploration (Section 3.1), adoption of stochastic gradient descent (SGD) in the exploration chains (i.e., those chains except the one with the lowest temperature) (Section 4), and use of adaptive tuning of learning rates (Section 4.2). The proposal is applied experimentally in Section 5 to demonstrate superiority of the proposal over existing approaches.

The initial review scores of the four reviewers were one positive and three negatives. Most reviewers positively evaluated the proposal, including the proposal of DEO* and its theoretical analysis, as well as its empirical usefulness in deep learning for a computer vision task. On the other hand, some reviewers showed concern about soundness of the proposal. Upon reading the reviews and the author responses, as well as the paper itself, I think that this paper lacks a clear statement on its objective.

* **What does "uncertainty approximation" mean?:** The paper title would imply that the objective of the proposal in this paper is for "uncertainty approximation," but I could not find any concrete description on what it exactly is.
* **Sampling versus optimization:** The methods of Langevin dynamics, or more generally Markov chain Monte Carlo methods, have been used for two distinct purposes: sampling and optimization. In any case fast relaxation towards equilibrium would be of practical importance. For sampling purposes it is also important to assure that the stationary distribution of the Markov chain corresponds to the target distribution (In Langevin dynamics the target distribution would be the canonical ensemble defined by the energy $U(\cdot)$ and the temperature $\tau$). For optimization purposes, however, the assurance of the stationary distribution to be equal to the target distribution would be less of concern. It seems that the authors  interest would be in optimization rather than in sampling, but it is not clearly stated.
* **Soundness issue:** As Reviewer mbau pointed out, DEO* does not have a guarantee of convergence to the target distribution. I thought that if the objective of this paper would be in optimization rather than in sampling, the existence of approximation already in DEO* might be thought of as a minor problem, as the proposal already has other approximations introduced in Section 4. The authors claim that this problem does not affect the main body of the paper, but I feel that it would affect the overall organization of the paper, as the current organization seems to presume that approximation only resides in the adoption of the SGD based exploration kernels with deterministic swap. In any case, this problem has been acknowledged by the authors themselves, as well as Reviewer ofJx.

In particular, the detailed discussion between the authors and Reviewer mbau has been very fruitful in clarifying technical subtleties in this manuscript, including the soundness issue mentioned above. At the same time, it would imply that this paper still has room for improvement.

An additional point I would like to mention is that this paper is not really self contained, in the sense that several key notions and quantities are not defined or only defined in the Supplementary Materials ($\tilde{U}$ is not explicitly defined at all, the terms "swap time" and "round trip time" are defined in Appendix A.5, $\sigma_p$ in Corollary 1 and Lemma 2 is defined in Appendix A.1). 

All these weaknesses make me to think that another round of revision would be appropriate to properly judge the quality of this paper, whereas there is no such option within the review procedure of ICLR. I therefore cannot recommend acceptance of this paper at least in its current form.

Minor points (page and line numbers refer to the revised version):
  Abstract, line 5: "given sufficient many $P$ chains" would be better phrased as "given $P$ chains", as the big O notation usually assumes the large P asymptotic.
  In several places, there are periods after "Figure" and "Table", which are not needed.
  Page 3, line 32: In Lemma 2 there is apparently no such term found as "the second quadratic term". It should appear only after having assumed the equi acceptance/rejection rates in equation (4), so that the sum becomes proportional to $P$.
  Theorem 1: "the maximal round trip time" should certainly be "the minimal round trip time". / is the ceiling function(. T  > , t)he round trip time
  Table 1: I did not really understand what "non asymptotic" / "asymptotic" mean, as the big O notation used here should by definition be asymptotic.
  Corollary 1: the optimal (number of) chains
  Page 4, line 34: The abbreviation SGLD is not defined in this paper.
  Page 4, line 36: similar(ly) to
  Equation (6): The sign of the last term should be " ".
The paper presents a new framework of synthesizing differential private data using deep generative models. Reviewers liked the significance of the problem. They raised some concerns which was appropriately addressed in the rebuttal.  We hope the authors will take feedback into account and prepare a stronger camera ready version.
The reviewers agree that the proposed method to create a more robust representation of a task for model based RL is interesting and has significant merits. After some revision, more critical reviewers improved their ratings of the paper, such that there is unanimous agreement that the paper can be accepted to ICLR.
The paper aims to solve the source free domain adaptation, specifically on measurement shift. The proposed method lowers the domain gap via restoring the source feature distribution with a lightweight approximation. The effectiveness and performance are well validated by extensive experiments on various datasets compared with other recent methods, and the ablation analysis supports the claim of the paper well. The paper is well written with clear logic to follow.
This paper present a way to fully binarize a BERT model. The authors convincingly demonstrate that a naive binarization results in large quality losses and then propose amendments. It is pretty impressive that it is possible to get a fully binarized model to work at all.
At the same time, the quality losses are still significant and in practice one might prefer to use distillation (as long as the hardware doesn t require binarization). One could also envision combinations of the proposed technique (perhaps in the 1 1 4 setting) with distillation.
### Summary

The key idea behind this approach is a new technique to map irregular sparsity to a regular, compressed pattern. The results can, in principle, therefore overcome several standard limitations with irregular data storage formats.  The results improve over existing (though related) techniques.

### Discussion

#### Strenghts

  An interesting and timely topic to study

  Results show non compute improvements

#### Weakness

The primary weakness noted among the reviewers was the lack of study on actual decoding performance. As I note below, this is a serious oversight that given the already existing theoretical work in the area warrants study as the community should begin to turn towards mapping that theory to practice.

### Recommendation

I recommend Accept (poster). This is a strong piece of theoretical work. However,  I would like to note that while I believe this work meets the current evaluation standards set in the area, it is time for follow on work to take the additional step to validate the practicality of the approach through a performance evaluation (either in simulation or FPGA/ASIC work).
This paper introduces a deep neural network sequence to sequence framework for modifying the length of a speech sequence.  It employs a convolutional encoder decoder architecture optimized under a Bayesian formulation with variational inference.  The proposed framework is evaluated on a voice conversion task and three emotion conversion tasks. The results show that it can successfully change the duration of an utterance without accessing the target utterance.  Almost all reviewers raised concerns with some strong or inaccurate claims made by the authors in the paper.  The literature review on related work also needs to be significantly improved.  Another major concern is on experiments. Other than the DTW compared in the work, the proposed method should also be compared with existing duration modification techniques. The MOS evaluation seems to be limited and needs further improvement to make the results stronger and more convincing.  Since the authors did not provide a rebuttal, all these major concerns remain unanswered.
There wasn t enough enthusiasm to push this paper over the bar, based on no reviewer championing the paper (the one score above 6 was consulted and thought this was a fair assessment). The reviewers appreciated the contributions of the paper but felt that in terms of technical depth, there was a lot of overlap with prior work, and the statements of the results themselves were good but not exciting enough to convince the reviewers. Some suggestions for further improvement that came up were to try to extend this to update time for low rank approximation, which was an application that other work that built off of Cohen et al did, see, e.g., https://arxiv.org/abs/1805.03765 . Regarding presentation, it would be great if in a re submission the authors handle the presentation concerns of some of the reviewers regarding the experiments.
This paper proposes an active intervention targeting mechanism for causal structure discovery. After the discussion, there was a consensus among the reviewers that this paper needs another round of revision to address the lingering concerns. These concerns include providing a more fair experimental setup (e.g. by properly distinguishing and designing proper experiments for the observational, random intervention, and targeted intervention settings). Since the paper lacks theoretical guarantees (which is OK and not a requirement for acceptance), the merits rest on providing a thorough and fair experimental evaluation.
This paper presents U WILDS, an extension of the multi task, large scale domain shift dataset WILDS. The authors propose an extensive array of experiments evaluating the ability of a wide variety of algorithms to leverage the unlabelled data to address domain shift problems. The vision behind sounds quite ambitious and convincing to me, namely, the proposed U WILDS benchmark would be a useful and well motivated resource for the ML community, and their experiments were very comprehensive. Although they did not introduce any new methods in this paper, U WILDS significantly expands the range of modalities, applications, and shifts available for studying and benchmarking real world unsupervised adaptation.

The clarity, vision and significance are clearly above the bar of ICLR. While the reviewers had some concerns on the novelty, the authors did a particularly good job in their rebuttal. Thus, all of us have agreed to strongly accept this paper for publication! Please include the additional rebuttal discussion in the next version.
The article introduces a Bayesian approach for online learning in non stationary environments. The approach, which bears similarities with weighted likelihood estimation methods, associate a binary weight to each past observation, indicating if this observation should be including or not to compute the posterior. The weights are estimated via maximum a posteriori. 

The paper is well written, the approach is novel and its usefulness demonstrated on a number of different experiments. The original submission missed some relevant references that have been added in the revision. The approach has some limitations, highlighted by the reviewers:
* it requires to solve a binary optimisation problem whose complexity scales exponentially with the size of the dataset; although the greedy procedure proposed by the authors seems to work fine on the examples shown, the approach may not be applicable to larger datasets
* it requires to store all the data
* it requires the traceability of the marginal likelihood

Despite these limitations, there was a general agreement that this paper offers a novel and useful contribution, and I recommend acceptance. 

As noted by reviewer o4TK, I also think that the title is not very accurate. Bayesian methods naturally allow recursive updates of one s beliefs, and therefore have "memory". Maybe change the title for "Bayes with augmented selective/adaptive memory"?
Kernel methods are among the most flexible and powerful approaches of our times. Random features (RF) provide a recent mechanism to also make them scalable due to the associated finite (and often small) dimensional approximate feature map (in the paper referred to as linearization). The focus of the submission is the linearization of the softmax kernel (defined in (1)) while making sure that the obtained RF approximation is accurate simultaneously for the small and the large kernel values. The authors present a hybrid random feature (HRF, defined in (8)) construction parameterized by base estimators and weights, and show that specific choice of these parameters is capable of implementing the goal. Some of the HRF estimators are also accompanied by theoretical guarantees (Section 3). Their numerical efficiency is illustrated (Section 4) on synthetic examples and in the context of natural language and speech modelling, and in robotics.

Scaling up kernel methods is a fundamental task of machine learning. The authors present a nice and valuable construction in this direction which can be of both theoretical and practical interest to the community.

The submission would benefit from implementing the remarks of the reviewers to improve its clarity.
The paper presents a new perspective on recommendation systems, categorizing them as linear predictors where the main difference between the various methods is the regularizer. The authors then propose an objective function that aims at optimizing the Frobenius norm while maintaining a low rank solution, and present algorithm that have closed form solutions based on the SVD.

The reviewers noted the novelty of the framework, but the overall assessment after the discussion was that the theoretical contribution was limited. The algorithm proposed by the authors does not provide any improvement on standard criteria (performance, computational complexity), which makes the algorithmic/experimental contribution limited as well.
The paper aims to improve our understanding of GNNs for relational reasoning. In this regard, authors develop a conceptual framework unifying popular models (GNNs, Transformers, etc.) for analyzing their expressiveness and learning capacity. We thank the reviewers and authors for engaging in an active discussion. Based on author comments, the goal of the paper was more of a conceptual exposition, however this did not come across to the reviewers from the manuscript at first. Thus, a better presentation would definitely make the paper much more accessible and useful to the community. Moreover, there were some concerns about the significance of the exposition and better positioning would help (e.g., how the results help improve our understanding of GNNs). Thus, unfortunately I cannot recommend an acceptance of the paper in its current form.
The reviewers uniformly suggested rejecting the current paper.

I concur and remain especially somewhat unconvinced by the authors comments on learning features.  In particular, any argument based simply on (current) performance seems rather weak.  There are methodological reasons one might want to keep features fixed, and there are a small subset of problems with well defined known useful features.  But in the long term surely we should want to be able to learn the features, and efficiently and elegantly handle the case where they are learnt continually.

I want to thank the authors for engaging.  This work has the potential to be improved and I would encourage the authors to carefully consider and incorporate the provided feedback by the reviewers into their work.
This paper focuses on using reference objects for long distance estimation by introducing a novel dataset and an attention based learning framework. While the presentation flows well and the methodology is practically useful, it is only marginally significant and novel. Some of the practical data augmentation aspect raise some question on whether the process wouldn t confuse the network   the authors provide empirical evidence of the contrary in their response, although I find the principled argument to be somewhat lacking.
The submission develops a rotationally equivariant scattering transform on the sphere.  Many developments in deep learning make use of spherical representations, and the development of a rotationally equivariant scattering transform is an important if not unexpected development.  The reviews are split with half of the reviewers believing it to be slightly above the threshold for acceptance, and half believe it to be slightly below the threshold for acceptance.  In the papers favor, it solves an important case of the scattering transform framework, which has been demonstrated to be important in diverse machine learning applications such as learning with small data sets, differentially private learning, and network initialization.  As such, continued fundamental development in this area is valuable, especially in the context of representation learning, the focus of ICLR.
*Summary:* Study expressive power of narrow networks. 

*Strengths:*
  Study the narrow setting, which is not as well studied as the wide setting. 
  Some reviewers found the paper well written. 

*Weaknesses:*
  Restricted class of targets and activations. 
  Similar results have appeared in previous works. 

*Discussion:* 

99iL asked about the possibility to remove certain assumptions and the extension to other activations. Authors answer negatively to both. 99iL acknowledges the response and concludes the so called maximum principle is the most interesting result, but also points out that similar results appear in previous work and that it would have been good to see some extensions. qHTG indicates that the paper is well written and has interesting contributions but that some of the theoretical results only apply in settings that are more restrictive than in other recent related works. Authors agree that generalizations deserve to be investigated in the context of the presented results, but point out that their principle does not apply in that case, and hence that such generalizations are out of scope. Although qHTG identifies several good aspects in this work, they maintain the overall assessment of just marginally above the threshold. PCRn finds the work very interesting but is concerned about the novelty and points out that although the work is technical, the main message is not very strong and that the extraction of insights to solve tasks is not as clear. PCRn concludes that the paper presents various relatively weak results but not a sufficiently significant message. Authors remark that some of their results constitute a mathematical tool for future works. 

*Conclusion:* 

One reviewer rated this work marginally below the acceptance threshold and three other marginally above. Considering the reviews and the discussion, I conclude that this paper obtains a few interesting results but leaves much for future work. Further development of the current results would make the article significantly stronger. In view of the very high quality of other submissions to the conference, I find that this article tightly misses the bar for acceptance. Therefore I recommend to reject this article. I encourage the authors to revise and resubmit.
The authors propose to use genetic algorithms to learn variational autoencoders (VAEs) with discrete latent spaces. Specifically they employ natural evolution strategies (NES) to avoid backpropagating gradients through discrete variables. Experiments show how the proposed approach is competitive with the current state of the art to train discrete VAEs.

Some concerns arose from the review and discussion phases, these included confusion around the justification and derivation of NES for VAEs in the presentation and the limitation of the experiments. Authors were responsive and provided the reviewers the needed clarifications, an updated presentation in the revised paper and additional experimental results which ultimately were successful in raising the reviewers  scores towards full acceptance.
*Summary:* Study inductive bias of natural gradient flow. 

*Strengths:* 
  Some reviewers found the invariance to reparametrization insightful, a good way to better understand the interaction of the learning rule and parametrization. 
  Experiments support the theory.   

*Weaknesses:* 
  Unclear takeaway message. 
  Comparison with Euclidean case not comprehensive. 
  Insufficient distinction between reparametrization (invertible) and different parametrization. No experiments on actual dataset/architecture.
  Some reviewers found the the cases considered in the paper are already well understood. 

*Discussion:* 

2Yhk found that although the author responses and other reviews clarified some of their concerns, particularly about reparametrization conditions, the result provided in the paper is not strong enough and could be further clarified. The authors found that this reviewer might have misunderstood the paper. Following the discussion period, the reviewer raised his/her score and lowered his/her confidence. In response to CBDn the authors added demonstration of NGD being worse than EGD on matrix completion. In one of the responses, the authors summarize their contribution as:  replacing EGD with NGD … disturbs the second mechanism [dynamics and GD trajectories] . I find the question really is what kind of quantitative conclusions can be made. gWo5 pointed out important related work that was not discussed in the initial submission. Authors added discussion. VPSX misses applications to less well understood settings. Authors however only offer to keep this in mind for future work. VPSX also asks to emphasize the insights into the inductive bias of NGD. Authors added some discussion, however mostly pertaining previous works and not specific enough in my opinion. 

*Conclusion:* 

One reviewer found this work marginally above the acceptance threshold and four other reviewers found that it does not reach the bar for acceptance. I find the topic worthwhile and that it deserves a thorough investigation. However, I concur with the reviewers that some concepts require a clearer presentation and that it would be desirable to see more general results and more comprehensive discussions. Several suggestions were made by the reviewers and acknowledged by the authors, but many of these were left for future work. In summary, I think that the article makes a good start but needs more work. Therefore I am recommending a rejection at this time. I encourage the authors to revise and resubmit.
This paper proposes a few shot learning method that uses Fisher information matrix based task affinity. The experimental results show that the proposed method achieved better performance than existing methods. This paper is well written. The newly proposed task affinity score is interesting. The experimental results and theoretical analysis support the effectiveness of the proposed method. The authors are encouraged to address the reviewers  concerns in the paper.  Although the distance between task representations is symmetric in neural processes, they do not use the symmetric distance for meta learning. They input the task representations into the neural network, so the output can be asymmetric.
This paper connects MAML to contrastive learning under some simplifying assumptions and with slight modifications in the setting. 
Specifically, the authors show that if the inner loop updates are only applied on the top linear layer, MAML is equivalent to supervised contrastive learning (SCL). This means that MAML learns a feature transformation that brings in class representations closer and representations across classes far away. The zeroing trick the authors propose seems to give some performance gain in the experiments and is an actionable insight from their theory. 

Overall the paper is very interesting and (as far as I know) novel. The proposed zeroing trick is supported by theory and experiments and is justifies the previous theoretical narrative. 

Some reviewers raised concerns on motivation and numerous clarification questions that the authors have addressed to a large extent in my opinion.
Strength
* The paper is relatively clearly written.
* A new method is proposed.

Weakness
* The evaluation is weak.  The experimental setup is not clear enough.  More quantitative evaluation is necessary. There are strong and new baselines that need to be compared with.
* Relation with existing work needs to be more clearly described.
* The novelty of the work is limited.  It is a combination of existing methods.
* Justification of the proposed method needs to be provided.
* The writing of the paper can be improved.
The paper presents a new computational framework, grounded on Forward Backward SDEs theory, for the log likelihood training of Schrödinger Bridge and provides theoretical connections to score based generative models. The presentation of the results is not satisfactory (the algorithm should be clarified in several places and the notation is not accurate which raises doubts about the soundness of the method). The paper is thus very hard to read for the non experts on the subject. Furthermore, some reviewers raise concerns about the similarity of this method to other algorithms that were never cited in the paper. Finally, the empirical analysis, as of now, is limited.

In the rebuttal the authors carefully addressed lots of the comments. However paper s presentation still needs to be substantially improved (de densification of the paper would be extremely important since now the main narrative is very convoluted). The authors made several changes in the manuscript, but detailed discussion regarding training time complexity still seems to be missing (main body and the Appendix) in the new version of the manuscript, even though this was one of the main raised concerns. Overall, the manuscript requires major rewriting. Since the comments regarding the content were successfully addressed (the reviewers are satisfied with detailed answers given by the authors), the paper satisfies the conference bar and can be accepted.
The manuscript proposes a black box optimization algorithm based on statistical hypothesis testing and proposes to use it to solve control problems posed in stochastic environments. While "Reinforcement Learning" appears in the title, this appears to have nothing to do with RL other than that it purports to solve problems that are typically used to benchmark RL algorithms, i.e. sequential decision making for reward maximization with stochastic dynamics. Baselines compared against also don t involve the reinforcement learning formalism. A key claim appears to be that the application of other gradient free methods to such problems incorrectly ignores stochasticity of the objective function owing to the stochastic sequential dynamics.

Reviewers found the paper both clear and well written, but questioned the novelty of the approach, its situation in the wider world of gradient free black box optimization methods, and the limited scope of the empirical results. Reviewer pTGV criticized apparent "magic numbers" in the algorithm description, to which the authors responded rather dismissively.

In their general reply statement, the authors seem to misinterpret reviewer Je5f s comment that “critical gradient information is hard to obtain”: my own interpretation is that Je5f is characterizing _black box optimization problems_ in general, and noting that the special structure of MDPs admits gradient estimation via the policy gradient theorem, etc. In response to Je5f directly, the authors state "[TD3 & SAC] are not BBO techniques, thus we did not feel as though it would be fair to compare HDCA to problems solved by these methods". ADBA raises some concerns about links to the existing literature, which appear to have been addressed to their satisfaction, though was unwilling to champion the paper for acceptance in light of the concerns of other reviewers.

The AC concurs with the reviewers  assessments regarding the limited nature of the experiments  The paper makes the claim that the newly proposed method is a compelling choice for the solution of stochastic MDPs, but fails to adequately defend this claim, and fails to benchmark against either policy or value based RL methods. When challenged on this, they assert that such comparisons would be unfair; this may be true in a limited sense, but if an RL algorithm can outperform the novel BBO method on all problems considered, it s unclear why this would be of significant interest to the ICLR community. A recurring theme in the discussion is that BBO methods are more scalable than RL, though batched A2C methods (e.g. Espeholt et al, 2018) or distributed Q learning (e.g. Kapturowski et al, 2019) would at least merit mention, and would appear to at least challenge this claim.

Finally, the AC would like to note that work on Evolution Strategies predates the work of Salimans et al, 2017, including for control problems (e.g. Cardamone et al, 2009) and in particular Wierstra et al (2014) propose many sophisticated techniques for more effectively utilizing function evaluations (including methods based on hypothesis testing) which are a natural point of comparison for this work. Similarly, CMA ES is a widely used and well regarded technique for gradient free optimization, and would be a reasonable baseline. In short, the empirical investigation is inadequate even if restricted to gradient free methods, and the fact that these methods may not explicitly account for stochastic dynamics does not obviate the need for comparison.

The method presented is interesting, and I encourage the authors to continue studying it, with a more diverse set of environments, strong baselines and careful ablations.
In this paper, the authors present a method that combines genetic data (using a hierarchical, graph convolution approach) with imaging data to predict schizophrenia. The reviewers raised several concerns that the authors have addressed. Some of the concerns were relevant to writing, the authors have clarified these points. Another important concern was about the baselines. The authors added other baselines. One of the baselines they added was GUIDE with random dropout, this baseline performs as well as GUIDE, but the authors argue that GUIDE leads to robust features. 

I ask the authors to move the other baseline results, specifically the GUIDE with random dropout, and the relevant discussion to the main manuscript, and to consequently temper the discussion of the bayesian feature selection. Currently these additional results are only in the appendix, and not the text. Conditional on this, I recommend acceptance.
This paper deals with the task of long text summarization. Inspired by earlier work on top down and bottom up architectures, this work focuses on improving the traditional bottom up converter encoder structure, and the fine resolution representations. 

Pros:
  Their model can model longer documents in coarse and fine granularity levels.
  The performance on benchmark datasets looks pretty good compared to strong baselines
  Computationally efficient. 

Cons: The reviewers have raised several concerns including:
  the experimental verification for calculation efficiency and memory usage of model is not sufficient.
  the novelty of this design is somehow limited since the bottom up and top down idea is not new. 
  several details about the figures and especially the experiments were missing.

The authors have addressed several of the suggestions, added new experiments results addressing the issues raised by the reviewers. During the rebuttal period, the authors further conducted empirical investigations showing that the top down update for token representations, especially with good top level representations, leads to good summarization because of enriched token level representations by the top down. Despite positive results, some reviewers raised concerns that with only using BART as a backbone, it is surprising to achieve this great performance boost with the top down/bottom up models on long document summarization when they compared to the state of the art transformer models (BigBird, Longformer and T5) that have been shown to encode longer sequences and beat several summarization models.
To address the problem of unauthorized use of data, methods are proposed to make data unlearnable for deep learning models by adding a type of error minimizing noise. Based on th fact that the conferred unlearnability is found fragile to adversarial training, the authors design new methods to generate robust unlearnable examples that are protected from adversarial training. In addition, considering the vulnerability of error minimizing noise in adversarial training, robust error minimizing noise is then introduced to reduce the adversarial training loss.
The authors have tried to respond to reviewers  comments along with adding more experiments.
Overall, this manuscript finally gets three positive reviews and one negative review, where the possible vulnerability or robustness of error minimizing noise against (simple) image processing operations was not verified.
In comparison with other manuscripts I m handling that got consistent positive comments, this manuscript is still recommended to be accepted (poster) with a further study of robustness under simple image transformations in the final version.
The authors propose a semi supervised novelty detection method which tries to identify out of distribution samples in the unlabeled data (consisting of in  and out distribution samples) using a disagreement score of an ensemble. The ensemble is generated by fine tuning the trained classiifer on the labeled training data plus the unlabeled data which all get a fixed label (which is repeated several times to generate the ensemble). The main idea is that one uses early stopping based on an in distribution validation set in order to avoid overfitting on the unlabeled points which allows then identification of the out distribution points via the disagreement score.

The reviewers appreciated the simplicity of the approach and the extensive experimental results. The authors did a good job in trying to answer all questions and concerns of the reviewers. 

However, some concerns remained:
  the setting assumes that the OOD data is fixed which was considered as partially unrealistic and thus evaluation of the OOD detection performance on unseen OOD distributions was requested in order to understand the limitations of the method (this was only partially done by the authors). 
  the theoretical result is for a two layer network and completely based on previous work. As the authors use much deeper networks later on in the experiments, this result cannot be used to theoretically justify the approach. 
  there remained concerns about the necessary diversity of the ensemble and the early stopping procedure

While I think that the paper has its merits, it is not yet ready for publication. I encourage the authors to to take into account the above points and other remaining concerns of the reviewers in a revised version.
This paper proposes a new dataset called ComPhy to evaluate the ability of models to infer physical properties of objects and to reason about their interactions given these physical properties. The paper also presents an oracle model (named oracle because it requires gold property labels at training time) that is modular and carefully hand designed, but shows considerable improvement over a series of baselines. The reviewers for this submission had several concerns including:
(a) [VByS] "concerns are about the complexity that the proposed method can handle"\
(b) [VByS] "the method is only demonstrated on a simple synthetic dataset"\
(c) [8BUA] "I am struggling to see any direct application"\
(d) [8BUA] "choosing 4 videos as reference"   why use ref videos, why use 4\
(e) [8BUA] "Baselines showing results with ground truth object properties should be reported"\
(f) [3cQE] "no innovation in the type or structure of questions asked"\
(g) [3cQE] "neither the CPL framework nor the implementation of any module is novel"\
(h) [DJEq] "The only difference is that this paper infers hidden properties instead of collisions"\
(i) [DJEq] "The dataset is not comprehensive enough"   only 2 properties and simplistic and synthetic videos\

The authors have provided detailed responses to these concerns and I discuss these below.

The authors have addressed (c),(d) and (e) well in their rebuttal.

I don t think (a) is concerning. The proposed model is not expected to solve the dataset entirely inspite of having access to gold properties at training time. As the authors mention, this indicates the complexity of the task at hand.

The authors also address (f) well. I dont think there is any need for innovation in the structure of questions asked. QA is merely a mechanism to probe the model, and using CLVERER style questions seems appropriate.

I disagree with the sentiment behind (g). The proposed oracle model clearly inherits modules from past works and assembles them to suit the needs of the dataset. It is this assembly that differentiates it from past works. This is true of most papers in our field, including ones that are widely acknowledged to be important papers. The underlying modules in proposed networks are rarely novel, but their assembly can lead to improvements on benchmarks. Furthermore, the oracle model, isnt the central contribution of this work. The dataset is, and hence, the requirement for novelty is reduced. The oracle is meant to serve as a guideline to show what one may achieve given gold labels at training, and it serves that purpose well.

Re (h), my takeaway is that inferring properties based on their dynamics and without any link to their appearance is an important step, and past datasets do not exhibit this characteristic. And thus, in spite of being a limited differentiation from CLEVERER, I think this is interesting.

Re: (b) and (i) I do agree with some aspects of these, with the reviewers.
I think its still valuable to have a dataset with synthetic videos, given that models today are unable to solve this dataset. Moving to more realistic videos is a next step.
However, as the reviewer [DJEq] points out, it would be desirable to add more physical properties and add more complex scene elements like ramps. That would have added a lot more diversity to the dataset   visually, with regards to physical properties and with regards to the types of reasoning required.

Having said that, I believe that the dataset in its present form is still valuable to the community, and hence I recommend acceptance.
I think adding more physical properties and scene elements will have made this a much stronger submission.
The paper considers the global convergence and stability of SGD for non convex setting. The main contribution of the work seems to be to remove uniform bounded assumption on the noise, and to relax the global Holder assumption typically made. Their discussions in Appendix A provide an example for which the uniform bounded assumption on the noise commonly assumed in the literature fails.  The authors establish that SGD’s iterates will either globally converge to a stationary point or diverge  and hence tehir result exclude limit cycle or oscillation. Under a more restrictive assumption on the joint behavior of the non convexity and noise model they also show that the objective function cannot diverge, even if the iterates diverge.

The reviewers are on the fence with this paper. While they agree that the paper is interesting, they only give it a score of weak accept (subsequent to rebuttal as well). One of the qualms is that while the authors claim the result helps show success of SGD in more natural non convex problems, they don’t provide realistic examples supporting their claim. Further, while the extension to holder smoothness assumption while is indeed interesting, unless practical significance is shown via examples, the result is not that exciting.

From my point of view and reading, while the reviews are not extensive, i do not disagree with reviewers sentiment. Technically the paper is strong but there is a unanimous lack of strong excitement for the paper amongst reviewers. While there is this lack of more enthusiasm, given the number of strong submissions this year, I am tending towards a reject.
While I understand and have empathy with the authors  viewpoint of their work and novelty, this unfortunately has not reached reviewers  hearts in the way that they intended. There has been no strong support for acceptance, as the questions about the amount of novelty piled up. Some interactions between authors and reviewers happened. It is clear that the authors made an effort to show the differences with respect to other tree related models and algorithms, as well as to highlight the strengths of the approach which was claimed to be too similar and simplistic. I believe the interactions helped with improving the first viewpoint of reviewers, however this improvement has not been enough, as reviewers did not significantly changed their stances. This is a short process and indeed it is not easy to change first impressions. If anything to add is that I hope that the impressions can be used to give a new presentation to the work that will enhance the work and its view by others.
There was consensus that though  the paper introduces an interesting question, but not enough exploration has been made. The reviews point out several mathematical in accuracies, and points out  several issues including that the delta criterion needs to be examined.
The paper worked on fully unsupervised anomaly detection and proposed to use self supervised representation learning to improve the performance of one class classification. This is a borderline case close to acceptance but cannot make it. Specifically, it is useful, but its novelty is the main issue, since it is not surprising that self supervised representation learning can improve one class classification without representation learning (this part is still much of the taste of ICLR) and an ensemble of multiple models can improve upon a single model (which is just "bootstrap aggregating" or "bagging" used everyday in practice and known to machine learning and statistics societies a very long time ago). After seeing the rebuttal, the concerns were not really addressed well and the issues were only partially solved. Thus, the paper is not enough to guarantee an acceptance to ICLR unfortunately.
The paper introduces an idea that was found interesting by all reviewers (including Gxxe who recommends a marginal reject). A majority of the reviewers also point out a few weaknesses of the paper, notably in terms of clarity of several statements that were found to be hand wavy (see the reviews of Gxxe and oSPE for more precise details). The area chair agrees with those statements, but overall, the originality of the idea introduced in this paper outweighs these weaknesses, and the experimental study is conducted in a reasonably convincing manner.

Even though there is room for improvements, the area chair is happy to recommend an accept, but encourages the authors to follow the constructive feedback provided by the reviewers for the camera ready version.
This paper proposes the use of Bayesian prior upon initialization for predicting generalization performance of a neural network, and empirically shows that it can outperform flatness based measures. Understanding the underlying reasons that control generalization performance on neural networks is of great theoretical and practical importance, and reviewers find efforts in this direction valuable. However, they believe the submission in current state is not ready for publication.

Specifically, ZCFx believes the setup considered in the paper does not resemble a realistic situation, which makes claims about the Bayesian prior being a more robust predictor than flatness unsubstantiated. ZCFx appreicates authors  response and clarifications, but finds the concerns unresolved. ohft believes the paper is weak in a certain aspects, such as comparing across different architectures (including number of parameters), and comparing with SAM optimizer whose goal is to find flat minima and has shown to greatly improve the generalization performance. ohft acknowledged reading authors  response but the response did not help with changing the score of the paper. r1hF has some reservations about the novelty of the work and the limited experiments, which remained unresolved. r1hF suggests that the authors revise the paper to emphasize on the author s contribution in light of the previous work.

Based on reviewers  feedback, I suggest authors to resubmit after revising the draft to address the issues raised above.
This paper studies the general problem of out of distribution (OOD) detection, where the goal is to detect outliers (i.e., points not in the distribution of training data) in the sample. The paper introduces a methodology for measuring robustness by using adversarial search/distributions. Experimental evaluation indicates that traditional metrics fail to fully capture OOD detection. The reviewers  evaluations of this work were mixed. Overall, there was consensus about the importance of the problem. Moreover, some of the reviewers argued that the submission contains some interesting new ideas. On the other hand, concerns were raised regarding lacking comparison to prior work, potential overselling of the contributions, and several aspects of the experimental evaluation. At the end, there was not sufficient support for acceptance. In its current form, the work appears to be slightly below the acceptance threshold.
Based on the observation that the eigenvectors with smaller eigenvalues are more non robust (i.e., adversary adds more components along such directions), the authors propose a method called Feature Spectral Regularization (FSR) to penalize the largest eigenvalue, and as a result, the other smaller eigenvalues get increased relatively.
In this paper, in addition to FSR, theoretical analysis along with experimental results on different datasets and models were presented.
Although the proposed FSR has some merits, the major concerns from the reviewers include (1) impractical use on large scale datasets and (2) lack of significant improvement over SOTA.
Compared with other submissions I m handling, I have to reject this manuscript.
The reviewers agree that the paper is addressing an interesting problem, and provides a valuable contribution for the learning of quasimetrics and would be useful for many real world applications.
The paper proposes an edge independent graph generative model that can capture heterophily. The authors propose a 3 stage process to obtain the node representations. The idea of factorization in the form of BB^T CC^T is an interesting approach to model heterophily.

The paper can be improved in terms of writing to better motivate the need for a 3 stage algorithm and how these individual steps are related to the existing techniques in the literature. The authors should elaborate on the implications of the theorems and the concerns raised by the reviewers in the body of the paper.

The algorithm faces scalability challenges, which are not studied well in the experiments. The reviewers also have raised concerns about degeneracy in network reconstruction experiments. Overall, the paper needs further improvements for publication.
The reviewers unanimously recommend rejecting this submission and I concur with this recommendation. The submission essentially introduces a regularization technique to solve the alleged problem of Adam getting worse out of sample error for typical image classification problems, e.g. training ResNets on ImageNet. Reviewers raised a variety of issues with the submission. Some found the experiments unconvincing, some were concerned that the submission duplicated closely related work without engaging with and citing that work, and some were concerned by what they viewed as insufficient analysis and comparisons. To me, the most severe issue with the submission is that the experimental evidence for its claims is not sufficiently convincing and the problem it purports to solve has not been convincingly demonstrated, making the work hard to motivate. The other issues raised by the reviewers are less damaging in my view.

Although this is a meta review and not a full de novo review, I would be remiss to not raise a few of the severe issues I see with the results that makes it hard for them to be convincing.
The Adam results in table 1 are far weaker than they should be, raising questions about the experiments as a whole. For example, https://arxiv.org/abs/2102.06356 reports 76.4% top 1 accuracy for ResNet 50 on ImageNet with Adam without increasing the epsilon parameter to a larger value as Choi et al. 2019 did (who also report good Adam results for ResNet 50 on ImageNet). This should also lead us to question one premise of the paper that there is some problem with adaptive optimizers for image classification.

Ok, but perhaps LAWN helps validation error even if there is no gap between SGD and Adam? Sadly, to demonstrate this subordinate claim, LAWN would have to be compared carefully with state of the art regularization techniques and compared with results that use any optimizer, not just Adam. With modern regularization techniques, it isn t hard to get 77%+ top 1 validation accuracy on ImageNet with ResNet 50. See, for example https://arxiv.org/abs/2010.01412v1 which gets 77.5% in 100 epochs and as high as 79.1 with longer training. Since LAWN is claiming to improve generalization, it must be compared with other regularization techniques. It is a type error to primarily compare it with optimizers so even if there weren t concerns with the performance of the existing baselines, there would need to be additional comparisons.

The claims about fixing issues that arise at large batch sizes are prima facie problematic since there isn t strong evidence of an actual problem at the batch sizes considered in the submission.
The paper presents a domain adaptation approach based on the importance weighting for unsupervised cross lingual learning. The paper first analyzes factors that affect cross lingual transfer and finds that the cross lingual transfer performance is strongly correlated with feature representation alignments as well as the distributional shift in class priors between the source and the target. Then the paper designs an approach based on the observations.

Pros: 
+ The paper is well written and the proposed approach is well motivated. 
+ The analysis about which factors affect cross lingual transfer is interesting and provides some great insight. 

Cons: 
  As the reviewer pointed out, the experiments for verifying the proposed approach are relatively weak.

Overall, the paper presents nice insights to connect cross lingual transfer with domain adaptation. All reviewers lean to accept the paper and I also found the paper is in general interesting.
I do not recommend accepting this paper, although I make this decision with reservations. The review quality for this paper was not particularly strong, and I wish to emphasize to the authors that I read the paper myself in detail in the process of writing this metareview.

This paper proposes a new structured pruning technique called LEAN. It involves computing an operator norm of the convolutions in a convolutional neural network, multiplying these norms over paths through the network, and keeping the paths with the highest such values (and pruning everything else). This paper makes the argument that this metric is robust to scaling and prevents network discontinuities. (In this way, the technique is very reminiscent of SynFlow (Tanaka et al., Pruning Neural Networks without any data by iteratively conserving synaptic flow) in terms of motivation and resulting technique, although SynFlow is unstructured. I do not mean this as a criticism   just a suggestion for the authors of a connection they might be able to make in the future.)

One big concern I have about this paper based on the methodology alone is as follows: the paper states a number of hypotheses about why this is a sensible way to prune (e.g., in the beginning of Section 4). I see no reason why any of these hypotheses are wrong, but the paper never makes an effort to evaluate any of them. I don t mean a theoretical justification here   that s difficult and unlikely to yield useful information about what happens in practice. I mean experiments to ablate the salient properties of the heuristic mentioned in the paragraph at the beginning of Section 4 (Does scaling invariance actually matter in practice? Is network disconnectivity actually a risk in practice?).

My biggest concerns about the paper, however, are in the evaluation. I share two major reviewer concerns that were mentioned:
(1) The paper compares to a very limited set of baseline pruning methods, and relatively older ones at that (2019 is indeed old in the world of pruning).
(2) The paper does not look at standard, "large scale" benchmarks for computer vision   namely, ResNet 50 on ImageNet.

Neither of these concerns is necessarily decisive in my view. For example, with respect to Concern 1, the reviewers unhelpfully do not suggest very many additional structured pruning benchmarks to consider, and I think the additional baselines added during the revision process have softened this concern. I would also recommend taking a look at "Growing Efficient Deep Networks by Structured Sparsification" (Yuan et al) for a useful method and a good set of baselines. There are an arbitrary number of baselines one could add and the structured pruning space is a confusing mess, but I think the claims in this paper merit more than are currently present.

With respect to Concern 2, I m even more conflicted. On the one hand, I have rarely seen any pruning techniques proposed for or evaluated on vision tasks beyond image classification, despite the fact that   in the real world   segmentation is much more popular than it would seem by reading the ICLR proceedings. To that end, I applaud the authors for focusing on those settings and I see substantial value in a paper that does so. On the other hand, ResNet 50 on ImageNet (among other standard classification benchmarks) is the de facto measuring stick for evaluating pruning methods in computer vision, and the exclusive focus on segmentation here means it is very difficult to compare the proposed technique to other benchmarks. If the paper is to focus on segmentation alone, this places a higher burden on adding many additional comparisons to other methods (i.e., Concern 1). Finally, I don t see any reason why the paper *couldn t* also include ResNet 50 on ImageNet or the like in addition to segmentation; if it is a limitation on the compute available to the authors (something I empathize with), they did not say so in any of the author responses. Upon reading the author responses, I was left asking, "Why not both?"

For those reasons, I do not recommend accepting the paper, although I think there are some good reasons to value the paper s contributions. At the end of the day, there are some relatively simple things that could be changed to make the paper much easier to contextualize within the pruning literature. As of right now, it would be very difficult for me to say whether or in what contexts this method should actually be used in practice.

(P.S. I agree with the reviewers that Figure 3 is exceptionally hard to parse.)
This work shows that the source filter model of speech production naturally arises in the latent space of a variational autoencoder (VAE). It is interesting that the fundamental frequency and formant frequencies are encoded in orthogonal subspaces of the VAE latent space   this opens up a possible way of easily controlling these.

The key motivation/goal of the paper has caused some confusion. The abstract highlights an observation about VAE’s learned representation. In retrospection, some reviewers have not found the findings very surprising. On the other hand, the authors also do not attempt at developing and evaluating a speech generation method. As is, the paper seems to be much more suitable to a specialized workshop on speech. Alternatively, the paper could be extended to other modalities to show steerability of a representation using a synthetic dataset. However, the current scope seems to be somewhat limited hence I am not able to recommend the current manuscript for acceptance.
The paper proposes a gradient based method for OOD detection. While the paper has some interesting contributions, all the reviewers felt that the current version falls below the ICLR acceptance threshold. I encourage the authors to revise and resubmit to a different venue.
This paper looks at a formulation of online multi objective optimization problem.

All reviewers agree on the score, 6, which is quite rare but is not really informative; none of them are very excited about the paper, but they all find it interesting.

I have read it as well myself. The paper is rather clear and well written. I have three majors concerns.
1) I am not fully convinced by the objective R_{MOD} as it reduces to the dynamic regret in the single objective problem, as the later cannot be minimized unless we make strong stationarity assumption. This is obviously the case here (see Assumption 2). Then the choice of parameters would depend on some "stationarity" quantity (V_T). I am not really enthusiastic about this either.
2) The analysis is rather classical once the problem is reduced to a single objective, so the analysis is not really breathtaking. Yet I admit that I quite enjoyed reading about this reduction, the idea is quite neat. 
3) Multi objective online optimization has already been considered in online learning, but the related works did not really mention it. For instance, Blackwell approachability is such an example [1,2,3] (yet I am not sure that it can cover the Pareto front idea). It would be interesting to see how those approach compares (notably, the online mirror descent has been widely studied in that case).

All in all, I do understand the reviewers, and this paper is certainly borderline, but I do not think it reaches the acceptance bar yet. As a consequence, I would rather recommend rejection this year. 

[1] J. Abernethy, P. Bartlett, D. Hazan. Proceedings of the 24th Annual Conference on Learning Theory, PMLR 19:27 46, 2011.
[2] V. Perchet. Approachability, regret and calibration: Implications and equivalences, Journal of Dynamics & Games,181 254, 2014.
[3] A. Rakhlin, K. Sridharan, and A. Tewari. Online learning: Beyond regret. Proceedings of the 24th Annual Conference on Learning Theory, PMLR, 19:559–594, 2011.
The authors study separable convolutions in the group convolutional setting, and describe experiments showing them to be more computationally efficient without loss of performance in the setting of some group augmented MNISTs, and show some promising results on un augmented CIFAR10, CIFAR100, and Galaxy10.  The reviewers are mixed; some of the reviewers have concerns about the completeness of the experiments and the novelty of the work, and in particular to what extent the experiments support the specific novelties claimed.  The authors have made some updates to address this in the revision, but my opinion is that the authors should resubmit to the next venue after further experiments and exposition to clarify.
This work adds the positional encoding (akin to those in transformers, but adapted) to GNNs.
In their reviews, reviewers raised a number of concerns about this work, in particular, lack of novelty, lack of ablations to demonstrate the claims of the paper, lack of comparison to previous work (e.g., position aware GNNS, Graphormer and GraphiT which would appear very related to this work), lack of motivation (e.g., the introduced positional loss do not actually improve performance), and whether the experimental results were really significant.
During the rebuttal, the authors replied to the reviews, to address. the concerns that they could. Of the reviewers, unfortunately only one reviewer elected to respond to the authors. It is disappointing that the four other reviewers did not respond and overall the reviewers did not discuss this paper further.

The authors chose to highlight privately to the AC that two reviewers who scored the paper unfavourably did not respond. The authors then argued this should be taken into account in the score (presumably to make acceptance more likely) however, two favourable reviewers also did not respond (not highlighted by the authors). I understand this kind of private request to the AC to dismiss unfavourable reviews (especially if they do not respond) is becoming common I find it unhelpful I can see who and who has not responded.

Nonetheless, looking at the responses to the original concerns of the reviewers highlighted above, I believe the authors have adequately addressed the concerns of the reviewers. Therefore i recommend acceptance but only as a poster.
Given the increasing scale of large models (e.g. CLIP), there s an argument that we need better automated techniques for properly utilizing (prompting) these models. Given the success of prompt learning within pure NLP models, the authors apply the same approach to the V+L domain and show that it also is applicable here.  Generally, reviewers felt that the results were clear and thorough, yet technically limited.  The approach is not novel and the result not surprising.  There is a documentary benefit to having this work out in the community for others to reference and extend.
This paper presents an approach for online continual learning where only a single pass over each task s data is allowed. Instead of the oft used softmax classification setting in continual learning, the paper proposes to use the generative setting based on the nearest class mean (NCM). The paper claims that it avoids the logits bias problem in the softmax classifier and helps combat catastrophic forgetting.

While the reviewers found the basic idea interesting, there were concerns about novelty and lack of clarity regarding the reasons for improved performance. In particular, there are several aspects from existing work that are leveraged in this paper (e.g, replay, metric learning loss, combination of generative and discriminative classification, etc) but the paper lacks in establishing which of these components affect the performance and in what ways.

The authors and reviewers engaged in detailed discussions; however, the reviewers were still unsatisfied and did not change their assessment. Based on my own reading of the paper as well as going through the reviews and discussions, I too concur with their assessment. It would be a stronger paper if the paper could shed more light on the above aspects as well as address the other concerns raised by the reviewers. However, in the current shape, it is not ready for publication.
This paper proposes two techniques for improving self supervised learning with a vision transformer. The first improvement is using a multi stage ViT, which is very similar to Swin transformer and authors recognized this is not a major contribution. The authors further found that using a multi stage ViT does not produce discriminative patch representation, thus proposing the second improvement with a region level loss. While both improvements are not particularly novel by themselves, combining both leads to a strong empirical result. However, It does looks like the multi scale vision transformer is the major improvement as removing the regional loss only leads to less than 1% decrease in performance in most cases. In general this is a good "engineering" paper with a practical approach for improving self supervised learning with vision transformation and obtained strong results, thus it s worthy of publication.
The paper studies gradient descent for matrix factorization with a learning rate that is large relative to the a certain notion of the scale of the problem. In particular, they show that the use of large learning rates leads to balancing between the two factors in the factorization.

The discussion between the authors and the reviewers was fruitful in dispelling some of the reviewers  doubts and at the same time improving the paper.

The paper seems to make some contribution on a relevant problem for the ICLR community. However, even in the restricted settings they consider, the problem does not appear to be completely solved. That said, I agree with the majority of the reviewers that the step forward seems enough to warrant the acceptance.

I would still encourage the authors to take into account the reviewers  comments in preparing the camera ready version. In particular, in the internal discussion it was suggested that the presentation of the paper could be improved by clearly stating the limitations of the current approach (e.g., the assumption of convergence in Theorem 5.1, a better discussion on large vs small learning rates w.r.t. the balancing effect).
Based on the contrastive learning loss wildly used in the NLP and computer vision domains, this paper presents Self GenomeNet, a contrastive learning method for representation learning of genomic sequences.
As shown in the experiment section, the improvement compared to baselines CPC, Language model, and even supervised learning method is considerable, on three benchmark datasets in both self supervised and semi supervised evaluation.

Even after the discussion phase, there exists disagreement among the reviewers.
AC considered all reviews, author responses, and the discussions, as well as read the paper.
While the paper has some merit such as an effective Self GenomeNet model for the particular problem setup, reviewers still have several reservations to directly accepting it:
+ Questionable impact. The proposed framework is overall a simple combination of existing methods and beyond genome datasets, the impact of this proposed method is questionable.
+ Limited inspiration. The proposed method is mainly constructed on the previously proposed contrastive learning loss wildly used in the NLP and computer vision domains, the benefits of the proposed method may be limited on the genome data (especially the domain specific data augmentation e.g., reverse complement). How can the insights foster future research?
+ Lack of justification. The innovations introduced by the paper seem ad hoc, and the reasons for the large observed improvement are not entirely intuitive.
Meanwhile, even with the provided response from the authors, the connection between motivation and the proposed method is still not crystal clear.

Given the above reservations, AC could not accept the paper for now but encourage the authors to fully revise the paper and strengthen their work.
To perform self supervised graph representation learning that is scalable to large graphs, the authors propose Bootstrapped Graph Latents (BGRL) that learns its graph representation by predicting alternative augmentations of the input, avoiding the need to construct negative examples. The weakness of the paper lies in its novelty, as it can be considered as a direct adaptation of the BYOL method, whose success has been demonstrated on self supervised visual representation learning, to learn graph node representations. While the novelty is limited, the paper has shown how to appropriately apply BYOL to graph representation learning, achieving state of the art results on graph node representation learning on large scale graphs. The overall assessment of the reviewers is that the empirical significance of the paper outweighs its shortcoming in novelty. The AC agrees with this assessment and hence recommends acceptance.
The authors propose WARM, a novel method that actively queries a small set of true labels to improve the label function in weak supervision. In particular, the authors propose a methodology that converts the label function to "soft" versions that are differentiable, which are in term learnable with true labels using proper updates of parameters. Empirical results on several real world data sets demonstrate that the method yields a pretty strong performance.

The reviewers generally agree that the idea of making the labeling functions differentiable is conceptually interesting. They are also positive about the simplicity and the promising performance. They share joint concerns on whether the idea has been sufficiently studied in terms of the design choices and completeness of the experiments. For instance, the authors can conduct deeper exploration of the trade off for differentiable LFs. They can also study active learning strategies that are beyond basic uncertainty sampling. While the authors have provided more studies about those exploration and ablation studies during the rebuttal, generally the results are not sufficient to convince most of the reviewers. In future revisions, the authors are encouraged to clarify its position with respect to existing works that combine active learning and weakly supervised learning.

The authors position the paper as more empirical than theoretical. So the suggestion from some reviewers about more theoretical study is viewed as nice to have but not a must.
The paper considers the setting of bi level optimization and proposes a quasi Newton scheme to reduce the cost of Jacobian inversion, which is the main bottleneck of bi level optimization methods. The paper proves that the proposed scheme correctly estimates the true implicit gradient. The theoretical results are supported by numerical experiments, which are encouraging and show that the proposed method is either competitive with or outperforms the Jacobian Free method recently proposed in the literature.

Even though the reviews expressed some initial concerns regarding the empirical performance of the proposed method, the authors adequately addressed those concerns and provided additional experiments. Thus, a consensus was reached that the paper should be accepted.
This paper introduces a soft gradient based subword tokenization module (GBST) that learns latent subword representations from characters. GBST enumerates candidate subword blocks and learns to score them in a position wise fashion using a block scoring network. The resulting model was tested on GLUE, and several cross lingual tasks. The performance is competitive with ByteT5 and often similar to subword models while being more efficient in FLOPs and RAM.

Reviewers are mixed on this. The negative reviewer points to how this not being a real tokenizer and does not produce a tokenization, that experiments that use the base model do not address bigger scales, and that there is a lack of code which is important for this kind of work, and the resulting accuracy gains are not significant and the method being not interpretable. The positive reviewers like the extensive experiments, the efficiency improvements and flexibility / simplicity of the GBST module. The authors seemed to have addressed most of the reviewer issues by providing larger scale experiments and code. I believe the results are fairly strong, since one would not expect a big performance difference in a learned tokenization method, but rather efficiency or flexibility gains. The paper is generally well written though details about the convolution should be included in the text (and not just the code).

Recommending accept.
The experimental part of the work has been reported by all reviewers as too limited and not convincing enough.
At this point this work cannot be endorsed for publication at ICLR.
This paper proposes a method for 4 bit quantized training of NNs (forward and backward), obtaining SOTA 4 bit training quantization, motivated by an analysis of rounding schemes (an important aspect) in quantized training. The main concerns from the reviewers were that the approach was not practical (both a general concern, and of specific note here since the word is used in the title and motivation of the work), due to lack of compatibility with (current) general purpose hardware, and lack of suitability of the approach for specialized hardware, so it is unclear what the actual use case is for the approach. The authors argued that (1) this is not a problem on some hardware and (2) that past works have not been held to this standard. I did not find the authors to provide a strong argument during the discussion period to address these concerns.
In this paper, the authors propose a new type of (missing not at random) model they call the MCM (mixed confounded missingness).
The authors further discuss that given their model, naive imputation strategies do not work, and a model tailored imputation strategy is needed.

The reviewers did not receive the paper favorably, with main complaints centering around: (a) outlining novelty compared to existing approaches to missing data, (b) whether imputation is a good strategy for dealing with missing data, and (c) whether the paper s results are actually sound.

Here s my perspective on these worries.

The paper aims to deal with missing data in a causal inference context (in other words, the target of inference is a causal effect, and our data happens to have entries missing not at random).  Further, the paper aims to work within a graphical modeling formalism for missing data models.  Finally, the paper points out that imputation is to be done with care if data is missing not at random (a point both myself, and reviewers agreed with).

Areas of improvement in the paper, in my mind, would be: (i) better literature review and putting authors  work in context of prior work, (ii) being clear about identification, and (iii) discussion of estimation strategies (not just imputation).

Dealing with missing data (in particular right censoring, but also more general types of missingness) in causal inference is a very old problem, with an established literature in statistics and public health.  In fact, methods for dealing with both causal inference and missing data together are a part of standard graduate curriculum in epidemiology and biostatistics in many Universities.

(i) Literature review and context.  Some papers the authors may find helpful to review:

James M. Robins, Andrea Rotnitzky, Daniel O. Scharfstein.  Sensitivity Analysis for Selection bias and unmeasured Confounding in missing Data and Causal inference models.  Part of the The IMA Volumes in Mathematics and its Applications book series (IMA, volume 116).

This paper discusses lots of relevant things, but in particular sensitivity analysis methods to violations of MAR in settings the authors worry about.

James M. Robins. Non response models for the analysis of non monotone non ignorable missing data. Statistics in Medicine, 16:21–37, 1997.

This paper is an early example of an MNAR model that may be represented by a directed acyclic graph.

Karthika Mohan, Judea Pearl, and Jin Tian. Graphical models for inference with missing data. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 1277–1285. Curran Associates, Inc., 2013.

Ilya Shpitser, Karthika Mohan, Judea Pearl.  Missing data as a causal and probabilistic problem.  In Proceedings of the Thirty First Conference on Uncertainty in Artificial Intelligence (UAI 15), pp. 802 811, AUAI Press, 2015.

Rohit Bhattacharya, Razieh Nabi and Ilya Shpitser. “Full Law Identification In Graphical Models Of Missing Data: Completeness Results.” In Proceedings of the Thirty Seventh International Conference on Machine Learning (ICML 20), pp. 7153 7163, 2020.

These papers deal with general models of missing data using graphs.

Since the authors use graphical models as well, I urge them to put their contribution in context with this prior work.

(ii) Identification.  The authors should clearly discuss whether treatment effects are identified under their model, and if so, by what function.  If this function is not closed form (which can happen in missing data), this should be discussed as well.  This should be contrasted with other missing data work that derives identification under MNAR, particularly using graphs.

(iii) Estimation.  The authors chose to use imputation.  Imputation is a sampling approach to inference in missing data.  Others include maximum likelihood or Bayesian methods (via EM), or semi parametric inference via influence functions.  If the authors chose to concentrate on imputation, specifically, they should explain why (as other methods have noted advantaged, e.g. statistical efficiency, quantification of uncertainty, etc.).

Cautioning against naive imputation is a fine thing to do, but everyone working on missing data problems already knows naive imputation does not work for MNAR data.  Please do not oversell your contributions.  Saying things like: "MCM being the first formalisation of a missingness mechanism when there are treatments at play." is neither true, nor helpful for the peer review process.

With all that said, the MCM model has the potential to be an interesting MNAR model, and placed in proper context of existing work, could be a very interesting addition to the missing data literature.  However, the draft needs a bit more work before it is ready for publication.
This paper studies online MAP inference and learning for nonsymmetric determinantal point processes (NDPPs). The main contribution is an online greedy algorithm. Surprisingly they show that their algorithm outperforms various offline algorithms on real world datasets. That said, the main concern was the novelty with respect to the prior work of Bhaskara et al. who gave an online approximation algorithm for MAP inference in DPPs. To compare the two works: (1) Bhaskara et al. give an algorithm for DPPs, and NDPPs are more complex (2) Bhaskara et al. give provable guarantees on the approximation ratio, but no such guarantees are known for NDPPs (3) And finally, some of the key ingredients in the online algorithm for NDPPs, like the stash, were already in the work of Bhaskara et al. Overall the reviewers felt that this submission would be improved with a clearer discussion of the contributions over prior work.
This paper tackles the problem of exploration in Deep RL in settings with a large action space. To this end, the authors introduce an intrinsic reward inspired by the exploration bonus of LinUCB. This novel exploration method called anti concentrated confidence bounds (ACB) provably approximates the elliptical exploration bonus of LinUCB by using an ensemble of least squares regressors. This allows ACB to bypass costly covariance matrix inversion, which can be problematic for high dimensional problems (hence allowing it to be used in large state spaces). Empirical experiments show that ACB enjoys near optimal performance in linear stochastic bandits. However, experiments on Atari benchmark fail to show any practical advantage of ACB over current methods, neither computation nor performance wise. That being said, the proposed ACB approach is theoretically transparent, which contributes to advancing our theoretical understanding of usable intrinsic rewards in deep RL and can inform theoretically motivated directions for improvement and further research, while being on par with SOTA. I believe that this makes the contribution of this work strong enough for acceptance.
The paper shows how to make use of a linear program for extracting logical rules for knowledge graph completion. Overall, the reviewers and I agree that this is an interesting and important direction for research. Moreover, the presented approach shows good performance with rather small sets of rules extracted. However, all reviewers point out that the related work is not well discussed. While the authors have improved the related work sections during the rolling discussion, overall the positioning of the new method has still to be improved, including a better empirical comparison across different datasets. Overall, we would like to encourage the authors to polish their line of research based on the feedback from the reviews.
The paper provides a high probability analysis for Adagrad for smooth non convex optimization and shows its rate of convergence to critical points. Both rates for deterministic optimization and for stochastic optimization are provided. The main contribution of the paper is that unlike for SGD they don’t require knowledge of smoothness parameter in advance and second, they prove high probability results. 

The reviewers lean positively towards the paper. One of the reviewers comments about the comparison with SGD which has some merit. The main comparison of this paper is w.r.t. ward et al 2019 and Zhou et al 2018 both of which prove high probability results. However, both these works require prior knowledge of smoothness parameter. The other axis of comparison is w.r.t algorithms like spider by Fang et al 2018 which uses variance reduction type techniques to obtain the optimal rate for critical point (here it is 1/sqrt{T} for norm square which is T^{ 1/4} for norm and spider is T^{ 1/3} for norm). Of course, an argument can be made for the fact that the algorithm here is closer to what is used in practice and more importantly, the assumptions there are somewhat stronger. 

In any case, the paper still has interesting results and I am leaning towards an accept.
This paper gives sample complexity lower bounds for differentially private empirical risk minimization (ERM). While the reviewers agreed that the results are non trivial, the general consensus was that the proofs are tweaks of previously developed techniques and that the main result is actually new in a rather narrow setting (specifically, for unconstrained ERM and sub constant error parameter). Another concern was that one of the proofs (the one on pure differential privacy) was incorrect in the submission; a different proof was provided subsequently (which also closely follows prior work). Finally, the reviewers pointed out several issues with the clarity of the presentation and comparison to prior work. Given the above, this work is below the acceptance threshold.
A deep Bayesian generative model is presented for multi omics
integration, using fused Gromov Wasserstein regularization between
latent representations of the data views. The method removes several
non trivial and practically important restrictions from an earlier
method BayRel, enabling application in new setups, while still
performing well.

Reviewers discussed the paper with the authors, resolving
misunderstandings of the differences from earlier work
(esp. BayReL). The authors reported more extensive experiments in the
rebuttal, though not comparisons. The main remaining weakness is that
the contributions are in a very narrow field, or at least aplications
have only been demonstrated in the narrow field of multi omics data
analysis. And even within that field, only in a narrow subfield. In a
machine learning venue that is restrictive. Another issue is
computational efficiency. The final decision then depends on how much
weight we place on the novel contributions vs these weaknesses.
This paper examines the extent to which a large language model (LM) can generalize to unseen tasks via "instruction tuning", a process that fine tunes the LM on a large number of tasks with natural language instructions.  At test time, the model is evaluated zero shot on held out tasks.  The empirical results are good, and the 137B FLAN model generally out performs the 175B untuned GPT 3 model.

All reviewers voted to accept with uniformly high scores, despite two commenting on the relative lack of novelty.  The discussion period focused on questions raised by two reviewers regarding the usefulness of fine tuning with instructions vs. multi task fine tuning without instructions.  The authors responded with an ablation study demonstrating that providing instructions at during tuning led to large gains.

Overall the paper s approach and detailed experiments will be useful for other researchers working in this fast moving area in NLP.
This work presents a novel method h to learn object dynamics from unlabelled videos and shows its benefits on causal reasoning and future frame prediction.  This paper received 4 positive reviews and 1 negative review. In the rebuttal, the authors have addressed most of the concerns. AC feels this work is very interesting and deserves to be published on ICLR 2022. The reviewers did raise some valuable concerns that should be addressed in the final camera ready version of the paper. The authors are encouraged to make other necessary changes.
This paper proposes use of a novel generative modelling approach, over both sequences and structure of proteins, to co design the CDR region of antibodies so achieve good binding/neutralization. The reviewers are in agreement that the problem is one of importance, and that the technical and empirical contributions are strong. There are concerns over the relevance of evaluating the method by using a predictive model as ground truth. Still, the overall contributions remain.
The reviewers agree that the problem tackled is important but raise several substantial issues that justify not to accept the paper in its current form. I would encourage the authors to clarify further the crypto part of the paper (dHiP, 2., 4.) and work on how to relax or improve the model assumptions (NGrb). Also, the author s reply to chCc, point 2. becomes more disputable as federated learning is further developed. The argument can be refined.

On a personal note, the statement of Theorem 3.3 could be made clearer, in particular in simplifying (while weakening a bit) the probability bound.

AC.
*Summary:* Study gradient flow dynamics of empirical and population square risk in kernel learning. 

*Strengths:* 
  Empirical results studying several cases in MSE curves.
  Explaining / solving certain phenomena in DL using kernels. 

*Weaknesses:* 
  More motivations would be appreciated. 
  Technical innovation not so high. 

*Discussion:* 

Ud7D found that the main strength of this paper is the take home message rather than innovations. They concluded 7 might be appropriate for the evaluation. This opinion was seconded by WyHh who considered 7 the most appropriate rating. 5uQz also found that 7 would be the most appropriate rating. qXRH maintained concerns about the novelty of the work and rating 5. Nonetheless, they agreed the study is valuable and would not oppose acceptance. 

*Conclusion:* 

Three reviewers found this paper is definitely above the acceptance threshold (suggesting rating 7) and one more reviewer found it marginally below the acceptance threshold however not opposing acceptance. I found the general impressions from the discussion well described in a comment from Ud7D, who indicates that although this is not a breakthrough paper, it is a nice paper showing that a lot of DL phenomena are can be explained by Kernels. I conclude that the paper makes a sufficiently valuable contribution and hence I am recommending accept. I suggest the authors take the reviewers’ comments carefully into account when preparing the final version of the manuscript.
This paper analyzes the effects of the weight decay hyperparameter, and based on this analysis, proposes methods to schedule the weight decay. Overall, while I m glad that more work is being done on understanding the effects of weight decay, I don t think this submission is of sufficient quality for ICLR.

Theorem 1 is simply re expressing the well known fact that if the regularization version of weight decay is used, then (simply because it s based on a single objective function) the stationary points are invariant to the choice of learning rate. This may not be apparent due to the misused terminology: "invariant" is referred to as "stable", but "stable stationary point" has a technical meaning very different from the one used here.

Corollary 2 essentially shows that the optimum of the regularized loss is different from the optimum of the unregularized loss. The authors conclude from this that the optimal value of lambda is 0 from the perspective of test error, which is unwarranted.

Overall, the paper centers around the interaction between learning rates and the weight decay parameter. However, as various reviewers point out, this interaction has been analyzed in detail for networks with normalization layers, and normalization completely changes the nature of the interaction. So any analysis would either need to take this into account or limit the scope to networks without normalization.

I encourage the authors to take the reviewers  feedback into account and improve the paper for the next submission cycle.
The paper proposes a framework for distilling deep directed graphical models where the teacher and student models have the same number of latent variables z. The key idea is to reparameterize both models in terms of standardized random variables epsilon with fixed distributions and train the student to match the conditional distributions of the observed variables/targets given the values of the standardized RVs epsilon. The approach aims to avoid error compounding that affects the local distillation approach, where the student is trained to match conditional distributions of the teacher model (without the above reparameterization). To deal with discrete latent variables and vanishing gradients the authors augment the target matching loss with the latent distillation loss that matches the local distribution for each z_i given the standardized variables epsilon it depends on.

Positives
 The paper tackles an important problem.
 The idea of using reparameterization for distillation in this way makes a lot of sense for continuous latent variables and could be impactful.
 The experiments provide some evidence in support of the idea.

Negatives
 There are considerable issues with the clarity of writing: for example, it is really not clear how (and why) the method is supposed to work for discrete latent variables. The explanation provided by the authors in their response to the reviewers was helpful but still not clear enough.
 The fact that the teacher and student models need to have the same number of latent variables (and perhaps even the same structure) is a big limitation of the method given the claim of its generality, and thus needs to be clearly acknowledged and discussed. For example, the method cannot be used to train a student model with fewer latent variables than the teacher, which seems like a very common use case.
 The experimental evaluation is extensive but insufficient, in large part due to the evaluation metrics. Given that VAEs are trained by maximizing the ELBO (and distilled by minimizing a sum of KLs), it makes sense to also evaluate them based on the ELBO rather than solely on the FID, is done in the paper. The VRNN experiment would be much more informative if it included a quantitative evaluation (e.g. based on ELBO).

In summary, the paper has considerable potential but needs to be substantially improved before being published.
This paper presents some insightful suggestions for researchers studying generalization in federated learning by separating two types of performance gaps between training and test performance, the participation gap (due to partial client participation) and the performance gap (due to data heterogeneity). They suggest that federated learning researchers use a three way split between participating clients  training data, participants clients  validation data, and non participating clients  data to measure the generalization performance of an FL model. The paper presents thorough experiments to support their conclusions. A common concern about the paper is that the authors  suggestions, although relevant and reasonable, are somewhat unsurprising and have been noted in different forms in other works in federated learning. Another concern is that the conclusions are purely based on experiments and are not supported by theoretical justification. Despite these concerns, the reviewers commended the overall insights presented in the paper. 

There was a healthy post rebuttal discussion and some reviewers reevaluated the paper and raised their initial scores. Therefore, I recommend acceptance of the paper. I encourage the authors to take the reviewer s constructive suggestions into account when preparing the final version of the paper.
This paper introduces a method to determine which precision to use for the weights, as well as a quantisation method using hysteresis to improve performance with low precision weights, including 4 bits.
Reviewers tend to agree that the two points presented are useful and can have a large impact on the field.
Generally, reviewers pointed out that motivations, notations and experimental studies could be improved. This has been partly addressed by the authors.
I recommend to accept this paper for ICLR 2022.
The work presents a theoretical analysis of data augmentation, presenting evidence that data augmentation enlarges the smaller the singular values of the network Jacobian. Based on this theory the authors present a method for selecting a subset of training data to use with augmentation that decently approximates performance of training w/ augmentation on the full dataset. Reviewers overall agreed that the theoretical analysis was interesting, and did not find any flaws (though it is worth noting that the theory is restricted to additive perturbations). However, multiple reviewers found the presented experiments unconvincing, and questioned the stated motivation. The AC agrees with reviewers that most simple augmentations are not prohibitive in training speed. Certainly training on less data with a fixed epoch budget would require less compute time, but this is has nothing to do with augmentation and instead is a result of fewer steps taken in training. In the rebuttal, the authors argued that training on Imagenet is prohibitive with a single GPU (taking 2 weeks to do full training). However, given the authors claim their method speeds up training by a factor of 6.3x, then reducing ImageNet training from 2 weeks to 2 days would be a more convincing application of their method and would strengthen the work.
The only positive reviewer has not decided to step forward to champion the paper. All others have had a negative first impression which has not sufficiently changed after the answers from authors. My recommendation is based on the data that I have available: unfortunately for the authors the need of more clarity throughout and compelling results cannot be ignore/resolved with the info at hand.
This is a borderline paper. 
This paper proposed feature kernel distillation (FKD), a new distillation framework, by matching the kernels obtained from the networks of student and the  teacher.  Theoretical justification is provided by  extending the results of Allen Zhu and Li(2020)(ALi20 hereafter). Empirical results show superiority of FKD over vanilla KD on several datasets.
There is however concern that the technical novelty is limited and  incremental, an opinion shared by DKJu, and 68WG, compared to ALi20. Reviewer DKJu suggests that the authors could highlight those results which are not straightforward extensions of ALi20. Another important point of concern is that the paper may have some Overstated claims.  The authors clarified that the language of the claims be suitably edited. In this regard Reviewer h8ud have some specific suggestions which should be easy to incorporate. 

In view of additional experiments conducted and detailed discussion during rebuttal addressed some of the concerns of the reviewers.
If accepted, the final version, should include most of the discussion and additional experiments.
The paper presents an empirical study of different strategies for fine tuning a large language model for the task of generating Java Unit tests *for a specific project*.

As several reviewers pointed out, the setup itself is fairly impractical, requiring fine tuning on an individual project, thus making it applicable only to the very tail end of very large projects where the investment of doing this would make sense and where one could reasonably collect sufficient data for that project. 

On top of that, the paper contributes relatively little in terms of novel techniques. This in itself would be OK if the paper presented some extremely important empirical evidence. However, reviewers also raised some important concerns with the empirical evaluation itself. For example, as reviewer 1jM4 pointed out, there is prior research explicitly showing that the BLEU score is not a good measure for code evaluation. 

Overall, the meta reviewer agrees with the reviewers that this paper is below the bar for publication.
The paper analyzes the learning behavior of deep networks inside RL algorithms, and proposes an interesting hypothesis: that many of the observed difficulties in deep RL methods stem from _capacity loss_ of the trained network (that is, the network loses the ability to adapt quickly to fit new functions). As the paper points out, some of these difficulties have popularly been attributed to other causes (such as difficulties in exploration) or to less specific causes (such as reward sparsity: the paper proposes that capacity loss mediates observed problems due to sparsity). 

The paper investigates its hypothesis two ways: first by attempting to measure how capacity varies over time during training of existing deep RL methods, and second by proposing a new regularizer to attempt to preserve capacity. These experiments are set up well, and their results are convincing &mdash; while there is likely no perfect way to measure or preserve capacity, the methods chosen here make sense. 

This is a strong paper: it proposes a creative, appealing, and interesting hypothesis about an important problem (difficulties in training deep RL methods), and conducts a well designed evaluation. We expect and hope that it will inspire interesting follow on work.

We thank the authors for their thorough and helpful participation in the discussion period, including updates to improve the clarity of the paper.
The reviewers think the topic is important and challenging. The results are novel, and the experimental section provides a nice illustration how the joint Shapley values can be used. However, the paper can be improved by including more real world applications and experiments.
The authors set up a simple combination of an energy based model and a flow based model that corrects the flow based model with an energy based term. The merits of this relative only an energy based model is improved sampling to compute the gradient. The advantage over a only flow based model is that the kinds of transforms that can be used are less limited.
Description of paper content:

The paper addresses the problem of credit assignment for delayed reward problems. Their method, Randomized Return Decomposition, learns a reward function that provides immediate reward. The algorithm works by randomly subsampling trajectories and predicting the empirical return by regression using a sum of rewards on the included states. The method is compared to a variety of existing methods on Mujoco problems in “episodic reward” settings, where the reward is zero except for the final step of the episode, where it is the sum of rewards from the original task. Theoretical argument suggests the method is an interpolation of return decomposition (regress based on all states, not a subsample) and uniform reward distribution (send episodic reward to all states equally). By regressing with a subset of states, the method reduces compute for longer problems and is suggested to be more scalable.

Summary of paper discussion:

The reviewers largely commended the simplicity of the method, the simplicity of the presentation, the novelty of the algorithm, and the quality of the empirical results. The negative reviewer maintained their initial review’s score on account of a bias introduced by the algorithm.
The paper received a majority voting of rejection, although the author response successfully convinced one reviewer to increase his/her score from 5 to 6. I have read all the materials of this paper including manuscript, appendix, comments and response. Based on collected information from all reviewers and my personal judgement, I can make the recommendation on this paper, *rejection*. Here are the comments that I summarized, which include my opinion and evidence.

**Presentation**

The presentation of this paper needs huge efforts to further improve. Several reviewers and I suffered from difficulties to understand the motivation and challenges of this paper. It seems that Section 3.5 is the novelty part of this paper, but I failed to catch their points. 

**Contribution**

Two contributions points were claimed in this paper. (1) The combination of data augmentation and MCR$^2$. Without knowing the challenges in this paper, it is difficult to evaluate this point. Based on my current understanding (The presentation heavily affects my understanding), this point is very incremental. (2) The proposed method achieved state of the art performance. This point is problematic. I will explain below.

**Related Work**

The authors failed to notice a huge body of manifold learning work and contrastive clustering work. Some state of the art methods are not included for comparisons.

**Experimental Evaluation**

(1) Lack of state of the art methods; (2) No standard deviation; (3) The experimental results are incomplete; and (4) It seems that the proposed method only achieved high performance on CIFAR 10 and CIFAR 20. I am not the person who requests the authors achieve the best performance on all the datasets. Everyone knows no algorithm always wins. But the authors should provide some analyses on the inferior performance for better understanding the model.

No objection from reviewers was raised to again this recommendation.
This paper provides a theoretical analysis for the Feedback Alignment (FA) algorithm, an alternative to backpropagation for training deep linear neural networks. The main drawback of the analysis is that it assumes that the initial weight matrices are diagonal, which makes the dynamics of the algorithm reduce to K independent one dimensional dynamic. Most of the reviewers feel that this assumption is too strong. Note that in many existing papers on the implicit bias/regularization of gradient descent (GD) for optimizing deep linear networks, they do not assume the initial weight matrices are diagonal. The authors provide some additional proof in appendix B during the rebuttal to try to relax the assumption on the diagonal initialization, but it is not critical clear if the same results still hold. While this paper studies a very important problem, I suggest the authors take into account the reviewers’ comments and improve the presentation/results. In addition, a comparison with the implicit bias of GD would help better position this work, as one of the reviewers suggested.
This paper continues the investigation on fairness and privacy in the context of federated learning. We appreciate the detailed response from the authors. During the rebuttal period, the authors have largely updated the set of experiments, since there was an identified bug in the previous implementation. Another drawback that the AC identified is that there is a lack of formulation and formal guarantees in the paper. In particular, is the proposed algorithm trying to satisfy example level or client level data privacy? The resulting noise scale can be quite different. Unlike prior work (e.g. Jagielski et al), the proposed algorithm does not seem to provide any fairness guarantee. Thus, it is not clear why the proposed approach is justified (even under some assumptions). In a similar vein, perhaps the authors could consider a more in depth discussion that compares their approach with prior work and articulate what advantages does their new method offers. Overall, the paper is not ready for publication at ICLR.
The paper received two accept and two marginally accept recommendations. All reviewers find value in the proposed supervised semantic segmentation methodology (making self supervised representation learning towards dense prediction tasks like segmentation or clustering without explicit manual supervision) and appreciate the experimental gains, but had (mostly practical) criticism that was reasonably well addressed in the rebuttal.
This paper has been reviewed by four reviewers with three borderline scores leaning towards an accept and one clear reject. Reviewers have raised a number of issues. They feel that *the paper is borderline* as *the paper may not have great novelty* due to the use of low rankness even though it is used for the low rank tensor approximation and that *larger datasets* should be used to demonstrate the effectiveness of the proposed approach (even though there are no papers doing it on the large scale graphs to be fair).

Also, reviewers note that they would like to see more theoretical justifications rather than just to see authors *propose a method for the adversary scenario* without full theoretical analysis. For instance, reviewers xxhm and WHUo were seeking the novel theoretical analysis in the context of adversarial robustness rather than a statement that *the problem of recovering the data under gross error has gained much attention* followed by the list of prior papers and an outline of their findings.

While all reviewers agree that the empirical results look very promising, they also agree that the theoretical analysis needs an improvement. For the above reasons, however tempting, even if overlooking the reject score from the reviewer xxhm, it is difficult for AC to advocate for a clean accept.
This paper studies an RL problem with vector rewards, where the goal is to maximize the expected minimum total reward (ex post max min fairness). This is different from prior works on a similar topic, where the goal is to maximize the minimum expected total reward (ex ante max min fairness). The authors propose an algorithm for solving the problem with $O(T^{2 / 3})$ regret and evaluate it.

This paper received two borderline reject and two reject reviews. The reviewers recognize the novelty of the objective. However, they are also concerned with its motivation and that the proposed algorithm relies on strong assumptions, such as that the used oracle knows the underlying reward and transition models, or at least has some estimate of them. At the end, the scores of this paper are not good enough for acceptance. Therefore, it is rejected.
The paper presents a general solution method for constrained RL problems using reward free exploration. While the reviewers found this reduction interesting in general, they had concerns about the price of this reduction in general (such as the increased regret or for suboptimal dependence of the bounds on some problem parameters), which is to be paid in exchange for the simplicity and flexibility of the proposed approach. This, coupled with the limited technical novelty used in the derivations, made all reviewers think that this is a borderline paper, and I also agree with this assessment. The paper could benefit a lot from presenting more evidence of the benefits of their approach (either theoretically or empirically). Based on the above, unfortunately, I am not able to recommend acceptance at this point.
This paper proposes a layer wise adaptive aggregation method for federated learning that seeks to reduce the communication cost. The frequency of aggregation is adjusted separately for each layer of the model that is being trained. The number of iterations $\tau$ after which each layer s parameters are averaged across clients is multiplied by a factor $\phi$ depending on the magnitude of changes to the parameters at each layer. The paper gives a convergence analysis of the proposed method and provides experimental results to demonstrate its effectiveness in reducing communication without compromising accuracy.  

Reviewer 7ks6 found some errors in the convergence analysis that were fixed by the authors in the discussion period. Reviewer YGZf increased their score to 5 after the discussion with the authors. The reviewers also gave the following suggestions to improve the paper:
1) Showing convergence curves to demonstrate that the communication reduction does not come at the cost of a slowdown in convergence.
2) The results presented in the paper shows only a small communication reduction for many of the layers. Perhaps the strategy can be improved in order to boost the communication reduction.
2) Use a more realistic model to calculate the communication cost so as to account for network delays and other costs rather than just considering the number of parameters communicated.

The scores are split on this paper. While one of the reviewers recommends acceptance, three others say that the paper is below the acceptance threshold. So I recommend a rejection while noting that the paper is close to the borderline.
This paper studies the problem of estimating the trajectory of a linear dynamical system when the covariances for the process and observation noise are unknown. The standard solution is to estimate these covariances from data, and this paper instead suggests an optimization procedure. They show promising experimental results. However there are two shortcomings: In terms of theoretical guarantees, they can only show convergence to a local optimum. Moreover they assume they have access to the ground truth hidden states. Although this is an assumption that has appeared in earlier works, it seems to limit the applicability.
This paper presents the use of the Ensemble Kalman Filter (EnKF) to solve the linear quadratic Gaussian (LQG) optimal control problem. After reviewing the paper and taking into consideration of the reviewing process, here are my comments:
  The related work is limited and needs more improvements to contextualize the problem and the solution.
  The reinforcement learning paradigm is not really appreciated in the proposal.
  The results are rather limited, so more experiments are needed to clearly validate the solution.
From the above, the paper does not fulfill the standards of the ICLR. I suggest improving the paper accordingly and submitting it to a control systems venue.
This paper introduce a protein pretraining framework that enhances representations learnt from protein language modeling with knowledge graph embeddings. The new framework, OntoProtein, optimizes jointly a masked Protein objective and a Knowledge Graph Embedding objective producing knowledge aware protein embeddings. These embeddings are evaluated on downstream tasks including protein protein interactions and protein GO association prediction. The paper also introduces a new large scale KG dataset, ProteinKG25.

The reviewers were in agreement that the paper presents an important research direction and that the work is well framed and motivated. The dataset contribution was also considered important by the reviewers and they are in agreement that the paper is clear and generally easy to understand. Reviewers were concerned that the novelty of the work is in the application of existing techniques to a new domain rather than introducing new domains, but generally the reviewers considered the novelty to be sufficient for publication. There were some other concerns about missing references and some of the presentation, but the authors addressed these concerns in their response and in the updated version that they produced.
This paper studies the performance of second order algorithms on training multi layers over parameterized neural networks. The authors propose an algorithm based on the Gram Gauss Newton method, tensor based sketching techniques, and preconditioning to train such a network, whose runtime is subquadratic in the width of the neural network. While some reviewers provide some weak support, none of them are in strong support, even after the author s response. I think one of the reasons is the lack of empirical experiments. Since the main claim of this paper is an efficient second order algorithm, some experiments are necessary to back up this claim. Unfortunately, the authors did not try to add such an experiment during the rebuttal. I would suggest the authors add such experiments in the revision.
This paper tackles the problem of Unsupervised Environment Design to train more robust agents. The proposed method trains RL agents by generating a curriculum of training tasks to enable agents to generalize to many tasks. The key contribution is an algorithm to generate this curriculum by incremental edits of the grid world environments. The reviewers all agreed that the paper is well written and the method is intuitive. However, the weakness of this work is also obvious: the proposed method is only evaluated in grid worlds, and it s unclear how the editing approach can be easily generalized to more complex environments. This submission would benefit from more comprehensive evaluation in non grid world environments, especially given that the compared baselines have results in other environments too.
The authors study a practical problem of selecting/combining existing multi label classification APIs under a budget constraint for a specific problem instance on hand. The task can be viewed as an (online) integer programming problem when given an accuracy estimator for the combination performance. The authors relax the integer constraints and propose a framework to solve the task in the dual form. They also run experiments to validate that the proposed framework is advantageous (cost or accuracy wise) over the best single API.

Most of the reviewers are positive about the practical value and the potential impacts of the work in applications/products/services. There are several disputes between the authors and some reviewers that cannot be fully resolved during the rebuttal. In the end, no reviewers express willingness to strongly champion for the acceptance of the paper, making the paper a borderline case. The decision is based on a careful examination of the current manuscript and every side s opinions.

* Novelty: Some reviewers question about the novelty of the work. There are two aspects about novelty: one is on whether the problem itself is novel (are the authors trying to propose a new multi label method?) In this aspect, the authors  response, which states that they are not aiming at proposing a new method, but at solving an automation task for MLaaS users, appears believable. The other aspect is whether the solution technique, namely the relaxed integer programming and other techniques, are sufficiently novel. Some reviewers find the novelty aspect satisfactory, while others believe that the proposed optimization technique have been widely used in machine learning community. The authors did not clarify the similarity/difference of the proposed technique to existing ones during the rebuttal. In this sense, the technical novelty is not well justified.

* Speed: Some reviewers are concerned about different aspects of the running time and other costs. The authors emphasized the rapid speed in inference phase, particularly in Figure 3. Less is discussed about the time needed for the training phase (although the authors claim to be much smaller than the inference time) somehow even the most positive reviewers have some questions about this aspect. The authors could add more clarification about the different "time" costs to the discussion. One dispute between some reviewers and the authors is about the *complexity* analysis of time, which is indeed missing in the current manuscript and can be a nice to have for future todos.

* Theoretical Guarantee: One major dispute between some reviewers and the authors is on the theoretical guarantee provided. The reviewers suggest a regret style bound, which compares the solution to the worst case sequence; the authors provide an optimization style bound, which compares the solution to the absolute optimal solution. Different bounds have their different roles for supporting the framework. Given that the authors have provided some reasonable bounds, the lack of regret bound is not taken against the authors.

* Specialty: One concern raised by some reviewers is that the technique does not seem particularly tailored for multi label classification (except some minor parts). In this sense, it is nice to have for the authors to discuss more on the wider applicability of the technique, and/or include some more specialty of the multi label classification problem into the technique design.

After taking all the factors above into account, and calibrating the received scores to the distribution across the papers, it seems that the paper could use some more revision before being mature enough as an impactful work.
The authors propose a VAE based architecture for generating multivariate time series. The base version of TimeVAE models a distribution over a fixed length sequences of observations using a latent vector of fixed dimensionality and a convolutional encoder and decoder. The Interpretable TimeVAE model incorporates additional features from traditional time series models such as explicit modelling of trends and seasonality. TimeVAE is compared to several baselines such as TimeGAN on four small times series dataset and seems to perform competitively according to two custom evaluation metrics and a visualization. 

The reviewers thought that the paper was interesting but not ready for publication due to the following:
 The paper s contributions and their significance are not clear
 Interpretable VAE was not used in the experiments and its interpretability has not been verified
 Coverage of related work is insufficient
This paper makes the following contributions   1) it shows that one reason behind the attributions being more interpretable for adversarial robust models is that for these models, the gradient with respect to the input is more closely aligned with the normal direction to a close decision boundary. 2) Using the previous fact, the authors devise two new attribution methods   BSM and BIG   which can be used to get more reliable explanations from even a normal (non robust) model. While the reviewers agree that the premise of this paper is interesting, some concerns remain post the rebuttal. More specifically, some reviewers opine that the AGI and BIG methods are somewhat similar, and other reviewers are not very convinced about some of the details e.g., the generalization of the orthogonality of SM to the decision boundary from the binary classification case (section 3.1) to the more general case of ReLU Net s multi class classifiers (section 3.2). Given this, we are unable to accept this paper at this time. We hope the authors find the reviewer feedback useful.
The author response addressed some reviewer concerns, and generally reviewers increased their scores. However, there are important, and unanswered concerns about the generalization of the model. The discussion raised the concerns that despite the paper claim of "a specific class of higher order reasoning" emerging, the result suggests relatively simple strategies. This might not be a limitation of the approach, but of the evaluation scenario. So, this either requires a more nuanced view of the findings, and further empirical evidence to support the claim.
This paper proposes a counterfactual explanation method, termed DISSECT, for image classification. While previous work is concerned with generating one single counterfactual, DISSECT aims to produce multiple counterfactuals, with each illustrating one possible way the class label could be altered. Intermediate images between the benign example and the counterfactuals are also generated to show how the decision boundary is crossed.
The reviewers find the idea novel, the presentation clear, and the empirical evaluation thorough.  However, there are concerns regarding whether the method will generalize to other domains because it relies on a strong generative model.  In addition, there is no human subject study to show whether and how much the method really help an end user.
The paper studies the behavior cloning based strategies of offline RL algorithms in different type of environments and reports that performance primarily depends on model size and regularization. The results contradict some of the earlier claims, and the authors conjecture that model size and regularization characteristics can explain past results.  

During the review period, the reviewers agreed that the paper has certain merits, and on the other hand, they also raised some concerns, regarding some missing technical details, whether the empirical finding could be trusted, the generalization of the findings to more scenarios, and the comparison with some highly related papers. The authors did a good job in their rebuttal, which removed many of the above concerns (although not all) and convinced the reviewers to raise their scores. As a result, we believe it is fine to accept the paper (although somehow like a weak accept).
This paper investigates a technique for projecting contextual embeddings into static embeddings. Neither the technique is ver novel, nor are the empirical results very strong. While the reviewers did not engage in a discussion, the area chair does not see this paper reaching the quality bar of the conference.
This is a borderline case and it s quite difficult to decide the recommendation. The paper works on a critically important problem, namely removing or reducing the in distribution accuracy drop when we need to also take the out of distribution accuracy into account. The proposed method is simple and it works, which is great. However, as the reviewers discussed, the demonstrated applications are not very representative, and the authors should consider more popular setups of few short learning and even other forms of domain generalization. Furthermore, adversarial examples are also OOD (in most cases, since the ID manifolds are thin films and the attacks can easily go out of the ID manifolds), it would be great if adversarial accuracy can be incorporated as a case of OOD accuracy. Since there is still room for improvement, we hope the paper would benefit from a cycle of revisions for a re submission.
This paper presents an approach to learn graph grammars for molecule generation in a very data efficient way.  The approach combines bottom up grammar construction by contracting graph substructures and evaluation driven learning of the parameters in grammar construction in order to optimize metrics of interest.  This paper is well written and the graph grammar learning approach is novel and can potentially have impact beyond just generating molecules.  All reviewers unanimously recommended acceptance of this paper.

A few things emerged in the reviews and discussions with authors, regarding in particular the computational cost and scalability of the approach and the actual molecule sampling process after learning.  I hope the authors can clarify these in the paper, and as the authors said in the discussion that exploring a more expressive model that uses learned probabilities on the production rules can make the model more powerful, which is a promising direction for the future.
This paper presents a new metric for disentanglement of learned representations, extending a prominent framework (DCI) to support object centric structured representations.

The reviewers agree on the importance of the question and find the metric a valuable contribution for addressing this problem. In the discussion, the reviewers identified some clarity issues that the authors have improved, leading to an overall much better writeup, as well as some deeper evaluation of learned matching agreements. The main remaining points that could be improved are
   making the results more robust with thorough hyperparam tuning
   connecting to other methods for inducing soft / probabilistic matchings, such as Sinkhorn or smooth&sparse optimal transport.

Please consider switching to the Times font as recommended by the ICLR style guide.
**Summary**

This paper proposes a method to do a sample efficient offline domain adaptation method where the setting requires one to have abundant amount of data from the source domain but limited amount of data available in the target domain. The proposed approach DARA achieves that by accounting for the the dynamics shift between the source and the target domain via a reward penalty. The paper shows promising experimental results.

**Final Thoughts**
Overall the paper is well written, and it addresses an important problem. The reviewers are mostly positive about the paper at the end. The authors did a very good job addressing the concerns raised by the reviewers. Reviewer 73ni had concerns about the novelty of the approach, it would be nice if the paper can make it more clear about the novelty of the proposed approach compared to the other existing methods in the paper. I  would also recommend the authors to incorporate the feedback and the suggestions made by the reviewer into the camera ready version of the paper.
This paper presents a method which selects feasible data augmentations suitable for contrastive time series representation learning. The topic in this paper is timely and interesting. One of 4 reviewers did not complete the review, not responding to a few reminders. So, one emergency reviewer, who is an expert in meta learning was added. While there is one review that strongly supports this work, two reviews remained unsupportive after the discussion period ended. I appreciate the authors for making efforts in responding to reviewers’ comments. However, after the discussion period, most of reviewers had concerns in this work, pointing out that the technical correctness needs further justification and experiments should be improved.  While the idea is interesting, the paper is not ready for the publication at the current stage. I encourage to resubmit the paper after addressing these concerns.
This paper proposes a novel representation for pose authoring, and was uniformly lauded by all reviewers.  The AC concurs this paper is far above the threshold for acceptance at ICLR.
This work describes an interesting approach of using a reinforcement learning algorithm for federated learning. The paper is well organized and the use case of performing federated learning while preserving patient privacy is also important. However, the paper has room for improvement. Important baselines used for client selection are missing and so the deep reinforcement learning approach is not well motivated. Many important technical details are missing such as hyperparameters and distributions for MNIST and CIFAR. The approach is also lacking novelty, DRL has been used for neural scheduling before and the authors do not suggest improvements to that. Finally, the experiments showing robustness to backdoor attacks is unconvincing and can benefit from more analysis.
### Summary

This work investigates effective sparsity: an assessment of the sparsity of pruned networks that accounts for the fact that unpruned neurons can still be completely disconnected through pruning.  Hence, the effective sparsity of a network may be much lower than otherwise reported.

### Discussion

#### Strengths

  The paper studies an important metric that deserves additional attention in the community, where a change in metric may guide either the theory or practice of pruning.

  The paper evaluates direct versus empirical sparsity for a healthy number of pruning techniques.


#### Weaknesess 

  While this paper appears to be the most direct study of effective sparsity at the moment, it is not the first. Appendix M of [1] defines effective sparsity and shows that direct and effective sparsity are similar for contemporary pruning at initialization techniques. However, that work does not evaluate random pruning. This work here will need to revise its novelty claims to account for these results as its characterization that [1] only considers direct sparsity is incorrect.


  "Computing effective sparsity:" the procedure in question is similar to that of Appendix M in [1], thus its relationship should be detailed.


  With the primary observations residing in the regime of extremely sparse neural networks, the elements of the response (and in the last paragraph of the paper) that claim this regime is productive for ensembling should make a more prominent appearance in the introduction of the work.

[1] Pruning Neural Networks at Initialization: Why Are We Missing the Mark? 
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, Michael Carbin. ICLR, 21


### Recommendation

I recommend Reject. Generally, the paper is well written and the empirical characterization of direct versus effective sparsity is thorough (except for ResNet 50 results). However, the results and the language around these results need significant rescoping to account for novelty and the relation of the work to an area in which it is anticipated that these results will change theory, practice, or thinking (e.g., ensembling).

Though I cannot speak for future reviewers, IMO, an extension of the results here to ResNet 50+ImageNet should suffice to establish the extent of the discrepancy between direct and effective sparsity. However, to satisfy additional demands from reviewers for more practical relevance, I suggest an evaluation that demonstrates a consequential difference in behavior for a task that maps more closely to the anticipated area of impact (e.g., ensembling)
This paper proposes a new link prediction algorithm based on a pooling scheme called WalkPool. The main idea is to jointly encode node representations and graph topology information into node features and conduct the learning end to end. The paper shows the superiority of the method against the baselines.

Strength
* The paper is generally clearly written.
* A new method is proposed, which is technically sound.
* Many experiments are conducted to verify the effectiveness of the proposed method.

Weakness
* The novelty of the work might not be so significant.  There is a similarity with the SEAL algorithm.

The authors have addressed most of the problems pointed out by the reviewers. They have also conducted additional experiments.
This paper considers an important problem, graph partitioning, from a transductive viewpoint: assuming that the graphs are generated by independent draws from an unknown distribution, learn some parameters in an ``offline” phase, and use these in the ``online” phase (much as in PAC learning). The authors have also answered many of the reviewer questions. In particular, the comparison with existing work is substantial. 

While I laud the positives of this work and the importance of the transductive approach, I see an issue: as a reviewer points out and as agreed by the authors, the paper does not provide a theoretical guarantee of the quality of the generalization to unseen graphs. It would have been useful, e.g., to consider this on Erdos Renyi G(n,p) models, stochastic block models etc.
This paper investigates defense against backdoor attacks for models that have already been trained. It proposes, in particular, a min max formulation for backdoor defense, in which the inner maximum seeks a powerful trigger that leads to a high loss, while the outer minimum seeks to suppress the "adversarial loss", so as to unlearn the injected backdoor behaviors. To solve the minimax, the authors also propose a method, Implicit Backdoor Adversarial Unlearning (I BAU). In addition, the authors also provide theoretical analysis including the convergence bound and generalization bound. Extensive experiments demonstrate the effectiveness and efficiency of the proposed method. 

The proposed method is interesting and the implementation is nice. Overall, there is a fundamental flaw in the formulation: if the trigger is not additive (where there are many such examples of poisoning attacks that are not additive) this approach should fail completely. Not having experiments that discuss such triggers that are not additive is a significant flaw in the presentation of the paper. Another flaw is that the trigger is assumed to be small norm. Unlike adversarial examples attacks (at test time), there is no reason for backdoor triggers to be of small norm. Given that the defense critically relies on these two flawed assumptions, and the extent of how the proposed algorithm is sensitive to these assumptions are not properly addressed in the experiments, this paper is on the border line.
The reviewers are unanimous that this is a strong submission that deserves to be accepted.
The reviewers remained concerned about the overall novelty of the paper, finding the contributions somewhat incremental. The authors are encouraged to better substantiate design choices that they make, to improve the overall presentation, and to contrast with the works/line of research brought up by the reviewers.
The authors consider the impact of designing fair algorithms on adversarial robustness. The particular focus is on poisoning attacks. They show experimentally that for some datasets and models/algorithms that using "fair" algorithms increase adversarial vulnerability compared to the standard training procedure (ignoring fairness criteria). The reviews have raised questions about whether the experimental results are extensive enough and I share their concerns. Most importantly, the authors have not addressed the question regarding to the quality of approximation at all.
In this paper, the authors generalize the univariate Shapley method to bivariate Shapley method. The authors first build a directly graph based on the asymmetric bivariate Shapley value (adding feature j to all sets contained feature i). Then several graph algorithms are applied to analyze the directly graph to derive (1) univariate feature importance available in univariate approach and (2) relations like mutually redundancy only available in bivariate approaches. Experiments on several datasets with comparison to existing methods demonstrated the superiority of the proposed method.  All reviews are positive.
This paper approaches personalized federated learning from the perspective of meta learning and use the mutual information framework developed in a recent work to regularize local model training. All the reviewers consider the writing very poor and hard to understand, and the contributions not sufficient for acceptance.
This paper proposes to implement posterior sampling for reinforcement learning for MBRL using three types of noisy convolutional layers inspired by object  and event based domain knowledge. These layers are used to augment the SimPLe agent (Kaiser et al, 2020), resulting in the EVaDE SimPLe agent, and experiments demonstrate that the EVaDE SimPLe outperforms SimPLe on average across twelve Atari games.

The reviewers  opinions on the paper were mixed. The reviews highlighted several strengths of the paper: that using posterior sampling for exploration in MBRL is well motivated (Reviewers 9oaA, XiQT) and that the simplicity of the proposed layers is appealing (Reviewers trzPm, XiQT). However, the reviewers also generally felt that the proposed method was overly specific to a particular domain (Reviewer gXzj, XiQT) and that there was not enough analysis demonstrating *why* the proposed layers work, in which cases they would not work, or why these modifications might be better than other similar modifications (Reviewers 9oaA, trzP, gXzj). Initially there were also some concerns raised by Reviewer trzP about the validity of the evaluation due to the number of seeds, though these concerns were addressed by the authors during the rebuttal.

I agree with the reviewers that the approach is interesting and that getting posterior sampling to work well in MBRL is an important problem. But I also find myself agreeing that the present approach is not analyzed in sufficient depth (the results are overly focused on just overall performance, rather than analyzing behaviors exhibited by the agents) and that it is unclear how well it would work in other domains (e.g. 3D settings). I therefore feel this work is not quite ready to be presented at ICLR, and recommend rejection.
This paper has been reviewed by four expert reviewers who gave diverging scores. The three negative reviewers have provided significant constructive feedback. The main criticism is the lack of novelty and clarity in the paper. The authors have submitted their rebuttal which did not improve the scores of these reviewers. After the discussion phase, the paper did not obtain any support for acceptance and stayed under the acceptance threshold. Following the reviewers  recommendation, the meta reviewer recommends rejection.
This paper considers a new setting of contextual bandits where the learning agent has the ability to perform interventions on targeted subsets of the population. The problem is motivated from software product experimentation but with more general applicability. The paper provides a method under this setting, with both empirical and theoretical support. Reviewers agree that this is an interesting setting, and the paper contributes new results. The initial concerns on assumptions, correctness and experiments were addressed in the rebuttal. I thus recommend accept. The authors should include the response carefully in the final version.
The paper proposes a method to change the graph structure for better robustness against adversarial attacks. The reviewers commend the authors for a clearly written paper and promising results. Several reviewers expressed concerns about experimental validation (specifically, comparison to truncated SVD and choice of baselines), complexity, and novelty. The rebuttal and follow up discussion alleviated some of the concerns, but the reviewers still have outstanding issues, therefore the AC does not recommend accepting the paper.
The paper proposes a method for learning to optimize (L2O) by distilling a numerical L2O optimization rule into a simple mathematical rule, mathematical equation, using special purpose student learning algorithm.  The motivation for using a symbolic distillation is to provide interpretability and scalability of the trained optimizers. 

Pros
   The paper addresses an important problem (better understanding learned optimizers).
   The experiments give good evidence that learned black box optimizers can be mapped to mathematical rules.

Cons
   The symbolic regression student learning algorithm, and many details of the experimentation, remain hard to understand.

Overall, after discussion, the paper was viewed as a solid contribution by the reviews, with some slight disagreement about whether the clarity was sufficient after revisions.  However, I believe that the main points of the paper and the general approach are quite clear, and that the details of the experiments and student learner are sufficiently well explained for other researchers to build on, at least given the appendix and the supplementary material.
This is a solid paper and considers the problem of training a wide neural network with a single hidden layer. This can be framed as an optimization problem in the space of probability distributions with a suitable entropy regularization, where each atom in the distribution corresponds to a hidden neuron. The dual of this problem (for finite data) is a finite dimensional optimization problem and the paper proposes a particle based coordinate ascent scheme.
The paper provides some convergence rate results. After the rebuttal, the authors have also included more experimental/numerical results.

The authors have answered the concerns raised by the reviewers and overall, the paper can be accepted:
The presented approach appears to be sufficiently novel and might be useful in other settings.
The presentation is clear and easy to follow for such a technical paper; the paper is well organized.
The limitations of the approach are clearly stated (dependence on the regularization parameter for entropy term that may be hard to select)
The submission provides a theoretical framework on the learning of group based disentanglement representations and proposes a novel method to learn such representations.

The reviewers appreciated the novel perspective of the paper in introducing the concept of group based disentanglement in unsupervised VAE. Furthermore, the approach was considered to be soundly theoretically motivated and experiments to be extensive. There was a lively discussion between reviewers and authors about certain ambiguities in the manuscript; however, they seem to have been largely resolved to the reviewers  satisfaction.

While there was a reviewer with a very low confidence recommending rejection, this paper brings an indisputably interesting novel perspective to the learning of unsupervised representations and I thus recommend acceptance.
This paper considers generalization of polynomial networks. It gives a characterization of the Rademacher complexity as well as Lipschitz constants for polynomial nets. Inspired by the theoretical results, the paper also proposed regularization schemes that empirically improves accuracy and robustness. Most reviewers found the theoretical results to be interesting (but there are some concerns about the mismatch between the upperbound in theory and used in practice, which was partially addressed in the response). There are some more concerns about the experiments but many of them are addressed in the new version. Overall although polynomial networks are not popular in practice, this paper provides some interesting theoretical results.
The paper studies two aspects of personalized federated learning: (1) Clients having their own labeling scheme. (2) Domain heterogeneity across clients. They propose a way to collaborate across clients by similarity matching. The key novelty is to measure similarity of client pairs, based on on how much their representation layer agrees (measured with cosine similarity). A second novelty is a low rank factorization of model weights. Empirical evaluations show wins on MNIST, CIFAR10, 100.  

Reviewers had various grave concerns. On the method side, they were concerned that thee is not enough theoretical insight and analysis of the proposed approach, esp. the kernel factorization and its effect.  On the empirical side, they were concerned that comparisons were not made with most recent baselines. There was a large number of PFL approaches published in 2021. e.g. FedBN.  Among these, its worth noting pFedHN (ICML2021) which actually discussed the case of heterogeneous (permuted) labels (their Sec 3.3).

In a discussion, reviewers appreciated the responses by the authors, the additional experiments and ablation studies. Unfortunately however, they found that the paper is not ready for publication in ICLR.
The paper describes a framework that unifies several previous lines under hindsight information matching.  Within that framework, the paper also describes variants of the decision transformer (DT) called categorical DT and unsupervised DT.  The rebuttal was quite effective and the reviewers confirmed that their concerns are addressed.  The revised version of the paper is significantly improved and consists of an important contribution that should interested many researchers.  Well done!
The paper proposes a GAN architecture with a ViT based discriminator and a ViT based generator. The paper initially received a mixed rating with two "slightly above the acceptance threshold" ratings and "three slightly below the acceptance threshold" ratings. Several concerns were raised in the reviews, including whether there are advantages of using a ViT based GAN architecture over the CNN based GAN and whether the proposed method can be extended to high resolution image synthesis. These concerns are well addressed in the rebuttal with most of the reviewers increasing their ratings to be above the bar. The meta reviewer agrees with the reviewers  assessments and would like to recommend acceptance of the paper.
I would like to thank the authors for having managed a thorough discussion despite the complexity of the task at hand (e.g. BEvM). during discussion, the reviewers clearly converged to accepting the paper, praising the importance of the problem tackled and the setup put in place to effectively tackle the challenge at hand.

All this makes the paper an important contribution and a clear accept (and an enjoyable read), for which I can only recommend a further polish before camera ready to follow the latest inclusions.

AC.
This paper opens the area of adversarial attack research on streaming data (e.g., real world settings such as self driving cars and robotic visual tasks for a robot). For instance, online adversaries can focus their attack on a small subset of the streamed/online data, but still cause much damage to downstream models. This work highlights the need for stateful defense strategies. Connections to online algorithms and the k secretary problem are made, along with improvements to some online algorithms work of Albers and Ladewig.  

Overall, the attack model introduced is important, and the bridge to online algorithms would be useful for the ICLR community. I also believe this topic lends diversity to the typical set of ICLR papers.
All reviewers suggested rejection of the paper. This is based on concerns regarding novelty of results, clarity of presentation, simplicity of conducted experiments and missing ablation studies (and several other points raised in the reviews). The authors also did not submit a rebuttal. Hence I am recommending rejection of the paper.
This paper studies the problem of learning a graphical model given observational and experimental data. The main novelty is the use of interventions to avoid the acyclicity constraint that plagues existing methods. Although this idea is quite standard and well known, the generality of the approach merits consideration. After the discussion, there was a consensus among the reviewers to accept this paper. Some valid concerns have been raised and we expect that the authors will take into account all of the suggestions raised by the reviewers.
This paper provides empirical results for one class classification problems.
The studied problem is important and the reviewers admire the challenge of this paper.
However, the empirical results are not still insightful enough to provide practical recommendations.

Some of the questions raised by the reviewers could potentially be answered by the authors, but we did not receive any feedback unfortunately.

Given that there is essentially no technical novelty in this paper, it cannot be accepted for ICLR.
The paper proposes a hierarchical meta imitation learning framework for few shot transfer in the context of long horizon control tasks. Underlying the framework is a hierarchical adaptation of model agnostic meta learning (MAML) that jointly learns the high level policy together with the set of modular low level policies (sub skills), both of which are fine tuned at test time based on a small number of demonstrations. Experimental evaluations on the meta world benchmark as well as a kitchen environment benchmark compare the proposed framework with recent baselines.

As several reviewers note, the problem of jointly learning modular policies together with the high level policy for composing these sub skills is both challenging and interesting to the robotics and learning communities. The manner by which the paper extends existing work in meta learning (MAML) and hierarchical imitation learning is novel and technically sound. The reviewers raised some concerns, notably those regarding (1) the the framework s sensitivity to various hyperparameter settings and its ability to generalize to other domains; (2) the merits of joint optimization over decoupled optimization of the sub skills and high level policy; and (3) the need for experiments/evaluations on different domains. The authors provided a detailed response to each of the reviewers that includes the addition of a different benchmark evaluation (the kitchen environment), new ablation studies, and updates to the text. After a thorough review, however, concerns remain regarding the reproducibility of the results, which call into question some of the key contributions that the paper claims to provide over the existing state of the art. The authors are encouraged to provide a more balanced discussion of the contributions along with evidence to support reproducibility in any future version of the paper.
This paper studies potential drawbacks in using softmax over attention in Transformers and evaluates other normalization approaches. Reviewers, while had been positive about the empirical analysis and the insights from the synthetic data experiments, agree that the paper lacks real world experiments/insights. I agree with that and believe the paper falls short in several areas.

1) Drawbacks of softmax: Paper states several generic drawbacks of softmax such as saturation issue leading to vanishing gradients. However the paper does not demonstrate if Transformer models used in practice suffer from this issue under standard training settings. Even the arguments that attention layer focuses on local information is quite vague and not well supported. Overall the analysis is quite weak without much concrete statements and demonstration in real settings. 

2) Experiments: The paper presents many synthetic experiments evaluating alternates to softmax varying from layer normalization to pooling. There are no experiments showing if the studied variations actually solve the issues discussed in earlier section. Finally due to the lack of any real world experiments (even small scale ones), it is not clear if the results apply in real world settings.

Overall I think the paper needs significant work in formalizing the drawbacks of using softmax in Transformers and demonstrating that the proposed solutions indeed solve this problem.
This is a deep theoretical paper with results that I consider very interesting. I have *not* had time to check them myself, but I have background in these theoretical matters and the results seem reasonable to me   the hardness of even checking the quality of a solution is well known for partition functions (as well as hardness of any reasonable approximation), but the undecidability seems new   I assume it comes naturally and it is a very interesting result   I have seen similar decidability issues for #P: general probabilistic polynomial time Turing machines (it is unclear if a connection was sought here). Reviewers are all positive about the content, and authors have acknowledge some points for improvement.
This paper is proposed to investigate the robustness of self supervised learning (SSL) and supervised learning (SL) in both balanced (in domain) and imbalanced (out of domain) settings. It can be concluded that SL can regularly learn better representations than SSL, and representations are better from balanced than from imbalanced datasets. The SSL is more robust than SL in the imbalanced settings, which is the crucial of this paper. Expect the experimental results, the authors also provided theoretical analysis to support their claims. The authors also extend a well established method SAM into the Reweighted SAM as the technical contribution to better address the imbalanced setting. The paper is well written with clear logic to follow.
This paper considers the so called partial label learning problem and proposes a class activation map that is better at making accurate predictions than the model itself on selecting the true label from candidate labels. The authors investigate the approach in experimental results on four benchmark image datasets. 

The reviewers appreciated the simplicity of the approach and its effectiveness in practice. The reviewers raised questions how to apply the approach to another weakly supervised learning problem such as semi supervised learning and whether the approach is an identification based strategy. The reviewers also raised several questions asking for more details.

The authors submitted responses to the reviewers  comments. After reading the response, updating the reviews, and discussion, the reviewers who took part in the discussion considered that their  “questions have been well addressed” and that the “authors’ responses basically provided the answers to the questions”.  

The feedback provided was already fruitful and the final version should be already improved. 

Accept. Poster.
This paper extends the recent work on continuous domain sparse attention mechanisms to use kernel parametrizations, and thus allow more flexible multi modal shapes. Continuous attention extends the standard attention mechanisms to continuous valued key/value/query functions, involving integrals over probability measures instead of sums over softmax weighted sums. 

Kernel methods fit very well in the framework and provide great expressivity. Reviewers agree it is an interesting and well motivated idea. The contribution of incorporating kernel families in continuous attention seems substantially novel in comparison to the previous work on the topic.

The main concern, however, is that the paper focuses too much on the theory and not enough on the modeling benefits  enabled by flexible kernels. I would stress that this isn t a question of *improving performance* purely (although quantitative results would help!) but perhaps more of qualitative results, demonstrating e.g. multimodality, selectivity, interpretability.

I very much look forward to a revised version, which I expect would be a strong paper.
The reviewers raised a number of major concerns including the limited novelty of the proposed, inadequate motivation of the design choices and, most importantly, insufficient and unconvincing experimental evaluation presented. The authors’ rebuttal addressed some of the reviewers’ questions but failed to alleviate all reviewers’ concerns. Hence, I cannot suggest this paper for presentation at ICLR.
The reviewers raised a number of major concerns including a poor readability of the presented materials, incremental novelty of the presented and, most importantly, insufficient and unconvincing ablation and experimental evaluation studies presented. The authors’ rebuttal failed to address all reviewers’ questions and failed to alleviate reviewers’ concerns. The authors explain that due to the lack of time they could not complete all experimental studies. A major revision of the paper is needed before the paper can be accepted for publication. Hence, I cannot suggest this paper for presentation at ICLR.
The paper extends the maximum entropy inverse reinforcement learning (IRL) framework by changing the optimal criterion used in reinforcement learning (RL). This novel criterion is an expectation of the Q values over a weighted distribution over states and actions induced by a policy, which is in contrast to the standard criterion that is an expectation over the initial state distribution.

All the reviewers agree that the topic addressed in this paper is interesting and novel. On the other hand, there are some concerns about the technical novelty and relevance of the paper. Since the authors have not provided any feedback, the reviewers did not solve their concerns and they reach a consensus on rejecting this paper.
This paper investigates a tighter bound for mutual information and proposes some novel estimators of MI from the importance sampling perspective. The proposed approach provides a unifying framework for mutual information bounds that can deduce many existing approaches. The theoretical and experimental analyses well justify the proposed approach and shows the bounds are much tighter than the existing ones.  
Overall, this paper is well written. The relevant literatures are exhaustively reviewed and well compared with the proposed method. The experimental results show remarkable superiority of the proposed method. The proposed framework would shed light on the literatures and open up a new direction of the relevant researches.  
For those reasons, I would like to recommend this paper to be accepted by ICLR2022 conference.
Thank you for your submission to ICLR.  The paper proposes a simple method for improving calibration performance using a loss based upon a Dirichlet KDE.  The method is appealing in its simplicity, but several reviewers (and myself) have concerns simply about the fact that the method ultimately seemed to give rather marginal improvement over the standard cross entropy baseline.  The authors attempted to address this point in the rebuttal, with their additional example on the Kather domain.  And while this is a nice addition, I m still not fully convinced that the improvement here is _that_ significant, to the point where I think it would be important to consider much broader sweeps of hyperparameters, etc, for all methods (which I believe should be reasonable here given the data set sizes).  I believe this has the potential to be a nice contribution, and its simplicity can be a positive, but ultimately I think a bit of additional effort is required to show the full empirical advantages of the method.
This paper presents ODConv, a convolution pattern which uses attention in the convolutions across all dimensions of the weight tensor.
The paper is well motivated and well explained, easy to follow.
This work is built on top of previous work, but reviewers all agree that the contributions of this paper are significant.
The experimental section is comprehensive, with several benchmarks, and show clear improvements.
The reviewers suggested a few additional remarks, and discussions to add to the paper, which the authors have addressed in the rebuttal. Reviewers seem in general happy with the authors answers to their concerns.
This seems like a sound and meaningful paper. I am fully in favour of acceptance, and I recommend this paper to be presented as a spotlight.
The paper proposes the anisotropic version of randomized smoothing. Evaluation metrics based on the volume of the certified region are proposed, allowing comparisons with the certified regions provided from isotropic randomized smoothing. Experimental results show the usefulness of introducing anisotropic randomized smoothing as it certifies larger regions.

Strengths:
+ The paper is well written, polished, and easy to follow.
+ The anisotropic part of the proposed approach is well motivated.
+ The evaluation section is quite thorough and obtains SOTA results.

Weaknesses:
  The sample wise (data dependent) part has several issues making it unsuitable to use in practice. The authors already discuss a know issue of data dependent classifiers which when not tackled can lead to certificates that are not sound. To address the issue they adapt the memory based procedure introduced in Alfarra et al. While this procedure does make the certificate sound it has other problems. For example, an issue is that the memory makes the certificate dependent on the order of the incoming test samples. This provides a new avenue for attack, i.e. the adversary can optimize the order of the test samples to decrease the utility of the final obtained smoothed classifier. In addition, the success of this memory approach also somewhat depends on the "sparsity" of the test samples. Namely, by using a small test set since the samples are in a high dimensional space the distance between them tends to be bigger than the (proxy) radii of the certified regions. However, in a real world application we are likely to have many more test samples which would increase the number of intersections when running Algorithm 1. Although the authors provide opposite empirical evidence on a specific dataset, it is not very clear how general it is for other datasets.

  Some of the theoretical results are not novel as they follow directly from prior work (as acknowledged in the paper). 

  Another issue with the proposed approach is the optimization procedure described in section C. The optimization suffers from issues such as: inconsistent estimation due to clamping and not using confidence bounds, sensitivity to initialization, high gradient variance, etc.
This paper introduces a technique to generate L0 adversarial examples in
a black box manner. The reviews are largely positive, with the reviewers
especially commenting on the paper being well written and clearly explaining
the method. The main drawbacks raised by the reviewers is that the method
is not clearly compared to some prior work, but in the rebuttal the authors
provide many of these numbers. On the whole this is a useful and interesting
attack that would be worth accepting.
This work proposes an observational (not counterfactual) link prediction method based on clustering and data augmentation. The total score went down during the rebuttal phase. One of the challenges is that the title confused reviewers. The authors proposed other titles. But in the end all proposed titles had variations of "counterfactual graph", which is what makes the title confusing. 

I think if the work had been more empirically focused and stayed away from a causal model, it would have been accepted. The idea is ingenious and it is likely that it indeed gives the empirical improvement in observational models that the authors claim. The authors have very scant citations to relevant works in the intersection of causality and representation learning anyways. But if the causal model must stay for future submissions, the authors should: (a) more clearly define and differentiate the causal and non causal aspects of the task (and properly contextualize the work), and (b) more clearly specifies the causal model that is being utilized (and be more mathematically rigorous in general).

Unfortunately, the original description of the causal model in the paper was both sloppy and incorrect. I had to ask a total number of five (5) long clarifying questions (visible to authors and reviewers only) in order to understand the causal assumptions. In my back and forth with the authors, the authors proposed a total of 7 different causal DAGs to explain their method. Some of these DAGs contradicted each other and most of the answers were vague. My first advice to the authors is to always be mathematically precise for the benefit of reviewers and readers. Causality requires very clear assumptions and precise notation. For instance, the variables in the causal DAG given in Figures 2 and 3 had little to do with the variables in the actual graph model. The causal DAG must have the variables $A_{ij}$, $T_{ij}$, $C_i$, $C_j$, $z_i$, $z_j$, for all $i,j \in V$. The final causal DAG proposed by the authors in the discussion was:
  $z_i \to A_{ij}$
  $z_j \to A_{ij}$
  $z_i \to T_{ij}$ (may not be needed)
  $z_j \to T_{ij}$ (may not be needed)
  $C_i \to T_{ij}$
  $C_j \to T_{ij}$
  $T_{ij} \to A_{ij}$

Regarding the nature of $z_i$ and $z_j$: They need to be formally described as structural node embeddings of the graph, otherwise the DAG is incorrect. After digging and reading some of the references the authors use, I came to the conclusion that the empirical method indeed relies on structural embeddings (and the authors later confirmed it). This should be made **very** clear in the SCM description.

This final causal model after discussion looks reasonable. And one challenge that remains, however, is sampling or MAP point estimates of the posterior $P(C_i,C_j|A)$. All standard clustering methods that can use this DAG rely on node embeddings $z_i, z_j$ being positional embeddings, not structural ones. So, how are we actually obtaining $P(C_i,C_j|A)$? The authors would need to prove that standard clustering methods can perform this task (I have my doubts this is even true). I fear this issue could again derail the paper in a future submission if the counterfactual justification is used again. One of the reviewers raised concerns about the quality of the embedding already.

The proposed method is observational and does not need a causal description. But if a causal description is provided, it must be correct and the method must be theoretically sound. The causality part of the paper needs a substantial revision, unfortunately. I suggest rejection.
This paper analyzes the latent concepts learned in BERT. In contrast to previous work which tries to map embeddings to predefined
linguistic concepts this paper sets out to discover what is inherently learned by BERT. This is however easier said that done, since 
there is no easy way to inspect the embeddings and draw conclusions on what is being learned. The authors adopt a methodology which could be used to inspect the inner workings of other pretrained models. They employ hierarchical clustering to discover latent concepts and then inspect these clusters by manually labeling them. The reviewers raised various issues regarding the number of clusters, and the amount of effort required which de facto renders the approach not very portable. The authors addressed the comments and flagged several difficulties with undertaking such an analysis. I will vote for the paper to be presented as an oral for two reasons a) it is difficult to analyze pretrained models, and although I am not convinced what the authors propose is feasible, it will at least get the discussion going, b) the manually annotated dataset is useful and will go towards allowing us to perform comparisons between models c) the annotation tool will be useful to others if the authors are considering releasing it.
The paper argues that adversarial training increases inter class similarities, therby increasingly the misclassification of some classes and lowering accuracy parity across classes. It proposes to combine existing adversarial training methodologies, PGD AT and TRADES, with a maximum entropy term to improve the classification fairness while remaining robust.

While they agree that the problem is timely and important, the reviewers identify the following issues that place the current iteration of the paper below the bar of acceptance: the comparison to other works on fair robust training and accuracy parity is incomplete; experimental evaluation is conducted only on CIFAR10, making the generalizability of the paper s claims about performance unclear; and the proposed methodology has low technical novelty.
This paper aims to address the problem of cross modal semi supervised few shot learning with noisy data, and proposed a robust cross modal semi supervised few shot learning (RCFSL) based on Bayesian deep learning. The approach combines several existing techniques for tackling a new problem in a non trivial approach. Empirical results demonstrate the effectiveness of the proposed method to some extent. 

While the proposed integrated complex approach seems to be novel in the proposed unique setting, there are some major concerns from the reviewers. One concern is about the lack of clear justification on technical contributions for the proposed methodology in the complex settings. In particular, it lacks of comprehensive ablation studies for analyzing and understanding the source of gains by the proposed complex method, and the baselines in the experiments also do not look strong enough. In addition, many aspects of the paper writing and presentation are not satisfied (e.g., the math formulation in Section 2 is densely presented making it difficult to follow). 

Overall, this is a borderline case, where the paper did contribute a new method for the interesting cross modal semi supervised few shot learning task, but some major concerns on the weaknesses remain at its current form. Therefore, it cannot be recommended for acceptance. Nonetheless, I hope authors can improve the paper by fully addressing these issues and hope to see it accepted in the near future.
The paper proposes a new pseudometric for action representations.  The reviewers generally liked the work and the rebuttal helped to clarity may concerns.  However, the degree of novelty of the approach remains a concern.  In addition, a technical error was discovered by a reviewer in the revised paper.  Hence the paper is not ready for publication.
This paper studies the transferability of adversarial attacks in deep neural networks. In particular, it proposes the reverse adversarial perturbation (RAP) method to boost attack transferability by flattening the landscape of the loss function.

The reviewers acknowledge the strengths of the paper, which include effectiveness of the simple RAP method proposed and the extensive experimentation presented.

However, a number of outstanding concerns still remain. Some of them include the technical novelty of the paper, insufficient theoretical justification of the proposed method, lack of grounded justification between flatness of the loss landscape and model generalization under the specific context of attack transferability, similarity of the optimization problem with some existing work, potential difficulties of the min max attack generation problem, among others.

As it stands, this is a borderline paper that is reasonably good, but not great. Addressing the outstanding concerns will make the paper more ready for publication in ICLR.
Reviewers all found the work well motivated in addressing uncertainty, a topic that has not seen much focus in meta learning and few shot learning. They describe the challenges well: small sample sizes and OOD shift. They then propose a solution they find works well empirically to overcome these challenges based on a set encoder and an energy function respectively.

The proposal is largely one of engineering components that have been found to work well in the literature. I m sympathetic to this style of research (particularly in today s neural network research), although the reviewers raise a primary concern about whether the choices leading to the proposal are justified. In particular, two Reviewers argue that there are no clear ablations compared to alternative simpler approaches, and so the approach of selecting a Set Transformer is rather arbitrary. My perspective is that theory provides one sufficient but not necessary angle to do this, and I do find the authors  replies to the two reviewers convincing. In particular, they add a baseline to estimate covariances suggested by Reviewer Zz5v and they describe how the current baselines do in fact use the shrinkage suggestion by Reviewer QrCN.

I recommend the authors use the reviewers  feedback to enhance their submission s clarity and overall quality.
This paper tackles a very important problem of detecting depression on Twitter. As the reviewers expressed in their reviews. this paper will be of interest for the community of researchers applying ML models to mental health domain. It is unfortunate that the authors did not respond to the reviewers  concerns and questions. I strongly encourage the authors to improve the paper based on the authors comments and questions and resubmit to a future venue.
The paper introduces a theory of mind benchmark. 

This paper certainly improved during the discussion period. However, the paper is still incomplete. The authors are working related work (paper was not updated in this regard). The experiments still need significant work. The original submission used only 3 runs (very, very low). Although the authors bumped up the # of runs, the learning curves in the appendix feature very large and overlapping error bars, and the main table of results presented in the paper contains no measures of certainty those a reported in a separate table in the appendix making comparison tedious. The paper has a fairly informal approach to dealing with hyper parameters that should be discussed and improved. The reviewers pointed out (in their reviews and dialog with the authors) several ways the experiments should be extended. 

The contribution of the benchmark is evaluated primarily via experiments; much work needs to be done before acceptance.
This paper is concerned with fairness in the generative setting, specifically the setting in which various groups have very different sizes, and are therefore treated disproportionately by the model, with the group memberships further being unknown.

The reviewers generally agreed that the setting was interesting and important. However, they were critical of the writing quality, significance, and quality of the theoretical contribution.

The authors made significant improvements in the review period, and while these were not quite enough to satisfy enough reviewers, opinions clearly changed in a positive direction during the discussion period. Future changes motivated by the existing reviewer concerns should significantly improve the paper.
Three experts reviewed this paper and all recommended acceptance. The reviewers liked that the work addressed a common problem in prior related work that it is hard to quantitatively evaluate slide discovery methods. Moreover, the proposed method achieves superior performance over prior arts. Based on the reviewers  feedback, the decision is to recommend the paper for acceptance. The reviewers did raise some valuable concerns, such as paper clarity, significance of the textual descriptions, that should be addressed in the final camera ready version of the paper. The authors are encouraged to make the necessary changes to the best of their ability. We congratulate the authors on the acceptance of their paper!
Authors developed a reparameterization scheme using QR decomposition to reveal symmetries in networks with radial activation. While I welcome new ideas and formalisms from other fields, the ideas presented in this manuscript are fairly straightforward under the radial activation assumption. Although the authors claim that the results may generalize, no evidence was provided. The practical contributions are marginal and lacks comparisons with related DNN compression schemes. Through the review process, the paper has been greatly improved, but unfortunately, this interesting paper does not meet ICLR s standard as is.
The work focuses on the observation that, given a certified epsilon robust model and a certified clean input x, many inputs within the epsilon ball around x are themselves not epsilon certifiable although they are correctly classified. The authors argue that an adversary can exploit this property to produce inputs which are correctly classified by the model yet are not certifiably robust. Reviewers agreed that the paper was overall well written, the methods were clear and overall evaluated thoroughly, and many felt that the main idea was interesting. There were some concerns regarding the significance of the contribution, the primary observation itself is arguably novel but somewhat obvious, and the proposed algorithm for finding non certifiable points isn t a significant contribution when standard techniques like PGD are sufficient. Much of the reviewer discussion concerned whether or not the proposed attack made sense as a threat model. It is the AC s opinion that this discussion did not reach any meaningful conclusions. It is important to remember that the lp threat model is intended as an abstract toy game so that a formal theory of neural network certification can be developed under idealized settings. It is not intended to model any realistic security scenarios, and even more generalized notions of "imperceptible" or "subtle" attacks aren t realistic when for the bulk of applied settings real adversaries are not restricted to small perturbations in the first place [1]. The example provided by the authors regarding small perturbations of a stop sign isn t a relevant example when the adversary has more effective options, e.g. knocking over stop signs [1, Figure 3]. 

For the sake of discussion, one could consider whether or not a degradation attack would make sense under more general threat models such as content preserving perturbations. An example discussed in [1] concerns adversaries uploading copyrighted content to public streaming services—this attacker defender game is being actively played in the wild where defenders produce statistical models which attempt to flag content as semantically matching existing copyrighted content in a private database, while attackers make large semantically preserving modifications in order to evade statistical detection. An example attack would be cropping 20% of the boundary pixels of a movie and replacing the cropped portion with arbitrary adversarially constructed backgrounds. Epsilon perturbations are possible, but are almost a measure 0 subset of the full attacker action space. Suppose in the far future neural network certification advanced to the point where we could certify that a classifier was robust to all possible content preserving perturbations of a specific movie. In this case the defender would be using the certification method on their private database of copyrighted content, they would not be running the certifier on any content uploaded by users. If a movie in the private database is certified, then we already know that an attacker cannot successfully upload an adversarial version of it, it would be unnecessary to certify whether or not user uploaded content could be further perturbed in a way to become adversarial. Perhaps degradation attacks could be possible as a training poisoning attack, but this seems a bit far fetched when more traditional training poisoning attacks would be preferred. Given this, at least in this example the AC does not see how a degradation attack would make sense as a threat model. 

Given that the primary contribution of this work is a novel threat model for ML security, it is crucial that the authors rewrite their work to make more realistic assumptions of the capabilities of realistic adversaries. Starting with some of the examples discussed in [1] may be useful to the authors. Although the example of adversarial attacks on copyright detection classifiers doesn t seem to fit the degradation attack threat model, perhaps other scenarios would.

1. Gilmer et. al, Motivating the Rules of the Game for Adversarial Example Research, https://arxiv.org/pdf/1807.06732.pdf
Between a reject, an accept, and a borderline accept, this is truly a borderline paper, though I d lean slightly on the side of accepting it. The most negative review raises issues of weak baselines, along with several more minor issues. The authors rebut this reasonably well, arguing several differences from the setting used in the suggested baseline papers. It is a little hard to follow who is correct between the review and the rebuttal, though the rebuttal makes reasonably convincing arguments. Other than the baselines most of the issues raised by the negative review are more minor and can be easily fixed in a revision. The specific issues mentioned are mostly not raised in either of the more positive reviews.

The borderline (positive) review is by far the most detailed, but overall praises the paper and mostly suggests fixes in terms of better positioning the paper. Overall both the positive and borderline positive reviews make persuasive arguments as to the paper s conceptual merits, which outweigh some more minor issues.
The authors set out on an important question of whether abstract and culturally specific concepts like offensiveness can be detected in images. The novelty of this work comes in part from tackling this question and attempting to create a technology which can operationalize it.  However, despite the authors  insistence that offensiveness is not "just another label", in practice the work treats it very much that way and therefore does not present a compelling innovation either in modeling or in juxtaposition to other labeling tasks.  Known training and inspection techniques are used on existing representations and more powerful models with more training data generalize better. It is unclear what is novel in the approach or unique to offensiveness over other labels (including abstract ones).
The paper suggests that robust overfitting could be viewed as the early part of a double descent phenomenon for adversarial training. The authors identify implicit label noise, i.e. the label distribution mismatch between the true example and the generated adversarial example as a possible explanation for this phenomenon in adversarial training. This claim is empirically supported by experiments using static adversarial examples. The authors propose a method using temperature scaling and interpolation to mitigate the effects caused by implicit label noise for robust overfitting. This method is evaluated on CIFAR 10/100 and tiny Imagenet. Concerns have been raised in the reviews about sufficient justification for the claim that implicit label noise leads to adversarial overfitting. The rebuttal answers this question to some extent. Concerns have also be raised about the writing and whether sufficient details of the experimental setup are present in the main paper. While I acknowledge the difficulty of fitting all details within page limits, I would think that these details are crucial given that primary support for the claims made are from empirical observations.
This paper presents a method for training neural networks with belief propagation based algorithms. The approach is to set a fully factorized prior over weights, compute a forward and backward pass of messages on a minibatch, then set the new prior to be a slightly higher temperature version of the minibatch approximate posterior. This new prior is then used for the next minibatch, and training iterates.

There is a huge range of opinions amongst reviewers. The main thing that reviewers appreciate is the novelty of using belief propagation instead of backpropagation for training neural networks. Finding alternatives to backprop with favorable properties could be hugely impactful, so even small gains in this direction are valuable. The posterior as prior update is interesting, and the authors have clearly put in care to getting things working. The main weaknesses are that some of the experiments aren’t always reasonable and fair, the paper is framed to overstate its contribution, and there’s not a clear advantage over standard approaches (e.g., MNIST error rates for a two hidden layer network are >2%).

In the end, this is a very borderline paper, but I find Rev nNGL’s position to be most informative. In particular, the paper frames the main contribution to be message passing as an alternative to SGD for training neural networks, but this is too broad of a framing given the existence of other closely related approaches like Soudry et al pointed out by Rev nNGL. I’d recommend that the authors frame their work as an advance over other message passing based approaches to training neural networks, and to focus on piecing apart precisely why the proposed approach improves over EBP and alternatives.
This paper investigates the role of representation learning when the distribution over the feature space has a long tail. The main motivation is to determine how much of the overall learning, in this case, is bottlenecked specifically by representation learning. The main findings are that vanilla learning gives brittle long tailed representations, harming overall performance. The paper suggests a form of data augmentation to remedy this. Reviewers acknowledge that this investigation is worthwhile. However, many concerns were raised as to whether experiments support the drawn conclusions. A more principled approach to the data augmentation methodology is also needed. The authors address some of these, providing further experiments, but these were not enough to sway reviewers. Since results are fundamentally empirical in nature, this shortcoming indicates that the paper is not ready to share with the community just yet. Stronger experiments with clearer evidence are needed to fully support the thesis of the work.
The paper studies an interesting question of the relationship between the eigenvalues of the Hessian matrix with the probability of the output in the logistic loss, and use this to propose a regularization that can improve the performance of the neural networks.

 All the reviewers agree that although the question is interesting, the paper lacks significantly in terms of representation and would benefit from another round of revision.
This paper gets decent performance gains (~2% on GLUE) by soft regularization to make negatives closer to positives in contrastive learning and hard correction of too close negatives to at least avoid synonyms. These are useful ideas which to some extent build on the simple technique of ELECTRA (controlling the size of the generator MLM in Electra encourages the negatives to in general be "close but not too close", right?). As such, the paper is correct and provides potentially useful gains, but it appears a quite small adjustment of existing techniques, and in addition the use of WordNet is fairly brittle (and its similarity calculations do not consider context at all). 

The authors should be commended for the thorough job they did at updating their paper to address particular questions and concerns of reviewers, and useful new information emerged. Relative to the question of whether this method can be applied with other MLMs, the new Appendix A results do show that the answer is Yes, but the gains turn out to be much more modest (~0.5% on GLUE). However, ultimately, while this is all useful information and side experiments, these improvements just can t fix the key problem that all the reviewers felt that this paper does not provide sufficient "Technical Novelty and Significance". As such without bigger new ideas, this improved paper would probably be best as a good workshop paper.

My recommendation is that this paper not be accepted to ICLR 2022 on the basis of its limited technical novelty and significance.
The focus of the paper is kernel thinning, i.e. the extraction of a core set from a sample with good integration properties meant in MMD (maximum mean discrepancy, hence worst case) sense. Particularly, the authors propose generalizations of the kernel thinning method (Dwivedi and Mackey, 2021) which relax the assumptions imposed on the kernel (k) and the target distribution (P), and possess tighter performance guarantees.

Designing compressed representation of samples for integration is a fundamental problem in machine learning and statistics with a large number of successful applications. As assessed by the reviewers, the authors deliver important new theoretical insights in the area which can be also of clear practical interest. They also pointed out that the self containedness of the paper could be improved and additional intuition would help the dissemination of the results among the members of the ICLR/ML audience.
This work proposes to use a transformer model and language model inspired self supervised training techniques to generate local modifications of organic molecules. The use of IUPAC names coupled with language inspired pre training is indeed an interesting idea worthy of exploration. The paper has a lot of promises in this regard but needs more work to deliver it through the finish line. In the rebuttal, the authors have provided strong arguments toward the advantages of using IUPAC representation. While these arguments make sense, they are more or less conceptual and better and more clear empirical evidences are required to back them up.
The paper proposes a variational dequantization method for categorical data, based on flows with learned truncated support. The problem has been studied before, but the paper makes it clear how the proposed method differs from existing ones. The method is empirically evaluated on a large variety of diverse tasks.

The reviews were initially borderline. In general, the reviewers did not identify major quality of technical issues with the paper, and appreciated the clarity of writing. On the other hand, the reviewers were not fully convinced by the motivation or the empirical performance of the proposed method. After discussion with the authors, some concerns were allayed (especially regarding motivation) and all three reviewers decided to recommend weak acceptance.

Seeing as there are no major technical or quality issues with the paper, and the paper is clearly written and well executed, I m leaning towards recommending acceptance, although some doubts remain about the significance of the contribution.
This work suggests using models of the environment as regularizers for performing explicit transfer in RL. Here are some of the highlights from the reviews and subsequent discussions:
  * Novel problem 
  * Unclear to some of the reviewers why the problem setting is in fact important.
  * Well written
  * Interesting theoretical results
  * Somewhat limited experimental results
Post rebuttal, while there is not necessarily a great consensus, the reviewers all feel that it s an improved piece of work. While I am myself not fully convinced that the problem setting motivation truly aligns with the kind of empirical results that the work provides, on the balance I think this work is interesting and has sufficient novel contributions to be accepted at ICLR.
This paper aims to make Stackelberg Deep Deterministic Policy Gradients practical and efficient. The main contributions are an analysis which suggests terms involving the Hessian can be dropped and a block diagonal approximation to an expensive matrix inversion.

Several reviewers who voted for rejection expressed concerns about the soundness of the theoretical arguments. The response provided by the authors did help alleviate some of the reviewers’ concerns but still left significant doubts. While some of the remaining concerns could be due to a misunderstanding of the deterministic setting it is up to the authors to convince the reviewers that their arguments are sound. Given the current scores and the low confidence of the reviewer voting for acceptance, I recommend rejecting the paper in its current form.
This submission received 4 ratings, all below the acceptance threshold. The reviewers expressed concerns around overall novelty of contributions and quality of produced results, and also pointed out lack of comparisons with some prior works and gaps in empirical evaluation. The authors responded to most of these comments, but did not convince the reviewers to upgrade their ratings.
The final recommendation is therefore to reject.
This work has triggered engaged discussions between authors and reviewers and also among reviewers.
These conversations have highlighted the potential weaknesses of the contribution, namely that the work
is a proof of concept experimentally (although arguably for ethical reasons) and that the
overall theoretical contribution is not strong.

Despite the merit of this work, and given the strong expectations of ICLR, I don t feel that this work
can be endorsed for publication at ICLR 2022.
The paper proposes a new technique to handle oversquashing in GNNs by introducing a novel rewiring technique. The reviewers are quite positive about the paper and the rebuttal phase greatly helped clarify the method and it s impact.
This paper extends neural processes (NPs) to the multi task setting (MTNPs). The approach uses a hierarchical Bayesian construction, where the latent variables of an NP are conditioned on a set of global task specific context variables. This allows the NP to share knowledge across related tasks.

There were a few issues raised in the reviews. Consistently, the reviewers noted that the writing could be improved. There were variables, like the context variables M, that lacked explanation. There was also confusion between the use of a Gaussian likelihood for classification vs regression. These were resolved with the author’s response and updated draft.

There were also requests for additional experiments and baselines: 1) a synthetic task, to which the authors included a 1D regression task. 2) More baselines against other multi task methods, to which the authors included a comparison to Guo et al., 2020, MTAN, and multi task Gaussian Processes.

Finally, there were questions around whether MTNPs are valid stochastic processes. This has been proven theoretically by the authors, albeit in the appendix.

Currently, this paper remains borderline. The main remaining criticisms are a) A desire for more experiments and analysis to highlight the particular strengths of the approach. b) That the approach is a straightforward extension of NPs, and may not be sufficiently novel. c) That the authors include more baselines from the recent multi task literature (Yu et al. and Sun et al.). In the end it was determined that the paper does not quite meet the bar for acceptance. I think in future submissions, it would be worthwhile to further highlight MTNP’s performance in the low data regime, where it particularly seems to do well, and to complete the full set of comparisons (e.g., Sun et al. and Yu et al.) that were requested by the reviewers.
This paper improves the training speed and decrease the computation cost of AdvProp, which is a method that leverages the adversarial example to improve the image recognition accuracy. The method achieves the speedup by leveraging a collection of practical heuristics, including reusing some gradient computation during training. The paper is well written, well justified with empirical supports, and can be potentially useful in many vision tasks. On the other hand, some novelty of the method is incremental, and the issues regarding empirical results and claims pointed out by the reviewers need  to be addressed in the revision.
This paper proposes a feature selection method to identify features for downstream supervised tasks, focused on addressing challenges with sample scarcity and feature correlations.  The proposed approach is highly motivating in biological and medical applications.  Reviewers pointed out various strengths including potential high impacts in biomedical applications, technical novelty and significance, and comprehensive and illustrative experiments.  The authors adequately addressed major concerns raised by reviewers.
This paper provides a very large scale study on the pretraining of image recognition models. Specifically, three scaling factors (model sizes, dataset sizes, and training time) are extensively investigated. One important phenomenon observed by this paper is that stronger upstream accuracy may not necessarily contribute to stronger performance on downstream tasks actually sometimes these two types of performance could even be at odds with each other

Overall, all the reviewers enjoy reading this paper and highly appreciate the empirical results presented in this paper. There were only a few concerns raised by the reviewers but most were well addressed during the discussion period. All reviewers reach a consensus on accepting this paper and believe this study is worthy to be heard by the community.
This paper proposes a new approach to graph based active learning, using the query whether the predictions made by the current model are correct or not.
Although the theoretical underpinnings of the proposed approach are a bit weak, the problem formulation that is newly proposed in this paper makes sense from a practical point of view, and the paper makes a simple and interesting proposal that would be worth sharing with the community.
This paper furthers recent work by Tian et al. 2021 to explain how representation learning with non contrastive self supervision works. The paper accomplishes this by analyzing a family of algorithms in which DirectPred from Tian et al. (2021) is a special case. Their theoretical analysis is performed with linear networks. Overall, the reviewers questioned the added value relative to Tian et al. 2021, noting that

"The analysis of DirectSet and DirectCopy succeeds at proving that it can successfully learn a projection matrix onto an invariant feature space subspace, but essentially boils down to a similar approach as DirectPred (albeit more efficient)"

The authors in their reply state "how the representation is related to the data distribution and augmentation process," however the relative contribution and why its important isn t transparent.
The paper studies semantic type detection.
 The problem is of practical significance to  i  tabular data.
 However, in its current form, there are concerns about  the scope of novelty and technical significance.
The manuscript develops a new kind of graph neural network (a Graph Mechanics Network; GMN) that is particularly well suited to representing and making predictions about physical mechanics systems (and data with similar structure). It does so by developing a way to build geometric constraints implicitly and naturally into the forward kinematics of the network, while still allowing for effective learning from data. The manuscript proves some essential properties of the new architecture and runs experiments both with simulated particles, hinges, sticks (and their combination), as well as with motion capture data. 
Reviewers were generally impressed by the writing and clarity of the work, as well as the main results. In addition, in those cases where reviewers thought that the experiments were lacking, the authors delivered effective new experiments to address those concerns (e.g. looking  at mocap and molecular datasets). One reviewer initially scores the manuscript as a Reject/3 on the basis of concerns about novelty and the scope of the theoretical and experimental contributions of the paper. However, they adjust their score 3 >5 based on the rebuttal presented by the authors (including new experiments). The reviewer also downgrades their certainty (from 3 >2) on the basis of the engagement from reviewers offering higher scores.
Overall, the manuscript presents a promising contribution to the graph networks literature and I agree with the general consensus in favour of publication.
All the reviewers liked the paper. The proposed method contains novel ideas of learning feature representation to maixmize the mutral informatio nbetween the latent code and its corresponding observation for fine grained class clustering. The model seems to successfully avoid mode collapse while training generators and able to generate various object (foregrounds) with varying backgrounds. The foreground and background control ability is an outstanding feature of the paper. Please incorporate the comments of the reviewers in the final version.

BTW, the real score of this paper should be 7.0 as Reviewer 5wFE commented that he/she would raise the score from 5 to 6 but at the time of this meta review, ths core was not raised. So the final score of the paper should be 8/8/6/6.
This paper got uniformly strongly negative reviews.  The issue of estimating or bounding generalization accuracy from performance on the training set has a huge history and literature.  After considerable discussion the reviewers uniformly find this paper lacking in making a contribution to that literature.
The paper is a nice addition to the developing theory of implicit bias in neural training. While the results are somewhat expected, the technical aspects are fairly involved due to the adversarial component.
The paper provides a unique contribution that uses Padde approximations to approximate non linear operators for solving initial value problems in PDEs. The paper contains also a non trivial experiment with a real world dataset that showcase the impact of the proposed model. The authors have provided a strong rebuttal and therefore I recommend Accept.
An algorithm for learning prototype based nearest neighbor regression model is presented. This algorithm minimizes an MSE on training examples w.r.t. the prototype centers and the prototype outputs by a block coordinate descent. The main contribution is the optimization algorithm finding the prototypes.
Major concerns in the reviews include missing mathematical rigor, poor description of the experiments, and unclear novelty. From my own reading I would like to add that the main theoretical contribution (Theorem 1) makes assumptions that are beyond any reasonable constraint, in particular as we know for more than 40 years, that such kind of assumptions are superfluous for many, many other algorithms.

In summary, a clear reject.
Overall, this paper receives positive reviews. The reviewers find the technical novelty and contributions are significant enough for acceptance at this conference. The authors  rebuttal helps address some issues. The area chair agrees with the reviewers and recommend it be accepted at this conference.
Following a recent line of work on the implicit bias of learning algorithms, the authors consider optimization methods that incorporate momentum. The reviewers found the topic timely and interesting, and generally appreciated the novelty of the technical contributions in the work. However, several critical issues concerning the presentation quality and the positioning of the paper have been raised. In addition, some of the reviewers felt that parts of the paper were somewhat rushed and potentially misleading (mainly, those concerning deterministic\stochastic ADAM and the complexity of the models considered in the paper), and others believed that the experimental section should be made more solid to properly corroborate the theoretical analysis provided in the paper. The authors are encouraged to incorporate the instructive feedback provided by the reviewers in future revisions of the paper.
This paper modifies the loss of supervised contrastive (SupCon) learning by adding a self contrastive loss. Utilizing a multi exit network and contrasting the multiple outputs of this network, the proposed self contrastive (SelfCon) learning removes the requirement of additional data augmentation samples for creating positive pairs. The proposed SelfCon loss is theoretically connected to the lower bound of a label conditional mutual information between the intermediate and last feature. The paper focuses its study on SupCon & SelfCon M, which are multi batch variates that first augment the images, and then contrast the views between both augmented and non augmented samples of the same class, and on SupCon S and SelfCon S, which are single batch variates that only contrast between the samples of the same class and do not require additional data augmentations. A wide variety of experiments have been done on CIFAR 10, CIFAR 100, TinyImageNet, ImageNet 100, and ImageNet, but mostly with relatively small networks. 

The ratings for the paper were mixed [3,5,5,8 before rebuttal; 5,5,6,8 after rebuttal]. All four reviewers had provided detailed initial reviews, pointing out a long list of issues. The authors had incorporated these reviews to make a large number of improvements to their initial submission. After the author rebuttal period, while one reviewer raised the score from 5 to 6, two reviewers maintained their negative positions: Reviewer ZiPE is clearly concerned about the risk of accepting a method that may break as soon as a slightly larger model (ResNet50 instead 18) is used, the model is trained a bit longer, or the baselines are tuned, while Reviewer MBzi is unsatisfied with how the paper motivates its empirical construction from the perspective of mutual information maximization. 

Given the disagreements between the reviewers, the AC has carefully read the paper to provide an additional review. Some concerning observations of the AC are summarized as follows:

1. Echoing the concern of Reviewer ZiPE, the performance gain of SelfCon S over SupCon diminishes in ImageNet with ResNet 18, as shown in Table 13, making it become even more important for the authors to conduct experiments following more standard settings (e.g., ResNet 50 on ImageNet).

2. The main paper seems to suggest SelfCon S outperforms SelfCon M and SupCon outperforms SupCon S, while Table 13 in the Appendix suggests the opposite. 

3. Table 3 that compares SelfCon S with SupCon appears very misleading, as SupCon consumes more memory and computation than SelfCon S simply because it has used data augmentations. If SupCon S is used, it would take less memory and computation than SelfCon S.

4. SelfCon S adds a subnetwork to the backbone to boost its performance, so technically, it has more parameters than the backbone. Comparing it with a baseline that only uses a backbone model does not seem to be that fair. This point has not been discussed in the paper. 

5. Last but not least, echoing the concerns of Reviewer ZiPE and MBzi, the paper seems to try to validate the motivating of the added loss with mutual information maximization.  However, establishing the causal relationship between maximizing the mutual information of the intermediate and last layers and the classification performance needs much more than the correlation analysis provided in the paper. 

Given the above mentioned concerns, the AC does not consider the paper to be ready for publication at its current stage.
This paper proposed an enhanced autoencoder for collaborative filtering by adding another element wise neural network for rating predictions. Overall the scores are negative, where reviewers pointed out concerns around the motivation, time complexities, and most importantly, using rating prediction as the evaluation setting which ignores the missing not at random nature of the recommender systems. The authors didn t provide any response. Therefore, I vote for rejection.
In this paper, the authors study "team zero sum games", where two teams are facing each other with opposite objective. 

The main result is that the complexity of finding equilibrium is CLS, hence probably not polynomial. This result is obtained via a reduction to some congestion games.

Three reviewers gave a mild positive score (6) while the fourth one had more concerns. I tend to agree with the first three reviewers, with a  a personal opinion around 5 6. The paper is interesting, but could benefit from polishing here and there (I acknowledge that the related work section is more precise after discussion). 

This said, I also kind of agree with the last reviewer in the sense that the result of this paper is a bit narrow (also not really surprising, but we cannot always have breathtaking results), and I am also not sure that most of the ICLR community will be interested by this kind of result. This is not really a criticism, but this paper is really borderline, and this is what makes it fall into the rejection pile.

For instance, I think this paper would be more suited to some other conferences, more concerned about games and computations for instance (or even a journal).
The authors analyzing the VC dimension of a class of neural networks
with hard thresholds at the hidden nodes that include a low rank weight
matrix and hard thresholds at hidden units.  The bounds are independent
of the number of weights used to represent functions mapping a hidden
layer to the output.  They also provided some experiments supporting the
practicality of networks like those treated in their theoretical analysis.

There was some question about whether the VC dimension continues to be relevant.
Also, while the upper bounds have attractive properties that were highlighted by the
authors, they also are not very strong in other respects.

The consensus view overall, though, was that this is a "nice result",
a clean illustration of a generalization affect of the type that has
been of wide interest lately.
The paper attacks an interesting problem: accurately estimating uncertainties in action value estimates in offline RL. It proposes a method based on ensembles of Q functions, where we alternately train an ensemble to estimate Q(s,a) for the current policy, and then adjust our policy based on the mean and uncertainty in this ensemble. By choosing mean + \beta * [standard deviation] as the basis for our policy updates, we can be either conservative (\beta < 0) or optimistic (beta > 0). The paper analyzes the ensemble training using the Gaussian process (NTK) view of deep nets.

The largest weakness of the paper is a lack of rigor in its analysis. While its main topic is uncertainty in Q estimates, the paper does not specify a valid probabilistic model on which such uncertainty estimates could be based. The theorems analyze only a part of the algorithm (policy evaluation), and don t take into account the interplay between this evaluation and any policy updates. The theorems also do not show that the computed output distribution is relevant to the actual uncertainty of the algorithm; e.g., they do not describe a prior for which the ensemble approximates the correct posterior (nor any other similar notion). Despite these omissions, the theorems are nonetheless presented as providing a reason to trust the output of the algorithm.

On the other hand, there definitely is valuable material in the paper; the experiments are interesting (and would be even more interesting if we could compare to some notion of a correct answer for at least the small ones), and the intuition and analysis could be enlightening if presented more clearly and formally, with a better description of the connection between theory and practice. Unfortunately, the paper as written doesn t enable the reader to accurately understand and assess the contributions.
This paper presents an attempt to infer controllable aspects of the environment dynamics by imposing an architecture on a pixel prediction model, such that the prediction is the sum of an action aware and action agnostic prediction, trained on a fixed dataset gathered under a uniform random policy, in such a way as to incentivize the action aware predictor to model the residual of what the action agnostic predictor is capable of predicting on its own. The utility of the method is demonstrated qualitatively and quantitatively by comparing both the effect identification performance and performance when used in the context of intrinsically motivated RL.

Reviewers found the work to be well motivated, highly relevant, clearly described and insightful. Several reviewers pointed to the relatively limited empirical evaluation, appropriateness of baselines, and questions as to how the method would apply to richer observation spaces. Reviewer hgSF, whose score improved by 2 points following discussion, in particular questioned the motivation for the baseline method (ADM) and asked for a hyperparameter ablation study which the authors provided. Reviewer Y2tc, whose score improved by 3 points, found that several experiments poorly supported the paper s claims and was concerned about the uniform policy requirement for training; after discussion, clarifications and a softening of some claims, the reviewer was satisfied. Reviewer fGPn raised concerns around metrics and questioned applicability to first person observations, and while the former were addressed to the reviewer s satisfaction, the latter were not, and the reviewer maintained their marginal accept leaning appraisal. MeF5 s biggest concern revolves around a conceptual issue around the expectation in equation 5 which judging by the discussion was clarified but not fully resolved.

The paper has improved significantly since submission through the diligent engagement of the reviewers. The method proposed is clever and effective in the domains considered but as fGPn points out is probably limited in applicability to observation spaces wherein non controllable effects remain stationary, ruling out 3D locomotion or camera control for example. It is unclear whether this limitation is addressable, and the current version of the manuscript does not discuss this a perspective a great deal (in fact, direct mention of this  limitation is absent from the "limitations" section).

After discussion among the senior program committee, it was decided that the manuscript, while much improved by subsequent revisions, fell short of the acceptance bar this year, particularly in terms of rigorous evaluation of the method. We are therefore unable to recommend acceptance at this time.
The paper addresses the problem of uncertainty quantification in deep neural nets. The authors introduces the CaPE calibration loss to deal with the inherent uncertainty in probabilistic prediction, e.g. medical prognosis, weather prediction or collision prediction.

The paper initially received contrasted reviews: two weak acceptance, one weak rejection, and one strong rejection recommendation. The main limitation pointed out by reviewers related to the unclear definition of the problem setting, the limited contributions, and clarifications on experiments (comparison with deep ensembles). After authors  feedback, the reviewers were not convinced by the clarification on the problem setting, and there was a consensus among reviewers to reject the paper. 

The AC s own readings confirmed the concerns raised by the reviewers, and also identifies additional shortcomings of the current submission. The paper addresses the problem of proper quantification of data uncertainty (generally referred as aleatoric uncertainty), and the CaPE calibration loss should be positioned with respect to the literature on the topic. The AC thus recommends rejection, but encourages the authors to re submit their work after specifying the focus and motivation of their work.
The paper proposes a model for heart rate estimation from video. Unlike previous approaches, the method does not perform pre processing of the video (face detection, cropping, etc). The empirical results are good   on par with state of the art or sometimes better.

Most reviewers are negative even after considering the authors  responses, but there is no full consensus. Some mentioned pros of the paper are that it s practically useful to have a model without pre processing steps and that overall the technical side of the paper seems sound. Some cons are that the technical novelty is limited, the empirical evaluation is somewhat limited, and appropriate ablation studies are missing.

Overall, I recommend rejection at this point. While the paper goes in a promising direction, it is an application paper and as such it would be expected to have a more solid experimental evaluation   ideally on multiple tasks (as the paper title might suggest) and on more datasets as mentioned by the reviewers and the commenters. Moreover, an ablation study (and/or other analysis) of the proposed model would be crucial to let the readers know what components of the model actually contribute to its performance.
The paper analyses theoretically the  Matthew effect  (disparate impact) in the setting 
of the semi supervised learning and its effect on fairness and performance. 

All reviewers agree that the paper deals with a very interesting topic and important problem. 
The paper discusses and presents a thorough and convincing analysis of the effect.
There were multiple concerns raised mainly around the lack of clarity at parts of the paper.
The authors did a very good job at resolving those and bringing their submission to a good standard. 
In the rebuttal I was glad to see a great dialog evolving among the authors and reviewers. 
I congraultate both sides. 

Happy to recommend acceptance.
This paper introduces a variant of DQN optimized for desktop environments to make large scale experiments more feasible for anyone.

This paper was close. The reviewers appreciated the effort and motivation, but in the end the reviewers all seemed to think that the paper was not ready. The main contribution is framed as making DQN training more feasible, but the reviewers expected the paper to show examples of what the workflow for another architecture would look like and ideally present results for domains beyond Atari. In addition, several reviewers thought the paper could be more precise about (1) ruling out speed differences due to hardware and low level software, and (2) contextualizing the speedups reported does 3x matter, what should we expect, etc.

This is certainly an interesting direction. The AC personally thinks that if the authors take some steps to address the points above this will be a great and potentially high impact paper.
This paper studies the stochastic shortest path (SSP) problem with a linear approximation to the transition model. The authors propose a doubling algorithm for regret minimization in this setting and bound its regret. This is a theory paper with no experiments.

This paper received three borderline reviews. All reviewers agreed on its strengths and weaknesses during the discussion. The strengths are that the paper is well written and that the results are novel. The weaknesses are that the proposed solution is standard and analyzed using standard tools. The reviewers noted departures from the standard analyses but these seem to be minor technical issues. Therefore, although well executed, this paper lacks novelty. No reviewer argued for the acceptance of this paper and therefore it is rejected.
The paper presents an actor critic type of method consisting of two types of features   dynamics and tasks, in the multi task continuous control setting. While the topic of the research is interesting and relevant to ICLR, the reviewers have concerns with the novelty and technical significance of the work. Specifically, the proposed method is very similar to several other works leading to an incremental novelty. In addition, the method is evaluated only on simple environments. The concerns remain after the discussion period. 

In the next version of the manuscript, the authors are encouraged to pursue more difficult settings and modify the method to work on those problems. That would make the paper stronger, and lead to a more novel method evaluated on harder problems.
The paper introduces a method to train neural networks based on so called stability regularisation. The method encourages the outputs of functions of Gaussian random variables to be close to discrete and does not require temperature annealing like the Gumbel Softmax. All reviewers agreed that the proposed method was novel and of interest. The authors conducted extensive experiments. They also adequately addressed the concerns raised by the reviewers (e.g., theoretical foundation and computational cost).
This paper proposes a technique to improve membership inference attacks by
carefully applying "difficulty calibration" to improve the attack success
rate. The reviewers are split on this paper. They all generally agree on
the facts: the paper introduces a (somewhat) new technique and performs a
solid evaluation, but the novelty on top of prior work isn t all that high.

On the whole I believe this paper should be accepted. This paper has identified
a very clear problem with existing attacks (poor performance at low false
positive rate) and has carefully developed a way to improve on this metric. A
thorough evaluation has convinced the reviewers that this paper does what it
set out to do.

It comes down to a question of novelty then. And here the question is this:
does someone who reads this paper learn something new that wasn t obvious
before? Part of this can be novelty in the method and I agree with the
reviewers that this paper lacks novely in the method. However the paper does
not lack novelty in the ideas on the whole. While Long et al., Sablayrolles et
al., and Carlini et al. do all use some kind of low false positive rate
evaluation and calibrate for low loss, none of these papers actually go out
of their way to evaluate this fact explicitly. And so even if this paper
had no technical contribution at all, the simple measurement study in and
of itself would be a useful insight.

Machine learning research at present focuses fairly heavily on novelty
of the techniques. While this is good, it s also important to go back and
actually evaluate what we have. That s what this paper does, and it does
it well enough to be worth accepting.

The paper would definitely be improved by following some of the advice of
the reviewers and including comparisons to prior work (e.g., especially
clarifying the relationship to Sablayrolles et al. and if it is true that
this attack is a simplification of this prior one and is thus less effective)
and I hope the authors will take the opportunity to do this.
The paper shows that active learning is an emergent property of pre trained models. They show that simple uncertainty sampling improves sample efficiency by 6 times (up to 6x fewer samples for the same accuracy). This is an interesting and important observation that has practical implications. 

Initially, there were various concerns regarding the message of the paper, including the tile and use of uncertainty function in AL and lack of enough experiments that were addressed through rebuttal period. 

However, there are still remaining concerns that lead to the paper not being ready for publication. Namely, 
  (1) Clear discussion on how much of the gains are due to active learning vs pre training with respect to different cases. it is also worth investigating additional causes for the failure cases.  
  (2) there are many observations here without a clear narrative or theory.
Moreover, making the story more cohesive will strengthen the paper.
This paper aims at improving AAEs with an intervention loss. While the topic is important, the reviewers agree that

  The paper has poor clarity,
  The related work is not adequately put into perspective,
  There are concerns with technical correctness,
  Experimental evidence is lacking,

As the authors have not addressed any of these concerns, the paper can not be accepted in its current form.
The paper studies real world ML APIs  performance shifts due to API updates/retraining and proposes a framework to efficiently estimate those shifts.  The problem is very important and the presented approach definitely novel. My concern is about limited novelty of the theoretical analysis and weak experimental evaluation (just two dates, limited number of systems tested, small number of ablations). As of now the paper looks like an interesting but unfinished proposal. Looking forward to the discussion between the authors and the reviewers to address the concerns.

In the rebuttal, the authors have addressed reviewers  comments, in particular by adding additional experiments that strengthen the paper. All the reviewers recommend the paper to be accepted. It is suggested that in the camera ready version the authors will add additional details regarding the experiments, as some of the reviewers mentioned.
This paper focuses on the extrapolation ability of graph neural networks and proposes a new pooling function based on vector norm. The proposed method can be applied to replace the commonly used pooling function like max/mean/sum, and is proved able for extrapolation in a simple example. 

Overall, all reviewers tend to reject this submission due to the following reasons
  The contribution to this paper is incremental. It builds on top of the well known L p norm pooing function and extends it to allow negative values of p and an additional learnable parameter q.
  However, this simple extension is not a well behaved function for gradient based optimization, which leads to unconvinced experiments, i.e., diverse performance compared with min/max.
  more recent baselines should be compared with and it would be better to see how GNP works on state of the art model architectures on real world applications
This paper proposed MIDI DDSP, a structured hierarchical generative model which offers both detailed expressive controls (as in traditional synthesizers) as well as the realistic audio quality (as in black box neural audio synthesis). Overall the reviews are very positive. All the reviewers unanimously agree that the paper is very well written and presented a very convincing model and a meaningful step up from the earlier work of DDSP. The authors also presented a well documented website for the project and promised to release the source code. The reviewers raised some clarifying questions and minor corrections which the authors addressed during the response. Therefore, I vote for accept.
In this paper, authors study adversarial examples from a distributional robustness point of view. Reviewers had several concerns about the work and all thought the paper is not above the accept threshold. In particular, they mentioned that the presentation and writing of the paper need to be improved and results (specially the ones presented in Section 2) are not significant contributions and novel. Given all, I think the paper needs more work before being accepted.
The paper proposes a GAN framework for dynamic point cloud superresolution. It does not need scene flow supervision for training and has an interesting adaptive upsampling mechanism. Results are shown on several datasets and are reasonably convincing. Overall, all the reviewers are slightly positive about the work. After the rebuttal, all the five reviewers converged to a marginally above the threshold recommendation. The meta reviewer agreed with their assessment and would like to recommend accepting the paper.
The paper proposes using unlabelled speech data for TTS by decoupling parts of the model.
However, all reveiwers agree that the technique is already known and the experimental results are not strong enough to make advantage of training on more data.
A reject.
This paper presents a steganographic approach called Variable Length Variable Quality Audio Steganography (VLVQ) that encodes variable length audio data inside images with varying quality trade offs. However, according to the reviewers, the proposal made in this paper is not novel enough, there are many details missing in the paper, and the experimental study is far from comprehensive and conclusive. Afte the reviewers provided their comments, the authors did not submit their rebuttals. Therefore, as a result, we do not think the paper is ready for publication at ICLR.
The paper proposes a method for predicting stock market crises using a deep learning approach which combines time series stock market data with text from news articles. Their experiments show that the proposed method works better than the same model using only news or only stock price data, and a couple of deep learning baselines. All the reviewers pointed out that this paper is lack of novelty and significant technical contributions. The experiments are performed on a single dataset with incomplete baselines, and hence insufficient to support the claimed advantages of the proposed method. The writing quality is not up to the standards of an ICLR papers, with too many grammatical mistakes, typos, and unjustified arguments/claims.  The clarity of the writing is poor.

The authors did not provide their rebuttal.
The paper investigates the neural tangent kernel NTK of infinitely wide ensembles of soft trees having a particular 
soft decision functions in their internal nodes. A closed form of the NTK is presented as well as a result bounding
the changes of the NTK during training. Implications for practical training procedures are briefly discussed and 
some experiments are also reported. 

The review and discussion phase were difficult with two rather uninformative but positive reviews and a negative 
but detailed review. The latter had, however, some serious flaws. For these reasons I carefully read the paper on my own,
too. In turned out that the criticized flaws in the presentation mentioned by the negative reviewer are baseless as long as 
one already knows what an NTK is. Given the title of the paper and the history of NTKs, I personally think that 
such knowledge can and should be assumed. 
Overall, I find the paper be actually very well written. The main issue I see is that the results are not overwhelmingly 
surprising. Nonetheless, this is a solid contribution, which deserves to be published.
The reviewers find the work to address an interesting and important problem but have several critical concerns about its insufficient treatment of prior work in this area,  lack of novelty in relation to the body of existing literature.
In this submission, the authors presented a framework (GIANT) for self supervised learning to improve LM by leveraging graph information. Reviewers agree that the method is somewhat novel, the (partial) theoretical analysis is interesting, and the evaluations are strong. We thank the authors for doing an excellent job in rebuttal which cleared essentially all the questions reviewers initially raised.
While all reviewers acknowledge the relevance of such an evaluation work for the MRI reconstruction field, they all agree that the contribution has a limited fit with a ML conference like ICLR. The work is solid experimentally and will surely interest the audience of conferences like ISMRM or MICCAI. For this reason, the work can unfortunately not be endorsed for publication.
The paper proposes a methodological improvement in the Langevin based training of energy based models. The idea is to initialize the Langevin flow used to train an energy based model with a normalizing flow which learns to mimic the Langevin flow as the energy based model is being trained. The method is empirically evaluated on synthetic data and image benchmarks.

The reviewers are currently divided: one argues for strong rejection, two for weak acceptance, and one for strong acceptance. In summary, the reviewers have expressed two main points of criticism: (a) that the motivation is unclear or not experimentally demonstrated; (b) that the convergence properties of the algorithm are unclear. Regarding (a), I believe the authors have adequately addressed this concern, and in my judgement the method is sufficiently motivated. Regarding (b), the authors responses have mostly relied on non rigorous argumentation and appeal to prior work, so I don t think the issue has been addressed to the reviewers  satisfaction. Having said that, in my judgement lack of clarity regarding convergence is not a sufficient reason to reject the paper, as there don t seem to be reasonable doubts that the method doesn t converge in practice.

On balance, although the paper has certain weaknesses, it proposes an interesting and potentially useful method without major technical inadequacies, so I m leaning towards recommending acceptance.
The paper proposes a novel ensemble method, CDA^2,  in which base models collaborate to defend against adversarial attacks. To do so the base models have two heads: the label head for predicting the label and the posterior probability density (PPD) head that is trained by minimizing binary cross entropy between it and the true label logit given by the label head. During inference the base model with the highest PPD value is chosen to make the prediction. During training base models learn from the adversarial examples produced by other base models. 

The evaluation of the manuscript of different reviewers was very diverse, resulting in final scores ranging between 3 and 8 after the discussion period. While the rebuttal clearly addressed the concerns of one reviewer and several additional experimental results were added for different adversarial attacks, it did not fully addressed the concerns of another reviewer, who rated his confidence higher. He was also not convinced by the update in the revised version of the manuscript, in which crucial changes in the pseudocode describing the proposed algorithm were made, which contradicted some statements in the first version. Therefore, the paper can unfortunately not be accepted in its current version. In a future version of the manuscript, the description of the algorithm and of he role of the PPD head should be improved and experiments on another dataset next to CIFAR 10 could be added.
Thanks for your submission to ICLR.

This paper explores zero shot adaptation from a theoretical perspective.  Three of the four reviewers are quite positive about the paper, particularly after the discussion phase.  One reviewer was more negative, citing a lack of compelling experiments and some possibly restrictive assumptions.

The authors responded to these concerns, as well as the concerns of the other reviewers.  One of the more positive reviewers increased their score from 6 to 8.  I did not hear from the negative reviewer, but my feeling is that I tend to agree with the authors that the focus of the paper is more on the theoretical side.  Moreover, the authors did add some additional results to the main paper, so I am of the opinion that the paper should indeed be accepted to the conference.

Even though this is paper is on the theoretical side, please do include as strong a set of empirical results as possible in the final version.  Also keep in mind the other suggestions from the reviewers when preparing the final manuscript.
This paper builds on previous work on supermasks. It  proposes to replace binary masks by a signed supermask, i.e. a trainable, trashold based mask that can take values from { 1,0,1}. This change (in combination with the use of ELUS activation functions and an ELUS specific initialization strategy) leads to a significantly higher pruning rate while keeping competitive performance  in comparison to baseline models. 

Most reviewers agreed that the paper is well written and that the proposed approach and the experimental findings are interesting. The motivation to improve interpretability was commonly perceived as misleading. Another  downside that was mentioned is the training time/efficiency. This however, should not be taken too much into account since the work focusses on finding the smallest possible subnetwork that still performs well (without changing the weight values) and  in line with work on the lottery hypothesis   aims at understanding more about the structure of the „winning tickets“ which is interesting for itself. The paper therefore should be accepted.
This paper presents a generative model for geometric graphs.  The main contribution is to separate the representation and generation of geometry from that of graph structure and features.  Based on this idea the authors assembled a set of existing ideas and built an auto encoder style generative model for geometric graphs.

This paper sits on the borderline, with reviewers split on both sides.  I appreciate the clarifications from the authors during the rebuttal and the interactions with the reviewers.  The main concern is the novelty of this approach, as the main contribution is the idea of separating geometry from graph structure, and most other components of the pipeline already exist in the literature.  Because of this I think the paper can probably devote a bit more to this ablation study.  In particular the paper currently lacks detail about whether the size of the models were controlled when doing the ablation, which could be a confounding factor that explains why the joint model with both geometry and graph structure works better.  Also the different architecture choices may also factor into the difference, it would be more convincing if for example the same combination of multi head attention blocks and GINE networks are used for the ablated graph encoder (you can simply concatenate the features from both on all layers, or even at the end).

Based on this I would recommend rejection at this time but encourage the authors to improve the paper and send it to the next venue.
The authors explore the hypothesis of whether grounded representations can be leaved from text only. They show that a language model trained with relatively little data can make conceptual domains such as color to a grounded world representation such as RGB coordinates. The paper was positively received by the reviewers, specifically after a fruitful discussion to further clarify the points that the authors were making and their conclusions. The authors have already edited some parts of the paper, I ask them to go back and include other points that the reviewers have made. I recommend this paper for acceptance, it will generate good discussion and ideas at ICLR.
In this study, the authors propose a new graph transformer network for dynamic graph representation. To solve the challenges of static graphs learning and the temporal information aggregating, this paper introduces a Dynamic Graph Transformer (DGT) which contains three components: (1) a two tower Transformer based method, (2) temporal union graph construction (3) a complementary pre training task. Extensive experiments on the two datasets of link prediction and node classification demonstrate the superiority of the model. The ablation studies justify the effectiveness of each module in the DGT model. The reviewers has various technical issues with the paper which the authors mostly addressed (e.g., whether the nodes are static or dynamic, whether DGT is robust to noise, whether it scales to larger datasets). Overall, the contributions seem incremental. There is confusion among the reviewers as to whether the proposed model differs from prior art. It seems to me there actual  differences  but whether they major or minor is open to interpretation. Overall there reviewers were not particularly excited about model/results/contributions.
In this paper the authors proposes to use partial optimal transport to align point cloud in the presence of noise and partially observed data. To this end they express the partial Wasserstein Kantorovich Rubinstein duality and use it to adapt the classical WGAN loss to partial OT. The optimal alignment between point clouds is then done by minimizing their proposed loss where the dual potential are modeled as deep neural network hence approximating the partial Wasserstein while being solved using mini batches. Experiments show the interest of the method on a few well understood examples with ground truth from the Stanford repository. 

The paper had originally borderline scores with some reviewers concerned about the theoretical contribution. The authors did a very good reply that that greatly appreciated by the reviewers, the one with the lowest score deciding to increase it. The new numerical experiments on the 3D human dataset were also appreciated. The consensus during the discussion was that the paper is worth accepting but that the authors should take into account the comments form the reviewers and better explain their contribution on the KR duality.

For these reason the AC recommends to accept the paper but urges the authors to take into account the comments from the reviewers. In particular the authors should better highlight their theoretical contribution and explain the link and differences with unbalanced minibtach OT.
This paper proposes asymmetry learning for learning counterfactual classifiers, i.e. classifiers which are invariant to certain symmetry transformations w.r.t. hidden variables that differ between the training and test sets.

The reviewers universally agreed that the proposed setting, and theoretical contribution, were interesting and novel. They also praised the writing quality, but had some quibbles about the quality of the experiments, and discussion of prior work. Neither of these concerns were considered significant enough to be a barrier to acceptance, but the authors should try to improve them, if possible.
The paper introduces an interesting new model for MDPs, where the time is divided into random segments, and at the end of each segment the cumulative reward for the given segment is communicated to the agent. Some theoretical results with a policy improvement algorithm, as well as a more practical algorithm are presented. While the reviewers valued these contributions, they all had issues with the presentation of the paper.

These presentation issues make the paper extremely hard to follow   this was a problem for all reviewers, and I also verified it myself. The reviewers also raised issues regarding the experiments, where the algorithms should be tuned properly to be able to draw valid conclusions.

While unfortunately the above issues prevent me from recommending acceptance of the paper, the authors are strongly encouraged to revise their paper and resubmit to the next venue, with a special emphasis on making the presentation proper. There are several problems/recommendations mentioned in the reviews which will certainly help in this regard (I would also add that special care should be made that everything is defined properly, e.g., the equation for your policy iteration should appear in the main text not in a proof in the appendix, or $\hat{Q}_\phi$ should be defined, etc.).
This is an extremely interesting and timely paper regarding the approximation ability, with statistical consequences, of circuits and (computation bounded) Turing machines by feedforward networks and transformers. The paper has an interesting and valuable setting, and also many unusual ideas, together which can inspire a lot of future work.  Unfortunately, the reviewers had significant difficulties with the presentation and setting; the Transformer material in particular lacks clarity.  As such, the paper could use more time and polish.

Separately, I will recommend in the future that authors consider making use of the rebuttal and revision phase.  While it is not strictly required, it seems that in ICLR, scores shift quite a lot in those phase, and it has (for better or worse) become standard to have a thorough involvement in this phase.  It was difficult to cause score changes after the initial phase due to the lack of review responses.  That said, I sincerely hope the authors continue with this valuable line of work.
This paper presents a Actor Critic Hedge (ACH) method for 1 on 1 Mahjong. It is is an actor critic method for approximating Nash equilibrium strategies in large extensive form games. ACH extends the CFR family of algorithms that uses deep learning and model free training (not using full game traversal). The propose ACH agent defeats several human players, including a Mahjong champion. This is impressive.

The reviewers and authors have extensive discussions and the authors managed to address most of the concerns from the reviewers. The overall opinions from the the reviewers favor acceptance. Below are some of the strength and weakness summarized from the reviewers:

Strength:
* Extensive experiments and impressive performance. 
* New policy based algorithm for competitive environments.
* Reviewers  questions are well addressed.

Weakness:
Lack of more tabular theoretical analysis. Need experiments to compare to existing methods. Theory and experiment does not match.
In this paper, the authors propose to use segmentation priors for black box attacks such that the perturbations are limited in the salient region. They also find that state of the art black box attacks equipped with segmentation priors can achieve much better imperceptibility performance with little reduction in query efficiency and success rate. Hence, the auithors propose the Saliency Attack, a new gradient free black box attack, that can further improve the imperceptibility by refining perturbations in the salient region.
The reviewers think that the proposed method is simple and important, and the authors have responded properly to some comments. 
However, the reviewers still are not satisfied with the experimental evaluation and comparisons, as the authors can only try to compare with other ideas and test more models in the future.
In summary, I think the manuscript at its current staus cannot be accepted.
The paper is well motivated and tackles a hard and long standing problem with seq2seq models: diversity and controllability.
The authors propose simple architecture for controllable text summarization. They use multiple decoders controlled by a gating mechanism which can be learnt or controlled manually. They control mostly the abstractiveness and specificity properties of the model.

Pros
+ the proposed approach is somewhat novel (several earlier work have proposed multiple decoder models to control the generation   as pointed by the reviewer team) 
+ the proposed modifications are motivated well, the approach is simple and easy to understand. 
+ the paper is well written and easy to read.
+ the authors made an effort to address most of the reviewers comments even added human evaluation scores (which was asked by reviewers)
+ It seems a highly flexible way of enriching existing models in a simple way for additional control behavior in output summary generation of documents.

Cons
+ During discussions, reviewers have circled around the novelty and continued to raise concerns about the weaknesses of benchmarks and comparison to related work and the fact that the proposed model has more parameters is potential advantage over other models that might contribute to the performance gains. Thus, the paper could be made stronger with further evaluations that could possibly make it stand out.
This paper adds to the literature of efficient sparse attention for long range transformer architectures: a learned hash function is proposed by building successfully upon contributions from previous work. A similar idea appears in contemporary work, but with clear and complementary differences.

The reviewers are convinced of the importance of attention complexity, bucket imbalance issues, and agree that learning to hash is is a promising solution. The authors have clarified almost all outstanding concerns, in some cases adding valuable new results (e.g. timing experiments.)

I echo the reviewers  concern and stress to the authors to clarify the precise meaning of "plug and play", as it may be misinterpreted (e.g., no fine tuning needed? or just no change to model but still fine tuning is needed.)

Some of the cited papers are accepted at conferences, please update your bib file with the correct information for credit attribution.
The paper proposes a method to learn the stride of downsampling in deep networks using a  gradient based learning approach. The main idea is to work in the frequency domain and to learn the cropping mask in that domain. The authors also introduce a regularization for applications seeking computationally and memory efficiency. The authors investigate the interest of the approach on a number of datasets with audio and image data. 

The reviewers praised the paper, appreciating the elegance of the approach and the effort made to thoroughly evaluate it. The reviewers also appreciated the clarity of the exposition and the care in the reporting of the results. The reviewers also expressed some concerns about several choices of the design (stride sharing) and the lack of detail in some experiments (computational /memory efficiency). Finally, the reviewers wished the paper had more theoretical grounding.

The authors submitted detailed responses to the reviewers  comments. After reading the responses, updating the reviews, and discussion, the reviewers found that the responses were ‘reinforcing [their] initial assessments  and their several concerns were satisfactorily addressed. Moreover several of the reviewer’s suggestions clearly already led to an improved manuscript with very thorough experimental evaluation and simpler approaches for stride sharing. 

The paper proposed an elegant, learning based, approach to one of the most important design choices in deep network architecture design: the strides in the convolutions. The authors provided a careful and thorough experimental evaluation, and moreover improved it during the review process following the reviewer’s feedback. 

Accept, definitely. 
The paper gives a framework for learning temporal logic rules from noisy unlabeled data. The key novelty is a formulation of combinatorial rule search as an end to end differentiable problem. The method is evaluated on a video dataset and a healthcare dataset. 

The reviewers liked the high level ideas behind the paper. However, the conclusion was that the experimental results, while interesting, are still somewhat preliminary (in particular, the baselines are weak).  I agree with this point and am recommending rejection this time around. However, I urge the authors to develop the paper further and submit to the next deadline.
This paper suggests that in multi label classification (where there are multiple y that could be correct), the usual conformal prediction setup could be too conservative (too large a set with too many false positives), because it asks for "full" coverage. They propose to change the error metric from coverage to precision, to instead output a smaller set, with higher precision, potentially with the loss of coverage. The paper thus produces two variants of conformal prediction: guaranteeing that the expected number of false positives is at most k, and that the probability that the #false positives being > k is < \delta. The experimental study is interesting.

I followed the extensive discussion thread, and appreciate the authors  and reviewers  willingness to engage. However, in the end, the (excellent) reviewers were somewhat still not enthusiastic about the paper, and with nobody willing to champion the paper, it ended up on the borderline, in the bottom half of my set. Nevertheless, I read through the paper in detail myself to make sure, but I find enough reasons that suggest that the paper is not ready for publication currently, and would benefit from a significant overhaul.

It seems like the final algorithm is a slight variant of nested conformal prediction (a well known and oft cited paper by Gupta et al, 2020 that the authors seem to miss) in the following sense. The usual conformal for classification framework would order the labels in terms of a score (like posterior probability) and then return a set of labels whose score is less than some threshold. (The same style of procedure can be used for the single label and multi label case also.) The nesting appears to be the same high level framework used in this paper, except that the threshold is chosen by a different rule from standard conformal prediction (ie same nesting, different threshold). Writing the algorithm more transparently will make for a tighter connection to the conformal literature   at the moment, the authors claim a fair bit of novelty, but this is partially due to the omission of this reference and the cleaner broad (nested conformal) framework under which this work (as well as standard conformal) sit.

At the same time, I was not fully convinced of the central theoretical claim of the paper, in Proposition 4.6. The authors claim that since Theorem 4.3 is simultaneously valid for all j in B satisfying a condition (the filtered set), the theorem can also be invoked for a data dependent index (chosen via (12)). This does not appear to the true, and at the very least requires careful justification. At a high level, dropping the conditioning for simplicity, Thm 4.3 reads as "forall j in [B], E[A_j] <  c", but this does not imply that for a data dependent j hat, we have E[A_{j hat}] <  c. It would have been true, if the forall j appeared as a sup_j inside the expectation (or as a forall j inside the probability, rather than outside). The authors may want to clarify more carefully, if it is indeed true.

Minor: I also continue to find typos in the main results and proofs. These are minor, but should be corrected. I believe that in the last line of Theorem 4.3, the X_i should be X_{n+1}. In the display after (25), that should be T_k, and not T_{k,\delta}. There appear to be some other potentially missing references as well. For example, while distribution free conformal approaches are cited, distribution free calibration approaches are not, within the related work section. At the same time, some very recent papers, such as by Bates et al, or Angelopoulos et al, appear to be overemphasized in the introduction. A more fair coverage of related work could be useful.
The paper introduces the CareGraph, a knowledge graph based recommendation approach. 
CareGraph is a deep neural network based recommender that can be used a mobile healthcare platform
for nudge recommendation. The main motivation is to use the knowledge graph to 
mitigate cold start problems when recommending nudge messages. 

The papers  main strength is the topic of interest. Research on recommending systems in the healthcare context is of great interest.
However, the reviews raised concerns that outweigh the strengths. 
The majority reviewers agree that the work is not ready for publication.
Main concerns focus on weak experimental section and lack of technical details.

I recommend the authors to incorporate all the reviewers  comments and make a 
stronger submission to a future conference!
Motivated by the recently proposed EigenGame, this paper proposes an unbiased stochastic update to replace the biased one in the original EigenGame. The new algorithm is asymptotically equivalent to EigenGame, enjoys better parallelism on big data, and beats EigenGame in experiments. Some reviewers are originally concerned about the lack of finite sample convergence results. After the author s response and reviewer discussion, this paper does get sufficient support. Therefore, I recommend acceptance and encourage the authors to think about how to deliver a finite sample analysis in future work.
The paper considers the problem of black box optimization and proposes a discrete MBO framework using piecewise linear neural networks as surrogate models and mixed integer linear programming. The reviewers generally agree that the paper suggests an interesting approach but they also raised several concerns in their initial reviews. The response from the authors addressed a number of these concerns, for instance regarding scalability and expressivity of the model. However, some of these concerns remained after the discussion period, including doubts about the usefulness for typical applications in discrete black box optimization and some concerns about the balance between exploration and exploration. 

Overall the paper falls below the acceptance bar for now but the direction taken by the authors has some potential. I encourage the authors to address the problems discussed in the reviews before resubmitting.
The paper proposes a modification of the Dreamer method to account for safety constraints.
There is agreement among the majority of reviewers that the paper lacks in novelty (with respect to Dreamer), in the safety analysis, and in convincing experiments.
I encourage the authors to take the detailed reviews into account when improving their work.
The paper presents an asymptotic analysis of the convergence of the last iterate of mSGD and Adagrad. This result extends previous work providing stronger results under weaker assumptions. Even if these topics received less attention from the community, they are key problems in stochastic optimization.

The reviewers and I had several doubts about the proofs and relation with previous work. However, the rebuttal phase essential acted as a minor revision process. In fact, the authors fixed all the issues, convincing the reviewers (and me) that the results are novel, correct, and interesting.

For the above reasons, I recommend the acceptance of this paper.
The paper studies the problem of character generation using reinforcement learning for generation/parsing. All the reviewers recommended reject due to insufficient experimental investigation to support the ideas. The authors did not provide a rebuttal. Hence, the reviewers  opinion still remains the same. AC agrees with the reviewers and believes that the paper is not yet ready for publication.
The paper proposes numerical method for solving SDEs that empirically are faster than previous approaches. Two reviewers felt the paper was above threshold, while two felt it was below threshold for acceptance. While the paper is borderline in this sense, all four reviewers noted that the paper lacked a theoretical justification and rested on empirical evidence for the usefulness of the approach. Several reviewers also pointed out that these empirical results are on the weak side. While the paper may add a potentially useful learning trick to the optimization literature, these two significant concerns put it on the side of a borderline reject.
This paper analyses generalization ability of graph neural network from the aspect of the distance between the test data point and the training data point, where the labels of a part of the vertexes are observed as the training data and a test data point is selected from the remaining vertexes. The theoretical result indicates that if the training data "cover" the whole vertexes of the graph, then the test accuracy will be better. This theoretical finding is supported by some numerical experiments. 

The problem this paper considers is interesting and would be worth investigation. However, the theoretical results presented in the paper are based on quite strong assumptions, and the statement of the theorem is not well exposed.  
  First, the paper assumes that a distortion map is obtained by training and the training procedure can produce zero training errors. Although these assumptions are far from obvious in practice, the paper lacks justification of these assumption. Hence, these assumptions seem to be made only for the sake of proof.  
  Second, the constants appeared in the theorems are not correctly specified. How different constants are correlated is not properly exposed.  

As for the experiments, they are not so strong: only Cora is experimented and training data size is small.  
For these reasons, this paper is not sufficiently matured to appear in ICLR.
The paper proposes a new perspective on the generalization performance of interpolating classifiers based on the entire joint distribution of their inputs and outputs. It conjectures that, when conditioned on certain subgroups, the output distribution matches the distribution of true labels. The conjecture is investigated empirically on a number of datasets and models, and proved to hold for a simple nearest neighbor model.

This paper generated varying responses from the reviewers and a detailed discussion. One main concern focused on whether the feature calibration conjecture is actually surprising, given standard expectations about generalization from learning theory. Indeed, from the discussion and the paper itself, it seems the authors conceived of classical generalization as a statement about whether train performance $\approx$ test performance, whereas one reviewer remarked that "what it really talks about is concentration of measure." I agree with the importance of this distinction in general, though it is perhaps less relevant in the current setting of modern interpolating classifiers, for which so little about generalization is understood in the first place. In particular, the empirical observations of varied forms of good generalization behavior for overparameterized models are likely to be interesting to the community, regardless of whether this behavior might be expected in the large sample limit.

As such, this is a very borderline paper, with many good arguments both for and against acceptance. After a detailed discussion among the chairs, it was decided that the current version is just shy of the acceptance threshold, but I would strongly encourage the authors to address the main reviewer concerns and resubmit a revised manuscript to a future venue.
In this paper, the authors study the decentralized empirical risk minimization problem with Reproducing Kernel Hilbert Space. I found the problem formulation and the solution quite interesting. The authors also answered the main comments of the reviewers. Even though part of the work is incremental, I feel that there is enough merit to accept this paper.
### Summary

This paper presents a technique to reduce the worst case latency of inference. The key idea is to use a combination of early exit and filter selection to achieve its results. The filter selection predicts the top k classes for the input and, using that indication, uses the filters that are the most relevant  (using DeepLIFT) to refine the result.

### Strengths (from Discussion)

  The idea is interesting. Early exit, mixtures of experts (one potential interpretation of the filter selection here), as well as pruning are interesting mechanisms for neural network efficiency. 
There may be new opportunities to find synergies in their combination.

### Weaknesses (from Discussion)

  The clarity of writing could be significantly improved, particularly in the description and illustration of the constituent techniques. Figures, such as those in https://arxiv.org/abs/2008.13006, that clearly present the constitution of various layers, in particular, would help.

  There are relevant and applicable baselines that a comparison would contextualize the strength of the approach (as per Reviewer vKUc examples)

  ImageNet experiments appear to be within reach of this experimental apparatus (i.e., without extreme cost). Hence, such experiments would validate the applicability of this approach to practice. 

  A small point arose that longest path inference was not motivated. Work on optimizing tail latency (https://research.google/pubs/pub40801/) may be helpful contextualization here.

### Recommendation 

My recommendation is Reject. The work here is a very promising start for a new idea. Though requests for additional experimentation and baselines can be ill defined recommendations. Here, scaling of the results to ImageNet as well as comparing against baselines in the literature (as per Reviewer vKUc s examples) would provide much stronger scoping for this work.
This paper proposes a theory for understanding the context representation in pretrained language models. The strengths of the paper, as identified by reviewers, are in the importance of an attempt to explain contextualization in language models, and in the novelty of using the category theory to model the connection between contexts and their representations. However, all the reviewers identify several major weaknesses, including flawed/incoherent definitions of concepts in the proposed theory and insufficient experimental results. Although the authors  rebuttal put a great deal of effort to address raised concerns, all five reviewers agree (and provide very detailed justifications along with suggestions for improvements) that the work is not yet ready for publication.
The submission evaluates the relationship between (logarithmic) Dice loss and cross entropy loss, arguing for a similar decomposition into ground truth and "hidden label marginal biases."  The submission received mixed reviews, with two reviewers voting for rejection, and two feeling that it is marginally above the acceptance threshold.  Setting aside the numerical scores, there are reasons to believe that this submission, while interesting, has shortcomings that limit its relevance to the wider ICLR community.  These include
  Very many losses have been proposed for imbalanced classification / (medical) image segmentation, such as Jaccard and Tversky index or ranking measures, although admittedly Dice is probably the most popular in the medical imaging literature due to historic reasons.  Arguably, Dice is less well behaved from a theoretical perspective compared to other options (e.g. it does not even form a metric), and may not be the most relevant point of departure for a representation learning conference.  The literature review misses many relevant papers on such losses, including papers that specifically are focused on the relationship between Dice and cross entropy, e.g. Eelbode et al., IEEE TMI 2020 and citations therein.
  The empirical results do not show substantially improved results compared to baselines.

On the balance, this does not cross the threshold for acceptance to a competitive venue such as ICLR.
The paper initially received negative reviews; the authors did a good job during the response period: two reviewers have updated their scores to 6. The AC has carefully read the reviews, responses, and discussions, and agreed that the authors have also mostly addressed the concerns of reviewer gsUt as well. It is unprofessional for reviewer gsUt to not engage in discussions after multiple requests.

The AC however also agrees with reviewer seqp that the new changes are major, and submissions are supposed to be evaluated in their initial form. Further, neither of the positive reviewers would like to champion the paper. 

The final recommendation is to reject the paper. The authors are encouraged to further improve and flesh out the paper based on the reviews for the next venue.
This paper investigated using bio inspired cumulative fatigue model to improve bipedal locomotion via deep RL. The proposed method marginally improved bipedal locomotion behavior. The size of the experiments should be improved, together with generalization to other symmetric walkers. AC agrees with the reviewers that the empirical performance is not significant enough. The paper may fit the scope of a bipedal locomotion journal/conference better than ICLR.
This paper studies meta learning in hierarchical RL, where the unknown hierarchy is learned during meta training and then applied to a test task. The authors propose an optimistic algorithm for solving this problem and analyze it. The main contribution of the paper is in the first end to end analysis.

This paper has three borderline reviews and one reject. Despite the differences in the scores, all reviewers share the same opinion. The idea is novel and very interesting. However, the algorithm and its analysis rely on many assumptions, many of which are introduced in this work and not properly discussed. Because of this, the paper needs a major revision and is rejected for now.
The paper proposes BitRand, a bit aware randomized response algorithm, to preserve local differential privacy in federated learning. The main idea is to take into account the bit indices and prioritise higher order bits focussed towards achieving a utility which is higher than other algorithms which are oblivious to the floating point bit representation. Additionally, the analysis allows the bit randomization probabilities not to be affected too much by the dimension of the data.

Overall, the paper lay right at the borderline of acceptance. The paper s core idea, their development and experiments were all liked by the reviewers. The main issue the reviewers brought up was the writing and structuring of the paper and the presentation of the overall results. Most reviewers agreed that the paper presentation was not ready to match the bar for ICLR. There are multiple suggestions that reviewers have made   through their questions and direct comments and addressing those and rewriting the paper to highlight these aspects better will significantly improve the paper.
The authors present an improved method to convert ANNs to spiking neural networks (SNNs). First, a network with quantized activations is constructed, then it is converted. They analyze the conversion errors theoretically. In addition to previously considered errors [Li et al. 2021] they also consider an error they call "unevenness error" and propose a way to compensate for that.
They test the method on data sets such as CIFAR 100 and show good improvements over previous methods with respect to classification accuracy and inference time.
The reviewers agree that the manuscript presents interesting and valuable work with a significant novel contribution.The manuscript is well written.

Weak points according to the first reviews were:
  Lack of ImageNet conversion experiments.
  Analysis of energy consumption was missing.
  More related work needs to be compared.
The revision addressed all these points, This was acknowledged by the reviewers with increased ratings. All reviewers propose acceptance.
The paper presents a new defense against backdoor attacks based on the discovery of homogeneous populations in the training data and subsequent filtering of poisoned data due to its difference from the said populations. The method has a solid theoretical foundation which, however, requires strong assumptions on attacks and benign data. Due to these assumptions the theoretical guarantees alone cannot ensure that the defense is robust against adaptive attacks. The experimental validation of the proposed method is limited to one benchmark datasets (CIFAR), additional results are briefly presented in the response but not elaborated on.
While all reviewers applaud the motivation to bridge the gap between machine learning and bioinformatics communities, they also raise a number of concerns regarding the choice of tasks and of baselines, and about the accuracy in their description. They feel the paper is not ready to be published in its current form, and we hope that their comments will help the authors prepare a revised version for the future.
This paper follows the recent line of work of theoretically analyzing the Neural Collapse phenomenon, by making certain simplifying assumptions on the problem setup. In this case, the authors use cross entropy loss on an unconstrained model where second to last representations become free variables. Their main results characterise the NC as the only global minimiser. 
Reviewers were positive about this work, and concluded it presents a valuable addition to the growing analysis of NC. They also pointed out several clarity issues that should be addressed in the final revision, including a more objective comparison to prior work. Ultimately this work will be an interesting addition to the conference, and therefore the AC recommends acceptance.
The paper proposes a new method for representation learning of time varying graphs which uses a streaming snapshot model to describe graphs on different time scales and meta learning for adaption to unseen graphs. Reviewers highlighted as strengths that the paper proposes an interesting approach for modeling temporal dynamics in graphs which of interest to the ICLR community. However, reviewers raised also concerns regarding the novelty of contributions, the empirical evaluation (also with regard to related work), as well as the clarity of presentation. In addition there was no author response. All reviewers and the AC agree therefore that the paper is not yet ready for publication at ICLR at this point.
The paper suggests a non random strategy for selecting minibatches of nodes for training graph neural networks. The main argument is that consecutive memory accesses are faster than random accesses, and thus they claim a 20x speedup per epoch by precomputing batches at a small cost to accuracy. 

There are a number of discussion points. One reviewer finds the results hard to believe because previous work has shown that runtime sampling can be fully pipelined. The authors agree but say their speedups are still better, which isn’t a fully satisfying response, and it calls into question the quality of the baseline implementation. Another concern is about the effect of deterministic minibatches. The authors argue that the empirical results speak for themselves, while the reviewer worries about robustness. There also are some concerns about methodology around hyperparameters and special casing of preprocessing for one dataset, though those appear mostly resolved.

On the whole, this is a borderline paper that lands just on the side of rejection. I’d encourage the authors to more thoroughly address the questions about quality of the baseline implementation and the reviewer’s concern about robustness of deterministic minibatches, and then resubmit to the next conference.
To overcome the challenge of lacking task specific unlabeled data in semi supervised learning (SSL) or knowledge distillation (KD) tasks, this paper presents a new framework called "generate, annotate, and learn (GAL)" that uses unconditional language models to synthesize in domain unlabeled data to advance SSL and KD. Extensive experiments on both NLP and tabular tasks demonstrate positive results of the proposed method.  

Reviewers generally agree on several key strengths of the paper, e.g., the paper is well written, literature review is comprehensive, experimental results are generally positive (the improvements over the standard baselines on GLUE benchmark looks solid despite not very significant). On the negative side, some reviewers did raise some major concerns about the novelty of the proposed framework and the lack of strong baselines for comparison. For example, the proposed GAL framework doesn’t seem particularly novel as neither of the proposed components is new, and the key value of the work seems on the contribution of evaluating the large LM s ability to generate good in domain unlabeled data (as agreed by authors). Therefore, it is very important to compare with other existing data augmentation baselines in the empirical studies. While authors did try to add one round trip translation (RT) data augmentation baseline for comparison during the rebuttal, more stronger SOTA data augmentation baselines should be compared. 

Overall, this is a good paper which is worthy of publication in near future but it still needs some more work on more extensive comparison of more baselines and improvements on the writing of novelty and contribution claims.
The paper proposes a set based neural subsampling model that selects both features and instances using a two stage model. The motivation is to allow for scaling to large datasets by first subsampling using an initial stage that does not model second order interactions (which would require work quadratic in the dataset size to model), and then following up with a second more sophisticated pass that includes second order interactions. Its results show an empirical improvement over previous methods in the case of very small subsample sizes.

The responses from the reviewers in discussion were varied—and often off base for all reviewers—and as a result I took an even more deep look at this paper than usual. I think the variance in reviewer response is a symptom of the fact that the paper is somewhat confusingly written, and sometimes has parts that give the opposite impression to what the authors intend. For example, the author response says "we emphasize again that our method is a subsampling method. This is very different from core set selection methods" but then Figures 11 and 12 explicitly label the subset produced by the algorithm a "coreset." There is general confusion as to what exactly is being subsampled (features or instances) and even what datasets were being evaluated on—it s not that the information is not there, but rather that it s easy to miss while reading the paper. We can see this happening where most of the reviewers were misunderstanding or making factual errors about the paper, and I can see how this happened by reading the paper.

The reviewers also shared some concerns about the baselines, and indeed some things about the baseline comparisons are confusing: for example, Figure 4 seems to report DPS having below 70% accuracy on MNIST while subsampling to a size of 25, but the original DPS paper (Huijben et al, ICLR 2020) in their Figure 2 reports a percent error of 6.6% (at Pixels removed: 96.8%, which I believe corresponds to keeping 25 pixels as 28*28*(1 0.968)   25). This does seem to back up the reviewer s speculation that "the baselines are most likely unfairly weaker." The presentation of the results should make this sort of thing more clear (if the setup isn t the same as DPS s MNIST setup, how does it differ? if the setup is the same, as seems to be the case the way the paper is presently written, why are the result accuracies so different from what is published in the DPS paper?).

The reviewer comment about theory is not one I count against the submission. Although it is certainly true that this paper would be greatly strengthened by some theoretical backing, it is also part of a line of work that eschews theory—so we cannot reasonably disqualify it for doing so.

To sum: although the technical contributions of this paper do seem to be significant, I expect that if the paper is published as presently written, it will confuse ICLR readers just as it has confused our reviewers. This leads me to lean against accepting this paper at this time.
All reviewers have agreed that the topic of evaluating compositional skills of agents is an important one and cast it as compositional learning as meta reinforcement learning is an interesting approach. At the same time, reviewers have raised concerns with respect to the benchmark itself, the exposition and clarify of the ideas as well as the experimental evidence used to support some of the claims. The authors have not provided an author response but have acknowledged the reviewers feedback. 

As this paper stands I cannot recommend acceptance for the current manuscript.
This work studies the problem of building powerful representations of low dimensional point clouds with permutation and rotational equivariance, with the motivation to tackle applications in the physical sciences. Their main technical contribution is the use of the so called geometric algebra, a series of operations between scalar and vector quantities that respect rotational symmetries, which the authors then combine with attention mechanisms to provide permutation symmetry. 

Reviewers generally found this work full of interesting ideas, in particular the novel geometric algebra structure to deal with rotational symmetry. However, they also found several issues, such as lack of clarity and somewhat unclear experimental validation. In particular, the authors are encouraged to formalise the rotational equivariance property, and to further address the "small" aspect of the title. Taking all these considerations into account, the AC recommends rejection at this time, but encourages the authors to pursue this exciting line of research.
This paper proposes an identifiable nonlinear ICA model based on volume preserving transformations. The overall approach is very similar to the GIN method published @ ICLR 2020. There is a weak consensus among the reviewers that this paper has some merit, although none pushed for acceptance. After reviewing the paper myself, I agree that the contributions here appear to be incremental, but the results do push this growing field of identifiable latent variable models forward.
The paper proposes Markov coding game (MCG), which generalizes both source coding and a large class of referential games. All the reviews are negative. The reviewers think the work is not ready for publication in its current form.
This paper proposes Hindsight Foresight Relabeling (HFR), an approach for reward relabeling for meta RL. The main contribution is a measure of how useful a given trajectory is for the purpose of meta task identification as well as the derivation of a task relabeling distribution based on this measure.

Reviewers agreed that the paper tackles an interesting problem and found the main insight to be simple and intuitive. While the initial reviews raised some concerns regarding novelty, the performance gap, and using the learned Q function to estimate post adaptation returns the rebuttal did a good job of addressing these concerns. Overall, the paper proposes a non trivial extension of hindsight relabeling to meta RL and while the results could be stronger I think the paper provides useful ideas and insights so I recommend acceptance as a poster.
This paper proposes a graph soft counter (GSC) model which is very simple and lightweight  compared to the conventional graph neural network for solving QA tasks that benefit from knowledge graphs. Compared to the conventional KG GNN combination, the proposed method is much simpler but produces better results for QA tasks. The paper originally dealt only with multiple choice QA tasks, but during the rebuttal process, the authors added more complex QA tasks which the reviewers appreciated. Additionally, there was (and still remains) some concern over the exact reasons and mechanisms behind this "too good to be true" result, and the authors addressed this with additional ablation studies, to be included in the appendix. With the publicly released code, others will be able to try GSC and its too good to be true performance and figure out how it actually works.
The paper proposes a transformer model of code that leaves "holes" at points of generation at which the model is uncertain. The model is evaluated on C# and Python programs and outperforms existing techniques. 

The reviewers found the Grammformer model and the RegexAcc evaluation metric to be useful and interesting. The experimental results are also compelling. Given this, I recommend acceptance. Please make sure to incorporate the feedback in the reviews and the additional experimental results into the final version.
Although the submission studies an interesting question, many parts are not clear enough and the presentation needs to be improved. I encourage authors to revise the paper accordingly and resubmit in future venues.
All reviewers agree that the presented approach to fair calibration of face verification models is interesting and needed in the field. The method does not require access to sensitive attributes for calibrating, which makes it sustainable. The reviewers are satisfied with the presented experimental studies in most cases. The rebuttal addressed a large majority of additionally raised questions. I believe that the paper will be of interest to the audience attending ICLR and would recommend a presentation of the work as a poster.
This paper carefully shows how all the stochastic elements in neural network training could be removed (by using full batch, and a dataset with fixed augmentation) and still maintain good performance, by adjusting hyper parameters and adding explicit regularization. 
All reviewers were eventually positive and recommended acceptance, except one reviewer, who was initially not aware of the recent theoretical interest in this question, and was therefore less surprised.

There are three remaining issues with the current version: 
1) Operations in cuDNN are, by default, non deterministic, but can be made deterministic. Though I believe this will not affect the final results, without it, the title and conclusions of this paper are technically unjustified. The authors have agreed to add this to the camera ready version of the paper.
2) Deterministic training was only shown on CIFAR10. I understand ImageNet would be too heavy for this task, but there are many other small and medium sized datasets, and I think showing that this on several such datasets would strengthen the message of this paper, and convince more readers. 
3) The question of how to achieve good performance with deterministic training is still mostly unknown, as it seems to require significant hyperparameter search (with unknown sensitivity), and no conclusion was reached regarding the how to properly adjust them. However, I agree that a good answer to this question might not exist.

Lastly, I recommend the authors to mention in the main paper the new baseline experiment, where the explicit regularization is added to SGD. I saw it in the appendix, but I didn t see it mentioned in the main paper (maybe I missed it). I think without it, many readers will not be fully convinced (as several reviewers requested it).
The paper proposes an interesting hypothesis about deep nets  generalization behavior inside RL methods: it suggests that the nets  implicit regularization favors a particular form of degeneracy, in which there is excessive aliasing of state action pairs that tend to co occur. It proposes a new regularizer to mitigate this problem. It evaluates the hypothesis and the regularizer empirically, and it provides suggestive derivations to motivate both.

The reviewers praised the comprehensive empirical analysis, the insights into learning, and the combination of empirical and theoretical evidence. The authors participated responsively and helpfully in the discussion period, and addressed any concerns raised by the reviewers.

This is a strong paper: it derives and motivates a novel hypothesis about an important problem, and analyzes this hypothesis both mathematically and experimentally.
This paper evaluates interpretation methods of neural networks on time series data. The reviewers find some values in this work, but were also consistently concerned with the main theme and novelty of this work. The authors have actively responded to reviewer comments, but the reviewers were not convinced with the major contributions and novelty. Thus the work is not ready for ICLR.
The paper was praised for being clearly written, well motivated, and for addressing an important problem: measuring intrinsic robustness.
It improves the previous results on intrinsic robustness based on concentration of data distribution, by incorporating the constraint on the label uncertainty of the models.
This requires information on label uncertainty for each data sample rarely available (here CIFAR 10H is considered), but could open new directions for future work on adversarial robustness, confidence calibration or label noises.
This paper adds an attention mechanism to deep variational autoencoders.  The authors develop a global + local attention method and achieve better log likelihoods than a variety of recent methods on MNIST and OMNIGLOT.  Overall the reviewers found this paper strong (8, 8, 8, 6), particularly after the author rebuttal.  They found the paper to be clear, the contribution sensible and novel and the experiments thorough and compelling.  In particular, the authors added additional experimental results on a larger dataset which addressed a common concern among the reviewers.  Thus the recommendation is to accept the paper.
Overall this paper was discussed at length given the high variance in scores, and it was ultimately felt that the paper was a borderline paper and there was not enough enthusiasm to warrant acceptance. Several concerns in the discussion could not be resolved, in particular the bounds might not be tight, or even useful, and more explanation on the dependence of various parameters involved and assumptions involved is needed. Specifically, as pointed out by a reviewer, there is a concern about the parameter epsilon_3. It seems for natural input distributions epsilon_3 would be so small that the upper bound would scale as n^3 (given the 1/epsilon_3^2 dependence), which is then trivial since it is larger than n. The reviewers were not satisfied with the authors response regarding this.
This is an interesting paper working on the difficult problem of learning from video demonstration. The authors provided convincing experimental solutions for visual representation, domain adaptation, and imitation. It would be a nice ICLR paper.
In this paper, the authors extend the performative prediction framework of Perdomo et al. (2020) to a multi agent, game theoretic setting, and they examine how and when multi agent performative learning may lead to performative stability/optimality.

The authors  results and contributions can be summarized as follows:
  They consider a multi agent location scale distribution map with parameters constrained in a simplex, and they study the dynamics of an exponentiated gradient descent algorithm (EGDA for short) inspired by Kivinen and Warmuth (1997).
  If the learning rate is small enough, the authors show that EGDA converges to a performatively stable point (under the same assumptions that guarantee existence of a convex potential).
  On the other hand, if the learning rate is large, the algorithm behaves chaotically.

The reviewers  initial assessment was mixed, but after the authors  rebuttal, some concerns were partially addressed and the scores of the paper were upgraded to borderline positive. On a point by point basis, the authors  result on the convergence of EGDA with a small learning rate was appreciated by the reviewers, but it was not otherwise deemed significant enough relative to existing convergence results for gradient methods. Instead, most of the discussion centered on the authors  result on chaos (Theorem 4.6), which was viewed as the most significant contribution of the paper. However, continued discussion between committee members revealed that this result follows directly from Theorem 3.11 and Corollary 3.12 of the arxiv preprint "A family of chaotic maps from game theory" by T. Chotibut, F. Falniowski, M. Misiurewicz, and G. Piliouras <https://arxiv.org/abs/1807.06831>, which is not discussed in the paper. [As was pointed out, the update map (7) of the paper coincides with the update rule (7) of the arxiv preprint, and the proof techniques are likewise identical.]

This overlap with previous work was considered a "big omission" and it pushed the paper below the acceptance threshold. In the end, the paper was not supported by any of the reviewers, so a "reject" recommendation was made.
In this paper, the authors proposed a method for causal inference under limited overlap   an important and understudied complication.  The authors propose to recover a prognostic score using a variational autoencoder, and thereby map a higher dimensional set of covariates with limited overlap to a lower dimensional set where overlap holds, and such that ignorability is maintained.

The paper was reviewed quite favorably by reviewers, and the authors updated the manuscript to address specific issues raised by reviewers.
The authors introduce a GNN based method for classifying irregular multivariate timeseries.
They represent the dependencies among sensors using a graph structure and deploy message passing to  
model the effect of a sensor on another)s . The approach jointly learns embeddings and the dependency graph. 

The manuscript gathered a clear accept (8) and two marginal below the threshold scores (5). 
I want to accept this work and I explain why. 

The reviews and the ongoing discussion during the rebuttal showed that the work is interesting 
with its main strength being the novel exploration of GNNs application on irregularly samples multivariate 
time series.

There were many concerns raised by a reviewer regarding important  theoretical and methodological issues in the paper. 
During the rebuttal phase, the authors clarified and resolved the majority of the concerns and there was an ongoing discussion among the two sides, authors and reviewer (which I have to admit was a pleasure to watch researchers communicating). 
The authors took into account the feedback and revised the manuscript accordingly. Having read the edits myself, I believe the submission is substantially improved and addressed the concerns sufficiently. 

I expect that this work will stimulate further research in the community and I would like to accept this.
This manuscript makes an interesting observation: there is no reason why planning based methods like MDPs must be limited to physical or grounded environments. One can plan about more abstract textual domains. It adapts the standard methods from planning to such text domains in a fairly straightforward way. The fact that concepts from MDPs map to these problems directly is an asset: ideas could flow between these domains in the long term. While the original submission was lacking clarity and significant technical details, the authors engaged with the reviewers and resolved lingering concerns. Reviewers are unanimous that this a strong contribution.
This paper presents a semantically controllable generative framework by integrating explicit knowledge. In particular, a tree structured generative model is proposed based on knowledge categorization. Reviewers raised concerns about technical details, experiments, and missing references. In the revised paper, the authors provided more justifications and clarifications, such as the definition of adversarial attack. During the discussion, reviewers agreed that the previous concerns have been partially addressed, but there are still concerns on experiments, e.g., more recent work should be considered as baselines.

Overall, I recommend to reject this paper. I encourage the authors to take the review feedback into account and submit a future version to another venue.
The paper studies poisoning attacks of small perturbations to training samples. Existing attacks introduce features that allow for easily fitting the data, but do not lead to good generalization. They use this principle to generate attacks based on random class specific patterns. They finally propose defense using a pre trained model whose last layer is fine tuned. Several criticisms of novelty of the work in comparison to prior work were raised during the reviews. At the same time the validity and effectiveness of defenses remains unsubstantiated to a large extent. Although the authors put an effort in addressing these concerns, for the most part some reviewers and myself remain critical of the contributions and novelty of this work.
This paper is proposed to deeply investigate the hot refresh model upgrades of image retrieval systems. The hot refresh model is very useful since the model can be quickly updated after the gallery is backfilled. To address the model regression with negative flips, this paper introduces a Regression Alleviating Compatible Training (RACT) method by reducing negative flips. The proposed method has been verified on the large scale image retrieval benchmark such as Google Landmark. The key contribution of this paper is the new setting targeting an important application in real world image retrieval systems. However, some of the technical details are not fully explained. Despite these minor concerns, the AC will rate this paper as a poster acceptance based on the overall contributions.
Current meta learning algorithms suffer from the requirement of a large number of tasks in the meta training phase, which may not be accessible in real world environment. This paper addresses this bottleneck, introducing a cross task interpolation in addition to the existing intra task interpolation. The main idea is very simple, which can be viewed as an incremental adding up to existing augmentation methods. However, the method is well supported by nice theoretical results which highlight the relation between task interpolation and the Rademacher complexity. In fact, this is not a trivial extension of existing work.  Authors did a good job in the rebuttal phase, resolving most of concerns raised by reviewers, leading that two of reviewers raised their score. All reviewers agree to champion this paper. Congratulations on a nice work.
This paper deals with solving the problem of scheduling machines in a semiconductor factory using an RL approach. As the different actions take a different amount of time to complete, the authors propose to use a predictron architecture to estimate the targets in DQN. The experimental results show that the proposed method outperforms the considered baselines on two scheduling problems.

After reading the authors  feedback and discussing their concerns, the reviewers agree that this paper is still not ready for publication.
In particular, the main issues are about the novelty/similarity with respect to related works, the lack of theoretical insights and formal definitions, the effectiveness of the presented benchmarking, lack of analysis of some unexpected results.

I encourage the authors to take into consideration the concerns raised by the reviewers when they will work on the updated version of their paper.
This is an interesting and carefully presented work which discusses how to implement finite width NTKs more efficiently.  Overall, the reviews were slightly tending positive, though with a variety of concerns, including some concern that the contribution is not sufficiently substantial.  In my own perusal of the paper, personally I feel it could be made more compelling if (a) more speedups could be considered, including ones with various tradeoffs, for instance via randomized linear algebra, (b) explicit consequences on various prediction tasks, rather than plotting wall clock times (e.g., as this paper cites many works which tried to use finite width NTK, and as this paper claims massive speedups, then it will be able to repeat some of those experiments at much larger sizes, which should lead to interesting and valuable larger scale experiments which ideally have some new phenomena, but are even interesting if they simply confirm the smaller scale phenomena).  As a separate concern, I second the comments of one reviewer, that part of this paper s contribution is to a single software package, which is moreover listed in the abstract (and not just part of the standard code release, e.g., as a footnote); this feels a little strange, like an announcement of a code release, and further limits the impact to general machine learning researchers (for instance, I feel completing some of my preceding suggestions could result in, say, researchers who use other software feeling eager to re implement this).  Overall, I urge the authors to continue with their interesting work and aim to resolve these concerns and those of the reviewers.
The premise of this paper is that the development of time series forecasting methods has traditionally focused on accuracy rather than other criteria such as training time or latency. This work presents a new benchmark, evaluating classical and deep learning based methods on a number of public datasets. They also propose a technique, ParetoSelect that is able to select models from the Pareto front that can efficiently select models in a multi objective setting. 

Reviewer XSBL liked the observation that classical methods do not always beat deep learning methods even for very small datasets. They thought that the empirical contribution was valuable and myth breaking. They also commented that the evaluation was robust. Their main concerns were: inadequate description of hyperparameters, lack of evaluations on *really* small datasets, missing confidence measures for latency results. They also made some suggestions for improving clarity. The reviewers responded, pointing to a description of the hyperparameters in Table 2 of the appendix. They also responded to the reviewer’s comment about very small datasets and explained the advantage of the ranking loss. They made some small adjustments to the paper based on the clarity comments. 

Reviewer PZ2f also thought that the large scale comparison was valuable for the community. Overall they thought it was well written though could be improved w.r.t. Notation and writing style. They even inspected the code. Their primary concern was that the paper lacked focus and “tries to do too much and too little”. Is this a benchmarking effort of previous methods, or is the main contribution the ParetoSelect algorithm? This reviewer thought that due to its superficial coverage of too many things, and it wasn’t ready yet for publication at ICLR. The authors provided quite a comprehensive response to reviewer PZ2f and pointed to some minor improvements in the manuscript.

Reviewer rQb3, like the others, viewed the benchmark analysis as valuable. They thought that the ParetoSelect approach was “natural” and that it was shown to be effective over baselines. Like PZ2f they had some structural criticisms and pressed for more insights. 

Reviewers XSBL and rQb3 continued to engage in discussion through the AC reviewer discussion phase. XSBL said that the authors’ response addressed some concerns yet raised others w.r.t. hyper parameter selection. rQb3 updated their review after considering the author s response, feeling that minor concerns were addressed but the paper could still use further development. Overall, after considering the discussion I think that it’s been difficult for the authors to provide any patterns regarding which model performs best for which datasets. To me, a benchmark paper should provide some deeper insight and the paper appears to be struggling to do that. On the other hand, the study is comprehensive. The authors have argued in their response to all reviews, that their evaluation is at quite a different scale compared to other published time series model evaluations. I think that this benchmark paper can provide value to the community yet it could use further work: specifically the authors need to focus the paper and communicate clearer insights from the study.
This paper proposes loss functions to encode topological priors during data embedding, based on persistence diagram constructions from computational topology.  The paper initially had some expositional issues and technical questions, but the authors did an exceptional job of addressing them during the rebuttal period nearly all reviewers raised their scores (or intended to but didn t update the numbers on their original reviews).

The AC is willing to overlook some of the remaining questions. For example, concerns that topology isn t well known in the ICLR community (8muq) are partially addressed by the improved exposition (and it s OK to have technically sophisticated papers so long as some reviewers were able to evaluate them).  And, future work can address scalability of the algorithm, which indeed does seem to be a challenge here (ey6b).

In the final "camera ready," the authors are encouraged to address any remaining comments and to consider adding experiments/discussion regarding scalability to larger datasets.
As pointed out by some reviewers, the proposed method basically puts progressive training in the federated context. The theoretical analysis only concerns the centralized or non federated setting and thus give no insight or guidelines for progressive training in federated learning. The main advantage of saving communication mainly comes from the simple observation that less parameters are computed and communicated during each round before the full end to end stage. However, this may cause extra overhead in hyper parameter tuning including number of stage, learning rate schedules and stage wise warmup. Despite its potential effectiveness in practice, the current version of the paper falls short of the acceptance bar due to the weakness in novelty and relevant theory for federated learning.
The paper proposed a new architecture called Regional to Local attention for the vision transformers. The idea is easy to understand, the model adopts the pyramid structure and adds a regional to local attention instead of using the global attention. The architecture is well motivated and the paper is generally well written.

The main concerns from the reviewers are mostly clarification questions. The authors did a good job addressing them. Apart from those, most reviewers raise the novelty issue of such architecture, which I would think is a drawback of this paper.

I am leaning towards the acceptance of this paper mainly because of its experimental results. It is the best in my batch and I think there is a significant improvement over the previous approaches.
This work proposes a new strategy for prioritized experience replay. It is based on the argument that the TD error itself may not be a good indicator for priority, so we should rely on other factors that are easier and more reliable to learn. The new method is based on two modifications: (1) modifying the critic s objective so that it learns a good model of the environment (reward and transition dynamics) as well, and (2) use the combination of the TD loss and the model loss in order to define the priorities in the ER queue.

The majority of reviewers are positive about this work. They believe the method is novel and the experiments are extensive. The authors improved the paper during the discussion phase, so many of the questions have already been answered.

There are some concerns though, some of them shared by reviewers and some after my own reading of the paper:

One concern is about the justifications for the method, which are based on the heuristic and intuitive arguments, rather than principled approach. Currently it is not clear, at least to me, why adding a model error to the objective is a good idea.

Another concern, which is not shared by the reviewers, is that there is much overlap in the confidence band of figures and confidence intervals of tables. For example, in many of the subfigures of Figure 2 or 4, there is a significant overlap in the shaded areas. Or many of the numbers in Table 1 (with and without MaPER) are within each others  confidence intervals. Are the results statistically significant?

Another comment, again not shared by reviewers, is regarding how the loss functions are defined. Consider the loss in Eq. (1):
  Is there a squared missing? I assume that it is missing. Although it does not matter at this stage, when you add other terms to the loss, it would, i.e, the minimizer of $f(x) + g(x)$ is not necessarily the same as the minimizer of $f^2(x) + g^2(x)$.

  Is the target value based on a fixed parameter (not optimized), or do you actually consider the expected of the TD error, which would be equal to the empirical Bellman error. If the latter, it would be a biased estimate of the Bellman error. And it is not what DQN or the TD method optimizes (that s why Sutton and Barto s textbook calls them pseudo gradient).

These requires some clarifications.

Another question related to the model: Is it assumed that the model $T_\theta$ is deterministic and predicts a next state, as opposed to predicting a distribution over them? (cf. equations after (5) )?

All strengths and concerns considered together, I believe this is a good paper overall, and can be accepted at ICLR. Hopefully we get a better understanding of what this method is actually doing in the future research.

I have the following suggestions to the authors:
  Perform statistical significant tests on your results. In some cases, it might be helpful to increase the number of runs from five to a larger number. It may also be more visually clear to provide standard error instead of standard deviation.
  Clarify issues about the definition of the loss function.
  Please consider improving and clarifying your argument of why you method works.
  Please consider the remaining comments by reviewers in order to improve your paper.
This paper presents an approach to high quality waveform synthesis using multi band decomposition. The resulting synthesis speed is substantially faster the past work on both CPU and GPU   a feature that all reviewers viewed as a significant strength. However, the majority of reviewers raised concerns about discussion of and contextualization within past work, as well the novelty of the proposed approach. Finally, one reviewer pointed out a potential concern with experimental evaluation (sample rate of proposed system outputs vs baseline s). Author response clarified the relationship with some past work but did provide evidence to mitigate the concerns about experimental evaluation. Overall, this paper could still benefit from another round of review.
All reviewers are very positive about this paper. The reviewer with the lowest score did independent experiments that show that the authors  method works well, and has had an extensive discussion with the authors that justifies a higher score. The paper is potentially very valuable to practitioners, since it shows how to compensate for a training set that is not representative of the test data.

Suggestion from the area chair to the authors: Briefly discuss the relationship between influence scores and propensity scores, which are standard in the literature on causal modeling and on sample selection bias, as in https://jmlr.csail.mit.edu/papers/volume10/bickel09a/bickel09a.pdf for example.
This paper is a fair effort, making some headway on a problem of practical importance. 
There was some discussion of scoping and whether the contribution was Machine Learning y enough. 
I m kind of ambivalent on that particular question: I think the general rule is that the further out of scope the paper seems, the better the results need to be for people to overlook it. 
I think in this case, unfortunately, even the two most positive reviewers did not evince enough excitement about this paper for it to get accepted in light of the scoping concerns. 
Given the various constraints involved, I don t think I can recommend acceptance.

In order to get it accepted into a future conference I would recommend either:
a) Submit to a more Software Engineering focused venue
b) Really shore up the evaluation such that the reviewers sympathetic to this kind of paper will find it unimpeachable and score it more generously.
This manuscript tackles an interesting and significant line of research of long term prediction and out of distribution generalization in time series models. I strongly believe this problem is an important one to solve. However, in its current form, its novelty is marginal, and the experiments fail to decisively show advantages. It also lacks of systematic improvements and error analysis. Further work could make it ready for publication at a next conference.
This work proposes a federated version of the classical $\chi^2$ correlation test. The key new step is the use of stable projection to reduce computational overheads associated with the use of secure multi party protocols. Overall while the contribution is of interest the novelty is rather limited. I also consider the work to be somewhat outside of scope for ICLR. It would be more suitable for a security or statistics focused venue. Therefore I do not recommend acceptance.
This work formulates the Adaptive Mesh Refinement (AMR) problem used in solving Finite Element Method (FEM) as an MDP, and suggests an RL based solution for it. Most reviewers agree that this is a novel problem and the solution is promising. There are, however, several issues raised by our reviewers, who have expertise ranging from ML to computational methods to solve PDEs. Some of the concerns are:

  As this is not a theoretical work, the burden of proof is on the empirical evaluations. Some reviewers found the experiments very small and not convincing enough.
  The paper does not compare with the state of the art AMR methods.
  The detail of how the problem is formulated as an MDP can be improved.

Given that four out of five reviewers are on the negative side, unfortunately I cannot recommend acceptance of this paper in its current form. Nevertheless, I believe this is a promising application of RL. I d like to encourage the authors to consider the reviews in order to improve their work, and resubmit it to another venue.
The paper provides new insights about how to identify latent variable distributions, making explicit assumptions about invariances. A lot of this is studied in the literature of non linear ICA, although the emphasis here is on dropping the "I". I think more could be said about how allowing for dependencies among latents truly change the nature of the problem since any distribution can be built out of independent latents, by some more explicit contrast against the recent references given by the reviewers. In any case, the role of allowing for dependencies in the context of the invariances adopted is discussed, and despite no experimentation, the theoretical results are of general interest to the ICLR community and a worthwhile contribution to be discussed among researchers in this field.
This paper presents a method to interpret neurons in the vision neural models by generating natural language description that specifies the activation selectivity of a given neuron. The proposed method first identifies an exemplar set of input image regions that corresponds to a neuron, then searches a natural language description by optimizing the point wise mutual information between descriptions and the exemplar set.

Strength:
  Reasonable method design and clear writing
  Important problem and broad applications
  Extensive experiments for evaluation of the proposed method

Weakness:
  Need more discussion on the limitations of the proposed method
  Elaboration on the human inter annotation agreement
  Analysis on method transferability across tasks.
The paper proposes to fine tune the belief states of a MDP, for later using the learned model for decision time planning, e.g. via search.
The contribution is well presented, motivated and focused to a specific scenario, which is generally considered challenging in the literature. This scenario is exemplified by the cooperative card game Hanabi, which takes the role of the benchmarking setting for the empirical evaluation of the fine tuning procedure.

The major concern raised in the review and discussion phases are about the limited evaluation, which is centered around only Hanabi, as well as the magnitudes of the improvements over previous baselines. However, three knowledgeable reviewers agreed that since the setting has been historically challenging, the reported improvements are in fact significant and potentially inspiring future works in this direction. 

The paper is accepted provided that the authors include and polish in the camera ready the additional experiments over the parameter sensitivities, the ablation tests and the discussions highlighted by the reviewers in the comments.
The reviewers generally agreed that the ideas presented in the paper are interesting and novel. However, all reviewers also agreed that the paper is quite preliminary in its current form: the particular approach, while sensible, appears to be somewhat heuristic, and the evaluations are not as complete as necessary to fully evaluate the proposed approach.

Generally, my sense is that there is something quite interesting in this work, but the present paper is too preliminary for publication. I would encourage the authors to take the reviewer comments into account and improve the work into a more complete submission for a future venue.
The authors propose a novel method for conditioning deep neural network. They replace the activation function with a linear combination of activation functions (e.g., ReLu). The weights for the activation functions are dynamically computed from the input during inference and training. The approach is evaluated on standard public tasks and shows improvement over well established alternatives.

Pros
+ A simple novel method for condition that is widely applicable
+ Adequate empirical evaluations to demonstrate it s effectiveness

Cons
  No major weakness

The reviewers provided several feedback. The authors incorporated the suggestions and clarified residual concerns. The revised version of the paper has improved the readability and utility substantially.
This paper introduces a convolution where the kernel is parametrised continuously over time (in the context of recurrent networks) to address vanishing gradients issues, by using another neural network to generate the kernels.
This is a meaningful idea, addressing an important problem.
The paper is well written and clear. The idea is novel (parametrised kernel already exist, but the way it s used here is new).
The experimental section is solid, although some reviewers suggests it could be extended with more baselines.
All reviewers recommend to accept the paper, therefore I also recommend accept.
We appreciate the authors for engaging in discussions with the reviewers and providing further experimental results to clarify and address the concerns raised by them in their original reviews, leading to changes in some of the recommendations.

While the (revised) paper with the clarifications and new results incorporated are more ready for publication, some outstanding concerns should preferably be addressed to further enhance its quality.

The authors are highly recommended to take into consideration all the comments and suggestions of the reviewers to further revise their paper to make it a scholarly work to contribute to the ICLR and the more general ML community.
The paper introduces a procedure to control the churn (i.e. differences in the predictive model due o retraining) using distillation.

This is a strong paper, with novel technique which is clearly presented, and is backed by sound theory. The experimental results were also deemed extremely convincing by reviewers TJ4g and pZBb. 

Reviewer nqfu raised a question about the similarities between churn reduction and domain adaptation. The authors have addressed this by pointing out similarities to their work but also noting that, in the settings mentioned by the reviewer, alternative approaches such as completely retraining the model might be more appropriate. This part of the rebuttal is convincing.

Reviewer TJ4g has pointed out several points of improvement, to which the authors have responded adequately.

All in all, this paper is ready for and deserving of acceptance.
This work gives an interesting perspective on combining options with exploration in the non tabular case. The reviewers have raised a number of important areas for improvement (primarily missing ablations to support the claims of the paper, but also specific suggestions about improvements to the text), and feel that sufficient work is required to address these that the paper should be rejected at this time.
All reviewers suggested acceptance of the paper based on that the paper addresses an important problem and presents and validates interesting ideas for approaching it. Therea are some concerns regarding limited experiments   I d like to encourage the authors to make an effort to address these concerns and also a few others raised in the reviews in the final version of their paper. The authors already made several updates to their paper in that regard during the discussion phase so I believe that the paper would be an interesting conttribution to the conference and I am recommending acceptance of the paper.
This paper studies the problem of kernel similarity matching using Hebbian neural networks. Specifically, the authors propose to compute the approximate feature map for the kernel using the least square loss function, Legendre transformation, and Hebbian parameter update rules.

Reviewers generally agree that the proposed method is interesting. However, there are major issues with the current manuscript, both theoretically and empirically. Theoretically, there is no guarantee for the convergence of the method. In fact, the non convergence of the approximation error, when the dimensionality (number of features) increases, indicates that the proposed method is not consistent, in contrast with other methods such as Kernel PCA or Nystrom approximation based methods. As observed by the authors, this may be related to the unstable convergence of the stochastic gradient descent ascent optimization procedure. For consistency,
the approximation error needs to approach zero as the number of features becomes large. 
Overall, empirical results compared with existing methods are not  satisfactory. As the authors themselves point out in their discussion, "this method does not provide the same sorts of theoretically guarantees or empirically observed robustness of sampling based methods".

The authors are encouraged to take  reviewers  comments and suggestions into account to improve their current work.
I thank the authors for their submission and active participation in the discussions. This papers is borderline with three reviewers leaning towards acceptance [3c96,7T33,Zhvq] and one leaning towards rejection [o38w]. Reviewer o38w s main concerns are around the lack of details about how the baselines were tuned and missing training details (specifically the connectivity test used to reject candidate environments). During discussion both, reviewers Zhvq and 7T33, agree that the paper requires substantial restructuring/rewriting to properly address the reviewer s feedback which is currently mostly addressed in the appendix. Based on the discussion with reviewers, my assessment is that this paper is not ready for publication at this point and that it will benefit greatly from another iteration. I want to very strongly encourage the authors to further improve their paper based on the reviewer feedback.
Three out of the four reviewers raised various concerns on motivation clarify, result significance, and unclear writing. While the authors provided their rebuttals, unfortunately no reviewer seems to have changed their mind. AC reads the paper and agreed this paper perhaps needs major revision before publishing in a major venue. However, the technical ideas are still interesting and promising; the authors are suggested to carefully take into account reviewer comments during revision.
This paper suggests the use of networks for supervised learning which are composed of a bijective network (e.g. a flow) followed by a separable function. This allows easy integration over the input space, which can be used to formulate novel regularizers (examples given are for local consistency, and for out of distribution detection).

The approach is pretty novel, and it s an interesting paper. The reviewers were very divided, however; one reviewer giving it a 1, and another an 8, with the other two reviewers arguing weakly to accept. The "1" took issues with the general formulation, feeling that the necessity to optimize bounds on the true objectives in cases such as softmax regression greatly limits the viability of the work. Personally I disagree with that reviewer s characterizations of the novelty and significance of this work, and I think the OOD detection / classification experimental setting is sufficient to make their point that their approach can be applied in such settings.

In the end I (AC) would agree with the "weak accept" / 6, given the draft of the paper at this time. While I think a few things could be presented more clearly, and I think the empirical evaluation could be more robust (e.g. exploring what goes wrong when the integrated objective functions are included on CIFAR and SVHN), and it would be nice to explore additional applications, I think this is a creative paper which it would be nice to include at ICLR.
This work proposes to study the generalization capabilities of RL algorithms using contextual decision processes (CDPs). CDPs allows to study generalization similar to how we are used to studying generalization in supervised learning, and can separate the generalization capabilities of a learned agent wrt observation, state and action space. This proposed measure for generalization is used in an extensive study on grid world domains to evaluate existing algorithms that aim to improve generalization.

**Strengths**
This manuscript is well written and the work is well motivated
A novel perspective and way of measuring generalization of learned agents
An empirical study that compares existing algorithms on how well they generalize in observation, state, action spaces

**Weaknesses**
Some clarity issues existed (missing links to existing literature, experimental details) 
empirical study is (out of necessity) limited to small scale grid worlds
no deeper analysis of the results, why do algorithms perform the way they do from this novel perspective of generalization, which makes it hard to understand how one could choose an algorithm for larger scale settings which don t allow for this type of analysis

**Rebuttal**
The authors updated the paper to improve the parts that were unclear, and had an extensive discussion with reviewers on the intuition of the results and converging on take aways. Unfortunately, this intuition and take aways have not been added. 

**Summary**
While I understand the authors wish to not speculate on intuition, I agree with the reviewers that without (experimentally supported) take aways the provided analysis is incomplete. Understanding why each algorithm achieves the performance they do wrt this novel way of measuring generalization is the only way the proposed method to measure generalization and the evaluation can be used to draw conclusions about more general problem settings. Thus, although this is a very promising direction on an important problem, the manuscript is not ready yet for publication.
In this manuscript, the authors study the relatively unexplored problem of how to characterize and assess the adversarial vulnerability of classification models with categorical input. Even certifying the robustness of such classification models is intrinsically an NP hard combinatorial problem, the authors show that the robustness certification can be solved via an efficient greedy exploration of the discrete attack space for any measurable classifiers with a mild smoothness constraint. 
Overall, the theoretical analyses in this paper are rigorous, and reviewers seem to be satisfied with the responses from the authors.
Based on the three positive reviewers, this manuscript is recommended to be accepted.
In the end, this paper essentially proposes a minor variation on an idea that 1) has been published before, 2) is not used extensively at all, and 3) seems applicable (in its current form) only on deterministic environments.  This, without additional insights or analyses, seems too marginal a contribution for acceptance.

The paper is not poorly executed, and the authors engaged well during discussion, for which I would like to thank them.  I would like to encourage the authors to consider the reviewers comments, and in particularly perhaps answer more clearly and directly what they are adding to the literature.  It could be that there is something particularly insightful in the detailed differences with past work, but this has not become sufficiently clear to me during this discussion phase.
This paper proposes a class of neural processes that lifts the limitations of conditional neural processes (CNPs) and produces dependent/correlated outputs but that, as CNPs, is inherently scalable and it is easy to train via maximum likelihood. The proposed model is extended to multi output regression and to capture non Gaussian output distributions. Results are presented on synthetic data, an electroencephalogram dataset and on a climate modeling problem. The paper parameterizes the prediction map as a Gaussian, where the mean and covariance are determined using neural networks. Non Gaussian prediction maps are obtained using copulas. 

Technically speaking, the reviewers found the approach to be incremental and only marginally significant and I agree with them. Issues such as estimates of computational cost, using fixed lengthscales for the covariances and relationships/using normalizing flows have been addressed by the authors satisfactorily. Empirically, the contribution of the paper is somewhat significant, as it provides similar flexibility to other more computationally expensive processes and more general assumptions than conditional neural processes.
Inspired by BERT and the corresponding masked language modeling objective, this paper proposes masked image modeling as a pre training technique for vision transformer. More precisely, the image is tokenized using a pre trained tokenizer, and the goal is to predict the token indices corresponding to masked patches of the image. As noted by the reviewers, the proposed method is simple, works very well in practice and the paper is well written. Since this work potentially opens a whole new research direction, my recommendation is to accept with oral presentation.
The paper proposes Data Dependent GCN (D2 GCN), which improves the efficiency of vanilla GCN by node wise skipping, edgewise skipping, and bit wise skipping. Gate functions are learned to prune the unimportant neighbor nodes in combinations, unimportant edge connections, and in the bit precision. The proposed method boosts efficiency while achieving comparable performance over benchmark datasets. Most reviewers agree that the paper is well motivated, and the writing is clear. However, two of the reviewers found the novelty of the paper compared to previous work (for example, [1]) is limited. Three reviewers raised concerns about the lack of theoretical or empirical analysis on how D2 GCN can alleviate the over smoothing problem, and how the proposed method can serve as an implicit regularization.

For the novelty concerns, the authors provided a detailed comparison with previous work during the rebuttal. For the lack of analysis on over smoothing, the authors provided an additional empirical analysis using the distance of different intermediate layers’ output as the metric for measuring over smoothing. But at least one reviewer is still not satisfied with those.

Given the current review scores (3, 5, 5, 6), the paper is below the acceptance threshold for the conference. The AC believes that the proposed method seems to be an effective and simple way towards more efficient graph neural networks and hence encourages the authors to submit the revised paper to another venue after addressing the reviewers’ concerns, especially on theoretical or empirical analysis on over smoothing and implicit regularization.


[1]: Gated graph sequence neural networks
A method for pruning neural networks is proposed.  Reviewers raised several concerns, including poor technical presentation and insufficient experimental validation with respect to both baseline methods and ablation studies.  All reviewer ratings lean toward reject and the authors did not provide a response.
The paper presents an optimization technique for optical networks based on federated learning. The motivation for using federated learning stems from the privacy of datasets arising from different operators. The performance of the method is compared to the one based on centralized learning. Despite demonstrating an interesting and promising application of a federated learning, the paper is rather weak in its methodical contribution. Its experimental evaluation, however, is rather artificial with an FL problem generated by splitting the dataset for a centralized problem into parts. No response to the reviewers  comments was provided.
This paper considers an important problem, Multi Agent Reinforcement Learning, and looks at a subclass of problem, the Markov Potential Game.

Even though this class is not the more generic one (as pointed out by a reviewer), one must start somewhere before (and maybe the results cannot easily or at all be extended to a larger class), so I will not personally take this as a strong negative concern.

The other reviewers are rather positive about the result and the techniques, and I concur with them. 

I will therefore recommend acceptance.
The paper considers a learning problem to determine the best low precision configuration within the memory budget.  It is an interesting problem that could be of interest to the community.  Overall, the reviewers were fairly positive on the paper and believe the paper give interesting insights into how to use limited memory for learning.
This is an interesting paper aiming to further advance the knowledge of implicit bias in deep networks.  Unfortunately, the reviewers had many concerns about technical details and presentation.  One concern was about section 5, on margins and implicit bias.  Oddly, this section 5 does not cite the extensive literature on margin maximization, implicit bias, and implicit regularization in deep learning (despite a mention of Soudry et al earlier on), whereas the choice of paper title and also this section title would suggest an advance over this work, or at least reference to this work (which goes far beyond that one paper); instead, that section left me a bit confused about the suggested bias and its implications on generalization.  As such, I suggest the authors spend more time on their submission, aiming to further separate their work from prior work, and address the comments of reviewers.
This paper aims to look at the relationship between disentanglement
and multi task learning.  The authors claim to show that disentanglement
emerges naturally from MTL.

The main discussion was whether the claim that disentanglement emerges
naturally from MTL has been adequately demonstrated.  The main  
issue is that MTL results in more extraction of information and that
is hard to disentangle from the disentanglement metrics used.

Reviewers agreed the work was interesting but not as complete as would
be desirable.  I also feel it is not ready for ICLR presentation, but  
with further work could be a nice future contribution.
The paper proposes to learn a state representation using bi simulation in an RL setting. The approach is thoroughly evaluated on several benchmarks. In its current form the paper is mainly an empirical contribution, with now some theoretical contributions tucked away in the appendices. Nevertheless, an interesting approach with promising results.

The reviewers appreciated the revised paper and the discussion. The replies and discussions successfully addressed all serious concerns of the reviewers. Please also clarify the discussed points in the next iteration of the paper, and run the experiments with more seeds, as promised.
This paper studies imitation learning from a causal inference perspective. The authors propose a method to remove the effects of confounders on expert action a using instrumental variable regression, which presumably leads to better estimation of P(a|s), and hence better imitation.  The reviews were negative overall at the start. After the discussions, one reviewer stated that he would change his recommendation to accept, although his score is not changed on the review form. However, another reviewer is still not convinced that the causal formalism introduced in the paper improves over the existing RL literature.
First this is the seed for a  very good paper on approximating manifolds and densities using injective flows.

Reviewers have done an admirable effort reviewing the paper giving detailed reviews and suggestions to improve the theory and  corrections  that resulted in an improvement of the paper during the rebuttal/ revision phase.

Unfortunately the paper still needs major rewriting and organization to be accessible by other readers, and should undergo another round of review in its last polished version to further vet the correctness of some of its claims as explained below .

The paper was discussed at length among reviewers and the AC and here are the suggestions to improve the paper. 

* Implementing Reviewer 4sjW suggestion w.r.t  to the narrative and adding explanations to improve the readability and accessibility  of the paper. 

* Another concerns were raised by reviewer eR1p  in the discussion  regarding the correctness of Theorem 1 and Corollary 1. " The proof of Corollary 1 is so rough that I could not confirm its correctness. For example, the functions $r$ and  $f$ are undefined."  Please revisit the proof of this Corollary. Theorem 1 builds on Lemma 7 point 5.  In point 5 of Lemma 7 :"The embedding  $r$ depends on $\epsilon$ , hence so is the measure $\mu$.Therefor the statement  $W_2(g \mu ,f \mu)< B_{K,W}(f,g) + \epsilon$ for all $\epsilon$, does not imply that  $W_2(g \mu ,f \mu)< B_{K,W}(f,g)$. One solution can be by  building a sequence of measures that would converge to that measure and see if the argument goes through. 

 We encourage the authors to implement all the feedback  and suggestions of the reviewers and to submit this interesting work to an upcoming venue.
This paper presents an approach to learn the solution operator of Markovian partial differential equations (PDEs) by combining the Fourier Neural Operator (FNO) with a hyper network.  In short, the hyper network g_\theta(t) is trained to output the weights of a FNO f_w(x), which (given an initial condition) outputs the PDE solution at the time given to the hyper network. The main claimed contributions of the proposed approach (as compared to, e.g., the original FNO architecture) are that the obtained solutions improve the learning accuracy at the supervision time points and that the solutions are able to interpolate and extrapolate to arbitrary times.  

The reviewers seemed to like the idea of using hyper networks for modelling continuous time FNOs. Several issues were raised by the reviewers, e.g., with respect to related work by Li et al. (2021, https://arxiv.org/abs/2106.06898), which I believe were addressed by the authors satisfactorily. However, it is still concerning that the reported performance for FNO in, e.g., 1d Burgers does not quite match those recently published  (Kovachki et al, 2021, https://arxiv.org/abs/2108.08481, Table 2).  Although the authors did report additional results using GeLU, these results are still very different to those in  Kovachki et al ( 2021) and it is unclear whether the improved performance is due to a lack of tuning the baseline FNO. I commend the authors for, as suggested by the reviewers, running more extrapolation experiments. However, I believe the reviewers also made a point about considering (training) times much longer than 1, as even the original FNO paper did this for Navier Stokes (NS) with T 50. 

Overall, the paper provides modest improvement wrt FNOs, although it does extend its capabilities to interpolation and extrapolation. The paper will also benefit from providing a brief overview of FNOs.
The authors have done a good job methodologically addressing reviewer concerns. The empirical results are good, and the application impact is clear. There were some concerns about the technical heft of the approach, but there s overall agreement that the effective application to the domain is interesting and done very well. The AC is a bit concerned about the impact of the regularities of the domain used on the results, especially with regard to semantic regularities (homes have very particular regularities). But even without answering this question (it should be discussed in the camera ready though), this paper makes a solid contribution.
The paper considers model based RL, and focuses on approaches that benefit from the differentiability of the model in order to compute the policy gradient. It theoretically shows that the error in the gradient of the model w.r.t. its input appears in an upper bound of the error in the policy gradient computing using the learned model. Motivated by this, it suggests a MBRL approach that learns two models, one of them minimizes the next state prediction error (as commonly done) and the other minimizes a combination of prediction error and the gradient error. 
The paper empirically studies the method through extensive experiments.

Reviewers are generally positive about this work. They believe that the paper is insightful and the method is original. At first, there were some important concerns raised by the reviewers, but the authors revised their paper in the discussion period, and it appears that the reviewers are all satisfied now. I also read the paper during the rebuttal phase, and I should say that I have some concerns myself, especially on the theory part of the paper. Given that the authors did not have an opportunity to answer my questions, I do not put much weight on my concerns (and I believe most of them can be addressed with some clarifications). Considering the positive response of reviewers and promising results, I am going to recommend **acceptance** of this paper.

I strongly encourage the authors to consider the comments by reviewers, as well as the following ones, in the revision of their paper.


**Comments**

1) The true dynamics $f$ is defined as a stochastic one, i.e., $s_{t+1}   f(s_t, a_t, \epsilon_t)$ (just before Eq. 1), and similarly for the learned model. Here $\epsilon_t$ is the noise causing the stochasticity of the model. But later, when the errors on the model and its gradient are introduced (i.e., $\epsilon_f$ and $\epsilon_f^g$), the role of stochasticity becomes unclear.
For example, we have
$\|| \tilde{f}(s,a)   f(s,a) \||  \leq \epsilon_f$.

What happened to the noise term?

The same is true for Eq. (5). The next state s  (either according to the true dynamics or the learned model) is random. In that case, it is not obvious how to interpret Eq. (5). Is it the error of the expected gradient of the next state? Or is it something else?

In case the dynamics is assumed to be deterministic, this should be clarified early in the paper.

2) The upper bound in Theorem 1 might be vacuous if the Lipschitz constant $L_f$ of the model is larger than 1.
To see this, consider Lemma 1. The constant $C_0$ is $\min [D/\epsilon_f, (1 L_f^{t+1})/(1   L_f)]$.
If $L_f$ is larger than 1, for large enough t, the term $(1 L_f^{t+1})/(1   L_f)$ blows up and $C_0$ becomes $D/\epsilon_f$. Therefore, the upper bound of Lemma 1 becomes $D$. Here $D$ is the diameter of the state space, which is assumed to be bounded.

This carries to in the next lemmas. In Lemma 4, $C_5$ would be of the same order as $C_0$ (multiplied by an extra $L_1 L_f / (1   \gamma) )$, so the upper bound of this lemma becomes proportional to D too.

The $C_0$ s appearance continues in the proof of Theorem 1, in which $C_8$ is proportional to $C_0$ and $C_5$. So, $C_8$ is also become proportional to $D/\epsilon_f$. When we have $C_8 \epsilon_f$ in Eq. (34), we get a constant term $D$.
A similar dependence appears in the proof of Theorem 2, where B_3 is proportional to $C_8 \epsilon_f$, which can be as large as $D$. And in Eq. (47), we have $B_3^2$. So the upper bound in Eq. (47), which seems to the be upper bound of Theorem 2, is proportional to $D^2$. This means that if $L_f$ is larger than one, the upper bound does not go to zero, no matter how small the model error $\epsilon_f$ is (unless it is actually zero). This makes the bound meaningless.

This might be unavoidable. I am not sure about it at the moment. But it definitely requires a discussion.

3) Assumption 2 has a term in the form of $E[\frac{s_{t_2}}{ s_{t_1}} ]$ (I have simplified the form). The states $s_{t_2}$ and $s_{t_1}$ are vectors in general. How is the division defined here?

4) Please improve the clarify of the proofs. For example, in Lemma 2 it seems that a negative sign is missing in Eq. (49). Also how do we get Eq. (50) and Eq. (52)? (I couldn t easily verify them).

5) I believe the "periodicity property" used in Assumption 1 should be "ergodicity property".

6) The paper still has a lot of typos, e.g., "To optimize the objective, One can ..." (P3), "argument data" (instead of augmented) (p4), "Superpose" (p5), "funcrion" (p6).
Three out of four of the reviewers are leaning (weakly or strongly) towards rejecting this paper. Unfortunately, the authors only responded to the concerns of the most positive reviewer, making it difficult to disregard the concerns from the three more negative reviewers.

I also took a look at the paper myself, and share a number of the reviewers  concerns. First, the proposed method appears to be performing transductive inference for its predictions, while many baselines it compares with rely on inductive inference. Transductive inference is generally known to outperform inductive inference, therefore some of the improvements in accuracy can potentially be accounted to that. The authors did mention in their one author response that they generated results in the inductive setting and still saw an improvement, however the submission was not updated with details around that new experiment, making it hard to rely on it. Second, the paper is using a 224x224 resolution for images, while the original mini ImageNet benchmark (and the majority of baselines evaluated on it) assume a 84x84 resolution. Here too, using the former resolution is known to outperform the latter. Third, I too found the paper to lack clarity at a number of places in the writing.

I also notice that the final predictions is made following the averaging of features from two models (A and B, as in Eq. 5). This is a form of model ensembling, which generally is a principle know to help improve generalization. It seems appropriate to wonder whether the baselines are worse partly due to not relying on any ensembling at all. 

Finally, I ve found a recent method from ICJAI 2021 (Cross Domain Few Shot Classification via Adversarial Task Augmentation) which appears to beat the proposed method in the cross domain setting for the majority of domains. 

Given the above, and the lack of rebuttals to the reviewers with the most concerns, I m afraid I must recommend this paper be rejected at this time.
This submission received 4 diverging ratings: 3, 5, 5, 6. On the positive side, reviewers appreciated the novelty of the approach and strong empirical performance. At the same time, all negatively inclined reviewers mentioned unfair comparisons with baselines (which was partially addressed in the rebuttal), flaws in the evaluation protocols and ablations not fully supporting claims made in the paper. After discussions with the authors most reviewers decided to stick with their original ratings.
AC agrees that the remaining open questions around empirical validation will need to be answered more clearly before the paper can be accepted. The final recommendation is reject.
The paper proposes to use intra class mixup supplemented with angular margin to improve OOD detection. 

Strengths:
+ Simple idea
+ Experiments on multiple datasets (although mostly focused on image benchmarks)

Weaknesses:
  Justification for the idea could be improved. It d be nice to understand when we expect this to (not) work.
  Differences from prior work "Angle based outlier detection in high dimensional data" could be better explained.

While the paper has some interesting contributions, the reviewers and I feel that the current version falls short of the acceptance threshold. I encourage the authors to revise and resubmit to a different venue.
Four knowledgeable referees recommend Accept. I also think the paper provides a unique contribution to the field of deep survival models and I, therefore, recommend Accept
The paper reviews and draws connections between several parameter efficient fine tuning methods.

All reviewers found the paper addresses an important research problem, and the theoretical justification and empirical analyses are convincing.
The paper proposes the replacement of the softmax layer in a neural network with one parametrized by a kernel. The kernel itself is learned during training from the space of radial basis kernels. The resulting models are compared against identical networks with softmax, linear kernels, second order pooling and kervolutions on several datasets, encompassing vision and NLP tasks.

First, the reviewers raised questions about the novelty of the work. Theorem 4.3, based on which the method is derived, has existed in the literature and seems to be related to the uniqueness of the power series expansions for kernels. There is novelty in using this theoretical result to write an approximation of a positive definite kernel in a way which can be learned. Specifically, it is written as a finite weighted sum of existing kernels, where the coefficients are learned. Reviewer pWF3 posed a valid question about the quality of the approximation, to which the authors responded with an equally valid, and comprehensive, appendix on the error bounds of the approximation. Still, it is worth tempering the statement that the search is  exhaustive  over the space of radial kernels or that the kernel is optimal (instead, the search appears over a large class of radial kernels, and the kernel is approximately optimal with an extremely low distance from the actual optimum).
Along the same lines of rephrasing claims, reviewer WDU4 also pointed out several statements and claims which were not entirely accurate, which the authors then proceeded to resolve, resulting in notable changes from the initial version of the paper. Specifically, there was mention of a "non parametric kernelized classifier". This has been fixed, but it did seem to have initially confused other reviewers, who suggested related work that, it turns out, are not necessarily suitable contenders. The changes made definitely improved the paper, and resolved most of the reviewer s concerns.
Nevertheless, the appendix added comparing the method to non parametric models could be improved. For instance the authors stated "Wilson et al. use Gaussian RBF and spectral mixture kernels. Our method has the capability to automatically learn any positive definite radial kernel. Note that Gaussian RBF and spectral kernels are all radial kernels."   is there any intuition, or proof, of a case when the method introduced here learns a network + classifier that the method by Wilson et al. cannot learn? Or for which deep kernel learning requires considerably more resources? (DKL has been optimized and made considerably faster since the initial paper in 2016).  https://proceedings.neurips.cc/paper/2016/hash/bcc0d400288793e8bdcd7c19a8ac0c2b Abstract.html
Also, while the present work is backed by 4.3, DKL also has a theoretical grounding.
https://www.jmlr.org/papers/volume20/17 621/17 621.pdf

There was some discussion on the exhaustiveness of the experiments, and it was concluded that the datasets are sufficient, while the reviewers were not in agreement as to whether the authors considered sufficient contenders. A comparison against DKL, at least, appears to be warranted.

Overall, the paper brings a contribution in terms of improving the performance of backbones with limited expressiveness through the use of a kernel parametrized classifier, learned by optimizing an approximation of a formulation that spans the entire space of radial basis kernels. The paper was updated considerably during the reviewer process, to its betterment, however, an experimental comparison against deep learning with non parametric kernelized classifiers is still missing.
The paper proposes a method to sample the parameters of the generator and discriminator in a BayesGAN (Saatci and Wilson, 2017) setting. The main innovation is a modified Hamiltonian Monte Carlo sampling scheme. Unfortunately the method is not clearly presented, to the point that all reviewers had difficulties understanding how the method works. The revision is making progress but still does not clearly explain the method. While the paper cannot be accepted for publication in its present form, the experimental results are encouraging so I encourage the authors to keep improving their manuscript.
The topic of this paper is non uniform priors and exploration in reinforcement learning with the graph Laplacian.

All reviewers appreciated several aspects of this work but they all also have several reservations. 

Looking at the paper, reviews and discussions, I see the potential a very nice more general contribution. This potential is not fully realised as the paper stands now. Acceptance can therefore not be recommended.
The paper presents a deep learning network architecture for (semi) supervised tabular data classification and regression problems based on a new attention mechanism between samples (rows) and features (columns).
The model is compared to 10 sota methods, studied on 30 diverse datasets (10 for binary classification, 10 for multiclass classification and 10 for regression).
contrastive learning approach for pre training on unlabeled data and fine tuning on a small number of labels
Explainability capabilities are not presented in a very convincing way. 
While the reviewers find the problem relevant, they criticise novelty and, in particular, the experimental comparison.
Concerns about hyperparameter tuning of own vs. comparison methods voiced by the reviewers.
While these concerns have partially been addressed in the author response, the reviewers still doubt the fairness of comparison.
This paper aims to model adversarial noise by learning the transition relationship between adversarial labels and natural labels. In particular, an instance dependent transition matrix to relate adversarial labels and natural labels. Reviewers agreed that the paper is well motivated and well written, and the proposed method is novel. Meanwhile, reviewers raised some concerns about experiments and paper presentation. During discussion, the authors provided a lot of additional results that partially addressed the reviewers  concerns. However, the reviewers still think the experimental part of this paper should be further strengthened before acceptance. 

Thus, I recommend to reject this paper. I encourage the authors to take the review feedback into account and submit a future version to another venue.
The paper explores and discusses the effects of incorporating prior domain knowledge for modeling fluid dynamics with neural networks, with a focus on smoothed particle hydrodynamics. Reviewers agree that the contributions are modest, and that they are not well presented with respect to issues of efficiency, scalability, robustness, etc.  More work need to be done to make it useful to the community.  Many of directions to improve the paper were in reviewer comments.
The reviewers are all weakly positive. The author response clarified important aspects of the paper. The new human evaluation was critical. However, the human evaluation result presentation is flawed: presenting Likert scores as means does not reflect them well. The authors should use something similar to a Gantt chart to fully reflect the distribution across Likert categories. Another detail in the human evaluation that are troubling: it does not reflect interaction with the system, but judgements through observation. Therefore, the human evaluation does not reflect the ability of the learned dialogue system to interact with users. Overall, the paper makes a nice, original contribution, but despite author improvement there are evaluation flaws (even if they are common in papers using these benchmarks).
The paper discusses propagating input uncertainty through non linear layers by a simple local linearization approach. This is a straightforward idea and the authors explain how this is an optimal approximation of the propagated distribution for Total Variation and reLU non linearity (for a single layer). This is an interesting (if quite limited) theoretical result. What is not clearly stated is that this result only holds for a single layer. It does not mean that the local approach is the best way to approximate (in the total variation sense) a distribution passed through multiple reLU layers.

By repeating the procedure, the authors are able to define closed form objective functions for noise robust training of deep networks.

The reviewers found this an interesting paper and there was a good effort by the authors to improve the results. However, technical innovation is modest and reviewer doubts still remain. For that reason, clarity of presentation is critically important. The overall numerical score isn t convincing and with one reviewer remaining very unconvinced.

I agree with the reviewers that the technical contribution is quite limited and I would argue is not particularly well explained. For example, a simple alternative would be to use a "global" linearization in which one can consider the network function $f$ as a whole, and then simply linearize this (rather than linearizing each layer). Indeed, the way that the paper is written, this would be a natural interpretation since $f$ is defined in the introduction as the network function, but is used later differently (eg section 3.2) as the transfer function. The approach is to recursively compute a new mean and covariance for each layer, propagating these through the network (similar to moment matching approaches). It would have helped if the authors had made pseudocode for their approach. It would be natural and interesting to compare to the simple global linearization approach (which is computationally faster).

The presentation of results and experiments could be improved. For example, in figure 3, it is not clear (nor is it explained in the text) what the definition of "robust accuracy" is.  

Given the modest technical innovation, I also feel that the clarity of presentation isn t yet at the level that would merit acceptance. The paper would also benefit from some deeper insight into why the approach might perform better than other approaches (such as local moment matching) at the network level (rather than a single layer).
The paper analyses the loss landscape induced by AUC loss. Reviewers found critical issues with the paper, and the Authors have not provided feedback. As such I have to recommend rejecting the paper. I thank the Authors for submitting the paper to the ICLR conference. I hope the reviews will be helpful in improving the paper.
This work proposes a new framework that can learn the object centric representation for video. The authors did a good job during rebuttal and turned one slightly negative reviewer into positive ones. The final scores are 6,6,8,8. AC agrees that this work is very interesting and deserves to be published on ICLR. The reviewers did raise some valuable concerns that should be addressed in the final camera ready version of the paper. The authors are also encouraged to make other necessary changes.
The work proposed an interesting source free adaptation setting, where one is asked to adapt a pre trained source model to a target domain without accessing data from the source domain. While reviewers find the setup interesting and the initial results encouraging, they expressed concerns on the limited novelty of the work as well as incomplete evaluation. Multiple reviewers (reviewer Lx65  and pNRq) raised concerns on the fairness of the evaluation, which was not fully addressed by the authors during rebuttal. Please consider addressing these comments in your draft.
The paper uses several types of information to predict one specific
lab test response for patients. The predictions are made by combining
and tailoring mainly existing techniques.

The reviewers raised a number of concerns, and the authors clarified
many of them and provided additional results. In particular the
following issues were discussed: Comparing to state of the art methods
and methods having the same information available, specifics of
empirical evaluations and of methodological novelty, choice of the
particular data sets, and justifiability of conclusions.

The main remaining weakness is the limited novelty, which should not
be interpreted as the contributions of the paper being trivial.

In contrast, the solid engineering work done by the authors in this
paper will be valuable in developing clinical decision support tools,
and the authors are encouraged to incorporate the new results and
feedback in future work and submissions.
This paper focuses on unsupervised image denoising and proposes a method to do so. It shows that using a designed operator based on domain knowledge can help improve unsupervised image denoising. The authors also provide experimental results demonstrating that the proposed methods outperform existing unsupervised denoising and behave similar in performance to supervised methods. The reviewers liked the improvements but (1) limited novelty/simple extension of noise2self, (2) example not convincing, (3) lack of clarity in 2.3, (4) a variety of other technical concerns. The authors partially addressed these concerns. However, I concur with the reviewers that the paper still requires more work and is not ready for publication in its current form.
The authors describe an approach to modeling data via an implicit representation that lives on a union of linear subspaces.  While the reviewers consider the authors  approach as novel and having potential, they (and myself) consider that the exposition and notation could be improved, and that the paper as it is is hard to understand and contextualize.
This paper extends the Contextual Graph Markov Model, a deep unsupervised probabilistic approach. The key idea is to leverage Hierarchical Dirichlet Processes to automatically determine each layer s latent representation s size. The paper conducts experiments on graph classification tasks to show the superiority of the proposed method.

Strength
* A new method is proposed.
* The proposed method appears to be sound.
* Experiments are conducted to demonstrate the effectiveness.

Weakness
* The novelty and significance of the work are not enough.
* The improvements on existing methods are not significant.
* The proposed method is also not so general.

 

After rebuttal

Reviewer ynws, who gave the highest score, says

“I agree with the overall review of the paper by other reviewers. The proposed method is limited to the CGMM model and not generic enough to extend to other more popular graph neural networks. The improvements don t seem to be significant enough as well.”
This paper presents a method for producing higher quality uncertainty estimates by mapping the predictions from an arbitrary (e.g. deep learning) model to an exponential family distribution.  This is achieved by using the model to map from the inputs to a low dimensional latent space and then using a normalizing flow to map to the parameters of the distribution.  The authors show empirically that this improves over a variety of baselines on a number of OOD and uncertainty quantification tasks.  This paper received 5 reviews who all agreed that the paper should be accepted (6, 6, 8, 8, 8).  The reviewers in general found the method novel compared to existing literature, compelling and the results strong.  Multiple reviewers asked for experiments with higher dimensional output distributions (e.g. CIFAR 100) and had concerns regarding the "entropy regularization" term (akin to the beta term in a beta VAE, this is a constant applied to the entropy term).  The reviewers seemed satisfied with the author response, however, and the concensus decision is to accept.
The authors present a new learning based algorithm for constructing index structure. Existing learned index algorithm use a fixed value, in contrast the authors show that a more refined methods can be used to obtain higher quality solutions for the problem.

The reviewers, after discussion, found the paper interesting and the experimental results promising but they feel that the paper in the current form is not yet ready for publication.  In particular,
  in the current form the theoretical motivation and the experimental results are a bit detached

Overall, the paper is interesting and the results are promising but it probably would benefit from significant re writing before being accepted.
The authors investigate the claim that agents in emergent communication games will converge to a symmetric homogenous state.  In particular, the authors show/argue for diversity in the population to close the gap between observed trends in neural agents and those expected when studying natural languages (e.g. around structure).  Reviewers were generally positive, though requested a number of rhetorical changes needed and additional literature.  These have been addressed.
This is a representation learning time series paper.

The reviewers appreciated aspects of the paper, but all agreed that primarily the experiments are lacking and to a lesser degree the presentation is unclear and needs further proofreading.

So definitely this work has merits. It is also much appreciated that the authors throughout the discussion have been engaged in adding results and further clarification. This can be used for an updated version for the next conference.
Dear authors,

I have carefully read the reviews, rebuttals and the subsequent discussion. The review scores are mixed (5, 5, 6, 6). Let me comment on some of the key issues raised by the reviewers. I will elaborate on some of them with my own insights.

1) You insist that (P4) cannot be regarded as a particular case of (P3). But this is trivially incorrect. The hard constraint in the reformulation (P6) you mention in the discussion can be written as a regularizer: *indicator function* of the constraint set. Indeed, let  $\cal C$ be the set of points $W (w_1,\dots,w_K)$ for which there exists $w$ such that $w_k   m_k^* \circ w$ for all $k$. Then the regularizer defined by ${\cal R}(W)   0$ if $W\in {\cal C}$ and ${\cal R}(W)   +\infty$ if otherwise does the job. This is a well defined regularizer. Such regularizers are routinely used in optimization to model hard constraints. So, the formulation you consider is a special case of (P3).  Moreover, as pointed out by Reviewer Zg2F, and acknowledged by the authors, "The idea of using sparse masks to model personalization for federated learning is not novel in this work. Prior works utilize this idea with other techniques (Li et al., 2020) (Vahidian et al., 2021). Moreover, several side benefits such as low communication cost, cheaper computation, and fewer memory requirements should also be attributed to those original works where sparse masks are used, and the same side benefits of sparsity were mentioned." The claim that one of the novel contributions of FedSpa is "we formulate a clear optimization problem for FedSpa" is weak, especially in the light of the above comment, and the "moral" existence of the formulation in prior work, albeit not expressed in a mathematical notation. The fact that previous works did not formulate this properly is a major issue with those works, and not a major contribution of this work. A clear mathematical formulation of what one wants to achieve should be a standard requirement. In any case, I appreciate the clarity nevertheless. 

2) The same reviewer states that the key idea of the paper that differs from the above two mentioned papers is how the sparse masks are handled. One of the two ideas proposed is trivial and is equivalent to standard non personalized FL (if all masks are the same, the submodes they defined can be considered a global model). The second idea does not seem to have any interesting/distinctive theoretical support. 

3) Sparse to sparse training in FedSpa may be novel, but the claim that "the masks continue to evolve (towards the optimal masks) in the training process" is not supported by theory nor experiments. If indeed you can show that the local masks evolve to some meaningful notion of an optimal mask, this would be interesting. 

4) I also agree with the other points raised by this reviewer. I have read the author response to these comments. (BTW: Language such as "you bet" is inappropriate). While some of them make sense, they do not reduce the severity of the concerns by a large enough margin. 

5) The comment about the weakness of the main theorem is particularly concerning. Indeed, the main theorem may be vacuous, and the authors need to do a thorough explanation of the result and its importance (on its own and in comparison with existing literature and rates). I do not believe such a comparison could be advantageous to the proposed method though. The expressions are complicated. It seems that for any meaningful mask size, the non vanishing term will be too large. The theorem is not a valid convergence result as the authors do not show that the right hand side can indeed be provably made arbitrarily small by some choice of the parameters of the method. For instance, it is not guaranteed that $dist(m_{k,t}, m_k^*)$ will converge to zero. In this sense, calling this theorem "Convergence of personalized models" is incorrect and misleading. This is a fatal issue, unfortunately. The authors should make it absolutely clear that the result does not prove convergence. 

6) Assumptions 1, 2, and 4 are very strong. For example, Assumption 2 is not provably satisfied for lower bounded nonconvex smooth functions when subsampling ( minibatching) is used to produce the stochastic gradient. Assumption 3 is also quite strong: it is not satisfied by convex quadratics. Assumption 1 is also strong   most recent works on FL do not require any similarity assumptions. 

In summary, while this direction of research is interesting, the level of contributions in this work is marginal at best. The key theoretical result is misleading in that it does not imply convergence while it is marketed as such. Moreover, strong assumptions (relative to what is achieved in the latest papers) are used to obtain it. Because of these concerns, and other concerns raised by the reviewers, I do not have any other choice but to reject the paper.

Area Chair
Two reviewers increased their scores after considering the responses from the authors, and all reviewers are somewhat positive. However, the increased scores are still 6 only, and as the area chair, I have concerns about the foundations of this research.

The authors write "there is no off the shelf baseline that can automatically disentangle the data from different domains in the open ended regression setting." This is not true for the standard situation of a mixture of regression lines, as in Section 4.1. Completely standard EM (not necessarily hard EM) will solve this problem, as long as the individual lines (sinusoids etc.) can be represented easily by the EM components. Another standard method that should be another baseline is a mixture of experts neural network. 

One thing that EM cannot handle is learning the number of components in a mixture model, as in learning the _k_ in _k_ means. To the extent that "open ended" here refers to a new approach for this problem, with mathematical guarantees, it is interesting. But this point of view needs more explanation.

The paper begins "A hallmark of general intelligence is the ability of handling open ended environments, which roughly means complex, diverse environments with no manual task specification." If there is one aspect of natural environments that is crucial and fundamental, it is the presence of noise. However, starting theoretically with Definition 1 and empirically with Section 4.1, the authors work in a world of deterministic functions. This mismatch undermines the conceptual significance of the paper.

Perhaps because of the universality of noise, the authors do not present a real world dataset or task for which the OSL method is directly natural or applicable. Rather, they impose restricted specifications on datasets such as MNIST and introduce metrics that are novel, hence hardly natural, undermining the empirical significance of the paper.
The paper proposes an unrolled algorithm to solve the l1 norm formulated dictionary learning problem, and focuses on the number of unrolling steps. It shows that it is better to limit the number of unrolling steps, and this leads to favorable performance over the alternating minimization baseline. The method can also be adapted to scale to very large datasets.

Most reviewers were positive or became positive after the rebuttals.  Reviewer njnY was still concerned about some issues, such as constraints and the choice of the l1 model over the l0 model; there also may have been confusion about unit sphere vs unit ball constraints.  However, given the recommendations of the other reviewers and my own opinion, I think the paper is a worthy contribution, and the point about not unrolling too deeply is an important one that is worth highlighting.
This paper introduces a new linear attention mechanism for transformer based models.  This is accomplished by replacing the softmax in the standard transformer self attention with a cosine based re weighting mechanism.  The empirical results are good, and cosFormer generally outperforms existing efficient transformers for autoregressive language modeling, fine tuning, and on the long range arena.

The reviewers were generally positive regarding the paper, with all reviewers voting to accept.  The discussion period focused on particular choices regarding the ReLU activation function vs. other non negative activation functions, further motivating the cosine operation, and comparing the speed of cosFormer vs. other efficient transformers.  The authors responded by providing additional ablations to empirically validate the choice of ReLU, motivated the cosine operation by noting that it introduces a locality bias, and further described the computation requirements of their transformer vs. prior work.

Overall, this is an interesting addition to the linear / efficient transformer literature, with solid empirical results supporting the various design decisions.
In this paper, the authors extend the FLAMBE to the infinite horizon MDP and largely improved the sample complexity of the representation learning in FLAMBE. Meanwhile, the authors also consider the offline representation learning with the same framework. Although there is still some computational issue in MLE for the linear MDP, the paper completes a solid step towards making linear MDP for practice. The paper could be impactful for the RL community. 

As the reviewers suggested, there are still several minors to be addressed:

  The extension of the proposed algorithm for finite horizon MDP should be added. 
  The directly comparison between the sample complexity of FLAMBE and the proposed algorithm in infinite horizon MDP is not appropriate. The authors should clarify the difference here. 
  The organization of the proof is not clear. As reviewer suggested, the one step back trick should be emphasized for better significance of the submission.
It can be prohibitively expensive to train a reinforcement learner from scratch &mdash; particularly in cases where experience is expensive to obtain, such as with a physical robot. So, we might hope to speed up RL in a couple of ways: first, by pre training a representation that makes subsequent RL need less data; and second by running our RL on a cheaper proxy environment such as a simulator. For pre training, we hope to be able to take advantage of available pre collected data, and we hope to be able to use supervised learning or reconstruction tasks since they can be cheaper than RL. For either pre training or a proxy environment, we have to deal with distribution shifts: the properties of the environment may change between pre training and RL, and between RL and testing the learned policy.

The paper presents an empirical study of how different pre trained representations and different distribution shifts affect RL performance. It evaluates a number of representations trained by different VAEs (differing in aspects such as loss and hyperparameter settings) under various scenarios of distribution shift. It also asks whether we can predict the performance of the learned policies from properties of the representations, before going to the expense of training and evaluating our reinforcement learner.

The paper concludes that it is possible to significantly reduce RL data requirements using pre trained representations, even in the presence of significant distribution shifts &mdash; including demonstrating zero shot sim2real transfer. And, the paper concludes that inexpensive measurements of OOD performance on supervised tasks can at least partially predict success in generalization.

The reviewers praised the extensive experimental evaluation, including a large number of experiments on a physical robot, as well as the investigation of less expensive ways to predict generalization.

Some reviewers were concerned that the choice of environments was limiting &mdash; e.g., that the distributional distance between in distribution and out of distribution tests was limited, or that the results might not generalize to other related robotic environments. However, in the end there was support for the conclusion that the experiments cover a sufficiently general and interesting question.
In this paper, the authors aim to work within the path specific framework to implement fair predictions by learning a causal graph in such a way that some path specific effect is removed.

Generally, the paper was not received very well by reviewers, with the primary concern being lack of novelty in particular in comparison with (Kyono et al.)

One additional comment I wanted to make is this: any prediction task that is a part of a pipeline that contains a graphical model selection step is properly a "post selection inference problem."  Such problems are very challenging because:

(a) Learning a graph from data is known to lack consistency at any rate (meaning that the algorithm is only pointwise consistent, but not uniformly consistent).  This issue propagates to "downstream" tasks in the pipeline, including prediction problems.  Probably, the way this issue would manifest in this work is unless sample sizes were very large, there would be no particular reason to assume the correct causal path is removed.

(b) Even if uniformly consistent modifications of structure learning algorithms were used, the uncertainty in learning the graph with error must be propagated to all subsequent steps in the pipeline.  Doing so appropriately is very challenging.

When revising the paper, in addition to taking reviewer comments into account, please consider how your method deals with post selection inference issues   I think this is a very interesting but challenging question that is likely to come in peer review.
This paper introduces a new layer for graph neural networks that aims to reduce the oversmoothing issue common to this model type. The reviewers find the paper well organized and easy to follow, and they recognize the importance of the problem that is addressed. However, they also identify critical errors in the mathematical derivations: the authors did not provide a response to the reviews, and hence these errors remain unaddressed. In addition, multiple reviewers indicate they find the experimental evaluation insufficient. For these reasons I m recommending rejecting this paper.
The paper provides an "asynchronous" method for multi agent actor critic with macro actions. A major contribution of this paper is the integration of the macro action value from the Q value based macro action MARL method into multi agent policy gradient. Although it appears an interesting contribution, reviewers found that several parts of the paper were not clear enough and there is a lack of fair comparison with previous works.
The paper provides a solid and thorough analysis to the two basic methods of fine tuning, linear probing (LP) and fine tuning (FT). The authors provide an important and highly interesting observation about the performance of both in and out of domain (OOD) setting. They validate the known phenomena that FT outperforms LP in the in domain (ID) setting, but demonstrate that when tested on OOD data, LP is in fact more performant and back this observation with a theoretical and empirical analysis.
The remedy provided is also a known, yet slightly less popular technique of setting the final layer (LP) first, then finte tuning (FT LP). The authors provide thorough experiments showing that this technique enjoys the best of both worlds, meaning ID and OOD. I found it worth noting that during the rebuttal period the authors provided experiments on additional larger scale datasets and models and the results of the paper carried over to these new setting.
The reviews agree that the analysis provided is both interesting and novel. Even though the paper does not provide a new technique, there is a consensus that the understanding it gives on known techniques is a welcome addition to ICLR.
The paper present results using syntactic information (primarily through constituency trees) on the task of recognizing argument discourse units. No reviewer recommends acceptance of the paper:
  The empirical results appear strong, though the reviewers raise questions about some of the experimental choices. 
  The writing is unclear and reviewers point out many missing or incorrect references in the bibliography.
  There is little methodological novelty   known techniques are applied to a topic that has not been studied much.
Overall, the area chair agrees with the reviewers that this work does not yet meet the bar for ICLR.
The paper used the Koopman operator theory to explain and guide the DNN pruning. All the reviewers deemed that such a viewpoint is novel (but at different levels). However, the paper still had some issues, including unclear technical details, vague/overselling statements, being computation and memory expensive, etc. The paper finally got 4 "marginally above threshold" (one being of low confidence), making it on the borderline. The AC read through the paper and agreed that the Koopman operator theory brings new perspective to DNN pruning, with potential for other analysis of DNNs. Although the paper is imperfect and not strong, it does not have severe problems either and the issues pointed out by the reviewers could be easily fixed (except the scalability issue, which can be left as future work). In order to encourage new ideas, the AC recommended acceptance.
Dear Authors,

The paper was received nicely and discussed during the rebuttal period. There is consensus among the reviewers that the paper should be accepted:

  The new result about query complexity of regression problem that the authors have added. Along with the result on 
 for (noisy) Vandemonde matrix, these make the paper lie above the accept bar.
  The authors have providing satisfying clarifications during the rebuttal that convinced reviewers to increase further their scores.

The current consensus is that the paper deserves publication.

Best AC
The paper relies on the analytical tools afforded by on the NTK theory to proposes an adversarial attack that uses the information of the model structure and training data, without the need to access the model under attack. While the reviewers found the problem interesting and well motivated, they feel that the theoretical analysis and the experimental results can be significantly improved. In particular, some of the points that the reviewers did not find convincing during the discussion include: (1) the technical novelty of the work, i.e., applying adversarial attack on NTK at inference time seems a trivial extension of PGD attack; (2) authors  argument that knowing the model is strictly stronger than knowing the original training data; (3) scalability and generalization of the proposed method to settings without training and test set; and (4) comparison to existing sota transfer attacks in the same setting, like no box attack. Addressing the above points will significantly improve the manuscript.
This paper presents a conditional variational autoencoder (CVAE) approach to solve an instance of stochastic integer program (SIP) using graph convolutional networks. Experiments show that their method achieves high quality solutions with high performance. 

It holds merit as an interesting novel application of CVAEs to the ML for combinatorial optimization literature, as well as for the nice empirical results which show a very nice improvement. Two reviewers had a concern that the contribution is a bit narrowly focused toward  MILP focused journal rather than a general purpose ML conference since the core contribution is the novel application. On the other hand, they believe that combinatorial optimization has received growing interest from the ML community in recent years. 

All three reviewers vote for borderline accept of this paper. The authors have addressed some of reviewers  concerns, hence some reviewers increased their scores throughout the discussion phase.
While authors have updated the draft to address reviewers  concerns, some parts are still not clear enough and the presentation needs further improvements. I encourage authors to revise the draft accordingly and resubmit in the future venues.
The reviewers think the proposed method is well motivated and interesting. However, the novelty needs to be improved. At the moment, the paper seems to be a minor improvement over existing works.
This paper studies the "shortcut" learning phenomenon in CNNs and proposes a simple and effective strategy (white paper) to alleviate specific shortcut patterns (e.g. "black squares" in the image). The proposed scheme is verified empirically and shown to improve over some existing solutions. All reviewers appreciate the simplicity of the idea, which allows its quick implementation and reproduciblity. However, reviewers y5Su and C42n believe the notion of shortcuts as studied in this paper are not only very limited, but also artificial. Consequently, they raise doubts about practical relevance/significance of the method for real world datasets with natural shortcuts. Based on these concerns, I suggest authors to identify a real setting (non artificial data) where, alongside their synthetic shortcuts, can show the practical effectiveness of the proposed can.
This paper presents a theoretical analysis of the approximation properties of linear recurrent encoder decoder architectures, obtaining universal approximation results and subsequently showing approximation rates of targets for RNN encoder decoders. It introduces a notion of "temporal products," which helps to characterize the types of temporal relationships that can be efficiently learned in this setting.

Overall, the reviewers and I all agree that this paper makes important theoretical contributions to the important problem of the approximation capabilities of encoder decoder architectures. The main weaknesses involve the rather simplified linear problem setup, but this limitation is easily forgiven in this first of its kind rigorous analysis. I recommend acceptance.
Motivated by addressing the problem of lacking parallel training data for supervised code translation, this paper proposed to construct noisy parallel source code datasets using a document similarity based approach, and empirically evaluated its effectiveness for code translation tasks. 

The paper is in general well written, easy to follow, and the method is simple and empirical results look positive. Some major concern by reviewers is that while the proposed method is simple and may be easy to use, the overall technical novelty/contribution is limited, e.g., there generally lacks of more thorough discussions on how to deal with the critical noise issue in a more robust or sophisticated way. In addition, there were also other concerns about the experimental issues, such as datasets, metrics, ablation analysis, usability, etc. 

Overall, the paper presents some preliminary positive results for an interesting research problem, but the overall technical novelty and contributions are incremental and the paper is not strong enough for the acceptance by this conference. Nonetheless, this work could be potentially valuable for the niche area of code translation research, and authors are encouraged to continue to improve this research with more thorough investigation for a future venue.
This paper proposes to learn a latent space representation such that some linear equivariance and symmetry constraints are respected in the latent space, with the goal to improve sample efficiency. One core idea is that the latent space is also the same as the space of linear transformation used in the constraints, which is shown to simplify some of the mathematical derivations. Experiments on the Atari 100K benchmark demonstrate a statistical improvement over the SPR baseline when using the SE(2) group of linear transformations as latent space.

Following the discussion period, most reviewers were in favor of acceptance. However, one reviewer remained unconvinced, and after carefully reading the paper, I actually share the same concerns, i.e., that it is unclear under which conditions the proposed approach actually works, and what makes it work. I believe that, as a research community, we should value understanding over moving the needle on benchmarks, especially when proposing such a complex method as this one (see Fig. 5).

More specifically:

1. The method is only evaluated on Atari games, showing some improvements when using SE(2), and arguing that there are corresponding symmetries in such games. There is however no analysis demonstrating (or even hinting at the fact) that the proposed technique is actually learning to take advantage of such symmetries (NB: I had a quick look at the animation added by the authors in the supplementary material, but I do not see if/how they help on this point). Even if analyzing representations on Atari may be tricky, I believe that given the motivation of this new algorithm, it *must* be evaluated on some toy example (e.g., the pendulum mentioned throughout the paper) to validate that it is learning what we want it to learn (although I also agree with the authors that experimenting on a more complex benchmark like Atari is equally important).

2. The idea of embedding states into the same space as transformations is interesting, and brings some advantages when writing down equations, as demonstrated by the authors. However, there is no justification besides mathematical convenience, and it doesn t seem intuitive to me at all that why this should be a good idea, considering that it ties the state representation to the mathematical representation of group transformations. For instance, what does the spcial group element $e$ mean for a state? And this coupling makes it difficult to interpret the effect of using a different group of transformations: for instance when moving from GL(2) to SE(2), is the observed benefit because we are using only specific transformations, or simply because we are reducing the dimensionality of the state embedding? (note that in Fig. 4(c) the MLP variant has similar performance to GL(2), and based on my understanding they use the same embedding dimensionality   > I believe it would be important to check what would happen with an MLP variant using the same dimensionality as SE(2))

3. The effect of the $L_{GET}$ loss is not convincing, as pointed out by several reviewers. I think it would have been an opportunity for the authors to investigate why, especially since it seems to work in some games and not others. But just focusing on "here are the 17/26 games where it works better" doesn t really bring added value here. Do these games have some specific properties that make them better candidates to take advantage of $L_{GET}$? This could have been a very interesting insight if that was the case, but as it is now, I am not sure what we can learn from that.

4. There are several implementation "details", some moving the final algorithm farther from its theoretical justification, that are not ablated, making it difficult to understand their impact (ex: using target networks, the choice of the value of M, using projections onto the unit sphere of some arbitrary dimensionality, how the $s $ state is chosen in $L_{GET}$)

As a result, we have here an algorithm with some interesting theoretical background, but with a lot of moving components which   when properly tweaked   can lead to a statistically meaningful improvement on Atari 100K   without really understanding why. I believe this is not quite enough for publication at ICLR, and I would encourage the authors to delve deeper into the understanding of their algorithm, which I hope will bring useful insights to the research community working on representation learning.
The paper proposes a data augmentation approach that extends Mixup with high  and low pass filtering operations, in order to regularize deep networks towards focusing on low frequency components of the input signal.  Reviewers are unconvinced about the significance of the contribution.  Reviewer 5zdd notes that the method does not improve over standard Mixup in the absence of corruption error.  Reviewer 3E2o notes that "the idea of spectral mixing itself is not particularly novel", and also asks for ablation studies concerning the hyperparameters of the method; the author response unfortunately does not provide enough detail on ablation experiments.  The AC agrees with the reviewers and does not believe the author response has addressed weaknesses in a satisfactory manner.
The paper deals with the problem of adjusting the learning rate during gradient descent optimisation. Unfortunately the proposed approach is very similar to methods already presented in the literature and no significant contribution can be recognised. During the rebuttal, the author(s) have acknowledged their ignorance about the relevant literature and provided some further clarifications that did not turn into a revision of the reviewers’ initial assessment of the work.
The paper proposed a novel approach that leverages the discrepancies between the (global) series association and the (local) prior association for detecting anomalies in time series. The authors provided detailed empirical support to motivate the above detection criterion, and introduced a two branch attention architecture for modeling the discrepancies and establishing an anomaly score.

All reviewers acknowledge the technical novelty of this work (including the key insight of modeling anomalousness with Transformer’s self attention and concrete training mechanism via a minimax optimization process) as well as the comprehensiveness of the empirical study. 

Meanwhile, there were some concerns in the positioning of the work, in particular in the clarity in connection to related work, and some reviews concern the clarity of the presentation (e.g. missing some details in experimental results), and the clarity of the exposition of the training process. The authors provided effective feedback during the discussion phase, which helped clarify many of the above concerns. All reviewers agree that the revision makes a solid paper and unanimously recommend acceptance of this work. 

The authors are strongly encouraged to take into account the feedback from the discussion phase to further improve the clarity concerning the technical details as well as the reproducibility of the results.
This paper proposes an extension of mixup (a data augmentation method) to k mixup using optimal transport. The idea is to select randomly at each iteration  two subsets of  k samples and compute the optimal transport solution. Each pairs of samples assigned by the optimal transport plan will then be used to perform mixup and promote smoothness in the prediction function. The authors also provide some theoretical results about preservation of the clusters. Finally numeric experiment show the interest of k mixup on toy and real life dataset classification and study the effect of k and the $\alpha$ parameter (of the $\beta$ distribution).

All reviewers found the paper interesting and acknowledge that it leads to some performance improvements in practice. But they had several concerns that lead to low scores. The justification of the method an more specifically the link with the theoretical findings was found lacking, indeed the result make sens fr a large $k$ which is not was is done in practice (but experiments also show a decrease sometimes for large $k$). One interesting discussion  between the proposed approach and minibatch OT is also missing. In addition the reviewers found the numerical experiments interesting but regret that some mixup approaches have not been compared and also noted a small gap in performance for the proposed approach (with no variance reported). Also the Adversarial robustness measure is now considered weak in the literature and those results could have been made stronger with more modern adversaries. Their final concern was the fact that the method now has two parameters that needs tuning and that can have a large impact on the performance for limited gain. The authors did a detailed reply a,d edition of the paper that was very appreciated by the reviewers but that did not change their opinion that this paper still deserves some more work before being accepted.

For these reasons the AC recommend to reject the paper but strongly suggests that the authors take into account the reveiwers  comments before resubmitting to a ML venue.
The paper introduces a transformer like architecture to perform network inference in network games. While the reviewers acknowledge that the research direction is interesting, they raise concerns regarding the significance of the contribution in terms of methodology, particularly in light of the state of the art, and the experimental evaluation, which in their view did not support the promise of the work. The authors did not reply/follow up on the reviews during the rebuttal period. I would encourage the authors to use the reviewers  comments to revise their paper and resubmit to another conference.
The paper studies the effect of different design choices related to learning a dynamics model. The reviewers uniformly agree that the topic of the paper, systematically studying different design choices, is important. Furthermore, the paper is very well written. However, there are a number of weaknesses as well, that limit the relevance of this work. Arguably, the main weakness is that the results are inconclusive: there is no single design choice that is better, a conclusion that provides little guidance for researchers working in this space. Another weakness is that the study focuses on only 4 domains. And while performing such a study on a much broader set of domains can be prohibitively expensive, that doesn t take away from the fact that it is hard to draw strong conclusions from such a small set of tasks. For these reasons, I recommend rejection.
The paper introduces a novel decoding algorithm that allows to dynamically integrate external knowledge with generative 
LMs. The proposed technique is plug and play, it does not require re training or fine tuning LMs with knowledge based objectives. 
The author report a series of experiments on several datasets and tasks showing improvements over competitive baselines and in some 
cases above SOTA. The authors have addressed the reviewers  queries and added experimental results (additional baselines) as
well as clarifications of their approach (e.g., Figure A1 in the Appendix). I think the topic  (e.g., constrained LM decoding) is of general interest to the ICLR community and the approach compelling.
This paper presents an analysis showing the equivalence between gradient and data poisoning attacks in personalized federated learning settings. The paper contains an analysis of an attack that requires only a single corrupt learning agent, providing results in the setting of PAC learnable models.
The reviewers had several criticisms of the paper, some of which were addressed in the rebuttal.  The first is that the presentation of the paper was at times confusing, and the theoretical results were hard to interpret.  This has been addressed by several changes to the paper writing, including major changes to the layout.  The reviewers feel that other criticisms were not entirely addressed.  This includes the criticism that the experiments are in a fairly simplistic setting (GD on MNIST and Fashion MNIST), and that the theoretical results require strong assumptions and focus mostly on classical models that are learnable in convex frameworks.  While the reviewers agree there are interesting questions posed in this paper, the consensus seems to be that the experimental and theoretical results in this paper should be further revised, and that a future version of this paper will be a great candidate for publication.
This submission receives mixed ratings initially. Two reviewers lean negatively while one reviewer is positive. The raised issues include 
whether the proposed method can be adapted to other vision transformers, the design choice of pooling strategy, the computational time cost, the similarity to an existing work, and the influence of the proposed method on downstream tasks. In the rebuttal, the authors have addressed several issues such as pooling strategy analysis and time consumption.

There are still some issues not completely solved. The proposed method introduces K mean clustering on tokens between different layers. The K mean clustering is prevalent and the weighted clustering does not make the technical contribution sufficient. Also as a general token pooling operation, the proposed method shall be integrated into various types of vision transformers (e.g., vanilla ViT [a], ConViT [b]), rather than one single DeiT. Besides, the downstream tasks in DeiT are not conducted in the proposed method. 

Overall, the AC feels the proposed method, although interesting, requires a major revision that addresses existing issues.  The authors are suggested to further improve the current submission and welcome to submit for the next venue. 

[a]. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Dosovitskiy et al. ICLR 2021.

[b]. ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases. Ascoli et al. ICML 2021.
Reviewer E8z9 remained with a number of serious concerns, including efficacy of the defense in higher poisoning setting, overclaiming contributions in terms of L0 defenses (which are mostly achieved by the baseline CutMix), as well as novelty and generalizability of the approach. The author responses were unconvincing, and all other reviewers participated in the discussion, conceding that they too were unable to provide compelling arguments against E8z9 s comments. Other reviewers claimed that these drawbacks may be "acceptable" for a first step, but were not willing to defend it very strongly. 

We note that E8z9 claims they are a reviewer for a previous version of the paper and these issues were present before. The authors claim that E8z9 s key points had been addressed in this version, but the reviewer maintains that the issues still persist in the latest version. The authors are advised to take their comments into account for further versions of this paper.
This paper proposes a scalable learning method for GNN that gradually increases the training data size by randomly adding vertexes generated from a graphon. Theoretical justification to the proposed method is given that bounds the difference between the gradients on the sampled network and on the graphon. A numerical experiment was conducted to support the validity of the proposed method. 

Unfortunately, this paper contains several issues as listed below:  
1. Novelty: There are already some existing work  to address the issue of scalability of training a graph neural network model. However, the relation to them is not appropriately exposed.  
2. Experiments: Although the main purpose of this paper is to resolve the scalability of GNN, the numerical experiments are conducted only on a small scale dataset ($\sim$1k).  
3. Practicality: There are several hyperparameters. However, the theory and methodology do not give a practical guideline to determine them (e.g., how many vertexes should be added at each epoch).  
4. Correctness: The proof of the theorems would contain some flaws, which should be resolved by the authors. However, there was no response from the authors.

For these reasons, this paper would not be appropriate to appear in ICLR.
The paper proposes a new architecture named Iterative Memory Network (IMN) to encode long user behavior sequence for recommendations. Reviewers appreciate the clarity of the writing as well as practicality and the O(L) complexity of the proposed architecture, however do raise questions on novelty. Different design choices employed in the paper are not well explained. The rebuttal was not able to convince the reviewers to accept the work at this venue, but reviewers do feel the paper could fly in an application oriented venue.
This submission has generated sufficient debate, including some messages that, in my viewpoint, have the wrong tone. It may well be that different colleagues see the work in different ways. It is very hard to evaluate submissions in a short time and mistakes can happen. In this case, I think there were and still are misunderstandings and unclarity wrt very crucial points of the paper. This does not mean that the work overall is weak not that there is no contribution. If the content is so interesting (as discussed by authors and multiple reviewers) in some way (which it seems to be), then a better presentation and argumentation will lead to a publication elsewhere soon, but based on all the data that I have here, I recommend rejection. I see to reason to list details about the content and possible concerns, as they should be clear from the multiple messages among authors and reviewers. Best of luck.
The authors combine TAGI with Q learning to create an approximate Bayesian Q learning algorithm. They evaluate their approach and show it has comparable performance to DQN.

All of the reviewers were positive about the potential of the paper. Unfortunately, the paper suffers from lack of clarity, of motivation, and comparison to relevant approaches. All of the reviewers brought up nearly the same concerns and I agree with these concerns. The authors have not address them and the reviewers do not think this paper is ready for publication at this time. I agree and recommend rejection.

Evaluating TAGI DQN is a valuable contribution, but alone it is not sufficient. The reviewers have made many suggestions on how to improve the paper, and I hope the authors follow up on these suggestions.
This paper has been evaluated by three reviewers with 2 borderlines leaning towards the accept, and with 1 accept. The reviewers have noted that the idea of alignment is not particularly novel per se. Nonetheless, they found some merit in the use of a network learning the alignment and they liked experiments.

AC has however some concerns about this work. Firstly, it is not clear why Lifted+SoftDTW and Binomial+SoftDTW completely fail in Table 1, and in Table 5, SoftDTW is worse by 30% than TAP. Is soft DTW set up properly in these experiments (the softmax temperature, the base distance used, the maximum number of steps away from the main path etc.)?

AC is also not convinced about the principled nature of the proposed alignment. Eq. 3 and the residual design above seem more as heuristics than a principled OT transport as Eq. 1 and 2 set out to suggest. With concatenation of distances between sequence features and positional encoding, the proposed alignment seems more similar to attention and transformers than OT.
This paper analyzes problems of existing threshold meta learners and attentional meta learners for few shot learning in polythetic classifications. The threshold meta learners such as prototypical networks require exponential number of embedding dimensionality, and the attentional meta learners are susceptible to misclassification. The authors proposed a simple yet effective method to address these problems, and demonstrated its effectiveness in their experiments. This paper discusses meta learning from a very unique perspective as commented by a reviewer, and clearly explained problems of widely used meta learning methods. However, this paper focus on prototypical networks and matching networks even though there have been proposed many meta learning methods. Some existing methods seem not to have the problems of prototypical networks and/or matching networks. In addition, the practical benefits of the proposed approach are not well demonstrated. Although the additional experiments in the author response addressed some concerns of the reviewers, they are not enough to demonstrate the effectiveness of the proposed method.
This work propose to learn hierarchical skill representations that, as opposed to prior work, consist of both discrete and continuous latent variables. Specifically, this work proposes to learn 3 level hierarchy via a hierarchical mixture latent variable model from offline data. For test time usage and adaptation on down stream tasks, the manuscript proposes two ways of utilizing the learned hierarchy in RL settings. 

**Strengths**
A novel method to learn hierarchical representations with mixed (discrete/continuous) latent variables is proposed
Detailed experimental evaluation, and baseline comparisons, show promising results

**Weaknesses**
There were various clarity issues as pointed out by the reviewers (fixed in rebuttal phase)
The related work was missing relevant work, and the proposed framework was not connected well to existing work (fixed during rebuttal)

**Rebuttal**
The authors significantly updated the manuscript based on the feedback of the reviewers, and improved both clarity of the manuscript (method+experiments) as well as the exposition of the proposed framework with respect to related work.

**Summary**
After the rebuttal, all reviewers agree that this is a good paper that should be accepted. Thus my recommendation is accept.
This paper studies off policy learning of contextual bandits with neural network generalization. The proposed algorithm NeuraLCB acts based on pessimistic estimates of the rewards obtained through lower confidence bounds. NeuraLCB is both analyzed and empirically evaluated.

This paper received four borderline reviews, which improved during the rebuttal phase. The main strengths of this paper are that it is well executed and that the result is timely, considering the recent advances in pessimism for offline RL. The weakness is that the result is not very technically novel, essentially a direct combination of pessimism with neural networks. This paper was discussed and all reviewers agreed that the strengths of this paper outweigh its weaknesses. I agree and recommended this paper to be accepted.
The authors introduce a neural network approach for solving the fixed point equations arising in deep equilibrium models. This consists of a tiny network that provides an initial guess for the fixed point, as well as a small network that computes coefficients inside an algorithm inspired by Anderson iteration.

Overall, there is consensus among the reviewers that the paper is well written and is a strong empirical study.

I recommend acceptance as a poster.

Additional remarks:

  The authors argue the DEQs / implicit deep learning models allow a decoupling between representational capacity and inference time efficiency. Yet, in the "Regularizing Implicit Models" paragraph, they write "Implicit models are known to be slow during training and inference. To address this, recent works have developed certain regularization methods that encourage these models to be more stable and thus easier to solve.", which seems like a contradiction to me. So while in theory I agree with this decoupling, in practice, it seems not completely true.

  Section 3 should include some discussion on conditions on f_theta for the existence of a fixed point.

  Since the initialization and HyperAnderson networks are trained using unrolling, there is some memory overhead compared to vanilla DEQs, that are differentiated purely using implicit differentiation. It would be great to clarify the amount of extra memory needed by these networks. It is necessary to justify that the initialization and HyperAnderson networks are smaller than usual neural networks.
This paper proposed a self supervised learning view for sequential recommendation with different forms of model augmentation: neuron masking, layer dropping, and encoder complementing. Overall the scores are negative. The reviewers raised concerns mostly around the motivation of the proposed approach (which wasn t fully supported by the experimental results) as well as the limited contribution (especially considering some of the augmentation strategies have been proposed in the past). One reviewer also brought out an interesting connection between model augmentation and model regularization. The authors responded that they will keep improving the paper and hopefully we will see a much improved version in the next submission.
This paper studies PCA under a generative model setup. The authors analyze the projected power method in a range of natural settings.
Moreover, experimental evaluation and comparison to other methods is performed on MNIST. The paper studies an important problem. Despite some initial concerns, the reviewers overall agreed that this is an interesting contribution. I recommend acceptance.
The main remaining criticism of the paper is reproducibility, i.e., "it is nearly impossible to verify the correctness of the result in the paper or to reproduce any of these results" (AC). We generally agree with this statement. While the authors do provide some details in the paper, reviewers, AC, SAC, and PCs agree that this is insufficient. Further points that came up in our discussions were the simplicity of the baselines and the choice of testing to demonstrate that the approach really works. Our impression is that the work lacks a rigorous experimental evaluation. These considerations led to the decision in the end.
This paper introduces a tree structured wavelet deep neural network to effectively extract more discriminative and expressive feature representations in time series signals.  Based on a frequency spectrum energy analysis,  the approach decomposes input signals into multiple subbands and builds a tree structure with data driven wavelet transforms the bases of which are learned using invertible neural networks. In the end, the scattering subband features are fused using a self attention like mechanism.  The effectiveness of the proposed approach is verified extensively on a variety of datasets from different domains including follow up experiments in the rebuttal.  Overall,  the work is technically novel and provides an interesting way of extracting adaptive finer grained features to deal with time series signals.  The authors  rebuttal is solid which has cleared most of the concerns raised by the reviewers with additional supportive experimental evidence. 

I would recommend accept.
This work studies the task of sound source localization from multi channel audio. An approach to design a wavelet like filter bank for audio feature extraction is proposed.

After discussion, all reviewers have given this work borderline ratings. Concerns were raised about the quality of the writing, missing related work, experimental methodology, and especially with regards to confounding factors in the experimental results, which make it difficult to assess the individual merit of the different components in the proposed system (i.e. features vs. transformer model). The authors have addressed this to some extent with additional experiments in the updated version of the manuscript, but this revealed that the gains obtained by the proposed feature extraction method in isolation are actually quite modest. This is in contrast with how the paper is written, with much more emphasis on this particular contribution than seems to be warranted by the empirical results.

Additionally, I believe the manuscript would benefit from a more careful and thorough revision to improve clarity and accessibility, beyond what is possible within a single review cycle. Therefore I am recommending rejection.
This paper studies the memorization power of Relu Neural networks and obtains sharp bounds in terms of parameters. The writing is very clear and the results very interesting.
This paper discusses the problem of cross domain lossy compression on the basis of its reformulation as an entropy constrained optimal transport. Two average distortion measures (without and with common randomness) are defined (Definitions 2 and 3), and some of their properties are investigated, as summarized in Theorems 1 3. The authors also demonstrated in Section 2.2 that in the Bernoulli Hamming case the common randomness can indeed improve the performance under some conditions. Results of the numerical experiments on super resolution and denoising are presented to illustrate the principles derived from the theoretical considerations.

This paper received 5 reviews, with score/confidence being 8/3, 6/3, 6/2, 8/3, 3/4, which exhibit a relatively large spread across the borderline. Upon reading the reviews and the author responses, as well as the paper itself, I think that this paper proposes an interesting framework of optimal transport with entropy bottleneck, as well as architectural designs supported by the theoretical development, with potential image processing applications. The authors have provided further numerical results in their response.

My main concern is that the arguments in this paper are somehow confusing in that they borrow several notions and terms from the context of lossy compression and rate distortion theory in the field of information theory, and use them in quite different meanings without explicitly stating so. (It seems to me that this would have been one major reason for the negative evaluation by Reviewer LfvG.) Examples are:
1. **Target distribution:** In rate distortion theory the target distribution $p_Y$ is not fixed, whereas it is fixed in this paper.
2. **Rate constraint:** In rate distortion theory the rate constraint is imposed in terms of the mutual information $I(X;Y)$ between the source $X$ and the target $Y$, in a form $I(X;Y)\le R$. The justification of this particular form of the rate constraint rather than any other forms is that it is compatible with the operational achievability/converse arguments via explicit construction of encoder/decoder pairs. In this paper, on the other hand, the authors consider a Markov chain $X\to Z\to Y$ and impose the rate constraint on the entropy $H(Z)$ of the intermediate random variable $Z$. Under the Markov assumption one has $I(X;Y)\le H(Z)$, so that the rate constraint in this paper is stronger than that in rate distortion theory, and one would have no control over how tight/loose the adopted constraint $H(Z)\le R$ is against $I(X;Y)\le R$. In relation to this, the expression "identify the tradeoff between the compression rate and minimum achievable distortion" (page 2, line 12) would be at best misleading, as the arguments in this paper might be suboptimal, not necessarily providing the theoretically best achievable results.
3. **Extension versus single shot:** In rate distortion theory one usually considers $n$th extension of a source and a block encoder/decoder pair with blocklength $n$. On the other hand, this paper considers what is called the "single shot" setting, in which one does not consider extension of sources. There are some pieces of work on lossy compression in the single shot setting [C1][C2], so that I would be interested in how such pieces of work and the development in this paper will be related, an issue not explored at all in this paper.

As a result, although the quantities $D_{\mathrm{ncr}}$ and $D_{\mathrm{cr}}$ defined in Definitions 2 and 3 would look quite like the distortion rate functions in rate distortion theory, they are actually not the distortion rate functions at all. Although the authors, perhaps carefully, did not call them distortion rate functions, there should still be some explicit explanation on the difference of their framework from the standard one in information theory.

[C1] Nir Elkayam and Meir Feder, "One shot approach to lossy source coding under average distortion constraints," IEEE International Symposium on Information Theory, 2389 2393, 2020. [link](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp &arnumber 9173943)

[C2] ibid., "One shot approach to lossy source coding under average distortion constraints," [Online]. Available: https://arxiv.org/abs/2001.03983

Another concern of mine, from the viewpoint of the theory of optimal transport, is regarding the optimal transport map. It is known that under fairly general conditions the optimal transport *plan* exists. However the optimal transport *map* is not guaranteed to exist, and even if it exists it can be highly irregular. (See, e.g., Villani, 2009) The descriptions in this paper, such as "computing the optimal transport map is not straightforward" and "learn approximately optimal mappings" on page 4 would be too naive in that they would assume existence and approximability of the optimal mapping.

Despite these concerns, most reviewers agree that this paper presents an interesting piece of work. I would therefore like to recommend acceptance of this paper, and would appreciate it if the authors consider addressing appropriately the above concerns of mine in the final version.
This paper introduces a way of generating adversarial speech samples to attack an ASR system based on speech synthesis.  The proposed approach, by the name of Speech Synthesizing based Attack (SSA) , does not rely on real speech to create adversasial samples but rather create them purely from text using  a conditional variational auto encoder.  Experiments are conducted on three datasets and the results appear to support the effectiveness of the proposed approach. 

All reviewers consider the paper technically sound and well written. Overall, the work is interesting. It pursues another possibility for a threat model on ASR and may inspire related work in the community. Reviewers also raised a number of concerns most of which have been cleared by the authors  rebuttal.  However, there are two major ones standing. One is the perceptual quality of the synthesized speech and the other is the justification of the application scenarios in the real world.  In the follow up MOS experiments, there seems to be a noticeable difference (4.09 vs 3.39) which means the synthesized speech does sacrifice quality.  From the real world application perspective, the scenarios proposed by the authors seem to be a bit contrived.  These two drawbacks are considered significant and need further investigation and justification.
This paper proposes orthogonalising loss gradients with respect to neural network parameters to speed up optimization and improve performance.

The reviewers are unanimous in recommending rejection of the paper. They highlight the following issues:
* weak baselines, which make it difficult to judge the contribution of this paper empirically
* lack of discussion of relevant literature and existing techniques
* arbitrary choices in the design of the algorithm, not backed up by theory or convincing arguments

The reviewers acknowledge the author response, but remain largely unconvinced of the merit of the proposed approach. I see no special reasons to disregard the reviewer assessments, and I therefore recommend not accepting this paper.
After reading the reviews and the rebuttal I unfortunately feel the paper is not ready to be accepted. 

The reasoning for this decision is as follows:
* the empirical evaluation is somewhat weak in its current format, and even adding experiments going from BlockStacks to MNIST would have improved the results, or potentially other synthetically generated data. Or playing with which relation is used during the transfer phase. Something to give a bit more weight to the empirical section and help it connect better with the theoretical one
 * But maybe more importantly (and to some extend this is true for the formalism introduced as well), I think there needs to be a bit more context. After reading the reviews, I went and read the paper, and for example in results provided, it is not clearly explained what is the relationship between the proposed method and some of the baselines. I noticed that the related work section ended up in the appendix, which is fine, to the extent the main text can connect to the literature a bit. But while I agree that the introduction of the method seems good and clear, and this is a hard and important problem that lacks a proper framework and the proposal in the paper is quite interesting. It is also important to understand its relation to other frameworks, and to explain clearly what it tries to fix in other proposal. And to interpret the result, maybe justifying or providing some intuition of why the proposed model performs better. I think this is very crucial particularly for a topic that is still in a growing phase, which makes it harder to judge.
I know in the appendix, the author mention domain adaptation which is also something that jumps in mind when looking at this architecture. However this point is not discussed or mentioned as much in the main paper.

In current form, while the paper reads well, one is left to trying to understand whether these results are significant. I think the work is definitely very interesting and I hope the authors will resubmit it with modification. I just feel in the current format it will not have the impact it should, because of a preserved weak experimental section and not a clear grounding in the literature, making readers unsure of the significance of the work.
This paper introduces a new (un)fairness metric for recommender systems based on mutual information and then develop an algorithm to account for this metric in matrix factorization based collaborative filtering. The reviewers all agree that the proposed metric and algorithm are sound at a technical level, however, they have concerns regarding the motivation of the introduced metric as well as the experimental evaluation. The rebuttal by the authors did not persuade the reviewers to reconsider their original assessment and they still argued that their concerns remained. In the final recommendation, the simplicity of the metric was not seeing as a weakness of the work.
This paper proposes a joint multi agent trajectory prediction framework for multiple agents using a "heatmap" estimation approach employing a hierarchical strategy and sparse image generation for for efficient inference. The method takes a set of predicted trajectories for each agent produces reorderings. The work yields a top result on a competitive leaderboard. 

While multiple reviewers were initially concerned about the paper not making a single major contribution, the author response discussion helped to clear up the degree of novelty. Further experiments provided during the review also led to multiple reviewers increasing their score. In the end, all reviewers recommend acceptance of this paper.

As such the AC recommends accepting this paper.
Motivated by the connections between privacy and generalization, this paper studies the correlation between MI attack accuracy and OOD accuracy on synthetic and real world datasets. It shows that the measurements are not always correlated. I found the connection between the motivation and actual measurements performed in the experiments to be rather tenuous. Therefore it is hard to draw any insightful conclusions from the empirical results. It should also be noted that somewhat related disconnect between accuracy of MIA and generalization has already been observed in prior work.
The manuscript proposes (TRansfer and Marginalize) TRAM method that integrates the privileged information into the learned network weight through weight sharing at training time and approximately marginalizes over the privileged information at test time. TRAM can also be combined with methods for dealing with noisy labels, distillation (Distilled TRAM) and heteroscedastic output layers (Het TRAM). Experiments are performed on both realistic and synthetic datasets including CIFAR 10H, re labeled ImageNet, and Civil Comments Identities.

Reviewers agreed on several positive aspects of the manuscript, including:
1. The proposed methods have simple architectures (not requiring specific modules, e.g., Gaussian dropout [Lambert et al., 2018], for the marginalization);
2. The proposed method can in principle be applied to any neural network model and has zero overhead at prediction time.

Reviewers also highlighted several major concerns, including:
1. The analysis is performed on edge cases such as linear and non linear sine models. There is no analysis for the classification case that this manuscript is targeted for. The simple cases are only true when the feature extraction network is kept unchanged during training;
2. Empirically, the experiments are conducted in a limited and counter intuitive; 
3. Lack empirical evidence suggesting that the representations learned with access to privileged information are more robust against label noise;
4. Lack quantitative (or even qualitative) evidence about how, how much, and what kind of privileged information is transferred through weight sharing in realistic deep neural network models.

Several new experiments have been added to show, among others: representations learned with privileged information outperform representations learned without access to privileged information (using a linear classification model on ImageNet), better quantitatively and qualitatively understanding how and how much privileged information is transferred in realistic deep networks.

Post rebuttal, reviewers stayed with borderline ratings, and they have suggested further improvements: simulating with more annotators by using different checkpoints and/or different hyperparameters, collecting a real world large scale dataset such that the privileged information is insignificantly expensive to obtain along with the main annotations, and disentangling the effect of the pretraining model on the denoising method.
I thank the authors for their submission and active participation in the discussions. All reviewers are unanimously leaning towards acceptance of this paper. Reviewers in particular liked that the paper is well written and easy to follow [186e,TAdH,Exgo], well motivated [TAdH], interesting [PsKh], novel [186e] and provides gains over baselines [186e,TAdH,PsKh] with interesting ablations [186e,Exgo]. I thus recommend accepting the paper and  I encourage the authors to further improve their paper based on the reviewer feedback.
This paper proposes a method for visualizing representations of neural networks trained with self supervised learning with conditional denoising diffusion probabilistic models. By generating multiple images conditioned on a representation, one can identify what aspects the representation is and is not sensitive to. The proposed method allows for high fidelity generated images that can be used to compare different self supervised methods and layers.

Reviewers agreed that the paper proposed reasonable methodology, targeted an interesting problem of understanding what is learned by self supervised methods, and presented interesting qualitative evaluations. However, there remained concerns on the novelty of results in comparison to other methods for probing representations (e.g. classification based), subjectiveness of interpretation of the qualitative results, and limited quantifications of the intuition gained from the visualizations. While the authors have argued that the point of the paper is to showcase the merits of qualitative visual analysis method, reviewers found that the presented results were insufficient to demonstrate the value of the proposed approach. A number of ideas were discussed with reviewers on how to highlight the value of visualization which could strengthen the paper in the future. Given the lack of novelty on the conditional generation side, and limited insight gained from the qualitative results, I cannot recommend this paper for acceptance in its current form.
The paper focuses on self supervised learning (SSL) in the federated learning setting (FedSSL). Research in this area is timely and of significance. The authors phrase their work as primarily being an empirical study providing insights into the building blocks of FedSSL. The evaluation in the paper is quite thorough and the authors have been active in a detailed exchange regarding questions raised in the reviews. I would encourage the authors to fully implement the changes they promised into the revised manuscript and work towards timely release of open source code. (I appreciate internal policies of various institutions, but I do agree with the reviewers that it is more important that the code and experimental details be made public for papers such as this one, compared to some other papers.) I have chosen to disagree with some of the concerns raised in one of the reviews, in particular, I do agree with the authors that insights into the building blocks through empirical studies is a significant contribution, and also that FedEMA is a novel contribution. The discussion on this forum will remain for interested readers to come to their own conclusions about the relative performance of various methods.
This paper computes channel attention by considering feature maps across different layers, and named it the previous knowledge channel attention module (PKCAM). The reviewers find the proposed idea too straightforward and naive. Lack of technical contribution is one of the major criticisms. There are also correctness concerns with the submission. The authors have not provided any rebuttal.

We recommend rejecting the paper.
This paper studies different inductive biases that would improve OOD generalization (and in particular under translation, rotation and scaling) for image tasks. The study is focused on a toy dataset which allows authors to have more control over the data generation process and the transformations. Authors further show that iterative training using an auto encoder and presenting data in log polar space helps with rotation and scaling transformations on their toy dataset.


Strong Points:
  The paper is well written and easy to follow.
  The data generation process and the resulting toy dataset are novel and interesting.
  The experiment design and evaluations are solid.

Weak Points:
  No natural image datasets: While using a toy dataset has several benefits it does not grant that the conclusions would generalize to realistic settings. Reviewers have suggested several realistic datasets and I encourage authors to evaluate their findings on some of these datasets.
  Limited Baselines: As reviewers have pointed, comparison with baselines can be improved by including stronger baselines as well as more clear discussion about other techniques such as augmenting the data with the transformations.
  Related Work: A proper discussion of related work to set the context and highlight the contributions of this paper is missing. In particular, reviewers have pointed to prior work on the benefits of presenting the image in log polar space.

Unfortunately, authors did not engage with reviewers during the discussion period. Given the prior work and lack of any natural image dataset, I think the novelty and significance of this work is limited. Therefore, I recommend rejecting the paper. However, I encourage authors to improve the paper by addressing the points raised by the reviewers.
I recommend a rejection of this paper.

My overall impression is that this is genuinely an interesting topic and this a good basis for a solid paper, however, as pointed by several reviewers, there are multiple unanswered questions due to a very large scope of this work. It might be that a format of a conference paper is not the most appropriate for this work. The authors should consider instead submitting to some of the leading journals on medical image analysis, e.g. IEEE Transactions on Medical Imaging or Medical Image Analysis. I expect this work, as it is mostly empirical, would be appreciated there and could in fact make a much bigger impact if published there.