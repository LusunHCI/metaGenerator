this is an interesting approach that applies the idea of dynamically controlling the amount of information from the input fed into the classifier (some of the earlier approaches have used this idea for, e.g., parsing, real time translation, online speech recognition, and so on...) this is also related to some of the recent work on hierarchical recurrent nets [Chung et al.]. unfortunately, two of the reviewers and other commenters found this manuscript needs more work to clarify motivation, implication and relationship to other existing works, with which i don t necessarily disagree. 
Reviewers recognize the proposed method of hierarchical extension to ALI to be potentially novel and interesting but have expressed strong concerns on the experiments section. The paper also needs to have comparisons with relevant hierarchical generative model baselines. Not suitable for publication in its current form.
sadly, none of the reviewers seem to have been able to fully appreciate and check the proofs.  but in the words of even the least positive reviewer: In general, I find many of the observations in this paper interesting. However, this paper is not strong enough as a theory paper; rather, the value lies perhaps in its fresh perspective.  i think we can all gain from fresh perspectives of LSTMs and DL for NLP :) 
Pros: + Nice demonstration of the equivalence between scaling the learning rate and increasing the batch size in SGD optimization.  Cons:   While reporting convergence as a function of number of parameter updates is consistent, the paper would be more compelling if wall clock times were given in some cases, as that will help to illustrate the utility of the approach.   The paper would be stronger if additional experimental results, which the authors appear to have at hand (based on their comments in the discussion) were included as supplemental material.   The results are not all that surprising in light of other recent papers on the subject. 
The proposed relational reasoning algorithm is basically a fairly standard graph neural network, with a few modifications (e.g., the prediction loss at each layer   also not a new idea per se).   The claim that previously reasoning has not been considered in previous applications of graph neural networks (see discussion) is questionable.  It is not even clear what is meant here by  reasoning  as many applications of graph neural networks may be regarded as performing some kind of inference on graphs (e.g., matrix completion tasks by Berg, Kipf and Welling; statistical relational learning by  Schlichtkrull et al).  So the contribution seems a bit over stated.  Rather than introduces a new model, the work basically proposes an application of largely known model to two (not so hard) tasks which have not been studied in the context of GNNs. The claim that the approach is a general framework for dealing with complex reasoning problems is not well supported as both problems are (arguably) not complex reasoning problems (see R2).  There is a general consensus between reviewers that the paper, in its current form, does not quite meet acceptance criteria.  Pros:   an interesting direction   clarity Cons:   the claim of generality is not well supported   the approach is not so novel   the approach should be better grounded in previous work   
Dear authors,  While the reviewers appreciated the idea, the significant loss of accuracy was a concern. Even though you made significant changes to the submission, it is unfortunately unrealistic to ask the reviewers to do another review of a heavily modified version in such a short amount of time.  Thus, I cannot accept this paper for publication but I encourage you to address the reviewers  concerns and resubmit at a later conference.
 + interesting approach for a detailed analysis of the limitations of autoencoders in solving a simple toy problem    resulting insights somewhat trivial, not really novel, nor practically useful  > lacks demonstration of a gain on non toy task    regularization study too limited in scope: lacking theoretical grounding, and more exhaustive comparison of regularization schemes.
Two reviewers recommended rejection, and one was on the edge. There was no rebuttal to address the concerns and questions posed by the reviewers.
The paper makes overly strong claims, too weakly supported by a hard to follow and insufficiently rigorous mathematical argument. Connections with a large body of relevant prior literature are missing.
The reviewers are in agreement, that the paper is a big hard to follow and incorrect in places, including some claims not supported by experiments. 
Dear authors,  Based on the comments and your rebuttal, I am glad to accept your paper at ICLR.
The reviewers agree that this paper suffers from a lack of novelty and does not make sufficient contributions to warrant acceptance.
meta score: 8  This is a good paper which augments the data by mixing sound classes, and then learns the  mixing ratio.  Experiments performed on a number of sound classification results  Pros    novel approach, clearly explained    very good set of experimentation with excellent results    good approach to mixing using perceptual criteria  Cons    discussion doesn t really generalise beyond sound recognition  
This paper presents a framework where GANs are used to improve detection of outliers (in this context, instances of the “background class”). This is a very interesting and, as demonstrated, promising idea. However, the general feeling of the reviewers is that more work is needed to make the technical and evaluations parts convincing. Suggestions for further work towards this direction include: theoretical analysis, better presentation of the manuscript and, most importantly, stronger experimental section. 
The authors introduce a new activation function which is similar in shape to ELU, but is faster to compute.   The reviewers consider this to not be a significant innovation because the amount of time spent in computing the activation function is small compared to other neural network operations.
Paper was well written and rebuttal was well thought out and convincing.  The reviewers agree that the paper showed BNNs were good (relatively speaking) at resisting adversarial examples. Some question was raised about whether the methods would work on larger datasets and models. The authors offered some experiments in this regard in the rebuttal to this end. Also, a public comment appeared to follow up on CIFAR and report correlated results. 
The submission modifies the SPEN framework for structured prediction by adding an inference network in place of the usual combinatorial optimization based inference.  The resulting architecture has some similarity to a GAN, and significantly increases the speed of inference.  The submission provides links between two seemingly different frameworks: SPENs and GANs.  By replacing inference with a network output, the connection is made, but importantly, this massively speeds up inference and may mark an important step forward in structured prediction with deep learning.  
This paper proposes a non parametric method for metric learning and classification. One of the reviewers points out that it can be viewed as an extension of NCA. There is in fact a non linear version of NCA that was subsequently published, see [1]. In this sense, the approach here appears to be a version of nonlinear NCA with learnable per example weights, approximate nearest neighbour search, and the allowance of stale exemplars. In this view, there is concern from the reviewers that there may not be sufficient novelty for acceptance.  The reviewers have concerns with scalability. It would be helpful to include clarification or even some empirical results on how this scales compared to softmax. It is particularly relevant for larger datasets like Imagenet, where it may be impossible to store all exemplars in memory.  It is also recommended to relate this approach to metric learning approaches in few shot learning. Particularly to address the claim that this is the first approach to combine metric learning and classification.  [1]: Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure. Ruslan Salakhutdinov and Geoffrey Hinton.  AISTATS 2007 
This paper presents a GAN training algorithm motivated by online learning. The method is shown to converge to a mixed Nash equilibrium in the case of a shallow discriminator. In the initial version of the paper, reviewers had concerns about weak baselines in the experiments, but the updated version includes comparisons against a variety of modern GAN architectures which have been claimed to fix mode dropping. This seems to address the main criticism of the reviewers. Overall, this paper seems like a worthwhile addition to the GAN literature.
The paper presents an empirical study into sparse connectivity patterns for DenseNets.  Whilst sparse connectivity is potentially interesting, the paper does not make a strong argument for such sparse connectivity patterns: in particular, the results on ImageNet suggest that sparse connectivity performs substantially worse than full connectivity (at the same FLOPS level, Log DenseNet obtains ~2.5% lower accuracy than baseline DenseNet models, and the best Log DenseNet is ~4% worse than the best DenseNet). On CamVid, both network architectures appear to perform on par.  The paper motivates the model architecture by the high memory consumption of DenseNets but, frankly, that is a very weak motivation: DenseNets are actually very memory efficient if implemented correctly (https://arxiv.org/pdf/1707.06990.pdf). The fact that such implementations are not well supported by TensorFlow/PyTorch is a shortcoming of those deep learning frameworks, not in DenseNets. (In fact, the memory management features that deep learning frameworks have implemented to make residual networks memory efficient (for instance, caching GPU memory allocation in PyTorch) are far more complex than the "thousand lines of C++" currently needed to implement a DenseNet correctly.) Such issues will likely be resolved relatively soon by better implementations, and are hardly a good motivation for a different network architecture.
This paper builds on top of Cycle GAN ideas where the main idea is to jointly optimize the domain level translation function with an instance level matching objective. Initially the paper received two negative reviews (4,5) and a positive (7). After the rebuttal and several back and forth between the first reviewer and the authors, the reviewer was finally swayed by the new experiments. While not officially changing the score, the reviewer recommended acceptance. The AC agrees that the paper is interesting and of value to the ICLR audience.
The submission proposes a method for quantization.  The approach is reasonably straightforward, and is summarized in Algorithm 1.  It is the analysis which is more interesting, showing the relationship between quantization and adding Gaussian noise (Appendix B)   motivating quantization as regularization.  The submission has a reasonable mix of empirical and theoretical results, motivating a simple to implement algorithm.  All three reviewers recommended acceptance.
This paper proposes training binary values LSTMs for NLP using the Gumbel softmax reparameterization.  The motivation is that this will generalize better, and this is demonstrated in a couple of instances.  However, it s not clear how cherry picked the examples are, since the training loss wasn t reported for most experiments.  And, if the motivation is better generalization, it s not clear why we would use this particular setup.
The reviewers agreed that this paper is not quite ready for publication at ICLR.  One of the reviewers thought the paper was well written and easy to follow while the two others said the opposite.  One of the main criticisms was issues with the composition.  The paper seems to lack a clear formal explanation of the problem and the proposed methodology.  The reviewers in general weren t convinced by the experiments, complaining about the lack of a required baseline and that the proposed method doesn t seem to significantly help in the experiment presented.  Pros:   The proposed idea is interesting   The problem is timely and of interest to the community   Addresses multiple important problems at the intersection of ML and RL in sequence generation  Cons:   Novel but somewhat incremental   The experiments are not compelling (i.e. the results are not strong)   A necessary baseline is missing   Significant issues with the writing   both in terms of clarity and correctness.
This paper presents impressive results on scaling GANs to ILSVRC2012 dataset containing a large number of classes. To achieve this, the authors propose "spectral normalization" to normalize weights and stabilize training which turns out to help in overcoming mode collapse issues.  The presented methodology is principled and well written. The authors did a good job in addressing reviewer s comments and added more comparative results on related approaches to demonstrate the superiority of the proposed methodology. The reviewers agree that this is a great step towards improving the training of GANs.  I recommend acceptance.
The reviewers feel that the novelties in the model are not significant.   Furthermore, they suggest that empirical results could be improved by 1:  analyses showing how the significance network functions and directly measuring its impact 2: More reproducible experiments.  In particular, this is really an applications paper, and the experiments on the main application are not reproducible because the data is proprietary. 3: baselines that make assumptions more in line with the authors  problem setup
The reviewers agreed that the paper was too long (more than twice the recommended page limit not counting the appendix) and difficult to follow. They also pointed out that its central idea of learning the noise distribution in a VAE was not novel. While the shortened version uploaded by the authors looks like a step in the right direction, it was not sufficient to convince the reviewers.
The reviewers agree that the work is high quality, clear, original, and could be significant.  Despite this, the scores are borderline. The reason is due to rough agreement that the empirical evaluations are not quite there yet. In particular, two reviewers agree that, in the synthetic experiments, the method is evaluated on data that is an order of magnitude too easy and quite far from the nature of real data, which has a much lower signal to noise ratio.  However, the authors have addressed the majority of the concerns and there is little doubt that the authors are capable of carrying out this new experiment and reporting its results. Even if the results are surprising, they should shed light on what seems to be an interesting new approach.
The pros and cons of this paper cited by the reviewers can be summarized below:  Pros: * The method proposed here is highly technically sophisticated and appropriate for the problem of program synthesis from examples * The results are convincing, demonstrating that the proposed method is able to greatly speed up search in an existing synthesis system  Cons: * The contribution in terms of machine learning or representation learning is minimal (mainly adding an LSTM to an existing system) * The overall system itself is quite complicated, which might raise the barrier of entry to other researchers who might want to follow the work, limiting impact  In our decision, the fact that the paper significantly moves forward the state of the art in this area outweighs the concerns about lack of machine learning contribution or barrier of entry.
the proposed approach nicely incorporates various ideas from recent work into a single meta learning (or domain adaptation or incremental learning or ...) framework. although better empirical comparison to existing (however recent they are) approaches would have made it stronger, the reviewers all found this submission to be worth publication, with which i agree.
this submission introduces soft local reordering to the recently proposed SWAN layer [Wang et al., 2017] to make it suitable for machine translation. although only in small scale experiments, the results are convincing.
The paper based on cGAN developed a data augmentation GAN to deal with unseen classes of data. The paper developed new modifications to each component and designed network structure using ideas from state of the art nets. As pointed out by reviewer 1 & 2, the technical contribution is not sufficient. We hence recommend it to workshop publication.
This paper brings recent innovations in reinforcement learning to bear on a tremendously important application, treating sepsis.  The reviewers were all compelled by the application domain but thought that the technical innovation in the work was low.  While ICLR welcomes application papers, in this instance the reviewers felt that the technical contribution was not justified well enough.  Two of the reviewers asked for a more clear discussion of the underlying assumptions of the approach (i.e. offline policy evaluation and not missing at random).  Unfortunately, lack of significant revisions to the manuscript over the discussion period seem to have precluded changes to the reviewer scores.  Overall, this could be a strong submission to a conference that is more closely tied to the application domain.  Pros:   Very compelling application that is well motivated   Impressive (possibly impactful) results   Thorough empirical comparison  Cons:   Lack of technical innovation   Questions about the underlying assumptions and choice of methodology
The authors propose to detect anomaly based on its representation quality in the latent space of the GAN trained on valid samples.  Reviewers agree that:   The proposed solution lacks novelty and similar approaches have been tried before.   The baselines presented in the paper are primitive and hence do not demonstrate the clear benefits over traditional approaches. 
The paper makes a good theoretical contribution by formulating the GAN training as primal dual subgradient method for convex optimization and providing convergence proof. The authors then propose a modified objective to standard GAN training, based on this formulation, that helps address the mode collapse issue. One weak point of the paper as pointed out by reviewers is that that the experimental results are underwhelming and the approach may not scale well to high dimensional datasets / high resolution images. Interestingly, the proposed approach is general enough to be applied to other GAN variants that may address this issue in future. I recommend acceptance.
Authors do not respond to significant criticism   e.g. lack of a critical reference Reviewers unanimously reject. 
Paper reviewed by three experts who have provided detailed feedback. All three recommend rejection, and this AC sees no reason to overrule their recommendation. 
the paper validates the benefit of multi task learning on MNIST datasets, which is not sufficient for ICLR publication
Simple idea (which is a positive) to regularize RNNs, broad applicability, well written paper. Initially, there were concerns about  comparisons, but he authors have provided additional experiments that have made the paper stronger. 
This is an interesting and well written paper introducing two unbiased gradient estimators for optimizing expectations of black box functions. LAX can handle functions of both continuous and discrete random variables, while RELAX is specialized to functions of discrete variables and can be seen as a version of the recently introduced REBAR with its concrete relaxation based control variate replaced by (or augmented with) a free form function. The experimental section of the paper is adequate but not particularly strong. If Q prop is the most similar existing RL approach, as is state in the paper, why not include it as a baseline? It would also be good to see how RELAX performs at optimizing discrete VAEs using just the free form control variate (instead of combining it with the REBAR control variate).
The reviewers are satisfied that this paper makes a good contribution to policy gradient methods.
While the use of RNNs for building session based recommender systems is certainly an important class of applications, the main strength of the paper is to propose and benchmark practical modifications to prior RNN based systems that lead to performance improvements. The reviewers have pointed out that the writing in the paper needs improvement,  modifications are somewhat straightforward and some expected baselines such as comparisons against state of the art matrix factorization based methods is missing. As such the paper could benefit from a revision and resubmission elsewhere.
This clearly written paper describes a simple extension to hard monotonic attention   the addition of a soft attention mechanism that operates over a fixed length window of inputs that ends at the point selected by the hard attention mechanism.  Experiments on speech recognition (WSJ) and on a document summarization task demonstrate that the new attention mechanism improves significantly over the hard monotonic mechanism.  About the only "con" the reviewers noted is that the paper is a minor extension over Raffel et al., 2017, but the authors successfully argue that the strong empirical results render this simplicity a "pro." 
This is a paper introducing a hierarchical RL method which incorporates the learning of a latent space, which enables the sharing of learned skills.  The reviewers unanimously rate this as a good paper. They suggest that it can be further improved by demonstrating the effectiveness through more experiments, especially since this is a rather generic framework. To some extent, the authors have addressed this concern in the rebuttal. 
This paper provides an interesting synthesis of ideas. Although the results could be improved, this is a good paper.
The reviewers agreed that the paper was somewhat preliminary in terms of the exposition and empirical work.  They all find the underlying problem quite interesting and challenging (i.e. spiking recurrent networks).  However, the manuscript failed to motivate the approach.  In particular, everyone agrees that spiking networks are very interesting, but it s unclear what problem the presented work is solving.  The authors need to be more clear about their motivation and then close the loop with empirical validation that their approach is solving the motivating problem (i.e. do we learn something about biological plausibility, are spiking networks better than traditional LSTMs at modeling a particular kind of data, or are they more efficiently implemented on hardware?).  Motivating the work with one of these followed by convincing experiments would make this a much stronger paper.  Pros:   Tackles an interesting and challenging problem at the intersection of neuroscience and ML   A novel method for creating a spiking LSTM  Cons:   The motivation is not entirely clear   The empirical analysis is too simple and does not demonstrate the advantages of this approach   The paper seems unfocused and could use rewriting  
The reviewers agree the proposed idea is relatively incremental, and the paper itself does not do an exemplary job in other areas to make up for this.
This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection.
Pros: The proposed regularization for GAN training is interesting and simple to implement.  Cons:   Reviewers agree that the methodology is incremental over the WGAN with gradient penalty and the modification is not well motivated.   Experimental results do not clearly demonstrate the benefits of the proposed algorithm and the paper also lacks comparisons with related works. GIven the pros/cons, the committee feels the paper is not ready for acceptance in its current state.
The reviewers have unanimously expressed concerns about clarity, novelty, sound theoretical justification and intuitive motivation of the proposed approach. 
The reviewers agree that the authors have made an interesting contribution studying the effect of data augmentation, but they also agree that the claims made by the paper require a broader empirical study beyond the limited number of tasks surveyed in the current revision.  I urge the authors to follow this advice and see what they find.
The authors addressed the reviewers concerns but the scores remain somewhat low. The method is not super novel, but it is an incremental improvement over existing approaches.
This paper presents a method for using byte level convolutional networks for building text based autoencoders.  They show that these models do well compared to RNN based methods which model text in a sequence.  Evaluation is solely based on byte level prediction error.   The committee feels that the paper would have been stronger if evaluation was on some actual task (say summarization, Miao and Blunsom, for example) and show that it works as well as RNNs, the paper would have been stronger.
meta score: 4  This paper proposes an activation function, called displaced ReLU (DReLU), to improve the performance of CNNs that use batch normalization.  Pros    good set of experiments using CIFAR, with good results    attempt to explain the approach using expectations Cons    theoretical explanations are not so convincing    limited novelty    CIFAR is relatively limited set of experiments    does not compare with using bn after relu, which is now well studied and seems to address the motivation of this paper (and thus questions the conclusions)
This paper was perceived as being well written, but the technical contribution was seen as being incremental and somewhat heuristic in nature. Some important prior work was not discussed and more extensive experimentation was recommended. However, the proposed approach of partitioning the graph into sub graphs and a schedule alternating between intra and inter graph partitions operations has some merit.  The AC recommends inviting this paper to the Workshop Track.
With scores of 7 7 6  and the justification below the AC recommends acceptance.  One of the reviewers summarizes why this is a good paper as follows:  "This paper discusses several gradient based attribution methods, which have been popular for the fast computation of saliency maps for interpreting deep neural networks. The paper provides several advances:   This gives a more unified way of understanding, and implementing the methods.   The paper points out situations when the methods are equivalent   The paper analyses the methods  sensitivity to identifying single and joint regions of sensitivity   The paper proposes a new objective function to measure joint sensitivity" 
Pros: + The idea of end to end training that simultaneously learns the weights and appropriate precision for those weights is very appealing.  Cons:   Experimental results are far from the state of the art, which makes the empirical evaluation unconvincing.   More justification is needed for the update of the number of bits using the sign of the gradient. 
The authors present GraphVAE, a method for fitting a generative deep model, a variational autoencoder, to small graphs.  Fitting deep learning models to graphs remains challenging (although there is relevant literature as brought up by the reviewers and anonymous comments) and this paper is a strong start.  In weighing the various reviews, AnonReviewer3 is weighed more highly than AnonReviewer1 and AnonReviewer2 since that review is far more thorough and the reviewer is more expert on this subject.  Unfortunately, the review from AnonReviewer1 is extremely short and of very low confidence.  As such, this paper sits just below the borderline for acceptance.  In general, the main criticisms of the paper are that some claims are too strong (e.g. non differentiability of discrete structures), treatment of related work (missing references, etc.) and weak experiments and baselines.  The consensus among the reviews (even AnonReviewer2) is that the paper is preliminary.  The paper is close, however, and addressing these concerns will make the paper much stronger.  Pros:   Proposes a method to build a generative deep model of graphs   Addresses a timely and interesting topic in deep learning   Exposition is clear  Cons:   Treatment of related literature should be improved   Experiments and baselines are somewhat weak   "Preliminary"   Only works on rather small graphs (i.e. O(k^4) for graphs with k nodes)
The paper is interesting, and the update to the paper and additional experiments has already improved it in many ways, but the paper still does still not have as much impact as it could, by further strengthening the comparisons and usefulness in many of situations of current practice.
The authors did a good job addressing reviewer concerns and analyzing and  testing their model on interesting datasets with convincing results.
Both R1 and R2 suggested that Conceptors (Jaeger, 2014) had previously explored learning transformations in the context of reservoir computing. The authors acknowledged this in their response and added a reference. The main concern raised by the reviewers was lack of novelty and weak experiments (both the MNIST and depth maps were small and artificial). The authors acknowledged that it was mainly a proof of concept type of work. R1 and R2 also rejected the claim of biological plausibility (and this was also acknowledged by the authors). Though the authors have taken great care to respond in detail to each of the reviewers, I agree with the consensus that the paper does not meet the acceptance bar.
The paper studied defenses against adversarial examples by training a GAN and, at inference time, finding the GAN generated sample that is nearest to the (adversarial) input example. Next, it classifies the generated example rather than the input example. This defense is interesting and novel. The CelebA experiments the authors added in their revision suggest that the defense can be effective on high resolution RGB images.
PROS: 1. All the reviewers thought that the work was interesting and showed promise 2. The paper is relatively well written  CONS: 1. Limited experimental evaluation (just MNIST)  The reviewers were all really on the fence about this but in the end felt that while the idea was a good one and the authors were responsive in their rebuttal, the experimental evaluation needed more work. 
As stated by reviewer 3 "This paper introduces a new model to perform image classification with limited computational resources at test time. The model is based on a multi scale convolutional neural network similar to the neural fabric (Saxena and Verbeek 2016), but with dense connections (Huang et al., 2017) and with a classifier at each layer." As stated by reviewer 2 "My only major concern is the degree of technical novelty with respect to the original DenseNet paper of Huang et al. (2017). ".  The authors assert novelty in the sense that they provide a solution to improve computational efficiency and focus on this aspect of the problem. Overall, the technical innovation is not huge, but I think this could be a very useful idea in practice. 
The authors propose a technique for weight pruning that leaves block diagonal weights, instead of unstructured sparse weights, leading to faster inference.  However, the experiments demonstrating the quality of the pruned models are insufficient.   The authors also discuss connections to random matrix theory; but these connections are not worked out in detail.
There is significant discussion on this paper and high variance between reviewers:  one reviewer gave the paper a low score.  However the committee feels that this paper should be accepted at the conference since it provides a better framework for reproducibility, performs more large scale experiments than prior work.  One small issue the lack of comparison in terms of empirical results between this work and Zhang et al s work, but the responses provided to both the reviewers and anonymous commenters seem to be satisfactory.
This paper presents a simple tweak to hyperband to allow it to be run asynchonously on a large cluster, and contains reasonably large scale experiments.  The paper is written clearly enough, and will be of interest to anyone running large scale ML experiments.  However, it falls below the bar by: 1) Not exploring the space of related ideas more. 2) Not providing novel insights. 3) Not attempting to compare against model based parallel approaches.
The paper addresses the training time of CNNs, in the common setting where a CNN is trained on one domain and then used to extract features for another domain.  The paper proposes to speed up the CNN training step via a particular proposed training schedule with a reduced number of epochs.  Training time of the pre trained CNN is not a huge concern, since this is only done once, but optimizing training schedules is a valid and interesting topic of study.   However, the approach here does not seem novel; it is typical to adjust training schedules according to the desired tradeoff between training time and performance.  The experimental validation is also thin, and the writing needs improvement.
+ clearly written and thorough empirical comparison of several metrics/divergences for evaluating GANs, prominently parametric critic based divergences.    little technical novelty with respect to prior work. As noted by reviewers and an anonymous commentator:  using an Independent critic for evaluation has been proposed and used in practice before.  +  the contribution of the work thus lies primarily in its well done and extensive empirical comparisons of multiple metrics and models    
Pros: The paper aims to unify classification and novelty detection which is interesting and challenging.  Cons:   The reviewers find that the work is incremental and contains heuristics. Reviewers find the repurposing of the fake logit in semi supervised GAN discriminator for assigning novelty strange.   The experiments presented are weak and authors do not compare with traditional/stronger approaches for novelty detection such as "learning with abstention" models and density models. GIven the pros and cons, the committe finds the paper to fall short of acceptance in its current form. 
The submission addresses the problem of multiset prediction, which combines predicting which labels are present, and counting the number of each object.  Experiments are shown on a somewhat artificial MNIST setting, and a more realistic problem of the COCO dataset.  There were several concerns raised by the reviewers, both in terms of the clarity of presentation (Reviewer 1), and that the proposed solution is somewhat heuristic (Reviewer 3).  On the balance, two of three reviewers did not recommend acceptance.
This submission presents intriguingly good results on k shot learning and I agree with the authors that the results are better than the presented previous work, and that the method is simple, so I took a deeper look into the paper despite the overall negative reviews. However, I think in its current form, the paper is not suitable for publication:    The previous work, that the authors compare to, were not really using comparable architectures: in fact, likely much worse base models with fewer parameters etc. I think any future version of this work would need to control for architecture capacity, otherwise how can we be sure where the gains come from? To me, this is a major unknown in terms of the credit assignment for the great results.   The authors should be comparing with MAML (and follow up work) by Finn et al. (2017)   I don t really understand why the authors claim to have no need for validation sets. That s a very strong claim: are ALL the hyper parameters (model architectures etc) just chosen in another, principled way? This issue would definitely need to be addressed in a follow up work.
The committee feels that this paper presents a simple, yet effective way to adapt language models from various users in a sufficiently privacy preserving way.  Empirical results are quite strong.  Reviewer 3 says that the novelty of the paper is not great, but does not provide any references to prior work that are similar to this paper.  The meta reviewer finds the responses to Reviewer 3 sufficient to address the concerns.  Similarly, Reviewer 2 says that the paper may not be relevant to ICLR, but the committee feels its content does belong to the conference since the topic is extremely relevant to modern language processing techniques.  In fact, the authors provide several references that show that this paper is similar in content to those submissions.  Reviewer 1 s concerns are also not sufficiently strong to warrant rejection.  The responses to each criticism suffices and the meta reviewer thinks that this paper will add value to the conference.
the problem is interesting, and the approach is also interesting. however, the reviewers have found that this manuscript would benefit from more experiments, potentially involving some real data (even at least for evaluation) in addition to largely synthetic data sets used in the submission. i also agree with them and encourage authors to consider this option.
The paper proposes a GAN based approach for disentangling identity (or class information) from style. The supervision needed is the identity label for each image. Overall, the reviewers agree that the paper makes a novel contribution along the line of work on disentangling  style  from  content . 
Some reviewers seem to assign novelty to the compression and classification formulation; however, semi supervised autoencoders have been used for a long time. Taking the compression task more seriously as is done in this paper is less explored.  The paper provides some extensive experimental evaluation and was edited to make the paper more concise at the request of reviewers. One reviewer had a particularly strong positive rating, due to the quality of the presentation, experiments and discussion. I think the community would like this work and it should be accepted. 
I ve summarized the pros and cons of the reviews below:  Pros: * The method for time representation in event sequences is novel and well founded * It shows improvements on several but not all datasets that may have real world applications  Cons: * Gains are somewhat small * The task is also not of huge interest to ICLR in particular, and thus the paper might be of limited interest  As a result, because the paper is well done, but drew little excitement from any of the reviewers, I suggest that this not be accepted to the main conference, but encouraged to present at the workshop track.
There was quite a bit of discussion about this paper but in the end the majority felt that, though the paper is interesting, the results are too limited and more needs to be done for publication.  PROS: 1. Good comparison of state space model variations 2. Good writing (perhaps a bit dense in places) 3. Promising results, especially concerning speedup  CONS: 1. The evaluation is quite limited  
The paper proposes interesting  deep learning based spectral clustering techniques. The use of functional embeddings for enabling spectral clustering to have an out of sample extension has of course been explored earlier (e.g., see Manifold Regularization work of Belkin et al, JMLR 2006). For polynomials or kernel based spectral clustering, the orthogonality of the outputs can be exactly handled via a generalized eigenvector problem, while here the arguments are statistically flavored and not made very clear in the original draft. Some crucial comparisons, e.g., against large scale versions of vanilla spectral clustering and against other methods that generalize to new samples is missing or not thorough enough. See reviews for more precise description of issues. As such the paper will benefit from a revision. 
although the authors argue that their experiments were selected from the earlier work from which major comparing approaches were taken, the reviewers found the empirical result to be weak. why not some real tasks (i do not believe bAbI nor PTB could be considered real) that could clearly reveal the superiority of the proposed unit against existing ones?
The paper proposes a new method for interpreting the hidden units of neural networks by employing an Indian Buffet Process. The reviewers felt that the approach was interesting, but at times hard to follow and more analysis was needed. In particular, it was difficult to glean any advantage of this method over others. The authors did not provide a response to the reviews.
The paper contributes to a body of empirical work towards understanding generalization in deep learning. They do this  through a battery of experiments studying "single directions" or selectivity of small groups of neurons. The reviewers that have actively participated agree that the revision is of high quality, impact, originality, and significance. The issue of a lack of prescriptiveness was raised by one reviewer. I agree with the majority that this is not necessary, but nevertheless, the revision makes some suggestions.  I urge the authors to express the appropriate amount of uncertainty regarding any prescriptions that have not been as thoroughly vetted!  
The author s propose to use swish and show that it performs significantly better than Relus on sota vision models. Reviewers and anonymous ones counter that PRelus should be doing quite well too. Unfortunately, the paper falls in the category where it is hard to prove the utility of the method through one paper alone, and broader consensus relies on reproduction by the community. As a results, I m going to recommend publishing to a workshop for now.
A version of GCNs of Kipf and Welling is introduced with (1) no non linearity; (2) a basic form of (softmax) attention over neighbors where the attention scores are computed as the cosine of endpoints  representations (scaled with a single learned scalar). There is a moderate improvement on Citeseer, Cora, Pubmed.  Since the use of gates with GCNs / Graph neural networks is becoming increasingly common (starting perhaps with GGSNNs of Li et al, ICLR 2016)) and using attention in graph neural networks is also not new  (see reviews and comments for references), the novelty is very limited.  In order to make the submission more convincing the authors could: (1) present results on harder datasets; (2)  carefully evaluate against other forms of attention (i.e. previous work).  As it stands, though it is interesting to see that such simple model performs well on the three datasets, I do not see it as an ICLR paper.  Pros:   a simple model, achieves results close / on par with state of the art  Cons:   limited originality   either results on harder datasets or / and evaluation agains other forms of attention (i.e. previous work) are needed   
This paper constructs a variant of deep CNNs which is provably invertible, by replacing spatial pooling with multiple shifted spatial downsampling, and capitalizing on residual layers to define a simple, invertible representation. The authors show that the resulting representation is equally effective at large scale object classification, opening up a number of interesting questions.  Reviewers agreed this is an strong contribution, despite some comments about the significance of the result; ie, why is invertibility a "surprising" property for learnability, in the sense that F(x)   {x,  phi(x)}, where phi is a standard CNN satisfies both properties: invertible and linear measurements of F producing good classification. All in all, this will be a great contribution to the conference. 
Clearly explained, well motivated and empirically supported algorithm for training deep networks while simultaneously learning their sparse connectivity. The approach is similar to previous work (in particular Welling et al., Bayesian Learning via Stochastic Gradient Langevin Dynamics, ICML 2011) but is novel in that it satisfies a hard constraint on the network sparsity, which could be an advantage to match neuromorphic hardware limitations.
The pros and cons of the paper under consideration can be summarized below:  Pros: * Reviewers thought the underlying model is interesting and intuitive * Main contributions are clear  Cons: * There is confusion between keywords and topics, which is leading to a somewhat confused explanation and lack of clear comparison with previous work. Because of this, it is hard to tell whether the proposed approach is clearly better than the state of the art. * Typos and grammatical errors are numerous  As the authors noted, the concerns about the small dataset are not necessarily warranted, but I would encourage the authors to measure the statistical significance of differences in results, which would help alleviate these concerns.  An additional comment: it might be worth noting the connections to query based or aspect based summarization, which also have a similar goal of performing generation based on specific aspects of the content.  Overall, the quality of the paper as is seems to be somewhat below the standards of ICLR (although perhaps on the borderline), but the idea itself is novel and results are good. I am not recommending it for acceptance to the main conference, but it may be an appropriate contribution for the workshop track.
This work takes dialogue acts into account to generate responses in a human machine conversation. However, incorporating dialogue acts into open domain dialogue was already the focus of Zhao et al s ACL 2017 paper, Learning Discourse level Diversity for Neural Dialog Models using Conditional Variational Autoencoders, and using dialogue acts in a policy for human machine conversation was also an idea that already appeared in Serban et al 2017, A Deep Reinforcement Learning Chatbot. Despite the authors  response that tries to adjust their claims and incorporate a more thorough overview, I encourage the authors to re work their research with a much more careful and reliable examination of previous work and how their effort should be understood in that more comprehensive context.
The idea of using the determinant of the covariance matrix over inputs to select experiments to run is a foundational concept of experimental design.  Thus it is natural to think about extending such a strategy to sequential model based optimization for the hyperparameters of machine learning models, using recent advances in determinantal point processes.  The idea of sampling from k DPPs to do parallel hyperparameter search, balancing quality and diversity of expected outcomes, seems neat.  While the reviewers found the idea interesting, they saw weaknesses in the approach and most importantly were not convinced by the empirical results.  All reviewers thought that the baselines were inappropriate given recent work in hyperparameter optimization (and classic work in statistics).  Pros:   Useful to a large portion of the community (if it works)   An interesting idea that seems timely  Cons:   Only slightly outperforms baselines that are too weak   Not empirically compared to recent literature   Some of the design and methodology require more justification   Experiments are limited to small scale problems
The paper presents a really interesting take on the mode collapse problem and argue that the issue arises because of the current GAN models try to model distributions with disconnected support using continuous noise and generators. The authors try to fix this issue by training multiple generators with shared parameters except for the last layer.  The paper is well written and authors did a good job in addressing some of the reviewer concerns and improving the paper.  Even though arguments presented are novel and interesting, reviewers agree that the paper lacks sufficient theoretical or experimental analysis to substantiate the claims/arguments made in the paper. Limited quantitative and subjective results are not always in favor of the proposed algorithm. More controlled toy experiments and results on larger datasets are needed. The central argument about "tunneling" is interesting and needs deeper investigation. Overall the committee recommends this paper for workshop.
Pros   Lays out bounds for multi domain adaptation based on earlier work on a single source target domain pair.   Shows gains over choosing the best source domain for a target domain, or naively combining domains.  Cons   The reviewers agree that the extensions are relatively straightforward extensions to single source target pair.   Hard max doesn’t consider the partial contribution of multiple source domains, and considers the worst case scenario.   Soft max addresses some of these issues; the authors provide reasonable justification for the algorithm but it’s not clear that the specific choice of \alphas leads to the tightest bound.  The reviewers noted that the authors significantly improved the paper during the revision process. The AC feels that the presented techniques would be of interest to the community and would help lead discussions towards theoretically optimal ways to do domain adaptation given multiple domains. The authors are therefore encouraged to submit to the workshop track. 
Two reviewers recommended rejection, and one is slightly more positive. The main concern is that the experiments are not convincing (ie, the number of base and added classes is very small). Furthermore, while the paper introduces several interesting ideas, the AC agrees with the second reviewer that each of these could be explored in more detail. This work seems preliminary. The authors are encouraged to resubmit to a future conference.
The paper proposes a new way to understand why neural networks generalize well. They introduce the concept of ensemble robustness and try to explain DNN generalization based on this concept. The reviewers feel the paper is a bit premature for publication in a top conference although this new way of explaining generalization is quite interesting.
The submission proposes to use GANs to learn a generative model of fMRI scans that can then be used for downstream classification tasks.  Although there was some appreciation from the reviewers of the approach, there were several important remaining concerns:  1) From Reviewer 1: "Generating high resolution images with GANs even on faces for which there is almost infinite data is still a challenge. Here a few thousand data points are used. So it raises too concerns: First is it enough?"  and  2) R1 and R2 both raised concerns about the significance of the improvements.  Looking through the tables, there are many reported differences that are reasonably small, and no error bars or significance are given.  This should be a requirement for an empirical paper about fMRI.
I m inclined to recommend accepting this paper, although it is borderline given the strong dissenting opinion. The revisions have addressed many of the concerns about quality, clarity, and significance. The paper gives an end to end explanation in Bayesian terms of generalization in neural networks using SGD.  However, it is my opinion that Bayesian statistics is not, at present, a theory that can be used to explain why a learning algorithm works. The Bayesian theory is too optimistic: you introduce a prior and model and then trust both implicitly. Relative to any particular prior and model (likelihood), the Bayesian posterior is the optimal summary of the data, but if either part is misspecified, then the Bayesian posterior carries no optimality guarantee. The prior is chosen for convenience here. And the model (a neural network feeding into cross entropy) is clearly misspecified.  However, there are ways to sidestep both these issues using a frequentist theory closely related to Bayes, which can explain generalization. Indeed, you cite a recent such paper by Dzugate and Roy who use PAC Bayes. However, you citation is disappointingly misleading: a reader would never know that these authors are also responding to Zhang, have already proposed to explain "broad minima" in (PAC )Bayesian terms, and then even get nonvacuous bounds. (The connection between PAC bayes and marginl likelihood is explained by Germain et al. "PAC Bayesian Theory Meets Bayesian Inference").  Dzugate et al don t propose to explain why SGD finds such "good" minima. So I would say, your work provides the missing half of their argument. This work deserves more prominent placement and shouldn t be buried on page 5. Indeed, it should appear in the introduction and a proper description of the relationship should be given. 
While the reviewers all seem to think this is interesting and basically good work, the Reviewers are consistent and unanimous in rejecting the paper. While the authors did provide a thorough rebuttal, the original paper did not meet the criteria and the reviewers have not changed their scores.
The paper proposes a multitask deep learning method (called Deep AMFTL) for preventing negative transfer. Despite some positive experimental results, the contribution of the paper is not sufficient for publication at ICLR due to several issues: similarity between the proposed method and existing method (e.g., AMTL), unclear rationale/intuition of the proposed model, clarity of presentation, technical formulation, and limited empirical evaluations (see reviewer comments for details). No author rebuttal was submitted. 
The paper proposes a novel method for training a non autoregressive machine translation model based on a pre trained auto regressive model. The method is interesting and the evaluation is carried out well. It should be noted, however, that the relative complexity of the training procedure (which involves multiple stages and external supervision) might limit the practical applicability and impact of the technique.
This paper proposes a “warp operator” based on Taylor expansion that can replace a block of layers in a residual network, allowing for parallelization. Taking advantage of multi GPU parallelization the paper shows increased speedup with similar performance on CIFAR 10 and CIFAR 100. R1 asked for clarification on rotational symmetry. The authors instead removed the discussion that was causing confusion (replacing with additional experimental results that had been requested). R2 had the most detailed review and thought that the idea and analysis were interesting. They also had difficulty following the discussion of symmetry (noted above). They also pointed out several other issues around clarity and had several suggestions for improving the experiments which seem to have been taken to heart by the authors, who detailed their changes in response to this review. There was also an anonymous public comment that pointed out a “fatal mathematical flaw and weak experiments”. There was a lengthy exchange between this reviewer and the authors, and the paper was actually corrected and clarified in the process. This anonymous poster was rather demanding of the authors, asking for latex formatted equations, pseudo code, and giving direction on how to respond to his/her rebuttal. I don t agree with the point that the paper is flawed by "only" presenting a speed up over ResNet, and furthermore the comment of "not everyone has access to parallelization" isn’t a fair criticism of the paper.
Dear authors,  While I appreciate the result that a convolutional layer can have full rank output, this allowing a dataset to be classified perfectly under mild conditions, the fact that all reviewers expressed concern about the statement is an indication that the presentation sill needs quite a bit of work.  Thus, I recommend it as an ICLR workshop paper.
Thank you for submitting you paper to ICLR. This paper was enhanced noticeably in the rebuttal period and two of the reviewers improved their score as a result. There is a good range of experimental work on a number of different tasks. The addition of the comparison with Liu & Feng, 2016 to the appendix was sensible. Please make sure that the general conclusions drawn from this are explained in the main text and also the differences to Tran et al., 2017 (i.e. that the original model can also be implicit in this case).
This paper with the self explanatory title was well received by the reviewers and, additionally, comes with available code. The paper builds on prior work (Sinkhorn operator) but shows additional, significant amount of work to enable its application and inference in neural networks.  There were no major criticisms by the reviewers, other than obvious directions for improvement which should have been already incorporated in the paper, issues with clarity and a little more experimentation. To some extent, the authors addressed the issues in the revised version.   
This work introduces a trainable signal representation for spherical signals (functions defined in the sphere) which are rotationally equivariant by design, by extending CNNs to the corresponding group SO(3). The method is implemented efficiently using fast Fourier transforms on the sphere and illustrated with compelling tasks such as 3d shape recognition and molecular energy prediction.  Reviewers agreed this is a solid, well written paper, which demonstrates the usefulness of group invariance/equivariance beyond the standard Euclidean translation group in real world scenarios. It will be a great addition to the conference. 
New effective kernel learning methods are very well aligned with ICLR s focus on Representation Learning. As a reviewer pointed out, not all aspects of the paper are algorithmically "clean". However, the proposed approach is natural and appears to give consistent improvements over a couple of expected baselines. The paper could be strengthened with more comparisons against other kernel learning methods, but acceptance at ICLR 2018 will increase the diversity of the conversation around advances in Representation Learning.
Joint optimization of dimensionality reduction and temporal clusters. Results suggest performance improvement in a variety of scenarios versus a baseline of a recent state of art clustering method.  Pro:   Joint optimization may be new and results suggest performance improvement when done on NASA Magnetospheric Multiscale (MMS) Mission.  Con:   Small datasets evaluated, impact unclear   Breadth of possible applications unclear   Similarities exist to prior works. Significance of novelty not clear.   Unanimous consensus among reviewers that work is not in a state to be accepted.
The submission formulates self paced learning as a specific iterative mini max optimization, which incorporates both a risk minimization step and a submodular maximization for selecting the next training examples.  The strengths of the paper lie primarily in the theoretical analysis, while the experiments are somewhat limited to simple datasets: News20, MNIST, & CIFAR10.  Additionally, the main paper is probably too long in its current form, and could benefit from some of the proof details being moved to the appendix.  
The reviewers present strong concerns about the lack of novelty in the paper. Further there are strong concerns about how the experiments are conducted. I recommend the authors to carefully go through the reviews.
In general, this seems like a sensible idea, but in my opinion the empirical results do not show a very compelling margin between using *entropy* as an active learning selection criterion vs the proposed methods. The difference is small enough that in practice it is very hard for me to believe that many researchers would choose to use the meta learning via deep RL method (given that they d need to train on multiple datasets and tune REINFORCE which is not going to be obviously easy). For that reason I am inclined to reject the paper.  In a follow up version, I would heed the advice of Reviewer 1 and do more ablation analyses to understand the value of myopic vs non myopic, cross dataset vs. not, bandits vs RL, on the fly vs not (these are all intermingled issues). The relative lack of such analyses in the paper does not help in terms of it passing the bar.
This paper proposes an interesting analysis of the limitations of WGANs as well as a solution to these limitations. I am not too convinced by the experimental part as, as some of the reviewers have mentioned, it relies on hyperparameters which can be hard to tune.  The more theoretical part, even if it could be written with more care as pointed out by reviewer 2, is nonetheless interesting and could stir discussion. I think it would be a good addition to ICLR as a poster.
The main idea of policy as inference is not new, but it seems to be the first application of this idea to deep RL, and is somewhat well motivated.  The computational details get a bit hairy, but the good experimental results and the inclusion of ablation studies pushes this above the bar. 
The reviewers have pointed out that there is a substantial amount of related work that this paper should be acknowledging and building on.
Thank you for submitting you paper to ICLR. The big picture idea is fairly simple, although the implementation is certainly challenging requiring a deep generative model to be trained as part of the final system. The experimental validation is not sufficient to warrant publication. A comparison to a larger number of competitors e.g. [1,2] on a greater range of tasks is required.  [1] Continual Learning Through Synaptic Intelligence Friedemann Zenke BenPoole SuryaGanguli, ICML 2017 [2] Gradient Episodic Memory for Continual Learning, David Lopez Paz and Marc’Aurelio Ranzato, NIPS 2017
The paper proposes to launch adversarial attacks in the latent space of VAE such that the minimal change in the latent representation leads to the decoder producing an image with class predictions altered.  Given the pros/cons the paper in its current form falls short of acceptance.  Pros: Reviewers agree that the paper is well written and easy to follow  Cons:   The paper lacks novelty and uses standard attacks and defense methodology.   Reviewers find the attack scenario presented is unrealistic and hence may not useful.   Experiments lack rigorous comparisons with baselines and it is not clear if the attack in the latent space will be stronger than the attack in the input space. 
The paper presents AdvGAN: a GAN that is trained to generate adversarial examples against a convolutional network. The motivation for this method is unclear: the proposed attack does not outperform simpler attack methods such as Carlini Wagner attack. In white box settings, a clear downside for the attacker is that it needs to re train its GAN everytime the defender changes its convolutional network.  More importantly, the work appears preliminary. In particular, the lack of extensive quantitative experiments on ImageNet makes it difficult to compare the proposed approach to alternative attacks methods such as (I )FGSM, DeepFool, and Carlini Wagner. The fact that AdvGAN performs well on MNIST is nice, but MNIST should be considered for what it is: a toy dataset. If AdvGANs are, as the authors state in their rebuttal, fast and good at generating high resolution images, then it should be straightforward to perform comprehensive experiments with AdvGANs on ImageNet (rather than focusing on a small number of images on a single target, as the authors did in their revision)?
this submission has two results; (1) it defines what it means for the optimal representation is, although this is rather uninteresting that it simply says that if the representation from a model is going to be used based on some given metric, the cost function should directly reflect it, and (2) it shows that different choices of encoding and decoding have different implications. as with most of the reviewers, i found these to be a rather weak contribution.
The paper proposes a simple modification to conditional GANs, where the discriminator involves an inner product term between the condition vector y and the feature vector of x. This formulation is reasonable and well motivated from popular models (e.g., log linear, Gaussians). Experimentally, the proposed method is evaluated on conditional image generation and super resolution tasks, demonstrating improved qualitative and qualitative performance over the existing state of the art (AC GAN). 
The paper introduces an interesting family of two player zero sum games with tunable complexity, called Erdos Selfridge Spencer games, as a new domain for RL.  The authors report on extensive empirical results using a wide variety of training methods, including supervised learning and several flavors of RL (PPO, A2C, DQN) as well as single agent vs. multi agent training.  The reviewers also appear to agree that the method appears to be technically correct, clearly written, and easy to read.  A drawback of the paper is that it does not make a *significant* contribution to the field.  In combing through the reviewer comments, none of them identify a significant contribution.  Even in the text of the paper, the authors do not anywhere claim to have made a significant contribution. As the paper is still interesting, the committee would like to recommend this for the workshop track.  Pros:         Interesting domain with tunable complexity         High quality extensive empirical results         Writing is clear  Cons:         Lacks a significant contribution         Appears to overlook self play, the dominant RL training paradigm for decades (multiagent training appears to be related but different)         Per Reviewer3, "I remain unconvinced that these games are good general tests for Deep RL"
Reviews are marginal. I concur with the two less favorable reviews that the metrics  for privacy protection are not sufficiently strong for preserving privacy. 
State of the art results on Squad (at least at time of submission) with a nice model. Authors have since applied the model to additional tasks (SNLI). Good discussion with reviewers, well written submission and all reviewers suggest acceptance. 
This is a meta learning approach to model compression which trains 2 policies using RL to reduce the capacity (computational cost) of a trained network while maintaining performance, such that it can be effectively transferred to a smaller student network. The approach has similarities to recently proposed methods for architecture search, but is significantly different. The paper is well written and the experiments are clear and convincing. One of the reviews was unacceptable; I am not considering it (R1).
Based on the positive reviews, I recommend acceptance. The paper analyzes when empirical risk is close to the population version, when empirical saddle points are close to the population version and empirical gradients are close to the population version.
The reviewers (all experts in this area) appreciated the novelty of the idea, though they felt that the experimental results (samples and Inception scores) did not provide convincing evidence value of this method over already established techniques. The authors responded to the concerns but were not able to address the issue of evaluation due to time constraints. The idea is likely sound but evaluation does not meet the bar, it may make a good contribution as a workshop paper.
PROS: 1. good results; the authors made it work 2. paper is largely well written  CONS: 1. some found the writing to be unclear and sloppy in places 2. the algorithm is complicated   a chain of sub algorithms  A few small points:   I initially found Algorithm 1 to be confusing because it wasn t clear whether it was intended to be invoked for each task (making the training depend on all the datasets).  I finally convinced myself that this was not the intention and that the inner loop of the algorithm is what is actually executed incrementally.  
The paper received three good quality reviews which were in agreement that the paper was below the acceptance threshold. The authors are encouraged to follow the suggestions from the reviews to revise the paper and resubmit to another venue.
This paper studies an important problem (visual relationship detection and generalization capabilities existing networks for this task). Unfortunately, all reviewers raise concerns (e.g. limited relations studied) and are largely on the fence about this paper. While this paper does not propose solutions, it does present interesting "negative results" that should get some visibility in the workshop track. 
The proposed method combines supervised pretraining given some expert data and further uses the supervision to regularize the Q updates to prevent the agent from exploring  nonsense  directions. There a significant problems with the paper: the approach is not novel, the assumption of large amounts of expert data is problematic, and the claim of vastly accelerated learning is not supported empirically, either in the main paper or in the additional mujoco experiments added in the appendix.
This is an interesting paper that provides modeling improvements over several strong baselines and presents SOTA on Squad.  One criticism of the paper is that it evaluates only on Squad, which is somewhat of an artificial task, but we think for publication purposes at ICLR, the paper has a reasonable set of components.
 + The paper proposes an interesting empirical measure of ""learnability"" of a trained network: how well the predictive function it represents can be learned by another network. And shows it empirically seems to correlate with better generalization.    The work is purely empirical: it features no theory relating this learnability to generalization    Learnability measure is somewhat ad hoc with moving parts left to be specified (learning network, data splits, ...)    as pointed out by a reviewer, learnability doesn t really provide any answers for now.    the work would be much stronger if it went beyond a mere correlation study, and if learnability considerations allowed to derive a new approach/regularization scheme that was convincingly shown to improve generalization. 
Meta score: 5  The paper explores an interesting idea, addressing a known bias in truncated BPTT by sampling across different truncated history lengths.  Limited theoretical analysis is presented along with PTB language modelling experimentation.   The experimental part could be stronger (e.g. trying to improve over the baseline) and perhaps more than just PTB.  Pros:    interesting idea Cons:    limited analysis    limited experimentation  
The authors propose a system for asynchronous, model parallel training, suitable for dynamic neural networks.  To summarize the reviewers:  PROS: 1. Paper contrasts well with existing work. 2. Positive results on dynamic neural network problems. 3. Well written and clear  CONS: 1. Some concern about extrapolations/estimates to hardware other than that on CPU. 2. Comparisons with Dynet seem to suggest auto batching results in a dynamic mode aren t very positive.  For 1) the AC notes the author s objections to reviewer 1 s views on the value of estimation/extrapolation to non CPU hardware.  However, reviewer 3 voiced  a similar concern and  both still feel that there is more to be done to be convincing in the experiments.
The paper defines a new measure of influence and uses it to highlight important features. The definition is novel however, the reviewers have concerns regarding its significance, novelty and a thorough empirical comparison to existing literature is missing.
This paper proposes a model which learns simultaneously the dynamics of sequential data, together with a static latent representation. The idea and motivation is interesting and the results are promising.  However, all reviewers agree that the presentation needs much more work to convey the messages correctly and convincingly. Moreover, the reviewers question some design choices and lack of discussion of the results. No rebuttal has been provided.  
this submission proposes a novel extension of existing recurrent networks that focus on capturing long term dependencies via tracking entities/their statesand tested it on a new task. there s a concern that the proposed approach is heavily engineered toward the proposed task and may not be applicable to other tasks, which i fully agree with. i however find the proposed approach and the authors  justification to be thorough enough, and for now, recommend it to be accepted.
This work proposes a hybrid system for large scale distributed and federated training of commonly used deep networks. This problem is of broad interest and these methods have the potential to be significantly impactful, as is attested by the active and interesting discussion on this work. At first there were questions about the originality of this study, but it seems that the authors have now added extra references and comparisons.  Reviewers were split about the clarity of the paper itself. One notes that "on the whole clearly presented", but another finds it too dense, disorganized and needing of more clear explanation. Reviewers were also concerned that methods were a bit heuristic and could benefit from more details. There were also many questions about these details in the discussion forum, these should make it into the next version.  The main stellar aspect of the work were the experimental results, and reviewers call them "thorough" and note they are convincing. 
The reviewers find the work interesting and well made, but are concerned that ICLR is not the right venue for the work.  I will recommend that the paper be accepted, but ask the authors to add the NMT results to the main paper (any other non synthetic applications they could add would be helpful).
This paper introduces a framework for specifying the model search space for exploring over the space of architectures and hyperparameters in deep learning models (often referred to as architecture search).  Optimizing over complex architectures is a challenging problem that has received significant attention as deep learning models become more exotic and complex.  This work helps to develop a methodology for describing and exploring the complex space of architectures, which is a challenging problem.  The authors demonstrate that their method helps to structure the search over hyperparameters using sequential model based optimization and Monte Carlo tree search.  The paper is well written and easy to follow.  However, the level of technical innovation is low and the experiments don t really demonstrate the merits of the method over existing strategies.  One reviewer took issue with the treatment of related work.  The underlying idea is compelling and addresses an open question that is of great interest currently.  However, without experiments demonstrating that this works better than, e.g., the specification in the hyperopt package, it is difficult to assess the contribution.  The authors must do a better job of placing this contributing in the context of existing literature and empirically demonstrate its advantages.  The presented experiments show that the method works in a limited setting and don t explore optimization over complex spaces (i.e. over architectures   e.g. number of layers, regularization for each layer, type of each layer, etc.).  There s nothing presented empirically that hasn t been possible with standard Bayesian optimization techniques.  This is a great start, but it needs more justification empirically (or theoretically).  Pros:   Addresses an important and pertinent problem   architecture search for deep learning   Provides an intuitive and interesting solution to specifying the architecture search problem   Well written and clear  Cons:   The empirical analysis does not demonstrate the advantages of this approach over existing literature   Needs to place itself better in the context of existing literature
The paper proposes to use simple regression models for predicting the accuracy of a neural network based on its initial training curve, architecture, and hyper parameters; this can be used for speeding up architecture search. While this is an interesting direction and the presented experiments look quite encouraging, the paper would benefit from more evaluation, as suggested by reviewers, especially within state of the art architecture search frameworks and/or large datasets.
The paper sets out to analyze the problem of exploding gradients in deep nets which is of fundamental importance. Reviewers largely acknowledge the novelty of the main ideas in the paper towards this goal, however it is also strongly felt that the writing/presentation of the paper needs significant improvement to make it into a coherent and clean story before it can be published. There are also some concerns on networks used in the experiments not being close to practice.  I recommend invitation to the workshop track as it has novel ideas and will likely generate interesting discussion. 
The reviewers really liked this paper. This paper presents a tweak to the LSTM cell that introduces sparsity, thus reducing the number of parameters in the model.  The authors show that their sparse models match the performance of the non sparse baselines. While the results are not state of the art but vanilla implementations of standard models, this is still of interest to the community.
The effectiveness of active learning techniques for training modern deep learning pipelines in a label efficient manner is certainly a very well motivated topic. The reviewers unanimously found the contributions of this paper to be of interest, particularly nice empirical gains over several natural baselines.
The reviewer scores are fairly close, and the comments in their reviews are likewise similar.  All reviewers indicate that they find this to be an interesting learning domain.  However, they also agree in assessing the proposed method as having limited novelty and significance.  They also critiqued the empirical evaluation as being too specific to Starcraft and not comprehensive, without providing evidence that the defogger contributes to winning at StarCraft.  The authors wrote a substantial rebuttal to the reviews, but it did not convince anyone to increase their scores.
Important problem and all reviewers recommend acceptance. I agree.
Meta score: 4  This paper presents an approach which uses attention across multiple speech or video channels.  After some synthetic experiments, presents experiments on chime 3, but has a rather weak baseline system  Pros:    addresses an interesting task  Cons:    does not take account of other recent papers in the area    experimental results are weak   very high errors in baseline system    limited novelty
The reviewers agree that the method is original and mostly well communicated, but have some doubts about the significance of the work.   
The reviewers agree that the paper studies and interesting problem with an interesting approach. The reviewers raised some concerns regarding the theoretical and empirical results. The authors have made changes to the paper, but given the theoretical nature of the paper and the extent of changes, another review is needed before publication.
Authors present a method for representing DNA sequence reads as one hot encoded vectors, with genomic context (expected original human sequence), read sequence, and CIGAR string (match operation encoding) concatenated as a single input into the framework. Method is developed on 5 lung cancer patients and 4 melanoma patients.   Pros:   The approach to feature encoding and network construction for task seems new.   The target task is important and may carry significant benefit for healthcare and disease screening.  Cons:   The number of patients involved in the study is exceedingly small. Though many samples were drawn from these patients, pattern discovery may not be generalizable across larger populations. Though the difficulty in acquiring this type of data is noted.   (Significant) Reviewer asked for use of public benchmark dataset, for which authors have declined to use since the benchmark was not targeted toward task of ultra low VAFs. However, perhaps authors could have sourced genetic data from these recommended public repositories to create synthetic scenarios, which would enable the broader research community to directly compare against the methods presented here. The use of only private datasets is concerning regarding the future impact of this work.   (Significant) The concatenation of the rows is slightly confusing. It is unclear why these were concatenated along the column dimension, rather than being input as multiple channels. This question doesn t seem to be addressed in the paper. Given the pros and cons, the commitee recommends this interesting paper for workshop.
This work presents a RNN tailored to generate sketch drawings. The model has novel elements and advances specific to the considered task, and allows for free generation as well as generation with (partial) input. The results are very satisfactory. Importantly, as part of this work a large dataset of sketch drawings is released. The only negative aspect is the insufficient evaluation, as pointed out by R1 who points out the need for baselines and evaluation metrics. R1’s concerns have been acknowledged by the authors but not really addressed in the revision. Still, this is a very interesting contribution.
The reviewers liked this paper quite a bit. The novelty seems modest and the results are limited to a fairly simple NER task, but there is nothing wrong with the paper, hence recommending acceptance.
This work deals with the important task of capturing named entities in a goal directed setting. The description of the work and the experiments are not ready for publication; for example, it is unclear whether the proposed method would have an advantage over existing methods such as the match type features that are only mentioned in Table 3 for establishing the baseline on the original bAbI dialogue dataset, but not even discussed in the paper.
The paper aims to combine Wasserstein GAN with Improved GAN framework for semi supervised learning.  The reviewers unanimously agree that:    the paper lacks novelty and such approaches have been tried before.    the approach does not make sufficient gains over the baselines and stronger baselines are missing.    the paper is not well written and experimental results are not satisfactory.
There are some interesting ideas discussed in the paper, but the reviewers expressed difficulty understanding the motivation and the theoretical results. The experiments do not seem convincing in showing that SQDML achieves significant gains. Overall, the the paper needs either stronger and clearer theoretical results, or more convincing experiments for publication at ICLR.
Dear authors,  The reviewers all appreciated your work and agree that this a very good first step in an interesting direction.
All 3 reviewers consider the paper insufficiently good, including a post rebuttal updated score. All reviewers + anonymous comment find that the paper isn t well enough situated with the appropriate literature. Two reviewers cite poor presentation   spelling /grammar errors making hte paper hard to read. Authors have revised the paper and promise further revisions for final version. 
Authors propose an approach to generation of adversarial examples that jointly examine the effects to classification within a local neighborhood, to yield a more robust example. This idea is taken a step further for defense, whereby the classification boundaries within a local neighborhood of a presented example are examined to determine if the data was adversarially generated or not.   Pro:   The idea of examining local neighborhoods around data points appears new and interesting.   Evaluation and investigation is thorough and insightful.   Authors made reasonable attempts to address reviewer concerns.  Con    Generation of adversarial examples an incremental improvement over prior methods  
Reviewers always find problems in papers like this.  AnonReviewer1 would have preferred to have seen a study of traditional architectures, rather than fully connected ones, which are now less frequently used. They thought the paper was too long, the figures too cluttered, and were not convinced by the discussion around linear v. elliptical trajectories.  I appreciate the need for a parametrizable architecture, although it may not be justified to translate these insights to other architectures, and then the fact that fully connected architectures are less common undermines the impact of the work. I don t find the length a problem, and I don t find the figures a problem.  After the back and forth, AnonReviewer3 believes that there are data compatibility issues associated with the studied transformations and that non linear transformations would have been more informative. I find the reviewers response to be convincing.  AnonReviewer2 is strongly in favor of acceptance, finding the work exhaustive, interesting, and of high quality. I m inclined to agree.  
The paper proposes a method to map input probability distributions to output probability distributions with few parameters. They show the efficacy of their method on synthetic and real stock data. After revision they seemed to have added another dataset, however, it is not carefully analyzed like the stock data. More rigorous experimentation needs to be done to justify the method.
The reviewers generally agree that the DDRprog method is both novel and interesting, while also seeing merit in outperformance of related methods in the empirical results. However, There were a lot of complaints about the writing quality, the clarity of the exposition, and unclear motivation of some of the work.  The reviewers also noted insufficient comparisons and discussions regarding relevant prior art, including recursive NNs, Tree RNNs, IEP, etc.  While the authors have made substantial revisions to the manuscript, with several additional pages of exposition, reviewers have not raised their scores or confidence in response.
This paper makes progress on the open problem of text generation with GANs, by a sensible combination of novel approaches.   The method was described clearly, and is somewhat original.   The only problem is the hand engineering of the masking setup. 
All the reviewers agree that this is an interesting paper but have concerns about readability and presentation. There is also concern that many results are speculative and not concretely tested. I recommend the authors to carefully investigate their claims with stronger experiments and submit it to another venue. I recommend presenting at ICLR workshop to obtain further feedback.
There was certainly some interest in this paper which investigates learning latent models of the environment for model based planning, particularly articulated by Reviewer3.  However, the bulk of reviewer remarks focused on negatives, such as:   The model based approach is disappointing compared to the model free approach.  The idea of learning a model based on the features from a model free agent seems novel but lacks significance in that the results are not very compelling.  I feel the paper overstates the results in saying that the learned forward model is usable in MCTS.   the paper in it’s current form is not written well and does not contain strong enough empirical results   
The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. The method compares well to baselines and has good accuracy in 2d and 3d envs. All three reviewers are in favor of acceptance due to the novelty and competitive performance of the approach.
This paper studies the approximation and integration of partial differential equations using convolutional neural networks. By constraining CNN filters to have prescribed vanishing moments, the authors interpret CNN based temporal prediction in terms of  pde discovery . The method is demonstrated on simple convection diffusion simulations.  Reviewers were mixed in assessing the quality, novelty and significance of this work. While they all acknowledged the importance of future research in this area, they raised concerns about clarity of exposition (which has been improved during the rebuttal period), as well as the novelty / motivation. The AC shares these concerns; in particular, he misses a more thorough analysis of stability (under what conditions would one use this method to estimate an actual PDE and obtain some certificate of approximation?) and discussions about pitfalls (in real situations one may not know in advance the family of differential operators involved in the physical process nor the nature of the non linearity; does the method produce a faithful approximation? why?).  Overall, the AC thinks this is an interesting submission that is still in its preliminary stage, and therefore recommends resubmitting to the worshop track at this time.
The paper considers Markov potential games (MPGs),  where the agents share some common resource. They consider MPGs with continuous state action variables, coupled constraints and nonconvex rewards, which is novel. The reviews are all positive and point out the novel contributions in the paper
The paper presents a modification of the Winograd convolution algorithm that reduces the number of multiplications in a forward pass of a CNN with minimal loss of accuracy. The reviewers brought up the strong results, the readability of the paper, and the thoroughness of the experiments. One concern brought up was the applicability to deeper network structures. This was acknowledged by the authors to be a subject of future work. Another issue raised was the question of theoretical vs. actual speedup. Again, this was acknowledged by the authors to be an eventual goal but subject to further systems work and architecture optimizations. The reviewers were consistent in their support of the paper. I follow their recommendation: Accept. 
The reviewers point out that most of the results are already known and are not novel. There are also issues with the presentation. Studying only depth 2 and depth 3 networks is very limiting.
Thank you for submitting you paper to ICLR. The consensus from the reviewers is that there are some interesting theoretical contributions and some promising experimental support. However, although the paper is moving in the right direction, they believe that it is not quite ready for publication.
The paper proposes a method for learning connectivity in neural networks, evaluated on the ResNeXt architecture. The novelty of the method is rather limited, and even though the method has been shown to improve on the ResNeXt baselines on CIFAR 100 and ImageNet classification tasks (which is encouraging), it should have been evaluated on more architectures and datasets to confirm its generality.
Paper presents and interesting new direction, but the evaluation leaves many questions open, and situation with respect to state of the art is lacking
The output kernel idea for lifelong learning is interesting, but insufficiently developed in the current draft.
Two knowledgeable and confident reviewers suggest rejection, while one not confident reviewer suggests acceptance. I agree with the confident reviewers. All reviewers also point out that the paper is confusingly written and difficult to understand.
Understanding the quality of the solutions found by gradient descent for optimizing deep nets is certainly an important area of research. The reviewers found several intermediate results to be interesting.  At the same time, the reviewers unanimously have pointed out various technical aspects of the paper that are unclear, particularly new contributions relative to recent prior work. As such, at this time, the paper is not ready for ICLR 2018 acceptance.
 Pros: + Interesting and promising approach to multi domain, multi task learning. + Paper is clearly written.  Cons:   Reads more like a technical report than a research paper: more space should be devoted to explaining the design decisions behind the model and the challenges involved, as this will help others tackle similar problems.  This paper had extensive discussion between the reviewers and authors, and between the reviewers.  In the end, the reviewers want more insight into the architectural choices made, either via ablation studies or via a series of experiments in which tasks or components are added one at a time.  The consensus is that this would give readers a lot more insight into the challenges involved in tackling multiple domains and multiple tasks in a single model and a lot more guidance on how to do it. 
The reviewers agreed that while this is a well written paper, it is low on novelty and does not make a substantial enough contribution. They also pointed out that although the reported MNIST results are highly competitive, possibly due to the use of a powerful ResNet decoder, the CIFAR10/ImageNet results are underwhelming.
This work extends Druckmann and Chklowskii, 2012 and demonstrates some interesting properties of the new model. This would be of interest to a neuroscience audience, but the focus is off for ICLR.
Pros   Competitive results on LibriSpeech. Cons   Limited novelty, and lacks enough comparisons.   Comparison with other end to end approaches, and on other commonly used datasets, like WSJ, missing.   Gated convnets have already been proposed.   Letter based systems have been shown to be competitive to phone based systems.   Optimization criterion is quite similar to lattice free MMI proposed by Povey et al., but with a letter based LM and a slightly different HMM topology.  Given the cons pointed out by reviews, the AC is recommending that the paper be rejected.  
The authors have proposed an architecture that incorporates a VIN with a DNC to combine low level planning with high level memory based optimization, resulting in a single policy for navigation and other similar problems that is trained end to end with sparse rewards. The reviews are mixed, but the authors did allay the concerns of the most negative reviewer by adding a comparison to traditional motion planning (A*) algorithms. 
This paper s idea is to augment pre trained word embeddings on a large corpus with embeddings learned on the data of interest. This is shown to yield better results than the pre trained word embeddings alone. This contribution is too limited to justify publication at iclr.
Overall, the committee finds this paper to be interesting, well written and proposes an end to end model for a very relevant task.  The comparisons are also interesting and well rounded.  Reviewer 2 is critical of the paper, but the committee finds the answers to the criticisms to be satisfactory.  The paper will bring value to the conference.
The authors make an argument for constructing an MDP from the formal structures of temporal logic and associated finite state automata and then applying RL to learn a policy for the MDP. This does not provide a solution for low level skill composition, because there are discontinuities between states, but does provide a means for high level skill composition.  The reviewers agreed that the paper suffered from sloppy writing and unclear methods. They had concerns about correctness, and were not impressed by the novelty (combining TL and RL has been done previously). These concerns tip this paper to rejection.
I tend to agree with the most positive reviewer who characterizes the work with the following statements:  "Kronecker factorization was introduced for Convolutional networks (citation is in the paper). Soft unitary constraints also have been introduced in earlier work (citations are also in the paper). Nevertheless, showing that these two ideas work also for RNNs in combination (and seeing, e.g. the nice relationship between Kronecker factors and unitary) is a relevant contribution."  The most negative reviewer feels that the experimental work could have evaluated the different components explored here more clearly. For this reason the AC recommends an invitation to the workshop track.
As identified by most reviewers, this paper does a very thorough empirical evaluation of a relatively straightforward combination of known techniques for distributed RL. The work also builds on "Distributed prioritized experience replay", which could be noted more prominently in the introduction.
The reviewers rightly point out that presented analysis is limiting and that the experimental results are not extensive enough. Moreover, several existing work that use raw waveforms have interesting analysis of what the network is trying to learn. Given these comments, the AC recommends that the paper be rejected. 
This paper presents a different method for learning autoencoders with discrete hidden states (compared to recent discrete like VAE type models). The reviewers in general like the method being proposed and are convinced that there is worth to the underlying proposal. However there are several shared complaints about the setup and writing of the paper.    Several reviewers complained about the use of qualitative evaluation, particularly in the "Deciphering the latent code" section of the paper.   One reviewer in particular had significant issues with the experimental setup of the paper and felt that there was insignificant quantitative evaluation, particularly using standard metrics for the task (compared to the metric introduced in the paper).   There were further critiques about the "procedural" nature of the writing and the lack of formal justifications for the ideas introduced. 
This is a good contribution, with the potential to become extremely good and significant if presentation is substantially improved. All reviewers comment on the lack of clarity of the paper, especially concerning its central contributions (Section 4 and 5), as illustrated also by the relatively low confidence scores. Reviewers also mention the current imbalance between the generality of high order compositional networks and the motivation and empirical evaluation of these models. Generalizations of graph neural representations based on higher order local interactions are particularly interesting in contexts such as combinatorial optimization, where heuristics typically exploit high order interactions.  In summary, we believe this work deserves a further iteration before it can be in proceedings in order to improve the exposition and the motivation of compositional networks, that will greatly improve its exposure to the community.  That said, the idea it lays forward is of potential interest, and thus the AC recommends resubmission to the workshop track. 
Not quite enough for an oral but a very solid poster.
The paper addresses the problem of continual learning and solutions based on variational inference. Updates to the paper have improved it and addresses many of the concerns raised by the reviewers during the discussion period.
This paper presents a fairly straightforward algorithm for learning a set of sub controllers that can be re used between tasks.  The development of these concepts in a relatively clear way is a nice contribution.  However, the real problem is how niche the setup is.  However, it s over the bar in general.
The paper studies the robustness of deep learning against label noise on MNIST, CIFAR 10 and ImageNet. But the generalization of the claim "deep learning is robust to massive label noise" is still questionable due to the limited noise types investigated. The paper presents some tricks to improve learning with high label noise (batch size and learning rate), which is not novel enough. 
I am somewhat of two minds from the paper. The authors show empirically that adversarial perturbation error follows power law and looks for a possible explanation. The tie in with generalization is not clear to me and makes me wonder how to evaluate the significance of the finding of the power law distribution..  On the other hand, the authors present an interesting analysis, show that the finding holds in all the cases they explored and also found that architecture search can be used to find neural networks that are more resilient to adversarial search (the last shouldn t be surprising if that was indeed the training criterion).  All in all, I think that while the paper needs a further iteration prior to publication, it already contains interesting bits that could spur very interesting discussion at the Workshop.  (Side note: There s a reference missing on page 4, first paragraph)
* the proposed fine tuning of only the last layer is not novel enough * experiments are not sufficient to isolate the differences to support the benefit of post training 
The paper proposes a regularisation technique based on Shake Shake which leads to the state of the art performance on the CIFAR 10 and CIFAR 100 dataset. Despite good results on CIFAR, the novelty of the method is low, justification for the method is not provided, and the impact of the method on tasks beyond CIFAR classification is unclear.
This paper addresses the problem of learning neural graph representations, based on graph filtering techniques in the vertex domain.  Reviewers agreed on the fact that this paper has limited interest in its current form, and has serious grammatical issues. The AC thus recommends rejection at this time. 
Reviewers unanimous in assessment that manuscript has merits, but does not satisfy criteria for publication.  Pros:   Potentially novel application of neural networks to survival analysis with competing risks, where only one terminal event from one risk category may be observed.  Cons:   Incomplete coverage of other literature.   Architecture novelty may not be significant.   Small performance gains (though statistically significant)
This paper studies trainable deep encoders/decoders in the context of coding theory, based on recurrent neural networks. It presents highly promising results showing that one may be able to use learnt encoders and decoders on channels where no predefined codes are known.  Besides these encouraging aspects, there are important concerns that the authors are encouraged to address; in particular, reviewers noted that the main contribution of this paper is mostly on the learnt encoding/decoding scheme rather than in the replacement of Viterbi/BCJR. Also, complexity should be taken into account when comparing different decoding schemes.  Overall, the AC leans towards acceptance, since this paper may trigger further research in this direction. 
Thank you for submitting your paper to ICLR. The reviewers agree that the idea of sharing the approximating distribution across sets of variables is an interesting one and that the Omniglot experiments are thorough. However, although the authors make the nice addition of some simple examples during the revision period and a new table of quantitative results on Omniglot, the consensus is that the experimental results are not quite persuasive enough for publication. Adding a second dataset, such as mini imagenet or the youtube faces dataset, would make the paper very strong.
All three reviewers recommend acceptance. Good work, accept
This paper does an excellent job at helping to clarify the relationship between various, recently proposed GAN models. The empirical contribution is small, but the KID metric will hopefully be a useful one for researchers. It would be really useful to show that it maintains its advantage when the dimensionality of the images increases (e.g., on Imagenet 128x128).
This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection.
The reviewers tend to agree that the empirical results in this paper are good compared to the baselines. However, the paper in its current form is considered a bit too incremental. Some reviewers also suggested additional theory could help strengthen the paper.
The proposed LAN provides a visualization of the selectivity of networks to its inputs. It takes a trained network as golden target and estimates an LAN to predict masks that can be applied on inputs to generate the same outputs. But the significance of the proposed method is unclear, "what is the potential usage of the model?". Empirical justification of that would make it stronger.  
Pros   A novel formulation for cross task and cross domain transfer learning.   Extensive evaluations.  Cons   Presentation a bit confusing, please improve.  The paper received positive reviews from reviewers. But the reviewers pointed out some issues with presentation and flow of the paper. Even though the revised version has improved, the AC feels that it can be improved further. For example, as pointed out by reviewers, different parts of the model are trained using different losses and / or are pre trained. It would be worth clarifying that. It might help if the authors include a pseudocode / algorithm block to the final version of the paper.
The reviewers thought the paper provides an interesting line of research.
The paper presents a good analysis on the use of different linear maps instead of identity shortcuts for resnet. It is interesting to the community but the experimental justification is insufficient. 1) As pointed out by the reviewer that this work shows "that on small size networks Tandem Block outperforms Residual Blocks, since He at. al. (2016) in Tab 1 showed a contrary effect, does it mean that the observations do not scale to higher capacity networks?", the paper would be much stronger if with experiments justify this claim. 2) "extremely deep networks take much longer to train" is not a valid reason to not conduct such exps.
This paper studies the problem of modeling logical structure in a neural model.  It introduces a data set for probing various existing models and proposes a new model that addresses shortcomings in existing ones.  The reviewers point out that there is a bit of a tautology in introducing a new task and a new model that solves it.  The revised version addresses some of those concerns.  Overall, it is a thought provoking and well written study that will be interesting to discuss at ICLR.
This paper proposes a new way of learning tensors representation with ring structured decompositions rather than through Tensor Train methods. The paper investigates the mathematical properties of this decomposition and provides synthetic experiments. There was some debate, with the reviewers, about the novelty and impact of this method, where overall the feeling was this work was too preliminary to be accepted. The idea, from my understanding, is interesting and would benefit from discussion at the workshop track, but the authors are investigated to make a stronger case for the novelty of this method in any further work and, in particular, to consider showing empirical improvement on "real" data where TT methods are currently applied.
This paper studies the control of symmetric linear dynamical systems with unknown dynamics. While the reviewers agree that this is an interesting topic, there are concerns that the assumptions are not realistic. Lack of experiments also stands out. I recommend the paper to workshop track with the hope that it will foster more discussions and lead to more realistic assumptions.
Good contribution. There was a (heated) debate over this paper but the authors stayed calm and patiently addressed all comments and supplied additional evaluations, etc. 
This paper proposes a real time method for synthesizing human motion of highly complex styles. The key concern raised by R2 was that the method did not depart greatly from a standard LSTM: parts of the generated sequences are conditioned on generated data as opposed to ground truth data. However, the reviewer thought the idea was sensible and the results were very good in practice. R1 also agreed that the results were very good and asked for a more detailed analysis of conditioning length and some clarification. R3 brought up similarities to Professor Forcing (Goyal et al. 2016)   also noted by R2   and Learning Human Motion Models for Long term Predictions (Ghosh et al. 2017)   noting not peer reviewed. R3 also raised the open issue of how to best evaluate sequence prediction models like these. They brought up an interesting point, which was that the synthesized motions were low quality compared to recent works by Holden et al., however, they acknowledged that by rendering the characters this exposed the motion flaws. The authors responded to all of the reviews, committing to a comparison to Scheduled Sampling, though a comparison to Professor Forcing was proving difficult in the review timeline. While this paper may not receive the highest novelty score, I agree with the reviewers that it has merit. It is well written, has clear and reasonably thorough experiments, and the results are indeed good.
This paper showcases how saliency methods are brittle and cannot be trusted to obtain robust explanations. They define a property called input invariance that they claim all reliable explanation methods must possess. The reviewers have concerns regarding the motivation of this property in terms of why is it needed. This is not clear from the exposition. Moreover, even after having the opportunity to update the manuscript they seem to have not touched upon this issue other than providing a generic response.
The subject of model evaluation will always be a contentious one, and the reviewers were not yet fully convinced by the discussion. The points you bring up at the end of your rresponse already point to directions for improvement as well as a greater degree of precision and control.
the reviewers seem to agree that this submission could be much more strengthened if more investigation is done in two directions: (1) the effect of different, available resources (e.g., in the comment, the authors mentioned WikiData didn t improve, and this raises a question of what kind of properties of external resources are necessary to help) and (2) alternatives to incorporating external knowledge (e.g., as pointed out by one of the reviewers, this is certainly not the only way to do so, and external knowledge has been used by other approaches for RTE earlier. how does this specific way fare against those or other alternatives?) addressing these two points more carefully and thoroughly would make this paper much more appreciated.
R1 was neutral on the paper: they liked the problem, simplicity of the approach, and thought the custom pooling layer was novel, but raised issues with the motivation and design of experiments. R1 makes a reasonable point that training a CNN to classify time series, then throw away the output layer and use the internal representation in 1 NN classification is hard to justify in practice. Results of the reproducibility report were good, though pointed out some issues around robustness to initialization and hyper parameters. R2 gave a very strong score, though the review didn’t really expound on the paper’s merits. R3 thought the paper was well written but also sided with R1 on novelty. Overall, I side with R1 and R3. Particularly with respect to the practicality of the approach (as pointed out by both these reviewers). I would feel differently if the metric was used in another application beyond classification.
This paper presents a novel and interesting sketch based approach to conditional program generation. I will say upfront that it is worth of acceptance, based on its contribution and the positivity of the reviews. I am annoyed to see that the review process has not called out the authors  lack of references to the decently body of existing work on generating structure on neural sketch programming and on generating under grammatical constraint. The authors  will need look no further than the proceedings of the *ACL conferences of the last few years to find papers such as: * Dyer, Chris, et al. "Recurrent Neural Network Grammars." Proceedings of NAACL HLT (2016). * Kuncoro, Adhiguna, et al. "What Do Recurrent Neural Network Grammars Learn About Syntax?." Proceedings of EACL (2016). * Yin, Pengcheng, and Graham Neubig. "A Syntactic Neural Model for General Purpose Code Generation." Proceedings of ACL (2017). * Rabinovich, Maxim, Mitchell Stern, and Dan Klein. "Abstract Syntax Networks for Code Generation and Semantic Parsing." Proceedings of ACL (2017).  Or other work on neural program synthesis, with sketch based methods: * Gaunt, Alexander L., et al. "Terpret: A probabilistic programming language for program induction." arXiv preprint arXiv:1608.04428 (2016). * Riedel, Sebastian, Matko Bosnjak, and Tim Rocktäschel. "Programming with a differentiable forth interpreter." CoRR, abs/1605.06640 (2016).  Likewise the references to the non neural program synthesis and induction literature are thin, and the work is poorly situated as a result.  It is a disappointing but mild failure of the scientific process underlying peer review for this conference that such comments were not made. The authors are encouraged to take heed of these comments in preparing their final revision, but I will not object to the acceptance of the paper on these grounds, as the methods proposed therein are truly interesting and exciting.
Thank you for submitting you paper to ICLR. The paper presents an interesting analysis, but the utility of this analysis is questionable e.g. it is not clear how this might lead to improved VAEs/GANs. The authors did add an additional experimental result in their revised paper, but questions still remain. In light of this the significance of the paper is on the low side and it is therefore not ready for publication in ICLR without more work. 
Reviewers were somewhat lukewarm about this paper, which seeks to present an analysis of the limitations of sequence models when it comes to understanding compositionality. Somewhat synthetic experiments show that such models generalise poorly on patterns not attested during training, even if the information required to interpret such patterns is present in the training data when combined with knowledge of the compositional structure of the language. This conclusion seems as unsurprising to me as it does to some of the reviewers, so I would be inclined to agree with the moderate enthusiasm two out of three reviewers have for the paper, and suggest that it be redirected to the workshop track.  Other criticisms found in the review have to do with the lack of any discussion on the topic of how to address these limitations, or what message to take home from these empirical observations. It would be good for the authors to consider how to evaluate their claims against "real" data, to avoid the accusation that the conclusion is trivial from the task set up.  Therefore, while well written, it is not clear that the paper is ready for the main conference. It could potentially generate interesting discussion, so I am happy for it to be invited to the workshop track, or failing that, to suggest that further work on this topic be done before the paper is accepted somewhere.
The reviewers concerns regarding novelty and the experimental evaluation have been resolved accordingly and all recommend acceptance. I would recommend removing the term "unsupervised" in clustering, as it is redundant. Clustering is, by default, assumed to be unsupervised.  There is some interest in extending this to non vision domains, however this is beyond the scope of the current work.
The paper proposes a semi supervised method to make deep learning more interpretable and at the same time be accurate on small datasets. The main idea is to learn dense representations from unlabelled data and then use those for building classifiers on small datasets as well as generate visual explanations. The idea is interesting, however, as one reviewer points out the presentation is poor. For instance, Table 2 is not understandable. Given the high standards of ICLR this cannot be ignored especially given the fact that the authors had the benefit of updating the paper which is a luxury for conference submissions.
The paper proposes a method to embed graph nodes into a gaussian distribution rather than the standard latent vector embeddings. The reviewers concur that the method is interesting and the paper is well written especially after the opportunity to update.
The paper proposes a new metric to measure GAN performance by training a classifier on the true labeled dataset and then comparing the distribution of the labels of the generated samples to the true label distribution. Reviewers find that the paper is well written but lacks novelty and is quite experimental does not present any new insights. The paper investigates well known model collapse and diversity issues. Reviewers are not convinced that this is a good metric to measure sample quality or diversity as the generator can drop examples far away from the boundary and still achieve a good score on this metric.
The paper presents a variant of network morphism (Wei et al., 2016) for dynamically growing deep neural networks. There are some novel contributions (such as OptGD for finding a morphism given the parent network layer). However, in the current form, the experiments mostly focus on comparisons against fixed network structure (but this doesn t seem like a strong baseline, given Wei et al. s work), so the paper should provide more comparisons against Wei et al. (2016) to highlight the contribution of this work. In addition, the results will be more convincing if the state of the art performance can be demonstrated for large scale problems (such as ImageNet classification). 
Important problem (analyzing the properties of emergent languages in multi agent reference games), a number of interesting analyses (both with symbolic and pixel inputs), reaching a finding that varying the environment and restrictions on language result in variations in the learned communication protocols (which in hindsight is that not surprising, but that s hindsight). While the pixel experiments are not done with real images, it s an interesting addition the literature nonetheless.  
This paper proposes a multiscale variant of Graph Convolutional Networks (GCN) , obtained by combining separate GCN modules using powers of normalized adjacency as generators. The model is tested on several node classification semi supervised tasks obtaining excellent numerical performance.  Reviewers acknowledged the good empirical performance of the model, but all raised the issue of limited novelty, relative to the growing body of literature on graph neural networks. In particular, they missed an analysis that compares random walks powers to other multiscale approaches and justifies its performance in the context of semi supervised learning. Overall, the AC believes this is a good paper, but it can be significantly stronger with an extra iteration that addresses these limitations. 
The paper proposes a data augmentation technique for image classification which consists in averaging two input images and using the label of one of them. The method is shown to outperform the baseline on the image classification task, the but evaluation doesn’t extend beyond that (to other tasks or alternative augmentation mechanisms); theoretical justification is also lacking.
The paper received weak scores: 4,4,5. R2 complained about clarity. R3 s point about the lack of fully connected layers in current SOA deepnets is very valid and the authors response far from convincing. Unfortunately the major revision provided by the authors was not commented on by the reviewers, but many of the major shortcomings of the work still remain. Generally, the paper is below the acceptance threshold, so cannot be accepted.
The method proposed in the paper for latent disentanglement and attribute conditional image generation is novel to the best of my understanding but reviewers (Anon1 and Anon3) have expressed concerns on the quality of results (CelebA images) as well as on the technical presentation and claims in the paper.  Given the novelty of the proposed method, I would *not* like to recommend a "reject" for this paper but the concerns raised by the reviewers on the quality of results and lack of quantitative results seem valid. Authors rule out possibility of any quantitative results in their response but I am not fully convinced   in particular, effectiveness of attribute conditional image generation can be captured by first training an attribute classifier on the generated images and then measuring how often the predicted attributes are flipped when conditioning signal is changed. There are also other metrics in the literature for evaluating generative models.  I would recommend inviting it to the workshop track, given that the work is novel and interesting but has scope for improvements. 
Dear authors,  The authors all agreed that this was an interesting topic but that the novelty, either theoretical or empirical, was lacking. This, the paper cannot be accepted to ICLR in its current state but I encourage the authors to make the recommended updates and to push their idea further.
This paper is somewhat incremental on recent prior work in a hot area; it has some weaknesses but does move the needle somewhat on these problems.
I appreciate the experimental results, which includes a comparison against several baselines, however, I echo some of the concerns raised by the reviewers that the formulation is unclear and hard to follow. Moreover, the novelty over [Nachum, 2017] and [Haarnoja, 2017] seems small. Especially because [Nachum, 2017] also used expert trajectories to improve the performance in their experiments.  Detailed comment: The use of log sum exp state values is only valid for the optimal policy, so it is not clear how an on policy state value is replaced with the log sum exp state value. Also, because the equations that you derive characterize the optimal policy, I am not sure if you need importance correction at all.
This is a fascinating paper, and representative of the sort of work which is welcome in our field and in our community. It presents a compiler framework for the development of DSLs (and models) for Deep Learning and related methods. Overall, reviewers were supportive of and excited by this line of work, but questioned its suitability for the main conference. In particular, the lack of experimental demonstrations of the system, and the disconnect between domain specific technical knowledge required to appreciate this work and that of the average ICLR attendee were some of the main causes for concern. It is clear to me that this paper is not suitable for the main conference, not due to its quality, but due to its subject matter. I would be happy, however, to tentatively recommend it for acceptance to the workshop as this topic deserves discussion at the conference, and this would provide the basis for a useful bridge between the compilers community and the deep learning community.
The reviewers think that the theoretical contribution is not significant on its own. The reviewers find the empirical aspect of the paper interesting, but more analysis of the empirical behavior is required, especially for large datasets. Even for small datasets with input augmentation (e.g. random crops in CIFAR 10) the pre processing can become prohibitive. I recommend improving the manuscript for a re submission to another venue and an ICLR workshop presentation.
The pros and cons of the paper can be summarized below:  Pro: * The improvements afforded by the method are significant over baselines, although these baselines are very preliminary baselines on a new dataset.  Con * There is already a significant amount of work in using grammars to guide semantic parsing or code generation, as rightfully noted by the authors, and thus the approach in the paper is not extremely novel. * Because there is no empirical comparison with these methods, the relative utility of the proposed method is not clear.  As a result, I recommend that the paper not be accepted at this time.
This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection.
Meta score: 6  The paper approaches the problem of identifying out of distribution data by modifying the objective function to include a generative term.  Experiments on a number of image datasets.  Pros:    clearly expressed idea, well supported by experimentation    good experimental results    well written  Cons:    slightly limited novelty    could be improved by linking to work on semi supervised learning approaches using GANs  The authors note that ICLR submission 267 (https://openreview.net/forum?id H1VGkIxRZ) covers similar ground to theirs.
Reviewers are unanimous that this is a reject. A "class project" level presentation. Errors in methodology and presentation. No author rebuttal or revision
 This submission explores recent theoretical work by Shwartz Ziv and Tishby on explaining the generalization ability of deep networks. The paper gives counter examples that suggest aspects of the theory might not be relevant for all neural networks.  There is some uncertainty surrounding the results where mutual information is estimated empirically. Even state of the art estimation methods might lead to misleading empirical results. However, the submission appears to follow reasonable practice following previous work, making the reported results at least suggestive. They warrant reporting for further study and discussion.  The reviewers generally found the paper interesting enough for acceptance, however strong objections were posted by Tishby. A lengthy public exchange resulted between the groups of authors. Not every part of this exchange is resolved. It is not clear whether Tishby s group would be able to fix the full connected ReLU demonstration in this paper, or whether the authors of this submission have anything to say about Tishby s ReLU+convnet demonstration. By accepting this work, we are not declaring where this debate will end. However, we felt the current submission is a constructive part of ongoing discussion in the literature on furthering our theoretical understanding of neural networks.
Understanding the generalization behavior of deep networks is certainly an open problem. While this paper appears to develop some interesting new Fourier based methods in this direction, the analysis in its current form is currently too restrictive, with somewhat limited empirical support, to broadly appeal to the ICLR community. Please see the reviews for more details.  
All three reviewers agreed that the paper was an interesting, giving a demonstration of what quantum computer could achieve. However, they all also felt that the topic was outside the main interests of the conference and better suited to other venues, e.g. a quantum computation workshop. The AC agrees with them. Thus unfortunately, the paper cannot be accepted.
The authors seem to miss important related literature for their comparison. They also tuned hyperparameters and tested on the same validation set. They should split between train/validation/test.  Reviews are just too low across the board to accept.
Pros: + Interesting perspective on training deep networks  Cons:   Not a lot of practical significance: why would one want to use this algorithm over standard methods like ResNets or highway networks given that the proposed algorithm is more complex than established methods? 
The submission is motivated by an empirical observation of a phase transition when a sufficiently high L1 or L2 penalty on the weights is applied.  The proposed solution is to optimize for several epochs without the penalty followed by introduction of the penalty.  Although empirical results seem to moderately support this approach, there does not seem to be sufficient theoretical justification, and comparisons are missing.  Furthermore, the author response to reviewer concerns contain unclear statements e.g. "The reason is that, to reach the level of L1 norm that is low enough, the model needs to go through the strong regularization for the first few epochs, and the neurons already lose its learning ability during this period like the baseline method." It is not at all clear what "neurons already lose its learning ability" is supposed to mean.
The paper proposes a method to disentangle style from content (two factor disentanglement) using weak labels (information about the common factor for a pair of images). It is similar to an earlier work by Mathieu et al (2016) with main novelty being in the use of the discriminator which operates with pairs of images in the proposed method. Authors also have some theoretical statements about two challenges in disentangling the factors but reviewers have complained about missing connection b/w theory and experiments, and about exposition in general.  The idea has novelty, although somewhat limited in the light of earlier work by Mathieu et al (2016)), and theoretical statements are also of interest but reviewers still feel the paper needs improvement in writing and presentation of results. I would recommend an invitation to the workshop track. 
The reviewers agree that the proposed architecture is novel. However, there are issues in terms of the motivation. It would be helpful in future drafts to strengthen the argument about why the architecture is expected to be better than others. Most importantly, the gains at this stage are still incremental. A larger improvement from the new architecture would motivate more researchers to focus on this architecture.
As the reviewers said, it is unclear what the main contribution of the paper is.
The reviewers feel there are two issues that make this paper fall short of acceptance: first, the lack of a clear emphasis and focus (evidenced by the significant revisions) and second, a lack of comparison to similar, existing methods for multi agent reinforcement learning.
This paper combines ideas from student teacher training and multi view learning in a simple but clever way.  There is not much novelty in the methods, but promising results are given across several tasks, including realistic NLP tasks.  The improvements are not huge but are consistent.  Considering the limited novelty, the paper should include some more convincing analysis and insight on why/when the approach works. Given the intersting results, the committee recommends this for workshop track.
All of the reviewers agree that the experimental results are promising and the proposed activation function enables a decent degree of quantization. However, the main concern with the approach is its limited novelty compared to previous work on clipped activation functions.  minor comments:   Even though PACT is very similar to Relu, the names are very different.   Please include a plot showing the proposed activation function as well. 
The authors propose an efficient LSH based method for computing unbiased gradients for softmax layers, building on (Mussmann et al. 2017). Given the somewhat incremental nature of the method, a thorough experimental evaluation is essential to demonstrating its value. The reviewers however found the experimental section weak and expressed concerns about the choice of baselines and their surprisingly poor performance.
This paper presents yet another scheme for weight tying for compressing neural networks, which looks a lot like a less Bayesian version of recent related work, and gets good empirical results on realistic problems.  This paper is well executed and is a good contribution, but falls below the bar on 1) Discovering something new and surprising, except that this particular method (which is nice and simple and sensible) works well.  That is, it doesn t advance the conversation or open up new directions. 2) Potential impact (although it might be industrially relevant)  Also, the title is a bit overly broad given the amount of similar existing work.
Differentiable neural networks used as a measure of design optimality in order to improve efficiency of automated design.   Pros:   Genetic algorithms, which are the dominant optimization routine for automated design systems, can be computationally expensive. This approach alleviates this bottleneck under certain circumstances and applications.  Cons:   Primarily application paper, machine learning advancement is marginal.   Multiple reviewers: Generalization capability not clear. For example, some utility systems may be stochastic (i.e. turbulence) and require multiple trials to measure fitness, which this method would not be able to model. Overall, the committee feels this paper is interesting enough to appears as a workshop paper.
The reviewers are unanimous that the paper is not sufficiently clear and could be improved with better empirical results.
The reviewers are concerned that the evaluation quality is not sufficient to convince readers that the proposed embedding method is indeed superior to alternatives. Though the authors attempted to address these comments in a subsequent revision but still, e.g., the evaluation is only intrinsic or on contrived problems. Given the limited novelty of the approach (it is a fairly straightforward generalization of Levy and Goldberg s factorization of PPMI matrix; the factorization is not new per se as well), the quality of experiments and analysis should be improved.  + the paper is well written   novelty is moderate   better evaluation and analysis are necessary 
Generally solid engineering work but a bit lacking in terms of novelty and some issues with clarity. At the end of the day the empirical gains are not sufficient for acceptance   the results are state of the art relative to published work, but not in the top 10 based on the official leaderboard (not even at time of submission). Since the technical contributions are small and the engineering contributions have been made obsolete by concurrent work, I suggest rejection.
The authors propose a hierarchical VAE model with a discrete latent variable in the top most layer for unsupervised learning of discriminative representations.  While the reported results on the two flow cytometry datasets are encouraging, they are insufficient to draw strong conclusions about the general effectiveness of the proposed architecture. Also, as two of the reviewers stated the proposed model is very similar to several VAE models in the literature. This paper seems better suited for a more applied venue than ICLR.
The paper proposes a method to detect and correct adversarial examples at the input stage (using a sparse coding based model) and/or at a hidden layer (using a GMM). These detector/corrector models are trained using only the natural examples. While the proposed method is interesting and has some novelty wrt to the specific models used for detection/correction (ie sparse coding and GMMs), there are crucial gaps in the empirical studies:    It does not compare with a highly relevant prior work MagNet (Meng and Chen, 2017) which also detects and corrects adversarial examples by modeling the distribution of the natural examples    The attacks used in the evaluations do not consider the setting where the existence (and architecture) of the defender models is known to the attacker    It does not evaluate the method on a stronger PGD attack (also known as iterative FGSM)
The paper presents a technique for feature map compression at inference time. As noted by reviewers, the main concern is that the method is applied to one NN architecture (SqueezeNet), which severely limits its impact and applicability to better performing state of the art models.
The reviewers find the gradient compression approach novel and interesting, but they find the empirical evaluation not fully satisfactory. Some aspects of the paper have improved with the feedback from the reviewers, but because of the domain of the paper, experimental evaluation is very important. I recommend improving the experiments by incorporating the reviewers  comments.
The paper presents a practical approach to compute Wasserstein distance based image embeddings. The Euclidean distance in the embedded space approximates the true Wasserstein distance, thus reducing the high computation cost associated with the latter.  Pros:   Reviewers agree that the proposed solution is novel, straightforward and well described.   Experiment demonstrate the usefulness of such embeddings for data mining tasks such as fast computation of barycenters & geodesic analysis.  Cons:   Though the empirical analysis is convincing, the paper lacks theoretical analysis of the approximation quality. 
The reviewers unanimously recommended that this paper be accepted, as it contains an important theoretical result that there are problems for which heavy ball momentum cannot outperform SGD. The theory is backed up by solid experimental results, and the writing is clear. While the reviewers were originally concerned that the paper was missing a discussion of some related algorithms (ASVRG and ASDCA) that were handled in discussion. 
The updated draft has helped to address some of the issues that the reviewers had, however the reviewers believe there are still outstanding issues. With regard to the technical flaw, one reviewer has pointed out that the update changes the story of the paper by breaking the connection between the generative and discriminative model in terms of preserving or ignoring conditional dependencies.  In terms of the experiments, the paper has been improved by the reporting of standard deviation, and comparison to other works. However it is recommended that the authors compare to NAS by fixing the number of parameters and reporting the results to facilitate an apples to apples comparison. Another reviewer also recommends comparing to other architectures for a fixed number of neurons.
Pros   Nice way to formulate domain adaptation in a Bayesian framework that explains why autoencoder and domain difference losses are useful.  Cons   Model closely follows the framework, but the overall strategy is similar to previous models (but with improved rationale).   Experimental section can be improved. It would interesting to explore and develop the relationship between the proposed technique and Tzeng et al.  Given the aforementioned cons, the AC is recommending that the paper be rejected. 
The paper provides a useful analysis of the role of gradient penalties and the performance of the proposed approach in semi supervised cases.
All the reviewers agree that the paper is studying an important problem and makes a good first step towards understanding learning in GANs. But the reviewers are concerned that the setup is too simplistic and not relevant in practical settings. I recommend the authors to carefully go through reviews and to present it at the workshop track. This will hopefully foster further discussions and lead to results in more practically relevant settings.
The reviewers agree this paper is not yet ready for publication.
The paper presents the Stein gradient estimator, a kernelized direct estimate of the score function for implicitly defined models. The authors demonstrate the estimator for GANs, meta learning for approx. inference in Bayesian NNs, and approximating gradient free MCMC. The reviewers found the method interesting and principled.  The GAN experiments are somewhat toy ish as far as I am concerned, so I d encourage the authors to try out larger scale models if possible, but otherwise this should be an interesting addition to ICLR.
This paper presents a neural compositional model for visual question answering.  The overall idea may be exciting but the committee agrees with the evaluation of Reviewer 1:  the experimental section is a bit thin and it only evaluates against an artificial dataset for visual QA that does not really need a knowledge base.  It would have been better to evaluate on more traditional question answering settings where the answer can be retrieved from a knowledge base (WebQuestions, Free917, etc.), and then compare with state of the art on those.
Two reviewers recommended rejection, and the last reviewer votes for acceptance. The authors provided a rebuttal, including the end to end experiment (although the AC agrees with the authors that this experiment is not crucial to the paper). The AC read the paper and the reviews. While there are clearly interesting aspects of this work, it somewhat falls short in terms of the technical contribution. Perhaps a better writing would alleviate this issue: for example, explaining the visual features is somewhat a distraction from the main point, and could be put in the end. The 3 stage training is somewhat ad hoc (or less elegant). Since there are many excellent papers submitted to ICLR this year, this paper unfortunately did not make it above the bar.
the problem is interesting, and the reviewers acknowledge it s worth an effort to tackle. unfortunately all the reviewers found the work to be too preliminary without a convincing evidence supporting the proposed approach against other alternatives (or on its own.)
Three reviewers recommend rejection and there is no rebuttal.
The paper proposes a method for architecture search using network morphisms, which allows for faster search without retraining candidate models. The results on CIFAR are worse than the state of the art, but reasonably competitive, and achieved using limited computation resources. It would have been interesting to see how the method would perform on large datasets (ImageNet) and/or other tasks and search spaces. I would encourage the authors to extend the paper with further experimental evaluation.
The reviewers agree that the formulation is novel and interesting, but they raised concerns regarding the motivation and the complexity of the approach. I find the authors  response mostly satisfying, and I ask them to improve the paper by incorporating the comments.  Detailed comments: The maximum entropy objective used in Eq. (13) reminds me of maximum entropy RL objective in previous work including [Ziebart, 2010], [Azar, 12], [Nachum, 2017], and [Haarnoja, 2017].
despite not amazing scores, this is a solid paper. it created a lot of discussion and was found to be reproducible. we should accept it to let the iclr community partake in the discussion and learn about this method of n gram embeddings 
This paper presents a distributed memory architecture based on a generative model with a VAE like training criterion. The claim is that this approach is easier to train than other memory based architectures. The model seems sound, and it is described clearly. The experimental validation seems a bit limited: most of the comparisons are against plain VAEs, which aren t a memory based architecture. The discussion of "one shot generalization" is confusing, since the task is modified without justification to have many categories and samples per category. The experiment of Section 4.4 seems promising, but this needs to be expanded to more tasks and baselines since it s the only experiment that really tests the Kanerva Machine as a memory architecture. Despite these concerns, I think the idea is promising and this paper contributes usefully to the discussion, so I recommend acceptance.
The reviewers are not convinced by a number of aspects: including originality and clarity. Whereas the assessment of clarity and originality may be somewhat subjective (though the connections between margin based loss and negative sampling is indeed well known), it is pretty clear that evaluation is very questionable. This is not so much about existence of more powerful factorizations  (e.g., ConvE / HolE) but the fact that the shown baselines (e.g., DistMult) can be tuned to yield much better performance on these benchmarks.  Also, indeed the authors should report results on cleaned versions of the datasets (e.g., FB15k 237).  Overall, there is a consensus that the work is not ready for publication.  Pros:   In principle, new insights on standardly used methods would have been very interesting  Cons:   Evaluation is highly problematic   At least some results do not seem so novel / interesting; there are questions about the rest (e.g., assumptions)   The main advantage of sq loss methods is that it enables the alternating least squares algorithm, does not seem possible here (at least not shown) 
To ensure that a VAE with a powerful autoregressive decoder does not ignore its latent variables, the authors propose adding an extra term to the ELBO, corresponding to a reconstruction with an auxiliary non autoregressive decoder. This does indeed produce models that use latent variables and (with some tuning of the weight on the KL term) perform as well as the underlying autoregressive model alone. However, as the reviewers pointed out, the paper does not demonstrate the value of the resulting models. If the goal is learning meaningful latent representations, then the quality of the representations should be evaluated empirically. Currently it is not clear whether that the proposed approach would yield better representations than a VAE with a non autoregressive decoder or a VAE with an autoregressive decoder trained using the "free bits" trick of Kingma et al. (2016). This is certainly an interesting idea, but without a proper evaluation it is impossible to judge its value.
Proposed network compression method offers limited technical novelty over existing approaches, and empirical evaluations do not clearly demonstrate an advantage over current state of the art. Paper presentation quality also needs to be improved. 
The paper proposes a way of detecting statistical interactions in a dataset based on the weights learned by a DNN. The idea is interesting and quite useful as is showcased in the experiments. The reviewers feel that the paper is also quite well written and easy to follow.
Thank you for submitting you paper to ICLR. The idea is simple, but easy to implement and effective. The paper examines the performance fairly thoroughly across a number of different scenarios showing that the method consistently reduces variance. How this translates into final performance is complex of course, but faster convergence is demonstrated and the revised experiments in table 2 show that it can lead to improvements in accuracy.  
The paper extends the earlier work on Prototypical networks to semi supervised setting. Reviewers largely agree that the paper is well written. There are some concerns on the incremental nature of the paper wrt to the novelty aspect but in the light of reported empirical results which show clear improvement over earlier work and given the importance of the topic, I recommend acceptance. 
All of the reviewers found some aspects of the formulation and experiments interesting, but they found the paper hard to read and understand. Some of the components of the technique such as the state screening function (SSF) seem ad hoc and heuristic without much justification. Please improve the exposition and remove the unnecessary component of the technique, or come up with better justifications.
meta score: 4  The paper uses a deep autoencoder to rating prediction, with experiments on netflix.  Pros    Proposed dense refeeding approach appears novel    Good experimental results  Cons    limited experimentation    main novelty (dense refeeding) is not well linked to existing data imputation approaches    novel contribution is otherwise quite limited  
Pros:   Addresses an important medical imaging application   Uses an open dataset  Con:   Authors do not cite original article describing challenge from which they use their data: https://arxiv.org/pdf/1612.08012.pdf , or the website for the corresponding challenge: https://luna16.grand challenge.org/results/   Authors either 1) do not follow the evaluation protocol set forth by the challenge, making it impossible to compare to other methods published on this dataset, or 2) incorrectly describe their use of that public dataset.   Compares only to AlexNet architecture, and not to any of the other multiple methods published on this dataset (see: https://arxiv.org/pdf/1612.08012.pdf).   Too much space is spent explaining well understood evaluation functions.   As reviewers point out, no motivation for new architecture is given. 
R3 summarizes the reasons for the decision on this paper: "The universal learning algorithm approximator result is a nice result, although I do not agree with the other reviewer that it is a  "significant contribution to the theoretical understanding of meta learning," which the authors have reinforced (although it can probably be considered a significant contribution to the theoretical understanding of MAML in particular). Expressivity of the model or algorithm is far from the main or most significant consideration in a machine learning problem, even in the standard supervised learning scenario. Questions pertaining to issues such as optimization and model selection are just as, if not more, important. These sorts of ideas are explored in the empirical part of the paper, but I did not find the actual experiments in this section to be very compelling. Still, I think the universal learning algorithm approximator result is sufficient on its own for the paper to be accepted."
I recommend acceptance. The two positive reviews point out the theoretical contributions. The authors have responded extensively to the negative review and I see no serious flaw as claimed by the negative review.
This paper proposes a specific architecture for training an ensemble of separate policies on a family of easier tasks with the goal of obtaining a single policy that can perform well on a harder task. There are significant similarities to the recently published Distral algorithm, but I am convinced that this work offers a meaningful contribution beyond that work. Moreover, the authors performed a thorough comparison between their method and Distral and found that DnC performs better.
This paper is lacking in terms of clarity and experimentation, and would require a lot of additional work to bring it to the standards of any high quality venue.
The paper proposes a method to robustify neural networks which is an important problem. They uses ideas from causality and create a model that would only depend on "stable" features ignoring the easy to manipulate ones. The paper has some interesting ideas, however, the main concern is regarding insufficient comparison to existing literature. One of the reviewers also has concerns regarding novelty of the approach.
This paper gives a coding theory interpretation of VAEs and uses it to motivate an additional knob for tuning and evaluating VAEs: namely, the tradeoff between the rate and the distortion. This is a useful set of dimensions to investigate, and past work on variational models has often found it advantageous to penalize the latent variable and observation coding terms differently, for broadly similar motivations. This paper includes some careful experiments analyzing this tradeoff for various VAE formulations, and provides some interesting visualizations. However, as the reviewers point out, it s difficult to point to a single clear contribution here, as the coding theory view of variational inference is well established, and the VAE case has been discussed in various other works. Therefore, I recommend rejection. 
The authors give evidence that is certain cases, the ordering of sample inclusion in a curriculum is not important.  However, the reviewers believe the experiments are inconclusive, both in the sense that as reported, they do not demonstrate the authors  hypothesis, and that they may leave out many relevant factors of variation (such as hyper parameter tuning). 
 + Paper proposes simple joint deep autoencoder + classifier training where the hidden representation is split between (observed) class and (unobserved) style nodes.    Empirical evaluation is very limited, focusing on only qualitative evaluation of reconstructions and interpolations (on MNIST and EMNIST).    Unclear goal: if it is improving classifier robustness, then quantitative classifier robustness improvements should be experimentally demonstrated. If it is as a (conditional) generative model, then it should be compared to strong generative baselines (in the GVAE or GAN families). The paper currently has neither.
This paper has been withdrawn by the authors.
An interesting new approach for doing meta learning incorporating temporal convolution blocks and soft attention. Achieves impressive SOTA results on few shot learning tasks and a number of RL tasks. I appreciate the authors doing the ablation studies in the appendix as that raises my confidence in the novelty aspect of this work. I thus recommend acceptance, but do encourage the authors to perform the ablation experiments promised to Reviewer 1 (especially the one to "show how much SNAILs performance degrades when TCs are replaced with this method [of Vaswani et al.].")
This work looks at what factors can lead to the emergence of selectivity (to certain categories) in units of a neural network. While this is an intriguing area to explore, this work uses settings that are quite toy ish, making it a very hard to see how the observations could generalize to more realistic architectures or tasks. 