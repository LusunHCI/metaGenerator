The work presents a modification to existing approaches of automatic learning rate adaptation (called TLR) via a second order approximation of the function mapping step size to the change in loss when optimized with SGD. This was easily the most controversial paper in the AC s stack, with 4 reviewers advocating accept and 2 reviewers strongly arguing for reject. The authors also went through considerable effort to address reviewer and AC concerns and uploaded multiple additional experiments and ablations to support the robustness and efficacy of the proposed method. Despite a long discussion and rebuttal period, reviewers were unable to reach a consensus. There were several different aspects of the work whose merits were thoroughly debated during the rebuttal period.

The first aspect regarded what the exact contribution of the work was. Initial reviewers who were very high on the work believed that the entire derivation from equation (1) to equation (9) was novel. However, as other reviewers correctly pointed out (1) to (7) is a standard derivation of adaptive learning rates and has appeared in several prior works. Instead it is primarily equation (9) that is the contribution. Given that multiple reviewers initially believed that (1  > 7) was a novel contribution, I feel it is safe to say that the authors do not adequately discuss their contributions with respect to prior work. However, all reviewers in the end agreed that equation (9) is novel and potentially interesting (though some remain skeptical of it s utility).

The second topic of debate regarded the short horizon bias raised by reviewer hkZ3. The short horizon bias presents a fundamental barrier to meta optimization of the learning rate. To summarize, greedily selecting the step size to minimize the loss will result in the optimizer taking too small of steps in the flat directions of an ill conditioned loss surface. This results in faster training in the short term but slower training in the long run. The presented method seeks to greedily optimize the loss over short time scales and thus will be subject to the short horizon bias. The initial draft of the work did not include any discussion of this prior work. During the rebuttal, the authors initially argued that their method can help mitigate the short horizon bias before later concluding that it is a limitation of the method. There was debate between the AC and reviewers regarding whether or not existing methods of adaptive learning rate schedules were already at a fundamental barrier presented by the short horizon bias. One reviewer even mentioned that in their own research they have abandoned the general approach of adaptive learning rates because they cannot overcome this issue. This debate was never resolved, it s plausible to the AC that there is room for increasing the robustness of existing approaches while not addressing the short horizon issue. It is the AC s opinion however that the work would be significantly strengthened with experiments directly addressing the short horizon issue.

The final item of debate regarded the strength of the considered baselines. The authors claim that the second order term in (eq 9) largely removes the need for tuning relative to Baydin et. al. and that the method outperforms multiple baselines across multiple workloads, including Adam (Kingma et. al.), SLS (Vaswani et. al.), and SPS (Loizou et. al.). Indeed multiple plots are given showing that the authors have found a configuration of their method that consistently outperforms certain fixed configurations of the considered baselines. Furthermore, ablations are presented which suggest that indeed it is the addition of equation (9) that is responsible for this strong performance. Despite all of this presented evidence some reviewers remained skeptical, and believed that they could produce a different but fixed configuration of say Baydin et. al. (or even Adam) which matched the proposed method on all of the considered workloads. There are compelling reasons for reviewers to consider the presented experiments with skepticism. Indeed the deep learning optimization literature has for years struggled to make progress despite publishing hundreds of papers see for example the results of [1] which perform an independent comparison of 100 s of published methods and found that none convincingly outperform Adam. Given this, it is clear that the current standard for evaluating optimizers in the literature is inadequate if we are to reliably make progress.

To give a more relevant example of the difficulty of comparing optimization methods, suppose for the sake of argument that we were not evaluating the efficacy of TLR but instead the method of Vaswani et. al. (SLS). Vaswani et. al. makes many similar claims as the proposed method, namely the method consistently outperforms Adam across multiple workloads and enjoys a similar robustness to hyperparameters (e.g. their Figure 6). However, in Figure 3 and 4 of this work we see SLS no longer outperforms Adam on the considered workloads. If Vaswani et. al. had argued for acceptance based on the author s Figure 3 and 4, I don t think any reviewer would have recommended acceptance. This begs the obvious question, why does SLS consistently outperform Adam in the experiments run by Vaswani et. al., but not in the experiments run in the considered paper? There are at least two possible answers here, both of which are concerning. Either Vaswani et. al. is yet another method that generally doesn t outperform Adam or in comparing SLS with TLR in this work the authors did not properly tune SLS in their baselines. Furthermore, what is going to happen if future work tries to compare against TLR? Will TLR still look better than Adam or will independent review find that TLR is yet another method that on average performs about as well as Adam? As a reviewer trying to compare the two papers I see very similar evidence given supporting the two methods and thus I am left with an unresolved contradiction.

Given all of this, I am forced to conclude that there is insufficient evidence presented in this work that the proposed method generally outperforms related methods such as SLS and Adam. A natural question thus is, what would have been sufficient evidence? Indeed the presented experiments seem to be about as convincing as what is shown in previous published methods such as SLS. In a sense the AC is also arguing that Vaswani et. al. presented insufficient evidence that SLS generally outperforms Adam (looking at Figure 3 and 4 perhaps SLS in fact isn t as useful as Vaswani et. al. claim). In looking at the experiments presented in this work, related prior works, and the 100 s of methods considered in [1] a common recurring theme is that when comparing with prior work authors consistently run their own implementation of baselines on workloads of their choosing, rather than directly comparing with published results. In doing so, this leaves open the question regarding whether or not the authors (perhaps inadvertently!) are only considering workloads and hyperparameter settings which favor their own method rather than giving a realistic assessment of the efficacy of their own methods relative to others. Thus, if the authors wish to argue that TLR generally outperforms SLS, a strong piece of evidence the authors could provide is to run TLR directly on the open sourced code provided by Vaswani et. al. Show the reviewer how TLR compares when added directly to (for example) Vaswani et. al. Figure 4. In doing so, the authors will have addressed any concerns reviewers may have about how well represented SLS is, as the authors will be comparing against SLS in a setting where there were actual incentives to make SLS look good.

1. Schmidt et. al. Descending in a crowded valley â€“ Benchmarking Deep Learning Optimizers, https://arxiv.org/abs/2007.01547
The paper proposes a new method for unsupervised text style transfer by assuming there exist some pseudo parallal sentences pairs in the data. The method thus first mines and constructs a synthetic parallel corpus with certain similarity metrics, and then trains the model via imitation learning. Reviewers have found the method is sound and the empiricial results are decent. The assumption on pseudo parallal pairs would limited the application of the methods in other settings where the source/target text distributions are very different. The authors have added discussion on this limitation during rebuttal.
Learning policies from video demonstrations alone without paired action data is a promising paradigm for scaling up Imitation Learning. As such the paper is well motivated. Two approaches P SIL and P DAC train rewards for RL training, based on learning Sinkhorn distances between trajectory embeddings and an adversarial approach.  The reviews brought up lack of clarity in presentation and experimental results and ablation studies falling short of convincingly demonstrating value of distance functions used and other design tradeoffs. As such the paper does not meet the bar for acceptance at ICLR.
The paper s primary contributions are:
* Contrary to previous claims, the authors empirically show that inheriting the weights after pruning can be beneficial when using *larger* fine tuning learning rates than previously done.
* As an explanation, the authors provide suggestive results showing that pruning breaks dynamical isometry, which they claim explains why larger learning rates are needed.
* They propose a regularization based technique to recover dynamical isometry on modern residual CNNs.

Generally, reviewers were positive about the ideas in the paper, however, even after the rebuttal 3/4 reviewers did not find the arguments were clear or strongly supported yet. One issue that came up several times is a request for more investigation of StrongReg+pruning. At this time, I have to recommend rejection, but I encourage the authors to follow up on the reviewers suggestions and submit to a future venue.
This paper proposes a spanning tree based graph generation framework for molecular graph generation, which is an interesting problem. The tree based approach is efficient and relatively effective in molecular graph generation tasks, and the empirical results are convincing. There were some concerns during the initial reviews, but all of them have been addressed during the discussion phase. Thus, I recommend this work be accepted.
This work presents ChIRo, a method that incorporates 3D torsion angles of a molecular conformer to specifically handle chirality. Specifically ChIRo uses trigonometric functions to encode the torsion angles, which are invariant to bond torsion but sensitive to chirality, thus capable of distinguishing between enantiomers.  Overall, although not groundbreaking, we found the idea presented in the paper to be sufficiently novel and its ability to handle chirality to be of significant practical values.
The paper formalizes the problem of gradient leakage through a Bayesian framework. They show that existing attacks can be viewed as approximations of a Bayesian optimal adversary. The empirical results show that heuristic defences are not good against stronger attacks and that the early part of the training is particularly vulnerable. There was a lively discussion in the reviews and rebuttal and the outstanding questions of the reviewers have been answered.
Authors present an approach to consolidate multiple teachers into a single student model that can be adapted to new tasks. The method involves using a proxy dataset to facilitate distillation to prevent having to replay images from the teacher datasets. A multi task multi head objective is utilized, agnostic to the loss function, in which two are studied. Downstream task performance is used as the performance measure.

Pros:
  The problem of how to best leverage multiple teachers for a downstream task is important and interesting.
  Presents a method to generate distilled students that can be finetuned to tasks that demonstrates performance gains over baselines (imagenet alone or task specific teacher). 
  Easy to follow and implement.
  Analysis across multiple datasets.

Cons:
  Multiple reviewers expressed concerns about current level of novelty / contribution. In some sense, it is natural to expect that combinations of task related and generalist distillation would improve performance.
  Main results demonstrate improvements in performance when teacher and tasks are related to one another. But authors do not address how to select task specific teachers for distillation. Related tasks and their matching to the target task are assumed to be known. Authors cited related prior works that attempt to do this matching, but do not apply it to their study for a full solution.
  Authors do not study variations of generalist teachers. How does changing the generalist teacher impact performance?
  Some reviewers expressed concern presentation is not clear. In particular, the style of figures may not be appropriate to best convey results and analyses of this type of work. Comparing different approaches is difficult looking at thin lines. Tables are perhaps better suited to convey these results.
  Multiple reviewers expressed concerns full finetuning results are not convincing (Fig 4), though few shot results look more convincing

Authors and reviewers had interaction, but reviewers maintained their recommendation of weak reject. All reviews unanimous in their decisions. Authors are encouraged to take into consideration all the comments and submit to another venue.
This paper is a resource and numerical investigation into the variability of BERT checkpoints. It also provides a bootstrap method for making investigations on the checkpoints.

All reviewers appreciate this contribution that can be expected to be used by the NLP community.
This paper focuses on understanding how the angle between two inputs change as they are propagated in a randomly initialized convolutional neural network layers. They demonstrate very different behavior in different settings and provide rigorous measure concentration results. The reviewers thought the paper is well written and easy to read with nice theoretical results. They did raise a variety of technical concerns that were mostly addressed by the authors rebuttal. My own reading of the paper is that this is a nice contribution. I therefore agree with the reviewers and recommend acceptance.
This paper presents a faster sampling method for diffusion based generative models which are usually slow in practice. The key idea is based a progressive distillation approach (e.g., how to distill a 4 step sampler into a 1 step sampler). The paper studies the various design choices for diffusion models which existing work hasn t looked at that deeply and sheds light on the effects of these choices. The paper also shows that DDIM can be seen as a numerical integrator for probability flow ODE. The experimental results are impressive. 

There were some concerns such as the effect of progressive distillation and the overhead of distilling the diffusion model but the authors provided a satisfactory response and backed it up with additional results.

Overall, this is a nice paper on making diffusion based generative models generate faster samples and also provides novel insights into the behavior of these models under various design choices. Given the significant recent interest in these models which are pretty impressive in terms of generation quality but slow, the paper indeed makes a timely contribution which will fuel further interest in these models.  All the reviewers have voted for acceptance. Based on my own reading, the reviewers  assessments, the discussions, and the authors  response, I would vote for acceptance.
This work proposed to insert backdoor into pre trained models, such that down streaming tasks can be attacked. 

One of the main issue indicated by most reviewers is that some important and closely related works are missed and not compared, which also studied the backdoor attack to pre trained models. The authors argued in the rebuttal that these missed works require some instances of down streaming tasks, while the proposed method in this work doesn t. However, this difference could not be the reason to miss and not compare with them. 
Besides, most reviewers also indicated the insufficient experiments, such as limited defense methods, and some experimental results are not well explained. 

After reading the manuscript, reviews and discussions between reviewers and authors, I think this work is not ready for publication. The reviewers  comments are supposed to be helpful to improve this work.
The reviewers raised concerns and the authors have not provided a response. All reviewers concur that this paper should be rejected at this time, and I agree.
This paper proposes a method that combines Bad GAN and Good GAN, in which Good GAN learns to generate the anomalies while Bad GAN reguralizes the anomaly pseudo anomalies at the boundary of inlier distribution. In addition, a new orthogonal loss is proposed to  regularize the generation of anomaly samples to be distributed evenly at the periphery of the training data. The proposed method is new and shows some improvement over existing methods.

However, there are some detailed technical concerns raised by reviewers. Some of the concerns still remain unresolved after the discussion. 1) The proposed method lacks a principled way to select hyperparameters. 2) The experimental setting is a bit simple to verify the effectiveness of the proposed method in challenging real world applications. Especially, there is no theoretical guarantee of the proposed method, empirical evaluation is the only way to show the effectiveness of the proposed method. 3) The overall performance improvement is not very significant compared to existing methods. For example, the performance is very close to a method F AnoGAN published in 2019. Addressing the concerns needs a significant amount of work. Thus, I do not recommend acceptance of this paper.
The paper presents an improvement to the core set active learning algorithm by leveraging distance measures weighted by uncertainty scores and using beam search instead of greedy search. 

The reviewers agreed that the paper provides a nice theoretical analysis as well as motivation for the proposal, as well an ablation that shows the proposal indeed empirically outperforms the original core set algorithm. However, the reviewers also agreed that additional important comparisons would make the paper more convincing, including Bayesian core set algorithms as well as other recent proposals based on the original core set algorithm.
The paper builds on ideas in test time adaptation and test time normalization to improve performance under covariate shift. Concretely, the paper proposes (i) alpha BN, a method to calibrate batch statistics by mixing source and target statistics and (ii) test time adaptation using the CORE loss (which was proposed by Jin et al., 2020). The authors compare the the proposed approach to existing approaches on multiple benchmarks. 

The reviewers found the idea interesting and appreciated the additional ablations. The main concerns were around novelty (as the idea is closely related to prior work in test time adaptation and normalization) and hyperparameter selection (e.g. how to choose alpha in practice).  Overall, the reviewers and I felt that the current version falls slightly below the acceptance threshold. I encourage the authors to revise and resubmit to another venue. 

Minor comment about Appendix C (this didn t affect the score, just a suggestion for future revisions): 
I think it might be interesting to include other alternatives to cross entropy that downweight easy examples, cf. focal loss https://arxiv.org/abs/1708.02002 and https://arxiv.org/abs/2002.09437. I m curious to see if CORE and focal loss consistently outperform cross entropy.
This contribution investigates and takes a step back on an important problem in recent ML, namely the impact of the noise distribution in density estimation using Noise Contrastive Estimation. The work offers both theoretical insights and convincing experiments.

For these reasons, this work should be endorsed for publication at ICLR 2022.
This paper led to significant discussion, and the AC is generally on the fence. First of all, thanks to the reviewers for the significant time they invested in the discussion, and thanks for the authors for promptly and patiently answering our questions. 

Overall, the reviewer recommendations are positive. However, the discussion showed that despite the positive recommendation, the reviewers struggled to distill the general contribution of the paper beyond performance on ALFRED. In discussion, the authors distinguished their contribution from existing work by focusing on using a set of low level policies at the root of the overall policy. This relies on the discrete set of behaviors that is defined within the ALFRED benchmark. It s not clear how it generalizes to the actual problem of instructing a robot to execute natural language instruction. In realistic scenarios, is it possible to define a set of behaviors in such a clean way, and at scale? And then train/manage a separate model for each behavior? The set of interaction policies in Figure 2 illustrates this challenge well. The answer to this scaling question is not clear. This corresponds to a concern raised repeatedly by the reviewers about the approach too specialized to ALFRED. The AC shares this concern. 

(which are roughly equal to the SOTA at the time of submission, but show significantly more overfitting to seen environments)

On the positive side, this is solid work, with good results. The paper is well written, and the authors largely addressed the concerns raised as much as possible. The results are not SOTA though. The current SOTA was submitted on 09/19/2021, prior to the ICLR deadline   it s not included in the results table in this paper. (To clarify, the fact that it s not the current SOTA does not affect the final decision, as they are considered as contemporaneous.) With concerns regarding the specificity of the approach, this paper may interest researchers working on ALFRED, but not clear to what depth, despite the clearly significant work and effort the authors put into the paper. 

(If the paper is accepted, the AC asks the authors to fix the standing errors with regard to previous work, as discussed below, and to include more recent results from the leaderboard)
This paper presents a theoretical analysis of self attention modules, using Lipschitz conditions.

It suffers from two main weaknesses: the clarity of the presentation, and the weak experimental section.
All reviewers agreed that the idea proposed by the paper is interesting and is well motivated for handling long tailed recognition problems. 
As suggested by the reviewers, it seems important that the limitations the paper be addressed in the final version of the paper.
This paper studies knowledge distillation and explores why distillation gains are not uniform. Reviewers consistently find this paper an interesting read, but had common concerns on generalizability and limited improvements/contributions.
In general, reviewers mostly gave a score that is below the acceptance threshold, or expressed concerns otherwise. Summing these up, we conclude this paper is of interest to the ICLR audience, but current form is not ready yet for acceptance.  

Summary Of Reasons To Publish:
interesting analysis of the causes of non uniform gains in distillation 

Summary Of Suggested Revisions:

 (1) the improvements are marginal and (2) the contribution of AdaMargin is limited, (3) generalizability to other KDs
This paper proposes an early exit method that uses class means of samples that is gradient free and is aimed for low compute cases such as mobile and edge data. The idea is novel in this setting (though class means have been used for other settings such as few shot classification) and empirical results show that it works well. There are two main concerns from reviewer concerns that were not addressed by the author rebuttal. First, applicability of the model in real world due to its memory requirements and two, experiments that show performance on more realistic datasets such as Imagenet. The reason the latter is required is the promise of mobile application for the proposed method. I suggest the authors explain the first concern more and add the requested experiments in the upcoming version of the paper.
This paper received 4 quality reviews. The rebuttal and discussions were effective. All reviewers raised their ratings after the rebuttal. It finally received 3 ratings of 8, and 1 rating of 5. The AC concurs with the contributions made by this work and recommend acceptance.
This paper introduces a new transformer architecture for representation learning in RL. The key ingredients of the proposed architectures are a novel combination of existing methods: (1) the use of LSTMs to reduce the need for large transformers and (2) a contrastive learning procedure that doesn t require human data augmentation.  The resulting approach requires less prior knowledge and provides higher sample efficiency. The paper is convincing, with comprehensive experiments on multiple challenging and well known benchmarks and an ablation study. The reviewers did expressed concerns that parts of the paper are a very difficult read and could use improvement, especially those relying on substantial external background. The intuition behind several components could be improved, and there are some clarity issues, as detailed in the individual reviews.
Meta Review for Invariance Through Inference

The motivation of this work is to address the problem of learning a model that generalizes well on a test distribution that samples outside of the training data distribution. Reviewer X3P2 wrote a good summary of the paper:

In this paper, the transfer of a reinforcement model (RL) setting from an idealized (training) environment to a more realistic environment with distractors in the observations is considered. Instead of augmenting the training environment with more data so as to make the system more resilient to variations and distractors, the system is adapted at test time to be invariant to the specific distractors found in the environment. Experiments in simulation show the benefits of the proposed approach. Crucially, the agent is not able to access any reward data at test time.

Reviewers, including myself, recognize the novelty of the approach, in particular appreciate the authors  motivation to provide a more principled way to model environment invariance that can possibly scale well in comparison to data driven approaches. However, the initial round of feedback is generally negative, in particular, most reviewers raise concerns regarding the lack of clarity in presentation, and also have issues with the narrow range of experimental evaluation. Clearly, this is promising work, but possibly had to be rushed for submission.

To the authors  credit, they devoted substantial efforts to completely revamp their paper, addressing many of the issues head on. The resulting updated manuscript is almost a complete rewrite of the paper. All reviewers acknowledge (and praise) the effort from the authors  to improve the paper, and 3 out of 4 reviewers had improved (or maintained) their scores from rejection to a 6. But as the paper is a complete revamp, reviewers did not have the time to assess the entire rewrite of the paper (it s like the need to review a paper from scratch), so the confidence is reduced.

While X3P2 did not change their score, they did lead a discussion amongst myself and other reviewers, and they spent the time to take a detailed look at the completely revised draft. Here are the comments from that discussion, for full transparency:

 

*The paper has been completely revamped, to the point in which the presented technique is actually different (the dynamics loss now includes a new forward term). The changes are overall welcome since it significantly improves the clarity of the presentation. Experiments still show promise.*

*I still have problems with the theoretical aspect of it, though. I think that it is unclear why the proposed system is working and fails to provide the minimal system that works.*

*Equation (4) is dimensionally incorrect. It s summing squared error over actions with squared error over latents. Both of these are arbitrary units that can lead to the forward or backward losses dominating. It s also unclear why both of these losses are necessary and not just the inverse one.*

*Equation (7) is similarly dimensionally incorrect. Again, units are arbitrary and for all we know the joint loss could be ignoring the dynamics loss or the adversarial loss.*

*The use of a GAN and the corresponding loss is unjustified. As the authors acknowledge, if the dynamic loss is very small, then the system should already work. They argue that finding that parameterization without the adversarial loss is challenging. There s a difference between using the right loss and finding the right way to optimize it, but here it seems that the right loss is being modified for optimization purposes. Is the adversarial loss something we really want to minimize or just something that helps find the best dynamics loss? What if we remove the adversarial loss after being close to convergence? What if we use multiple restarts or other techniques to help with the optimization of only the dynamics loss? My take from the theory is that the adversarial loss term shouldn t be needed and that the challenging optimization problem should be addressed (rather than modifying the loss).*

*The new ablation experiments are also confusing: If the dynamics loss is the actual driver, and the adversarial loss only helps with finding a good solution, how come that we get almost equally good results when we remove the dynamic loss? Matching the latent distribution shouldn t be enough to have aligned latents. Maybe there s something about the architecture of F that matches the ground truth, so that matching the distribution aligns the latents. This hints towards the adversarial loss actually playing an important role beyond helping with the optimization problem. This is not supported at all by the theory, since matching distributions should result in arbitrary latents and potentially performance of a random system. In fact, given the problems with units, the joint loss might be dominated by the adversarial loss, which would explain this result.*

*It seems like there s something here, but I think more work is necessary to really understand which pieces are necessary in this system and whether there s some sort of adaptation between the experimental setup and F that would explain why distribution matching results in latent alignment, which is not expected (Zhu et al. 2017). Also, the units problem makes the ablation results even harder to interpret: Maybe the dynamics loss is playing a small role in the joint loss, and that s why removing it completely has a small impact.*

 

After much assessment, while I do find this work to be interesting and potentially highly impactful (since they introduce an alternative approach to data centric one OOD), the final manuscript s assessment is still borderline (the reviewers all mentioned that while they recognize the improvement, they list issues from preventing their full endorsement), and X3P2 still found several issues with the revision (which I do believe can be addressed in due time). While I m fully confident that with additional work, this paper could have the potential to be an impactful one, I am currently on the side of not recommending it for acceptance for ICLR 2022.

Note to the PC s, that this is a borderline decision. If the PC s want to flip the decision to an accept, and think the post rebuttal issues are small enough, I ll be fine with that. But in any case, I look forward to seeing a further improved version (of the revamped manuscript) published in a journal or presented at a future conference. Good luck!
In this paper, a new method is proposed to discover diverse policies solving a given task. The key ideas are to (1) learn one policy at a time, with each new policy trying to be different enough from the previous ones, and (2) switch between two rewards on a per trajectory basis: the "normal" reward on trajectories that are unlikely enough under previoiusly discovered policies, and a "diversity inducing" reward on trajectories that are too likely (so as to push the policy being learned away from the previous ones). The main benefit of this switching mechanism is to ensure that the new policy will be optimal, because the reward signal isn t "diluted" by the diversity inducing signal as long as the policy stays far away from the previous ones.

After the discussion period, most reviewers clearly recommended acceptance of the paper. One reviewer remained on the "reject" side though, especially due to an unconvincing theoretical analysis of the method, in spite of several back and forth with authors. I also had my own concerns regarding that part after reading the paper, and further discussions with authors eventualy led to a significant rewrite of the corresponding theorems and proofs. I believe the final version (shared in comments by authors after the dealine for paper revisions) to at least be technically correct, though the relevance of the theory w.r.t. practical usage of the method is still not entirely convincing (e.g., assumptions regarding the number of distinct global optima, and the need for positive rewards).

That being said, in spite of these concerns regarding the practical significance of the theoretical analysis, I believe the paper has a strong enough empirical validation, and the method is (1) simple, (2) intuitively reasonable, (3) original due to the trajectory switching mechanism, which makes me recommend acceptance.
This work studies a variant of a message passing scheme,  aiming to improve the efficiency of GNNs to heterophilic graphs, as well as improving its stability to noise. The authors provide a new architecture, called $p$ Laplacian message passing, as well as some theoretical analysis and empirical evaluation. 
Reviewers highlighted several positive aspects on this work, such as the general idea of considering p Laplacians, as well as the extensive empirical evaluation. However, during the review discussions, several important issues arose, namely important concerns regarding the theoretical contributions, as well as concerns in calibrating the baselines in some empirical evaluations. Overall, the AC is of the opinion that this paper requires a further iteration before it can be considered for publication, and encourages the authors to take the time to address the comments raised by the reviewers.
### Description

The paper enhances flow based generative models by putting them into a coarse to fine multi resolution framework. The key technical challenge as I understand is designing up scaling conditional flow modules. Since the operation needs to be invertible, the paper carefully designs what degrees of freedom need to be injected in addition to the low resolution image to compose a higher resolution one.

### Decision
The paper received 5 expert and rather detailed reviews. I have read and understood the paper and all reviews. Reviewers remark that the paper is well written, addresses a challenging problem. However reviews were in a consensus on that the contribution of the paper is marginal. The average score was 4.4. The authors did not respond to reviewers  and did not update the paper. There was no post rebuttal discussion and or additional feedback from reviewers. Therefore, must reject.

### Comments
I have only minor comments on the writing and organization of the paper.
There are many self repetitions in the text, restating what was already said above in same or very similar sentences. Some questions studied in appendices are not presented in the main papar.
The paper considers FL with periodically shifting distributions, which is a very relevant and timely research question in the area of federated learning, and learning under distributions shifts. The paper proposed an interesting unsupervised way to learn grouping clients into different branches during training, using a federated version of the EM algorithm. Overall the paper contains several solid contributions in some novel combinations, but remained borderline in terms over overall scores. While reviewers were generally positive about the approach, technical soundness and the importance of the question, still some concerns remained.

Concerns included on the level of novelty relative to several recent similar related FL works, and them being included as baselines. Several of these are now discussed in the rebuttal and revisions, but not all in sufficient depth. While the datasets used seem to offer sufficiently hard task from the split between the  day  and  night  distribution, several questions were raised if the treatment of priors is realistic enough. This includes the question of fair hyperparameter tuning with respect to the temporal priors, as well as potential misspecification of the same, towards a more principled treatment of the priors. The authors have answered several of the concerns in the revision, and have added more baseline comparisons, raising the paper narrowly above the acceptance bar in my assessment.

Time wise, the very related paper Marfoq et al 2021 "Federated Multi Task Learning under a Mixture of Distributions" seems to have been available 6 weeks before ICLR deadline. We thank the authors for having included it in discussion and experiments, but the discussion of related contributions needs to be expanded (main difference seems to be supervised vs unsupervised group assignment).

My impression is these points can be addressed in a camera ready version, and I hope the detailed feedback here by all reviewers below will be incorporated.
The authors claim that backdoored classifiers are "fundamentally broken" by demonstrating that other backdoors can be generated for such classifiers without the knowledge of the original backdoors. The proposed method, however, requires manual intervention and is not justified by theoretical arguments. Numerous questions asked by the reviewers were not addressed in the rebuttal period.
Reviewers unanimously vote for rejection for several reasons. First, the draft is incomplete and difficult to read. Second, one of the proposed methods (contextual sentence encoder) appears the same as past work, while the other proposed method (graph encoding) is difficult to interpret from what is written. Third, the draft is missing comparisons with recent work, and some included comparisons may be unfair due to data conditions. No author response was provided. The reviewer consensus is that this draft is underdeveloped, and not yet ready for submission or publication.
This paper proposes an MLP based neural network specifically designed for speech processing. The proposed Split & Glue layer is used to capture multi resolution speech characteristics. The method achieved better performance in both command recognition and speech enhancement tasks.

Two major concerns raised by the reviewers:
The proposed split & glue layer is similar to convolution. Although the authors revised the paper with more clarification on the differences, the op is equivalent to frame wise convolution which has been explored in speech literature. This limits the novelty of the paper.
The experimental justifications are relatively simple and limited. On the voice command and speech enhancement tasks presented in the paper, stronger and better baselines would be more convincing to justify the benefit of the proposed method. Moreover, testing on large scale ASR tasks instead of the relatively simple voice command task would be more convincing.  

The decision is mainly based on the limited novelty and experimental justification.
The authors conduct extensive experiments to show that there were some errors in the original claims of the WMD paper and as opposed to what was claimed in the original paper, WMD does not outperform simpler baselines like BOW and TF IDF. The authors claim that this is significant because WMD is widely used in the literature and hence pointing out errors in the original paper may help the community. 

Out of the 4 reviewers, 1 reviewer wrote a very short review and despite reminders did not elaborate on the reasons for a "Strong Accept". The other reviewer with a "Strong Accept" rating also did not champion the paper in the final discussions. The main objection of the two reviewers who were not in favor of accepting the paper were that (i) it focuses on cirticising a single paper and (ii) some of the criticism is not fair. In response, the authors claim that given the huge amount of derivative work which uses or builds upon the original WMD metric it is crucial to point out these errors. 

Having read the reviews and the responses, it is not clear to me whether such a paper which focuses only on such criticism of a single paper (not matter how popular it is) has enough merit in being accepted. Alternatively, if such criticism was a part of a broader work (maybe a work on new document similarity metrics) then it would have more merit. Further, it should be noted that of the 4 misleading conclusions of the original paper identified by the authors at least 2 are debatable (one being an error in the dataset and the other being a normalisation technique which was not mentioned in the paper but used in the code). The authors have also rephrased one of the original 4 misleading points and from the discussion it seems that they agree it is not misleading. It would have been easier for me to accept the paper if it had a new metric and ablation studies which showed that (i) Hey, normalisation is important and should be done for all baseline algorithms that are being compared (ii) Hey, there are errors in the dataset which affect the results
This manuscript presents a method to refine high level task descriptions into mid level executable steps. The idea of using language models to generate steps for a robot to follow is very interesting. Reviewer concerns focused on the general applicability of the approach and the evaluation.

Reviewers pointed out that the method is tied to VirtualHome which has various properties that are in general not true: the action space is small, the action space is very sparse, and objects tend to be unique.

First, the method enumerates a sentence for every possible action and object combination in the environment. The fact that VirtualHome has few verbs and few objects and that neither of these has complex additional structure (adjectives, adverbs, etc.) means that this is practical. But in any other practical setting this will be impossible. The manuscript mentions this limitation and hints at possible ways to resolve it.

Second, the method requires that the action space must be incredibly sparse. Moreover, a set of common sense rules are needed which are environment specific and must be hand curated. VirtualHome disallows microwaving a cup for example. It also disallows opening the TV. Both of these are valid actions that happen all the time.

Third, the method requires that objects be unique. If multiple plates, vacuum cleaners, lotions, etc. existed and had to be manipulated, e.g., there is no mechanism to refer to any one plate consistently. The model could generate something like "the first plate" but how to actually execute such an action is far from clear.

This third issue is related to the problem of grounding. Normally, grounding means connecting an abstract concept to something concrete in the environment. All of the grounding that is performed here is by virtue of VirtualHome having unique objects in its environments and the actions not requiring multiple instances of the same object. This is not addressing the problem of grounding. Reviewers requested that grounding be removed from the manuscript. This would significantly enhance it, as the model is inherently incapable of grounding as the authors say: "Indeed, one limitation of our approach is that we do not condition on environment state"

Reviewers took issue with details of the evaluation, which are largely a consequence of the choice of VirtualHome. Sometimes this manifested as strange results like models outperforming humans in terms of correctness. As reviewers pointed out, this is worrisome.

Reviewers were also concerned about the title. It implies that language models are zero shot planners, but this is not the case. They are instead able to decompose actions into mid level steps. Reviewers suggested that it would be better to focus the title and tone of the manuscript on extracting task/subtask structures from language models.

The idea presented here, that language models can break tasks into subtasks is interesting. But the manuscript goes a step further and discusses embodied agents which to reviewers appeared to be a reach: there is no grounding and in no sense is the output of the language model any different if the agent is embodied. Even the most positive reviewers felt that discussing embodied agents is unhelpful: it would be better to focus on task/subtask structures. And indeed, this would be more general. All of the concerns that reviewers had around the evaluation would be alleviated by focusing on a language task instead. And the effect of a narrow space of actions, constraints on those actions, and multiple objects of the same class, could be evaluated and reported. Even if the authors had to collect such a corpus, given the difficulties they describe in evaluating on VirtualHome, this would be less of a burden. This could be a strong submission in the future.
The paper aims to integrate Stein variational inference methods into the existing probabilistic programming language NumPyro. The implemented methods include variantions of Stein variational gradient descent with different types of kernel functions, non linear scaling of update terms, and matrix valued kernels. The paper includes empirical results with a comparsion with existing baselines in real world problems. Using this framework, the authors developed a new Stein mixture algorithm for deep Markov models, which shows better performance than existing methods.

Strengths:

  The paper is overall well written and the method is clearly explained. 
  The literature review is thorough.
  Integration of SteinVI into numpyro seems useful. Users can easily take advantage of the state of the art SteinVI algorithms for their own Bayesian modelings.
  Extending the stein mixture method to deep Markov models is a novel application.

Weaknesses:

  The originality is low the authors propose algorithms that are very similar to previous work and there is a lack of experiments to verify the usefulness of the proposed method, for example,
  to verify the decreased variance of the gradient estimates claimed by the authors.
  Efforts are required to illustrate why ELBO within Stein is preferred over the existing work.
  Some important Stein VI methods seem lacking.
  No experiments to support the usefulness of EinSteinVI for Non linear Stein VI, Matrix valued kernel stein VI, and message passing stein VI.

All reviewers vote for rejection. I recommend the authors to addrss the limitatoins mentioned above and improve the paper before its resubmission to another venue.
The paper presents an approach to neural analysis of programs whose main feature is that it operates on assembly code, therefore can account for issues that depend on things like compiler settings. The other important claim of the paper is that by combining information from the control flow and data flow graphs extracted from the assembly code, they are able to produce an embedding that can support a variety of tasks. 

The meta reviewer agrees with the reviewers that this paper is not suitable for acceptance at ICLR. The novelty in the approach is quite limited, and the gap between the extremely bold claims of the introduction of the paper and what is actually proven in the experiments is quite significant. The evaluation is not strong enough to be considered state of the art.
The paper proposed a new kind of neurons for 3D spherical data classification. All the reviewers agreed that the new kind of neurons makes a good contribution. However, all the reviewers also agreed that the experiments are too weak: only at the proof of concept level and no comparison with the state of the arts. Only reviewer FkmY advocated accepting the paper because we need new ideas, and all other reviewers leaned towards rejecting the paper. The AC had exactly the same feeling as the reviewers. Particularly, the AC also agreed with reviewer FkmY that we should not look at experimental results only. However, the AC would like to point out that this by no means means that the experiments can be too simple. Note that this paper is to propose a new tool to improve classification performance, rather than a new theory to explain or predict something. So some basic requirements on the experiments are necessary. If the authors could provide comparison with the state of the arts and with reasonably good performance, not necessarily exceeding or even on par with the state of the arts (namely can be inferior but not too inferior so that others can believe adding engineering tricks could fill in the gap), the AC would consider accepting the paper.
This paper introduces a VAE based generative model of 3D point clouds inspired by SPAIR that can do unsupervised segmentation, named SPAIR3D. The model uses both global and local latent variables to encode global scene structure as well as individual objects.

The proposed model is relatively complex, but the presentation is overall clear. 

Experimental results on simple synthetic datasets look promising. However, one might argue that for these simple tasks a direct application of a simpler mixture of VAEs (such as IODINE) might be sufficient, so it would be informative to make a direct comparison between these methods and/or show results on a problem clearly out of the scope of these simpler methods (e.g. with high imbalance in the point clouds).
The authors present a GAN for learning a continuous representation of disease related image patterns from regional volume information generated from structural MRI images.
The reviewers find the problem relevant and appreciate the proposed solution. They find the paper well written and find the empirical results on Alzheimer brain MRIs relevant for the neuroscience community.
 
The overall objective function includes several hyper parameters. As pointed out as the main weak point by multiple reviewers this may hint at overengineering/overfitting to a data set. However, the reviewers also mention that the regularizers are all sufficiently well motivated in the paper and the author response.

Reviewers highlight comparisons on the real data as a strong result demonstrating that Surreal GAN was able to isolate two major sources/locations of atrophy in Alzheimerâ€™s disease. Overall, the reviews are positive in majority.
This paper derives a generalization bound on target loss based on training loss and reverse KL divergence between source and target representation distributions. Then, proposes an algorithm for DA using inverse KL on representations. they show that inverse KL term can be estimated efficiently without the need for additional networks and minimax objective. The experiments show the efficiency of the proposed algorithm in terms of improving target accuracy. The paper touches an important problem and the proposed idea is simple and effective. 

There were several concerns regarding the paper that were addressed during rebuttal period, such as strength of assumptions, experiments with different values of beta, experiments on office31 dataset, novelty of theoretical results, significance of derived bounds and comparison to [3]. The remaining concern is on comparing the proposed method to recent work in domain adaptation.

I ask the authors to add the following to the camera ready (1) visualizations they have promised that depicts their method leading to better alignment and (2) add the points raised in defending the novelty of theoretical results.
This paper proposes an approach for multi label text classification. The method constitutes appending few "label" tokens to the beginning of the text input instead of the traditional single <CLS> token. The paper shows improvements over a competitive baseline on two datasets.

Reviewers agree that the novelty and contribution of the paper are marginal. The method of appending extra "fake" tokens has been used in other works as a "trick". It is also unclear how adding a few extra tokens allow for the model to represent label dependencies better.

The authors did not respond to the reviews, so there was no further dicussion.
The problem studied in this paper is interesting and the high level motivation of the proposed research is reasonable. However, as pointed out by reviewers, it is not convincing that the developed components in the proposed method are able to address the issues mentioned in the high level motivation. Furthermore, the experimental results are not convincing to verify the motivations either. Though the authors provided some clarifications in the rebuttal, reviewers  major concerns still remain.  

The authors are encouraged to take reviewers  concerns into consideration to revise the proposed method to make it a stronger work for future submission. Based on its current form, this work is not ready for publication at ICLR.
This paper is a scholarly examination for how to conduct continual learning evaluations, proposing six rules that in large part synthesize work from other papers. While there is certainly scholarly benefit to such an exploration, all reviewers believe that the contribution is not substantial enough in its current form to warrant acceptance. It is certainly true that not all continual learning papers follow all of the guidelines/rules for evaluation, and consequently, papers such as this are useful to improve the scientific process.  However, the contribution needs to be substantially deepened, including more extensive and in depth experiments with novel insights as described in the reviews, before the paper is ready for publication.
This paper presents a study of methods for tabular data imputation. In particular, the authors compare deep learning methods with k NN based approaches. Experiment results demonstrate k NN to be competitive with deep learning methods.

Reviewers have concerns about the characteristics of datasets used in the experiments and hyperparameters used for evaluation, which I agree with. They also think that the main contribution of comparing k NN and deep learning methods is not strong enough for acceptance to ICLR. I recommend rejecting this paper.
This paper presents a Bayesian GAN approach designed for a federated learning setting. In contrast to recent Bayesian GAN approaches that use Gaussian priors or iteratively updated priors on GAN parameters, this paper proposes a more complex prior motivated by expectation propagation, dubbed as EP GAN, and uses this formulation to construct a federated GAN. The paper claims that this prior better captures the multimodal distribution structure of the non iid heterogeneous data across the different clients.

The paper looks at an interesting problem, i.e., federated training of GANs, which is indeed a problem that has received a lot of interest lately. The paper received mixed reviews. The reviewers raised several concerns, some of which included (1) weak baselines, (2) not considering what happens when we switch to more advanced GAN models, (3) performance of the approach when the number of clients is large, and (4) lack of clarity in the presentation. 

The authors responded to some of these concerns and it is commendable that they reported some additional results during the discussion phase. However, after an extensive discussion among the reviewers and between reviewers and authors, and after my own reading of the manuscript, concerns still lingers over many of the above mentioned points. Another concern is the overly complex nature of the approach as compared to other recent federated GAN approaches which raises the question as to whether the actual improvements warrant the complexity of the proposed approach. From the report experiments, the improvements appear to be rather slim.

Considering these aspects, unfortunately, the paper in its current shape does not seem ready for acceptance. The authors are advised to consider the feedback from the reviewers which will strengthen the submission for a future submission.
Thank you for your first (hopefully of many!) submissions to ICLR.
This work describes a method for allowing nodes to be processed concurrently instead of sequentially, allowing for a reduction in computation time.
The reviewers identified a number of concerns about the paper (lack of citations and baselines, an additional experiments demonstrating scale, and a number of clarifications and motivation in the text). The authors addressed the majority of these concerns due the rebuttal. I m afraid a promise of a revised manuscript is not a sufficient substitute for the reviewers seeing a revised manuscript, and due the nature of the feedback, a revision is needed, which the reviewers have not seen to check their concerns are fully addressed. Therefore, at this stage, unfortunately, I recommend rejection.
This paper proposes a method that uses conditional moment restriction methods to estimate causal parameters in non parametric instrumental variable settings.  This is done by converting to an unconditional moment restriction setting common in the econometrics causal inference literature.

The paper was reviewed quite favorably by reviewers, and the authors updated the manuscript to address specific issues raised by reviewers.
This paper received 3 quality reviews, with 2 rated 5 and 1 rated 6. While the reviewers recognize the various contributions and insights made by this work, it was also pointed out that this work lacks technical novelty. The authors agreed with this concerns and argued that this work provides a service to the community, citing imageNet and COCO papers. The AC agrees with the contribution and major concerns. Furthermore, the AC would like to point out that in term of the level of efforts, this work might not be on par with the imageNet and COCO. All things considered, the AC believes that this work is not ready for publication at its current form, and hence recommend rejection.
This work studied an important issue, i.e., adversarial transferability, in adversarial examples. It provides a novel perspective that samples in  the low density region of the ground truth distribution where models are not well trained have stronger transferability across different models. Based on that, it proposed a metric called Alignment between its Adversarial attack and the Intrinsic attack (AAI) to indicate transferability. Inspired by the connection between AAI and transferability, this work further proposed to replace the regular ReLU activation with some smooth activation functions, to enhance the transferability. 

Most reviewers appreciate that the observation is interesting, and the theoretical analysis and the proposed method are intuitive. The reviewers posed some important comments on experiments, and the relationship between the proposed method and the proposed metric. The authors provided satisfied responses to most of these concerns. Although there is one remaining concern that AAI may be not the best metric to choose the structural hyper parameters, the reviewer still thought it is a good theoretical starting point to further analyze the adversarial transferability. 

After reading the submission, reviewers  comments and the discussions between reviewers and authors, I believe that this work has provided a valuable perspective, a reasonable theoretical analysis and an effective solution for adversarial transferability. It could inspire further studies on adversarial transferability.
This paper proposes a way to train Gaussian variational autoencoders that does not require the computation of empirical expectations but instead approximates the decoder network by its Taylor series. Results on 3 datasets show the competitiveness of the approach.

Based on the limited novelty of the approach, three (out of 4) knowledgeable reviewers recommend rejection and I agree. Variational autoencoders are simply doing variational inference in a specific model and, as one of the reviewers has pointed out, these types of approximations have been exploited in the inference world (before the popularization of the reparameterization trick)  for many years. Methods, where we replace a term in the joint distribution with a simpler function, are known in the variational inference world as local variational approximations, see, e.g. Murphyâ€™s book (Machine Learning: A Probabilistic Perspective, 2012, Sec. 21.8) as a reference. The community has departed from such approaches as using the re parameterization trick is unbiased, more general (e.g., not limited to Gaussian encoders) and allows for highly automated methods (no need to do derivations on a case by case basis).  Nevertheless, I encourage the authors to thoroughly explore the literature on variational inference with regards to these types of approximations. It may well be the case that, in the future, we revert back to these methods if they perform well in practice with modern architectures. For this, more comprehensive evaluations and comparisons are needed.
This paper proposed a spatial smoothing layer for CNNs which is composed out of a feature range bounding layer (referred to as prob) and  a bluring layer (referred to as blur). An empirical analyses shows that the proposed layer improves the accuracy and uncertainty of both deterministic CNNs and Bayesian NN (BNNs) approximated by MC dropout. The paper further provides theoretical arguments for the hypothesis that bluring corresponds to an ensemble and represents the proposed method as a strategy to reduce the sample amount during inference in BNNs.

Reviewers valued the extensive (theoretical as well as practical) analyses. However, the theoretical analysis should still be improved. First of all, the  the proposed technique is motivated in the context of BNNs, which is not very strongly supported. Second, the argument that â€žthe smoothing layer is an ensembleâ€œ is based on the observation that it has some properties ensembles have as well: (1) they reduce feature map variances, (2) filter out high frequency signals, and (3) flatten the loss landscape. But two things sharing the same properties do not need to be the same thing. Moreover, the proofs of the prepositions stating the properties are difficult to follow and may contain some flaws. Furthermore, the paper is not well self contained and highly depends on the appendix.
Given these, the paper can not be accepted in its current state. 

A future version could improve over the current manuscript by making the theoretical statements and proofs more clear. Another option would be to analyze the contribution without connecting it to a Bayesian setting and ensembles, and instead focus on showing that the proposed smoothing layer has those good properties, doing detailed empirical studies, and showing that CNN components like global average pooling and ReLU + BN are special cases of the propose method.
The paper proposes a model of agent collaboration to improve outcomes for any participating agent in a setting where every agent does not always benefit from collaborating with all other agents. The reviewers did find some of the theoretical results interesting, however, in its current (revised) form, they still argued during the discussion post rebuttal that: (i) the game theoretic formulation of this problem is not entirely new and has been studied in various forms before and (ii) the particular application of the results to federated learning comes after making various (questionable) assumptions. I would encourage the authors to take into account (i ii) for preparing a revised version of their paper and resubmit to another conference.
Strong submission that analyses the unsupervised skill discovery setting from the perspective of information geometry, which leads to some interesting conclusions. In particular, it is shown that this does not lead to skills that are optimal for all reward functions, but does provide a good initialization for methods that aim to find optimal policies.
 
Across the board, the reviewers believe the analysis provided by this work is both important and novel. And while there were some initial concerns raised, such as lack of empirical confirmation of some of the claims and some questions about the analysis, the authors have addressed all of these concerns convincingly.
 
Hence, I strongly recommend acceptance of this submission.
The main contribution is a way of analyzing the generalization error of neural nets by breaking it down into bias and variance components, and using separate principles to analyze each of the two components. The submission first proves rigorous generalization bounds for overparameterized linear regression (motivated in a general sense by the NTK); there are settings where this improves upon existing bounds. It extends the case to a matrix recovery model, showing that it s not limited to the linear regime. Finally, experimental results show that the risk decomposition holds empirically for neural nets.

The numerical scores would place this paper slightly below the cutoff. The reviewers feel that the paper is well written and have not identified anything that looks like a critical flaw. They have a variety of concerns, mostly centered around whether the results apply to practical situations. Specifically, they re worried about (1) the theory not applying directly to neural nets, (2) the high noise setting being less relevant for modern deep learning, and (3) whether there s a realistic situation where it improves over past bounds. Regarding (1), the theory covers not only the linear regime, but also the nonlinear matrix recovery regime; combined with the empirical results, this seems pretty solid by the standards of a DL theory paper. Regarding (2), even though the most common benchmarks indeed have low label noise, the high noise regime still seems worth understanding (after all, we d like our nets to work in domains like medicine). I haven t dug deeply enough to properly evaluate (3), but the author response seems believable to me.

Overall, the paper strikes me as creative and well executed. Regardless of whether the theory is easily extendable to neural nets, this seems like an interesting paper that can be built on in future work. I recommend acceptance.
The submission proposes a method to make a pre existing model equivariant to desired symmetries: frame averaging. The strategy relies on a significant reduction of the number of symmetries to average over (with respect to the Reynolds operator) and uniform subsampling. The paper also demonstrates the usefulness of this method theoretically (universal approximation result) and practically (competitive performance). The contributions are clear and the core idea is simple.
I recommend this paper for acceptance with spotlight.
This well written and well motivated paper has been independently reviewed by four expert reviewers. They all voted for the acceptance with three straight accepts and one marginal. The feedback provided to authors was constructive and the authors responded comprehensively. I recommend acceptance of this work for ICLR.
This paper proposes a cognitive science inspired interaction setting between two agents, an "architect" and "builder", in which the architect must produce messages to guide the builder to achieve a task. Unlike other related settings (such as typical approaches in MARL, HRL, or HRI), the builder does not have access to the architect s reward function, and must learn to interpret the architect s messages by assuming the architect is telling it to do something sensible. At the same time, the architect determines what is "sensible" by building a model of the builder s behavior and planning over it. This setting is common particularly in human agent interactions, where humans may not be able to either (1) accurately communicate a scalar reward or (2) provide demonstrations, but can still provide information that the agent ought to be able to learn from. The paper demonstrates that the learned communication protocol generalizes well to new settings.

While this paper generated a lot of discussion, the reviewers did not come to a consensus on whether the paper should be accepted or rejected, with those in favor of the paper maintaining it should be accepted and those not in favor maintaining that it needs work. I have therefore done a particularly close read of both the paper and the discussion in order to weigh the pros and cons brought up by the reviewers.

The positive reviews clearly indicate that this work is insightful and of interest to researchers in the ICLR community (in fact, all reviewers mentioned they found the work interesting and well written). In particular, Reviewer hMeT wrote: "I am positive about this framework as it presents a better model for multi agent communication, especially enriching the communication among agents over the fixed, restricted reward based communication protocol in traditional RL." I am inclined to agree with this assessment and find the communication setting studied in this paper to be much more ecologically valid for human agent interaction settings than having humans communicate scalar rewards or provide demonstrations: humans are typically poor at the former and may not have the same embodiment to achieve the latter.

The negative reviews focused on a few cons: (1) the assumption that the architect has access to a ground truth environment model, (2) confusion about differences from other related fields (e.g. feudal RL, MARL), and (3) lack of analysis of the communication protocol. I have considered these points, but do not feel any of them are fatal flaws: (1) From the perspective of human agent interaction, I think it is very reasonable to assume that a human architect would have a good model of the world and would be generally proficient at solving tasks in the world. Making this approach work in the setting where the architect is *also* learning how the world works seems squarely in the domain of future work. (2) The authors have done an extensive job of clarifying the differences between these related areas, and as discussed above, other reviewers found the way in which AGP is different to be insightful and ecologically valid. (3) This is potentially the most serious con: as the discussion with Reviewer BHGy brought up, the learned communication protocol may just be a simple mapping between messages and environment interactions. After further discussion in which the authors argued that learning a simple mapping is not a problem the main question is how to even induce such a mapping in the first place the reviewer acknowledged that this is not a fatal flaw but that makes the results somewhat less interesting.

In summary, the positive reviews highlighted the interestingness and insightful nature of the questions studied in this paper and have convinced me that this paper will be of interest to the ICLR community as it has provides a new perspective on the problem of agent agent interaction (particularly for the special case of human agent interaction). The negative reviews did highlight a few limitations of the paper, but I expect these can be addressed by future work and do not feel they outweigh the interestingness of the problem. In light of this, I recommend acceptance as a poster.

Suggestion for the authors: I found the discussion with Reviewer BHGy to be particularly insightful and helpful in understanding the aims of the paper. I would encourage you to incorporate some of this into the camera ready version of the paper, and perhaps to lean more heavily on the special case of human agent interaction as motivation of this work (as also hinted at by Reviewer hMeT).
For this paper initially the reviews were 6,8,5,5. All the reviewers have provided constructive and substantial feedback. The authors have incorporated changes to address some of these comments and some of the comments could not be addressed. The main criticism of the reviewers have been that the Reviewer tkQp finds two clear limitations in the paper, Reviewer 3o7Z finds that the proposed idea is similar to the parameter space adversarial attacks and Reviewer sCeW questions the generalisability of the method to other tasks. After the rebuttal the reviewers have reached the consensus that the paper may not be above the acceptance threshold (final scores: 6,6,5,5). Following the reviewers  recommendations, the meta reviewer recommends rejection.
This paper offers a refinement of the information theoretic characterization of the generalization of models obtained via SGD. This is assessed on some basic neural architectures and inspires the use of new regularizers. Overall, even though the perspective of this paper is not novel, the presented results appear to be clearer and tighter than prior instances of the same ideas. This was appreciated by most reviewers. The few clarity and organization concerns that were raised by the reviewers were adequately addressed by the authors. Overall, the paper deserves to be shared with the community.
This paper studies discontinuities (i.e., holes) in the latent space of text VAE. Analysis of previous hole detection methods are conducted, and a new efficient hole detection algorithm is proposed. It is an interesting work, but the paper in its current form has a few weaknesses/flaws regarding the proposed algorithm, experiment designs and the resulting conclusions. Reviewers have made various constructive suggestions, which the authors acknowledged.
This paper proposes an autoregressive framework that combines RNN and local linear component for the problem of meta forecasting of time series. The linear model can domain adapt to different time series while the RNN component is shared across series. Reviewers thought the problem was important, the paper was generally clear and the experiments extensive. However they found the significance to be limited and all took issue with some of the ways that the comparisons were done. FZbR also raised the issue of complexity of the matrix inversion component of the method.  I believe this paper does fall on the rejection side of the fence due to the issues of complexity, significance and evaluations. With some development, the paper could certainly be ready for acceptance.
This paper develops a method for decomposing scenes into object specific neural radiance fields.  After the discussion phase, two reviewers support acceptance.  Empirical results on multiple synthetic datasets and benchmarks appear convincing; the rebuttal also added an initial demonstration of generalization to real images.
This paper formalizes the setting where an autonomous RL agent operates with zero or very few resets, and provides a novel benchmark for this setting with diverse environments ranging from simple manipulation to complex manipulation/locomotion. The paper then uses this benchmark to analyze current methods and provide insight into those crucial factors that affect performance in this setting. The insights into current methods especially are appreciated. As one reviewer stated, "This paper isolates one problematic assumption in the way of [progress in RL], the environment reset problem, and provides the groundwork for [such] progress, i.e. baselines, clear metrics, etc. I believe the community is much better off with this paper published, since prior works don t seem to have used compatible methodologies."
The authors discuss the disconnect between log likelihood and sample quality of VAEs and relate it to an undesirable focus of the model on high frequency signals. They propose to alleviate it through a two stage training scheme for VAEs.
As it is, the paper does not explain well its contributions, especially compared to the rate distortion balance discussion in "Fixing a Broken ELBo" by Alemi et al. (2018) (see [reviews sh3z](https://openreview.net/forum?id  0LuSWi6j4&noteId D52ninjThn1), [7Pio](https://openreview.net/forum?id  0LuSWi6j4&noteId 9qMQNUGk6bx), and [LBJj](https://openreview.net/forum?id  0LuSWi6j4&noteId gyG86hghxsU)), and lacks the experiments to back up its claim (see [LBJj](https://openreview.net/forum?id  0LuSWi6j4&noteId gyG86hghxsU), and [KKon](https://openreview.net/forum?id  0LuSWi6j4&noteId zeFApaHliSv)). While the authors have made a more precise statement about their contributions in their rebuttal, the writing remains unclear.
I recommend this submission for rejection.
The paper studies improving model for abductive natural language inference task. Specifically, they introduce information interaction layers and the joint softmax focal loss. 

On positive notes, their method shows persuasive empirical gains. However, reviewers found (1) the technical novelty of the approach to be limited (reviewer croc, 3Vwo, W1Sp), (2) approaches (especially focal loss) not well motivated (reviewer hk5y), (3) there are limited take away from the paper (reviewer imYG, hk5y) and (4) claims not well supported and experimental details missing (reviewer hk5y). The reviewers further provided detailed comments that would be helpful for authors to improve the paper. Because of such limitations, in its current form, the paper is not ready for publication.
The paper presents interesting new results for pruning random convolutional networks to approximate a target function. It follows a recent line of work in the topic of pruning by learning. The results are novel, and the techniques interesting. There are some technical issues that are easy to fix within the camera ready timeline (see comments of reviewers below). I would also suggest refining the title of the paper: the lottery ticket hypothesis has an algorithmic component too, which clearly is not covered by existence results.
All three reviewers suggest acceptance of the paper. The authors study an interesting problem (understanding non stationary and reactionary policies) and propose a solution to the problem which compares favorably to baselines in experiments. However, some of the reviewers also criticize unclarities in the presentation of the paper and the made assumptions. The authors clarified those points quite well in their rebuttal. Further concerns regarded design decisions and the comparison to failure cases of baselines. The authors addressed those in their rebuttal and promised to include corresponding material in their updated paper. Hence I am suggesting acceptance of the paper. Nevertheless, I would like to urge the authors to carefuly revise their problem presentation in the paper in order to improve clarity and add the promised additional insights to the final version of the paper.
The paper presents a new method for detection of model extraction attacks. It is based on the intuition that typical model extraction attacks involve samples submitted by users that are harder to classify than "benign" samples submitted by users. By introducing the notion of hardness, a metric is developed for identifying malicious users submitting their samples for the purpose of model extraction. While the proposed method is original, it incurs a substantial overhead. Experimental evaluation of the proposed method also has several deficiencies, in particular, in the assessment of its overhead as well as in modeling of benign users.
The paper argues that existing evaluation metrics for GGMs are insufficient and perform an extensive empirical study questioning their ability to measure the diversity and fidelity of the generated graphs. To solve these limitations, they propose a new evaluation metric that computes the Maximum Mean Discrepancy (MMD) between graph representations of the sampled and real graphs, as extracted from an untrained GGM model. 

All the reviewers agreed that the research problem is interesting and the overall idea behind the proposed metric is sound and novel. While there were some concerns regarding some details/comparisons/conclusions of the experimental evaluation, the rebuttal managed to cleared up these concerns and all the reviewers eventually supported acceptance.
The authors consider the task of interpretable video classification. First, a set of binary â€œconcepts  is predicted, and these concept features are then used for classifying a video. The set itself is automatically generated from natural language descriptions, instead of relying on expert annotations. The authors collect two datasets to validate the proposed approach and show that the model can match the performance of a standard video classification model, while being interpretable.

The reviewers felt that the paper was well written and that the method and empirical results were clearly outlined. They also appreciated the empirical results whereby interpretability doesnâ€™t necessarily come at the expense of accuracy and consider interpretability as a desirable property. The main reason for the borderline results is the heuristic nature of the proposed automatic concept labeling and the empirical evaluation against alternative baselines. In particular, one needs to **show that the proposed method generalises to other datasets**. Secondly, one of the main contributions, namely the automatic **concept extraction, still ends up requiring human annotation in the form of narrations**, and this cost should be quantified and contextualised.

I suggest the authors address these points and resubmit.
The paper contains *fresh* new ideas connecting mental models and SCMs and providing interpretations (explanations) from DAG models learned from data, including those learned by using deep learning. The usefulness of the theory is illustrated with experiments. The paper contributes some theoretical results, but the presentation has serious issues. In general, the reviewers found the paper hard to follow due to a lack of clarity in some notations, definitions, and assumptions. 

The paper was discussed in depth and at length, including the reviewers, the AC, and the senior AC. After all, the gap between the current writing and what is expected from the camera ready is a bit too large, and we feel it could be a disservice to the authors and community to have the paper accepted in its current form, without passing through another round of reviews. Unfortunately, we do not have any version of "conditional acceptance." 

Having said that, we feel the paper has the potential for having a significant impact, and we appreciate the novelty of the proposed approach and the connection among different fields. To avoid issues in the future, we would like to suggest the authors pay attention to the detailed feedback provided by the reviewers, including the discussion and the conversation with the AC, following the exchange on Nov/28. Some examples of points that could make the presentation clearer include 1) clarifying the contributions and providing more examples of the theoretical results, 2) making explicit that the results work for Markovian and additivity models, and 3) perhaps changing the title accordingly.
This paper investigates several fine tuning methods for adapting the pretrained vison language model CLIP to different downstream tasks.  They found that LayerNorm (Ba et al., 2016) is a rather effective and competitive approach and investigated different ways of combining LayerNorm tuning with other adaptation methods. Reviewers generally agree that the technical novelty (combination of existing techniques) is limited and contribution is marginally significant. While some empirical results look interesting, the overall contribution is incremental. Overall, the paper has done a good job of thorough empirical evaluations, but the overall technical novelty and new empirical findings are not significant enough for publication at this conference.
The paper presents a new algorithm for augmenting RL training with human examples, and this is applied to learning safe driving policies. This algorithm is properly tested and compared to other relevant algorithms with favorable results. Reviewers agree that this is good work and that it should be published. Reviewers had multiple questions, which were in my opinion answered satisfactorily by the authors. Notably, the authors ran additional tests against other human in the loop RL algorithms with good results. In sum, this seems to be a solid paper, worth accepting.
This paper has been independently evaluated by four expert reviewers. After discussion with authors, three of them set their recommendations at marginal acceptance, one at straight accept. Perhaps the key criticism involved limited rigor of theoretical justification for the proposed method, but it appears to be applicable in practice as the empirical results suggest. All things considered, I am leaning towards recommending that this paper is accepted for ICLR 2022.
The paper looks at the favorable properties of feature representations of an adversarially robust model, which are interesting but not surprising, especially in the context of much existing literature has talked about this. All reviewers gave negative scores. The main issues are: 1) The paper only provides experimental demonstration of this phenomenon without going into a more detailed explanation of the phenomenon. This is not enough when the observations, in question, are not very novel and have already been explored in various forms in past published literature. 2) limited novelty since the current submission does not introduce a new approach or algorithm or theoretical results. The paper also lacks comparison/discussion of recent works. Thus, I cannot recommend accepting the paper to ICLR.
To tackle the problem of classification under input dependent noise, the authors proposed the posterior transition matrix (PTM) to achieve statistically consistent classification. Specifically the information fusion approach was developed to fine tune the noise transition matrix. Experiments demonstrated the effectiveness of the proposed approach.

I would like to thank the authors for the detailed feedback to the initial reviews and also further feedback to the reviewers  additional questions. Many concerns were clarified by the feedback, and the additional experiments still demonstrate the effectiveness of the proposed method.

The issue of data augmentation still remains, which should be at least experimentally investigated,
but the contribution of the current manuscript is still valuable to be presented as ICLR2022.
One might assume that the k means problem has already been beaten to
death, but this paper shows there are still remaining questions. And
rather interesting ones at that, with a novel angle of having
additional help from a prediction algorithm of cluster
memberships. This connects to learning augmented algorithms research.

The reviewers agreed that the problem is interesting and gives a novel
angle, and the interestingness stems from novelty, and the ability to
"escape" from NP hardness.

The reviewers and authors had nice discussions about details and
conclusions, on how limiting is it that the authors focus on
reasonably accurate predictors, for instance, and where could the
predictors come from. This is a good paper, and hopefully the
discussion helped make it even better.
This paper introduces a novel SE(3) equivariant graph matching network, along with a keypoint discovery and alignment approach, for the problem of protein protein docking, with a novel loss based on optimal transport. The overall consensus is that this is an impactful solution to an important problem, whereby competitive results are achieved without the need for templates, refinement, and are achieved with substantially faster run times.
The paper presents an empirical study on the impact of pertained model on lifelong learning. It concludes that the generic pertaining can benefit the lifelong learning duet the flatter loss landscape and evaluates on CV and NLP tasks. The paper is well written with detailed analysis. However, there is concerns on its limited setting and the conclusion is known in the community and based on empirical studies.
The paper contributes to the literature on federated learning by introducing a hybrid local SGD (HL SGD) method. HL SGD is motivated by the setups where edge devices are grouped into clusters with fast connections within the cluster, but slower connection between the devices and the server. HL SGD uses hybrid updates: decentralized updates within the clusters and federated averaging steps between the clusters and the server. 

Initially, the reviews expressed concerns regarding comparison to prior work, empirical results, and privacy of the proposed scheme. However, the authors adequately addressed all of the concerns, added relevant discussions and results to the paper, and a consensus was reached that the paper should be accepted.
The paper makes a significant contribution in the rather sparse and challenging field of convergence analyses of actor critic style algorithms, under the linear MDP structural assumption, showing that there is a natural bias towards being high entropy. As one of the reviewers points out, although it is unlikely that the strategy actually proposed is amenable to implementation, the paper nevertheless provides a clean and novel analysis of convergence of learning by eschewing the usual mixing time type assumptions often found in the theoretically oriented RL literature. Based on this strength of the paper, I am glad to recommend its acceptance.
Thanks for your submission to ICLR.

Reviews were fairly mixed on this paper, with two reviewers advocating for accepting the paper and two advocating for rejecting the paper.  There were some concerns raised by the reviewers, most notably novelty and some issues with the experiments.  After rebuttal, the negative reviewers maintained their scores and the positive reviewers were somewhat less enthusiastic.  In the end, the paper is quite borderline and could really go either way, but it seems that the paper could use another round of reviewing, particularly to make sure the issues raised by the reviewers are adequately addressed.

Please do keep in mind their comments when preparing a future version of the manuscript.
The paper proposes a method to perform self supervised model ensembling by learning representations directly through gradient descent at inference. The effectiveness is evaluated by k nearest neighbors accuracy.

The reviewers agreed that the paper studies an important and interesting problem of leveraging model ensembling for self supervised learning, which could improve both the performance and robustness of the learned representations. However, the reviewers also agreed that there were issues with the soundness of the empirical evaluation, which was a key reason for rejection.
The paper examines the approach of modeling aleatoric uncertainty by fitting a neural network, that estimates mean and variances of a heteorscedasitic Gaussian distribution, based on log likelihood maximization. The authors identify the problem that gradient based training on the netgative log likelihood (NLL) may result in suboptimal solutions where a high predicted variance compensates for the predicted mean being far from the true mean. To solve this problem, the authors suggest to adjust the log likelihood objective by weighting the log likelihood of each single data point by the corresponding beta exponentiated variance estimate. This adjusted objective is referred to as beta NLL.

All reviewers agreed that the identified problem and the proposed solution are interesting, that the paper is well written and organized, and that the contributions are significant and somewhat new. The main criticism was on the side of the empirical evaluation. It was criticized that the empirical analysis did not compare the proposed method to other approaches to solving the same problem, that the identified problem and the proposed method should be also analyzed on high dimensional data, that the results on the synthetic experiments could be improved by investigating more than a single run and by incorporating the the MSE in corresponding Figure 1,  and that standard errors were not reported. 

Based on the reviews the authors added several new experiments and investigations in the revised version of their manuscript to improve their empirical analysis: 1) new experiments on high dimensional data sets were conducted applying variational autoencoders on MNIST and Fashion MNIST and performing Depth map prediction from images on the NYUv2 dataset. 2) For comparison several baseline methods were added to the experiments on the UCI and the dynamics datasets. 3) Three more UCI datasets (carbon, superconductivity, wine white) were included in the empirical analysis. 4) An evaluation of calibration of predictive variance for the heteroscedastic sine dataset was added. 5) Several more repetitions of the experiment represented in Figure 1 were conducted. (6) An analysis of undersampling behavior on FetchPickAndPlace was added. Moreover, the authors reported two errors in their previous experiments that they discovered and corrected. 

All reviewers were satisfied with the changes in the revised version and the answers to their specific questions and increased their scores accordingly, now commonly voting for acceptance. The paper should therefore be accepted.
Meta Review for Recurrent Parameter Generators

This work investigates a method for reducing the parameters of a deep CNN by having a recurrent parameter generator (RPG) produce the weights, in effect achieving this compression via parameter sharing across layers (similar to earlier works, such as the 2016 Hypernetworks paper, as discussed in between xUeP and the author during the review period). But unlike previous work, this work conducts extensive empirical experiments on classification and even pose estimate tasks, and proposes an additional method, such as the use of pseudo random seed to perform element wise random sign reflection in the weight sharing. The novelty and experimental results are clearly displayed in this work, and shows a lot of promise, but after much discussion, I currently cannot recommend acceptance for ICLR 2022.

In my assessment, and also looking at reviewers and discussion, I believe this work is a great workshop paper at present, but there are a few items that would make it much stronger. There are outstanding issues in the paper that need to be improved. In particular, during discussions, reviewers noted that the paper has a problem with the design and presentation of the experiments. It somehow shifts the readerâ€™s focus to the compression task (3 of the 4 reviewers raised concerns about the compression performance and questioned the baselines). In their rebuttal, the authors emphasized that their contribution is not limited to compression but is rather more fundamental, and the authors propose an approach for understanding the relationship between the model DoF and the network performance. But if that s the main narrative of the paper, rather than the compression aspects, the authors need to clearly articulate why decoupling the DoF from the underlying architecture is advantageous (and also make the narrative more clear in the writing). While there are novel innovations in the method proposed, the authors also need to explain clearly why their method works well, why the even weight assignment and random sign flipping are so effective?

There is discussion between the authors and reviewers about what constitutes vector quantization, and I believe the author has clarified their position effectively (with regard to cgCS s review), and I believe this will be explained in great clarity in future revisions. But even with that disagreement out of the way, we still believe that this work needs improvement to meet the bar of ICLR 2022. Reviewers, including myself, do acknowledge the novelty and are excited about the method proposed, and we look forward to seeing an updated version of this work published or presented at a future journal or conference. Good luck!
Reviewers viewed the proposed approach to image retrieval as both simple and effective, the manuscript as well written and well motivated, and the presented experiments as relatively compressive (spanning three datasets). There was some discussion about novelty   all reviewers viewed the approach as simple, but one had concerns about the approach s novelty relative to past work. This concern, as well as some concerns about experimental evaluation, were adequately addressed in author response.
This paper proposes the notation of DB variability, which is essentially prediction variance. It is also closely related to algorithmic stability which is a theoretically more sound notation to derive generalization bounds. The paper is a mixed bag of empirical observations and "theory". However, looking at the "theoretical results" in the paper, it is clear that the authors lack adequate theoretical background. 

I d be more positive if the paper has been focused more on the former, and could be judged by the empirical part only. While the reviews were positive, I looked at them and realized that similar to the authors, the reviewers also lack theoretical backgrounds. 

First, the fact large variance implies a generalization lower bound is trivial, to the degree it is not worth stating as a "result". Second small variance implies a generalization upper bound isn t true. One can have a predictor that perfectly overfits the training data and predicts class 0 everywhere else. This has small prediction variance but poor generalization. In this context, the upper bound analysis of the paper is clearly misleading. Usually one compares training error to generalization error, where the estimator depends on the training set. In such case, one cannot use the simple argument of the convergence of the empirical mean of sum of independent random variables to the mean due to the dependency of estimator on the training set, e.g. in Thm 3. One needs to use uniform convergence and exponential probability (instead of Chebyshev) inequality to obtain such results. The right hand side of Thm 3 (the theorem itself is also very poorly stated. and shouldn t be allowed to be published) could not be interpreted as training error as should usually be the case for such bounds, but only as validation error. Such a result (comparing validation and test error when distribution isn t changed) has no value. 

I would not elaborate on other similar issues.  My recommendation is to focus on the empirical study if the authors are not familiar with theoretical analysis.
The reviewers all appreciated the novel concept behind the work. I agree with this, I think the principles behind the work are novel and interesting, and I would encourage the authors to improve the validation of this method and publish it in the future.

However, reviewers also raised a number of issues with the current paper: (1) the evaluation appears a bit preliminary, and could be improved significantly with additional datasets and more ablations/comparisons; (2) it s not clear if the improvements from the method are especially significant; (3) the writing could be improved (I do see that the authors made a significant number of changes and improved parts of the paper in response to reviewer concerns to a degree). Probably the writing issues could be fixed, but the skepticism about the experiment results seems harder to address, and while I recognize that the authors made an effort to point some existing ablations in the paper that do address parts of what the reviewers raised, I do think that in the balance the experimental results leave the validation of the work as somewhat borderline.

While less important for the decision, I found that the paper is somewhat overselling the contribution in the opening   while the particular concept of using gradients as features in this way is interesting, similar ideas have been proposed in the past, and the paper would probably be better if it was more clearly positioned in the context of prior work rather than trying to present a new "framework" like this. It kind of feels like it s biting off too much in the opening, and then delivering a comparatively more modest (but novel and interesting!) technical component.
This paper empirically studies when, why, and which pretrained GANs are useful.  
All the reviewers are positive about this work, that they all consider very valuable for practitioners and the community.  
First building intuition through toy examples, authors conduct a large scale study of transfer learning in GANs (with the stylegan2). They propose a way to understand the relevance of a pre trained generator and discriminator, as well as heuristics to select good initialization.  
Overall, this paper makes a solid contribution that should to be accepted.
The submission addresses the problem of whether or not to update weights for a previous task in continual learning.  The approach is to specify a trust region based on task similarity and update weights only in the direction of the tasks that are similar enough to the current one.  The paper was on the balance well received (3/4 reviewers recommended acceptance, 2 with scores of 8) and complemented for its simple but effective approach, and good discussion of related literature.  The submission attracted a reasonable amount of engagement and discussion between reviewers and authors, which should be taken into account in the final version of the paper.
While one reviewer remained concerned about the possibility of convergence to bad equilibria and felt that the proposed method appears to be four minor changes from prior work (PAIRED), the authors demonstrate empirically that the proposed changes make a significant difference in their evaluation. Other reviewers were positive about this work and all others rated this work as an accept. Post rebuttal the most positive reviewer increased their score to an 8 and felt did a good job answering their concerns. They wanted to see an analysis of systems with larger numbers of agents, but felt that the current manuscript was more than sufficient to warrant acceptance, and fell into the category of a good paper with the additional ablations provided during the rebuttal.

The AC recommends accepting this paper.
The paper presents an approach for learning inter object relations. The relationships are represented in terms of scene graphs, and are processed with graph convolutional networks. All of the reviewers find the problem interesting and meaningful, which is the main strength of the paper. The approach assumes good object segmentations and state changes are provided to the agent, which the AC believes is a very dangerous assumption. Object segmentation or the change detection from raw data is still an active research area and lack of end to end training capability of the proposed approach is a limiting aspect. Still, the authors were able to convince all the reviewers that the problem formulation and the proposed approach is valid. Experimental results were provided to support the argument.

Respecting the opinions from the reviewers, the AC recommends accepting the paper, although the AC himself/herself is very reluctant to give an accept rating.
The paper proposes a method for segmentation of thin structures in 2D and 3D, based on persistent homology and using a new filtration. The method performs similarly to state of the art methods on 2D datasets and outperforms some baselines in 3D.

After considering the authors  response and discussing, the reviewers have not arrived at a consensus. 

Pros include:
  Simple and reasonable approach
  Fairly strong experimental results

Some cons are:
  Missing theoretical contributions
  Experimental results on 2D datasets are not that strong, while on 3D datasets important baselines are missing
  At times unclear/unconventional presentation 

Overall, at this point I recommend rejection. The paper is promising, but since the main claim is good performance on 3D data, it is important to have a thorough empirical evaluation there, with the relevant baselines (as mentioned by the reviewers). I very much encourage the authors to polish the paper and submit to a different venue.
The paper proposes and studies a method for the responsible disclosure of a fingerprint along with samples generated by a generative model, which has important applications in identifying "deep fakes". The authors establish both the detectability of their fingerprint without significant loss of fidelity as well as the robustness to perturbations. The reviewers found the problem and contributions to be important and significant, well substantiated by an extensive experimental study.
This papers studies the classical problem of relational learning from a probabilistic perspective. The authors propose four reasonable constraints to encode relational properties, and develop a PGM based variational method for learning relational properties from data. After extensive discussion with the authors, a majority of the reviewer reviewers agree the approach is interesting, if not without some flaws.

The problem studied is interesting, novel, and could lead to new developments in the area of relational learning. It is expected that the experiments have some limitations given the authors have approached the problem from a fresh new angle, which the reviewers have appreciated.

Please pay attention to the suggestions from the reviewers, and in particular, please add a more detailed discussion with statistical relational learning: This material may not be familiar to the broader ML audience, and therefore it is essential to make these comparisons explicit.
The paper proposes "Delaunay Component Analysis", a novel manifold learning technique. Reviewers raised several concerns regarding novelty, computational complexity of the method, and presentation. The authors provided a thorough rebuttal and engaged in discussion with the reviewers that addressed the concerns in a satisfactory manner. After the discussion, all the reviewers and AC recommend acceptance.
This paper proposed a novel phase oriented algorithm to efficiently construct imperceptible audio adversarial attacks. It leverages the spectrogram consistency of STFT to adversarially transfer phase perturbations to the adjacent frames and dissipate the energy that is crucial for ASR systems. Empirical evaluations show that the attack effectiveness of the proposed attack is high.

As agreed by the reviewers the method is very interesting, but the experimental justification is limited, lacking strong SOTA baseline ASR systems, different ASR model architectures, the adversarial transferability analysis etc. The author did add DeepSpeech 2 results to the initial version s Low rank Transformer only results, which is still not convincing enough. The author also commented that they will "add more comprehensive experiments with different systems and â€¦architectures in our next version of the paper" which is not in the current paper yet. Hence, resubmission with more experimental evaluations is recommended. 

The decision is mainly due to the weak experimental justification.
The reviewers highlight that several of significant claims of the paper are not backed up by experiments, and the experiments themselves lack sufficient detail, therefore, at this stage, I recommend rejection. I suggest the authors address the questions and comments they have received before considering whether they might resubmit or not.
Ricci flow is a central topic in geometric analysis. It has had stunning applications in mathematics, most notably the proof of the Poincare conjecture. The major issue is that it, while it can be used to make a manifold more well behaved, it frequently develops singularities. The main contribution fo this paper is in introducing linearly nearly Euclidean metrics. They give a proof of convergence in both short and infinite time, under the Ricci DeTurk flow, and exploit connections to information geometric and mirror descent to develop methods for approximating the gradient flow. The paper is confusingly written (compounded by poor organizational structure and many grammatical mistakes). Perhaps the biggest issue is that it does not have any clear relevance to machine learning. Some sections mention connections to neural networks, but the reviewers found these sections to be indecipherable.
This paper received 5 quality reviews. The rebuttal and discussion were effective and addressed many concerns from the reviewers, after which most reviewers increase their ratings of this paper. The final rating is 6 from 4 reviewers, and 8 from 1 reviewer. The AC concurs with the positive recommendation from the reviewers and recommends acceptance.
This paper integrates model ensembles with randomized smoothing to improve the certified accuracy. The methodology is motivated theoretically by showing the effect of model ensemble on reducing the variance of smooth classifiers. Moreover, it proposes an adaptive sampling algorithm to reduce the computation required for certifying with randomized smoothing. Extensive experiments were conducted on CIFAR10 and ImageNet datasets.

The strengths of the paper are as follows:
+ In terms of significance of the topic, the problem tackled in the paper is significant and highly relevant. 
+ The motivation of using model ensemble is clearly illustrated via a figure and well justified with theoretical analysis. 
+ Algorithmically, the paper proposes Adaptive Sampling and K consensus algorithms to reduce the computational cost, making the method more practical.
+ Experimentally, the paper exhibits competitive results against several frameworks for training smooth classifiers and on several datasets.
This work proposes to extend the invariance/equivariance properties of GNNs by focusing on distance preserving and angle preserving transformations, given respectively by the Euclidean and Conformal group. Preliminary experiments are reported that demonstrate the advantage of such architectures. 
Reviewers found this work generally interesting, tackling an important problem and proposing a valid solution. However, they also raised important concerns, namely the relatively minor novelty relative to recent models (such as EGNN), as well as the lack of convincing real world experiments that would validate the modeling assumptions. Taking all these considerations into account, the AC recommends rejection at this time, and encourages the authors to address the points raised by reviewers in a revision.
All reviewers have converged to an unanimous rating of the paper, highlighting, in the paper or during the discussion, many strengths, including a compelling approach clearly relevant to applications and its solid range of experiments. 

A clear accept, and I would encourage the authors to push in the final version the experiments and discussions following the threads with reviewers (in particular, Vo8C and ULvk).

Thanks also to authors and reviewers for a thorough discussion which helped to strengthen further the paper s content.

AC.
The paper studies why existing deep GCNs suffer from poor performance and propose DGMLP to improve over existing GCNs. However, the reviewers think there are still many unjustified claims and the paper. Further, several reviewers question about the novelty of the proposed method, which seems to be a combination of existing approaches. 

I suggest the authors to revise the paper by defining terms like model degradation and smoothness mathematically and try to justify each claim (e.g., the effect of disentangling) with solid experiments. These will significantly improve the analysis part and make the conclusions stronger.
The paper proposes a new defense against backdoor attacks utilizing an improved version of defenses against noisy label attacks. The connection between these two problems is interesting and novel, which is acknowledge by all reviewers. The main drawback of the paper is, however, its experimental evaluation. The experiments are carried out only on one benchmark and the considered attack ratios are indicative for indiscriminative poisoning attacks rather than targeted backdoors. The AC addressed the summarized response and is not convinced that the reviews are biased. Despite the scarce response of the reviewers to the author rebuttal, the limitations of the experimental evaluation seem to persist in the revised version of the paper. While acknowledging the novelty and the overall good quality of the paper, the weakness of its experimental evaluation puts at in the position marginally below the acceptance threshold. The AC encourages the authors to revise the paper and improve on the pointed out weaknesses and is confident that this work will be well accepted by the scientific community.
This paper considered the computational budgets of adversarial training in the context of Federated Learning and studied the propagation of adversarial robustness from affordable parties to low resource parties. Although the authors conducted the extensive experiments to show the effectivenss of FedRBN, there are still important concerns from the reviewers,

(1) The novelty is marginal compared to FedBN, DBN and previous insights, which moves the similar framework to adversarial robustness and changed the rules, especially given the competitive ICLR. More theorectical novelty will be preferred. 

(2) Many technical details are not well explained and some parts need to be improved, which make the reviewers not well convinced about FedRBN. 

Given above points, I will recommend rejection and encourage the authors to improve the paper in the future.
The reviewers have improved their scores after the rebuttal, and I agree that the work has value. It proposes a model driven data augmentation approach to environment invariant graph representation. Just like most data augmentation works in graph representation learning, the approach relies on graph proposal generator. The work has an implicit mechanism hidden in the graph generator (the authors  reply to a reviewer that "we target the extrapolation via a new invariant learning approach that is agnostic to specific GNN models" is a misunderstanding of why & how these types of methods work). The submission could significantly improve the introduction by properly describing the work w.r.t. other OOD efforts in graph representation learning. While the tasks of different works may be different (e.g., graph classification vs node classification), it is important to emphasize what each different approach brings to the table (rather than just state that the tasks are different). I hope the authors take this opportunity to improve the introduction. This work is more similar to the former OOD graph representation works than IRM & REX, since (without modeling assumptions) both IRM and REX require the support of the environments in test to be a subset of the ones in training. The present work does not need this support assumption since it uses an implicit mechanism in the graph generator.

The toy example in Section 3.1 is informative, thank you. The theory looks solid and easy to understand. The technical novelty is limited, since once the input graph has been decomposed into a set of ego graphs the definitions, formulations, and theory are all straightforward adaptations from the respective versions for IID data. 

Minor:
  In Assumption 2, m() should be defined inside the assumption.
This is an interesting paper trying to answer how contrastive learning can be used in multi label classification. The reviewers however had raised several doubts about motivations, novelty, or the impact of contrastive module on final results. For many of them the authors had delivered satisfying responses, but after a long discussion, we decided that the paper needs revision to improve in these aspects. For example, the authors should make it clear whether the image retrieval application from Section 4.4 is the main motivation of the method. If so, what are the competitive approaches to solve such problem? How to measure the performance of such methods? Answers for the above questions are crucial to find the right motivations for the contrastive module used by the authors. 

We hope that the authors will follow the recommendations and resubmit the paper to another top conference.
This paper proposes an amortization strategy for MC sampling from a single chain rather than per datapoint chains, and uses this strategy to define a new Bayesian autoencoder based on Langevin dynamics.

The reviewers find the line of thought very promising, and a potentially interesting addition to the latent variable literature, while also raising some concerns. The dimension of the single chain must match the dataset size, which limits the computational benefits coming from amortization, and in fact this restriction seems hard, as empirical results (added in the discussion period) are qualitatively worse in the `d<n` case. This could be emphasized much more strongly in the current version, and seems worth deeper investigation. In the discussion, the authors agreed that in the case when the feature matrix G is the identity matrix then there can be no amortization improvement, but for other choices of (fixed) features, amortization *can* yield improvements; this is quite unclear. In addition, in response to the reviewers  observation, the authors improved in the discussion period the implementation of the EBM baseline, leading to much less clear cut differences on metrics. To improve the work further, the authors should clarify the source of amortization improvement, and discuss more the relationship to Bayesian Neural Networks (perhaps by evaluating against Bayesian / hyper net / hyper GAN generative models.)
The manuscrupt studies an unexplored problem: How to reverse engineer adversarial perturbations from an adversarial image? This leads to a new adversarial learning paradigmâ€”Reverse Engineering of Deceptions (RED). The authors formalize the RED problem and identify a set of principles crucial to the RED approach design. By integrating these RED principles with image denoising, they propose a new Class Discriminative Denoising based RED framework, termed CDD RED. 
The reviewers recognize that this topic is important and a promising research direction.
The reviewers are also satisfied with the respones from the authors.
In summary, this paper is recommended to be accepted as it is well formulated, easy to follow, and has some merits, despite that it needs to be evaluated further.
This paper derives a PAC Bayes generalization bound for SGD and uses the results to postulate a functional form for the generalization error as a function of the ratio of the learning rate to the batch size. This functional form is then leveraged to develop a kernel function GP hyperparameter optimization.

The reviewers favorably viewed the novel PAC Bayes bound, but were not convinced by the subsequent analysis. In particular, the reviewers expressed some skepticism about the soundness and generality of the proposed functional form, and were unconvinced that the method would be useful in practice. As such, I cannot recommend the paper for acceptance.
The authors propose an adversarial training method to increase network robustness to parameter variations. The proposed approach performs adversarial attacks on network parameters during training. They demonstrate that their method flattens the loss landscape of the network. Experiments were performed on F MNIST, ECG data, and speech command detection datasets using a conventional CNN and a recurrent spiking neural networks (SNNs).

The manuscript is well written and the method is interesting.

One reviewer was somewhat concerned about the novelty of the work, but acknowledged that the application to recurrent SNNs was new.
The main initial criticism was the question of scalability of the method, as it was tested only on networks with a relatively small number of parameters.

In the revision, the authors addressed these issues. Their method was compared to related approaches, and experiments on CIFAR 10 with a ResNet32 were performed.
The reviewers acknowledged these larger size experiments, but were not fully convinced as much larger models are typically used today.

Nevertheless, the reviewers acknowledged the improvements and ratings were increased, so all are voting for acceptance.
This paper studies text style transfer which aims to edit a given sentence to possess a desired style value (e.g., positive sentiment) while keeping all other styles and content unchanged. The paper specifically focuses on a challenging setting where besides the target style (e.g., sentiment) to transfer, there exists confounding attributes (e.g., product category) that correlate with the target style, making it hard to change only the target style while preserving the other. The proposed approach is to learn an invariant/unbiased style classifier using Invariance Risk Minimization (IRM), together with an orthogonal classifier for monitoring style independent changes (e.g., product category), to supervise the generator training. The main concerns are on the experiments   it s suggested to include experiments on other styles besides sentiment; human evaluation and/or other metrics are needed for more convincing comparison; it s also encouraged to experiment with large language models (e.g., GPT 2, BART) besides the small LSTM/CNN networks as in the present work.
The reviewers thought this paper tackles an interesting question around whether MaxEnt RL already provides an important form of robustness. Such work helps us better understand the intersection between generalization, regularization and robustness. The reviewers had a number of comments, questions and clarifications and were generally satisfied with the detailed responses provided by the authors. There was some concern over the strength of the experiments and the authors also ran additional experiments. These addressed one reviewerâ€™s concerns, though the other still thought the existing experiments were a bit too simple.
After carefully reading all reviews and rebuttal, I actually think the paper provides sufficient new insight in understanding MAML that is worth being accepted. I want to thank the authors for actively engaging with the reviewers, and providing sufficient changes to the paper in order to clarify and improve its contributions. 

Theoretical results tend to be harder to judge, as they often need to happen under assumptions that make them tractable. Nevertheless they provide intuitions and understanding of the underlying principle that end up having an impact even in more realistic scenarios where these assumptions might not hold. I think this is such a scenario, and I think better understanding the relationship between ERM and approaches as meta learning is important for the field moving forward.
The manuscript develops a novel method for uncertainty prediction that can be used in the context of active or reinforcement learning problems. They consider experiments such as an OOD Detection task wherein a ResNet is trained on CIFAR10 and predictions must subsequently be made for in  versus out of distribution (SVHN) data. 
The work develops an approach based on directly estimating epistemic (as opposed to a aleatoric) uncertainty by learning to predict generalization error and then subtracting estimated aleatoric uncertainty.
Reviewers found the essential approach to be novel and creative. However, there were several issues raised by reviewers that are not well addressed by responses by the authors. For example, Zaec worries about the dependence of the approach on an oracle for estimating aleatoric uncertainty. Multiple reviewers were concerned that this would make the approach unsuitable for many situations and thus limit the applicability of the ideas. 
Multiple reviewers also found the manuscript to be difficult to understand. I agree with the sentiment. While there may indeed be an interesting and important idea here, the text and explication of the algorithm and approach are complicated and leave the reader unsure about the contribution. I would recommend that the authors invest time and effort in simplifying and streamlining the narrative and presenting the technical innovation so that it is easier to judge. In it s current form, the manuscript is premature for publication.
Although the reviewers found the idea of the work interesting, they all think it is not ready for publication. The experiments do not properly support the claims. Discussion on the connection to some related work is missing. And also the proposed method is not well motivated. I suggest the authors to take the reviewers  comments into account, revise their work and prepare it for future venues.
The manuscript extends the popular "RL as inference" framework with a generalized divergence minimization perspective. The authors observe that most policy optimization can be thought of as minimizing a reverse KL divergence, which has potentially undesirable mode seeking properties. The authors propose a particle based scheme wherein samples generated via Langevin dynamics are used for learning.

Several reviewers found the ideas presented interesting, and cited potential novelty and high potential for tackling an important problem. Unfortunately, all reviewers found major shortcomings, from presentation ("messy" presentation, lack of definition of notation and inconsistent use, issues around motivation and logical flow, vague and imprecise use of language, etc.). Several reviewers also had more fundamental criticisms, notably Uu6f who helpfully provided quite actionable feedback on the presentation. Unfortunately, discussion ended with the reviews: the authors offered no rebuttal or updates. The AC considers this a missed opportunity.

The AC concurs with, first and foremost, the concerns around presentation. The current state of the manuscript makes it difficult to parse apart the contribution being made, and in light of all 4 reviewers recommending rejection either strongly or weakly and with no rebuttals or responses put forth, I have no basis to recommend anything other than rejection.
The paper proposes a planning framework that uses a transformer based architecture as an attention mechanism that guides the search of a traditional sample based planner (e.g., RRT*). More specifically, features extracted from a sliding window over the 2D search space serve as input to a transformer that produces a mask indicating where to draw samples from. By constraining the search space for the sample based planner, the method reduces the time required for planning. The method is compared to both traditional and learning based planners on different 2D navigation tasks and found to improve sample complexity (and, in turn, computation time), while also being capable of generalizing to unseen and real world maps.

The manner by which the method combines the advantages of sample based planning with an attentional mechanism as a way to constrain the sampling process is interesting. As the reviewers emphasize, the experimental evaluation shows that this approach results in performance gains over both traditional (sample based) and learning based planners, while also being able to scale to larger maps as well as better generalize to out of distribution settings (compared to learning based methods). These results support the value of both the overall approach as well as the architectural components (e.g., the transformer and the use of positional encoding). The reviewers initially raised a few concerns with the paper, the most notable of which are the need to include preprocessing in the overall computation time, the accuracy of some of the claims in the paper (e.g., with regards to generalizability), generalization to higher dimensional domains, and the performance on the Dubins car domain. The authors responded to each of the reviews and updated the submission to address many of these concerns. However, questions still remain regarding whether or not the approach can be adapted to state/configuration spaces with more than two dimensions, something that traditional planners are readily capable of, and the unconvincing results on the Dubins car domain.

Overall, the paper proposes an interesting approach to an important problem that is relevant to the robotics and machine learning communities. The paper makes promising contributions to improve the efficiency of planning, however the significance of these contributions needs to be made clearer.
The paper presents a deep learning approach encodes codebases as databases that conform to rich relational schemas. Based on this, a biased graph walk mechanism efficiently feeds this structured data into a transformer and deepset approach. The results shown a quite good, compared to other approaches present at ICLR. Moreover, one reviewer is strongly voting for accepting the paper, arguing that "that this paper is of significance to the ML4Code research community, as it shows how to offload the engineering cost of extracting semantic information from programs to a standard tool." Overall, I have really enjoyed reading the paper, and the use of relational database as codebase together with a transformers is sweat. On the other, it also presented in a rather engineering way, as pointed out by several reviewers, suggesting that some software engineering venue might be a better place for the work. But then ICLR had similar papers, and the present paper demonstrates a benefit of using a relational encoding. Thus, I weight the leaning towards rejects borderlines votes less and suggest an accept overall. We all should keep in mind that also deep neural architecture are full of design choices.
This paper studies group equivariant neural posterior estimation which seeks to endow conventional neural posterior estimation method with equivariance of both the data and parameters simultaneously. To test the efficacy of the proposed approach the authors experiment with gravitational wave data and show that the proposed GNPE achieves considerable performance gains.


Strengths:

  The approach is independent of neural architectures and does not necessitate knowledge of exact equivariances.
  The method seems to be much better than regular NPE in cases where there are known equivariances.

Weaknesses:

  the writing of the paper is not clear, which makes the paper difficult to read and evaluate.
  It is hard to check the correctness of the conclusions and algorithm due to lack of necessary assumptions and derivation steps.
  the authors are knowledgeable about the subject but present material in a slightly callous way which prevents a precise understanding of their techniques.


This is a borderline paper with two reviewers in favor of acceptance and two with a slight tendency to reject. The two negative reviewers did not engage in a discussion with the authors or did not complete that discussion, failing to confirm their ratings or provide an update of those ratings. They also do not seem to give strong arguments for rejection. Based on this, I recommend the paper for acceptance. However, I encourage the authors to take into account the reviewers  comments, especially the part on clarity and rigor, to improve the paper for its camera ready version.
I think there is good research behind this paper, but the presentation issues make it difficult to argue for acceptance. 

On the positive side, the paper has made a clear advance in terms of the ability to do full SAT based verification of neural networks. However, there are also important issues with the paper that prevent it from being accepted: 
* The paper argues for the value of the new approach for *both* verifiability and interpretability, where interpretability is measured in terms of the ability to make targeted adjustments to the network to change its behavior. These are very different goals, but they are conflated in different parts of the paper, leading to confusion, for example, from reviewer RhEH. 
* The paper only compares against SAT/SMT based verification, but completely ignores other approaches to verification that are arguably more effective for many problems. In particular, there is an emerging literature on Abstract Interpretation based verification that is significantly more scalable than SAT based verification and which this paper ignores.
* The paper s claims sometimes get ahead of the presented evidence, as pointed out by reviewer garj. 

So overall, I think this paper needs another iteration before it is ready for acceptance.
The paper is well written and deals with a simple yet interesting introduction of approxiThe paper is well written and deals with a simple yet interesting introduction of approxiThe paper is well written and deals with a simple yet interesting introduction of approxiThe paper is well written and deals with a simple yet interesting introduction of approxiThe paper is well written and deals with a simple yet interesting introduction of approxiThe paper is well written and deals with a simple yet interesting introduction of approxiThe paper is well written and deals with a simple yet interesting introduction of approximate Boolean logic activation functions. A number of comparison experiments showed intriguing differences for problems with potential logical structures. Authors suggest a probabilistic rational/motivation, i.e., computation in logit space, however, more theoretical investigation is critically needed to answer why they perform the way they do. There are a lot of activations in the literature, so perhaps it is not easy to make a distinct contribution in performance. Despite the large number of experiments the reviewers were not convinced on how they support authors claims and contributions. The reviewers and AC strongly encourage the authors to keep the direction and improve the paper for another conference.
The paper proposed a sketching algorithm for empirical risk minimization (ERM) for linear regression and classification. The technique is based on LSH with non standard hash functions. The reviews indicate that the paper is well written and easy to follow. However, there are several concerns raised regarding its quality. A major one regards the novelty of the paper. 

MTPW: â€œThe technique novelty is limited since previous work (Coleman & Shrivastava, 2020) has used LSH to approximate kernel density estimation on streaming setting.â€œ, 7LQM: â€œMy review of the theoretical results and data structure design is that the results are believable and seem correct, but lack technical novelty.â€ â€œOther than using non standard hashing functions, what distinguishes the STORM sketch from the RACE sketch?â€

An additional concern is a claim of weak experimental evidence. There seems to be a need for more thorough experiments isolating different components rather than the system as a whole, and in addition the bottom line results provide only a slight lift over a naive baseline (7LQM:  â€œThe experiments suggest that using the STORM estimator is only slightly better than returning the mean of your data.â€). 

Whether it is the case that the techniques should be improved or that these concerns could be addressed by improving the presentation of the paper, the conclusion is that the paper now is not ready to be published.
Authors study robustness properties of arbitrary smoothing measures with bounded support using Wasserstein distance and total variation distance. Reviewers pointed out several weaknesses about this work. In particular, they mentioned the paper is not well organized, comparison with prior work is lacking, the conclusion of the theoretical analysis is not novel and the experiments are not comprehensive. I suggest authors to take these comments into account in improving their work.
This paper proposes an innovative method for continual learning that modifies the direction of gradients on a new task to minimise forgetting on previous tasks without data replay. The method is mathematically rigorous with a strong theoretical analysis and excellent empirical results across multiple continual learning benchmarks. It is a clear accept. There was good discussion between the reviewers and authors that addressed a number of minor issues, including clarifying that the method has the same computational complexity as backpropagation. The authors are encouraged to make sure that these points are addressed in the final version of the paper.
The paper provides a framework for recourse (i.e. counterfactual explanations) that is robust to model shifts. The setup for the proposed method is a min max optimization problem, where the max is over a neighborhood around the distribution over model parameters. The model parameters are drawn from a mixture of K distributions, so that the neighborhood is specified by the Gelbrich distance on each component. The authors propose a finite dimensional version of the robustified optimization problem, which can be optimized using projected gradient descent. They evaluate their approach on the German credit dataset, the Small Business Administration dataset, and the Student performance dataset, each of which demonstrates a different type of data distribution shift.

Strengths:

  Most existing work on recourse actions do not consider model change, so the problem addressed by the paper is relatively new
  The experiment results demonstrate the superiority of the proposed method over baselines.

Weaknesses:

  The solution provided is somewhat limited as it relies heavily on the structural properties of the mixture distribution and Gelbrich distance to reformulate the optimization problem.

Most of the reviewers voted initially for rejection. The paper is borderline, tending to rejection after the rebuttal. The authors have also considerably updated the paper with new results after the initial reviews. It seems therefore that the paper may benefit from another round of reviewing and, because of this, I recommend rejection and the authors to use the reviewers  comments to improve the paper before resubmitting to another venue for another round of reviewing.
The reviewers are rather critical about the paper and the authors did not take a part in the discussion phase. Let me also add that the paper ignores a vast number of papers dealing with a similar problem. The column generation algorithm is a core of LPBooting also used for rule learning ("Rule Learning with Monotonicity Constraints", "The Linear Programming Set Covering Machine"). There are many other papers also using linear relaxation of integer programming to build rule models. Logical analysis of Data is also a well known method being close to such approaches. There is also a plenty other rule learning systems that should be compared in the experimental study such as Ripper, Slipper, MLRules, or Ender (to mention only a few of them).
This paper proposes a hierarchical reinforcement learning approach that exploits affordances to better explore/prune the subtasks, and thus making the overall learning more efficient. 

The idea of the paper is novel and interesting. 

After the rebuttal, all the reviewers agree that the paper is a solid contribution.
Therefore, I recommend acceptance of this paper.
This paper proposes a Role Diversity metric, meant to quantify how different roles are in a multi agent RL setting. There s actually three versions of this metric, or three aspects (the distinction is not entirely clear to this area chair).

The reviewers are generally not very enthusiastic about the paper, with scores hovering at or just below the acceptance threshold. There has been extensive discussion between reviewers and authors, but there a sense that there is confusion about the exact purpose and contribution of the paper. This is reinforced by the authors  "letter to area chair", which outlines several ways the reviewers have not gotten the message. Reading the paper, it appears to me that the root cause is that the authors are indeed not communicating clearly what the paper contributes and why. It is, after all, the authors  responsibility that the reviewers understand the work. My own impression is that the text is dense and not particularly easy to get through. Perhaps the authors are simply trying to cram too many contributions into a single conference paper? This is a classic error which leads to hard to read papers. In addition to this, there is a lingering concern about the generalizability of the proposed methods.

I think the authors need to work more on their presentation, and perhaps reconsider which parts to include in their paper and exactly which measure they want to send, before they submit to another venue.
The paper considers the high resolution continuous limit of Nesterov s Accelerated Gradient (NAG) algorithm and its connections to sampling (MCMC methods). The paper develops a Hessian Free High Resolution (HFHR) ODE and injects noise into it to obtain an accelerated sampling algorithm. Further, the paper provides a discrete time variant of the algorithm by appropriately discretizing HFHR using simple discretization schemes. For strongly log concave potential functions (log densities), the paper proves convergence of the order $\tilde{O}(\sqrt{d}/\epsilon)$ in Wasserstein 2 distance. In the asymptotic sense, the result matches the convergence of the underdamped Langevin algorithm; however, the paper argues that the constants in the proposed algorithm are smaller and empirically shows that the proposed algorithm is faster in practice. The main contributions of the paper are theoretical; however, the theoretical results are supplemented by numerical experiments.

Overall, the reviewers found the contributions interesting and the theoretical contributions of the paper technically sound. The main concerns that were not completely addressed were related to the presentation of the results and reproducibility of some of the numerical experiments. While both seem minor and possible to address, ultimately there was not enough support to recommend acceptance. However, the paper is solid and merits acceptance after suitable revisions. Thus, the authors are encouraged to revise the paper and resubmit it to one of the conferences in the equivalence class of ICLR.
All reviewers are in agreement to reject this paper. The main objection is that the tasks chosen are small scale and that the mixed results are not strong enough. The authors did not attempt to raise substantial issues to be discussed.
This paper investigates TD based off policy policy evaluation. This topic is of interest as most SOTA DRL methods are built upon unsound algorithms, whereas more sound variants are difficult to use in practice and have not been widely adopted. This paper introduces a new variant of ETD that addresses the variance issue with the existing algorithm, along with theory characterizing sample efficiency. The paper includes a well done illustrative empirical study to support the theory. The reviewers all scored the paper highly. 

The AC pointed out several minor issues in the presentation that the authors should address for camera ready.  In addition the grammar and word usage is rough in some places. Please take time to improve the text.
The paper proposes the physics informed neural operator. It combines the operating learning and function optimization frameworks, which improves convergence rates and accuracy over traditional methods. While the paper was well written, several reviewers raised their concerns on the novelty of the paper, especially regarding the difference from PINN DeepONet (Wang et. al.). Following this, there have been a long discussion between the authors and the reviewers, as well as among the reviewers. As a consequence, we think the authors somehow overclaimed their contributions on combining PINN and operator learning, and there are some important references missing and baselines not compared empirically. With this, the conclusion is that we cannot accept this paper in its current form, and we hope that authors can take all the review feedback into consideration and better position the novelty and impact of their work in the future submissions.
This paper proposes a learning to optimize approach that is "flatness aware", i.e. it tries to find flatter minima in the loss landscape. The idea is accompanied by both theoretical and empirical verification. However, in the current state, the paper did not convince the reviewers about its potential impact. In particular, reviewer ZY6B points out that comparison with "sharpness aware" minimization (SAM), which is a non learned optimizer for seeking flat minima, is an important comparison that is currently missing. Another issue mentioned by the reviewers  ZY6B and wp2U relates to the lower performance of the learned optimizer compared to standard SGD in large models (lower than 80% test accuracy). These reviewers are interested in the question of whether this method can still produce good results in the competitive setting (e.g. > 90% accuracy on CIFAR). Considering the performance/baseline issue even on small datasets such as MNIST and CIFAR, the impact of the proposed method is unclear. I encourage authors to adopt more conventional baselines to better indicate the potential impact of their method, and do consider comparison with optimizers that directly aim to improve flatness, such as SAM.
Previous approaches to model based offline RL require carefully tuning the trade off between model return and uncertainty. The authors propose an approach that produces a diverse pool of policies on the Pareto front of this tradeoff. On the D4RL offline RL benchmark, P3 outperforms competing approaches when the experience is collected with low or medium return policies.

Before the rebuttal, reviewers identified the following primary concerns:
* The experimental evaluation of P3 uses many policy evaluations to select the policy, which results in an unfair comparison with existing methods.
* P3 underperforms existing methods on some datasets. Why?

Overall, reviewers were satisfied by the response and raised their scores accordingly. The authors responded by including a modification of P3 that uses FQE to select the policy for evaluation, resolving the first concern. The authors explain that P3 underperforms on high return datasets because it splits its updates across the pool of policies. The authors state, "We believe (and in theory it does hold) that P3 can achieve the same performance of UWAC on high quality datasets, if provided with more computational budget." I suggest that the authors conduct at least one experiment to verify this claim.

The proposed idea is interesting and the revisions the authors have made resolved the primary concerns from reviewers, so I recommend acceptance. The reviewer/author discussion has many substantial points that I recommend the authors integrate into the revision.
This work proposes an instance specific label smoothing method, which is formulated as a two stage optimization problem for finding the optimal label smoothing. The authors show that the proposed approach can be equivalent to an efficient variant of self distillation techniques (i.e. no need to store the parameters or the output of a trained model). Experiments on image classification (CIFAR 10 and CIFAR 100) and natural language understanding datasets (the GLUE benchmark) demonstrate that our method is competitive against strong baselines.

The reviewers find the proposed approach reasonable, and the presentation clear. However, they all rated the paper as borderline, due to some concerns that the submission has in its current form. These include limited novelty (by CUiW) [the link between label smoothing and knowledge distillation is largely based on previous research findings (e.g., Yuan et al., 2020)], nonconvincing results on the effectiveness of the proposed method (by bwuZ) [Improvement of Pseudo KD in practice is not significant in terms of test accuracy gains], and lack of comparison with some recent related methods (by JBRd as well as other reviewers). The authors responded to these (and other concerns), but this did not convince the reviewers about the concerns listed above. I recommend the authors to resubmit after addressing these issues.
Summary: This paper studies a contextual bandit problem where the decision maker must communicate its intended actions (given observations of the contexts) to a controller through a constrained communication channel. The original part of the paper is that the â€œbandit algorithmâ€ must encode its actions into a compressed version that then serves to the controller. In that sense, the controller must cluster the problems for the decision maker to simplify communication.

Discussion: Most reviewers appreciated that the paper is well written and proposes an original problem. The main commonly issue is that of a lack of regret analysis. The authors included an additional theoretical result giving a necessary condition for sublinear regret but the committee would still appreciate a more in depth study of the performance of the proposed algorithm, given that the condition is satisfied. One possible direction is to connect this work with the literature on "Clustering of bandits" (CoB) as raised by reviewer mXFx. The authors claim that this paper is only mildly related but the committee would kindly insist that linear rewards are just a generalization of multi armed bandits, that there is also a finite state space in CoB (finite population) and it seems possible to reduce the proposed problem to CoB under some assumptions. In that regards, it would be important that a more thorough review and comparison of that line of work is done in the main paper (note also "Latent Bandits" and related papers), even though we agree that the proposed approach is different and we appreciate its originality.

Overall, the paper is borderline, and the committee did not reach a consensus. 

Decision: Reject
Three out of the four reviews rated this paper well below the acceptance threshold. Although the review scores show a relatively large spread, I think that the review contents are more or less coherent across the four reviewers.
The equivalence of the state equations (SEs; a set of equations that macroscopically characterizes optimal solutions of certain high dimensional regression problems) derived from three different approaches (AMP, CGMT, and LOO) is well expected to hold, as the optimal solutions should be independent of how their macroscopic characterization in the form of an SE is derived, and this paper concretely showed such equivalence to hold for three problems. More concretely, Theorem 1 states the equivalence of the SEs for M estimator derived from the three approaches, Theorem 2 states the equivalence of the SEs for LASSO derived from AMP and CGMT, and Theorem 4 states the equivalence of the SEs for logistic regression derived from LOO and CGMT. The main concern raised by all the reviewers is that this paper does not provide novel and significant insights as to why and how the equivalence arises.
Some reviewers also pointed out that this paper lacks citation to the relevant statistical mechanics literature, as well as that this paper contains so many typos, grammatical errors, and inappropriate typesetting styles. The authors responses were not instrumental in persuading the reviewers with negative evaluation. On the basis of these I would not be able to recommend acceptance of this paper for presentation at ICLR 2021.
This paper received a majority voting of rejection. In the internal discussion, one reviewer updated his/her score from 1 to 3 according to the author response. I have read all the materials of this paper including manuscript, appendix, comments and response. Based on collected information from all reviewers and my personal judgement, I can make the recommendation on this paper, *rejection*. Here are the comments that I summarized, which include my opinion and evidence.

**Interesting Idea**

Every reviewer including me agree that the idea of modelling Bayes label transition is novel and interesting. 

**The motivation lacks of supportive evidence**

The second motivation that "the feasible solution space of the Bayes label transition matrix is much smaller than that of the clean label transition matrix" is not well supported. The authors should theoretically or empirically demonstrate this point. The current description on uncertainty is not strong enough. Moreover, if so, the benefits are not illustrated. The feasible solution space, even with a small coverage area is continuous with infinite solutions. 

**A new concept**

The authors tried to sell the concept of a new transition matrix, but failed. I believe it might result from the organization and presentation. The authors spent too much pages introducing others  work. At least, a formal definition of the new concept should be given. In the current version, Definition 1 is from Cheng et al., 2020 on distilled examples.

**Title**

Literally from title, I guess DNN is a key component or a selling point of this paper. Actually no. We expect the authors could provide the insights on what benefits are using DNN over other techniques and how to apply DNN to estimate the transition matrix. If this is not a selling point, this word might be removed from the title.

**Algorithm 1**

I am a little surprised that the only algorithm listed in this paper is label noise generation. Instead the proposed algorithm of this paper is expected.

**Experimental Evaluation**

The experimental results look much better than other baselines. It is a little confusing that some best results are bold, some not. 

**Presentation**

Although I did not notice obvious grammar errors, some sentences are very long (3 lines). They made difficulties to follow the idea. I have to read these sentences several times. In my eyes, this is the biggest one! Presentation means how to sell the idea to audience (not only reviewers, but also future readers) in an easy understood way. The current version spent much space introducing others  work; on the contrary, the original or key part is not well illustrated. 

Although this paper has a novel idea and good experimental support, other issues listed above demonstrate the current version is not ready for a top tier conference. No objection from reviewers was raised to again this recommendation.
The paper provides an analysis of the well known method of Iterative Magnitude Pruning (IMP) for DNN compression. The problem tackled is undoubtably an important one, and IMP is likely one of the most known solutions for DNN compression. As such, there is no doubt that the paper is well motivated. In addition to the motivated task, the reviews indicate that the paper is well written and provides a thorough review of the related literature, making the paper easy to read and follow.
The main weakness of the paper seems to its novelty, as it seems that similar analyses have been done in the past. This issue was raised by the reviews and remained after the correspondence with the authors:

 WMeJ: â€œAs I described previously the consistent references and experimental structure borrowed from existing work hinder the novelty of the workâ€, dL1d: â€œWhile the paper introduces inspiring findings on how SLR (or CLR) help IMP, most components are from existing techniquesâ€.

Given the discussion and concerns related to the novelty of the paper, I feel that the paper requires too major of a revision to be accepted, either improving its core analysis, or presenting it in a better way that clearly distinguishes it from previous art.
This work presents an approach to learning good representations for few shot learning when supervision is provided at the super class level and is otherwise missing at the sub class level.

After some discussion with the authors, all reviewers are supportive of this work being accepted. Two reviewers were even supportive of this work being presented at least as a spotlight.

The approach presented is well motivated, experiments demonstrate its value and include a nice application in the medical domain, making the work stand out relatively to most work in few shot classification. Therefore, I m happy to recommend this work be accepted and receive a spotlight presentation.
The main concerns from the reviewers is the novelty of the algorithm and analysis from the CIVR algorithm of Zhang and Xiao (2019b). The author rebuttal clarified the main contributions as reformulation of DRO as composite finite sum optimization, solving heavily constrained optimization problems through composite optimization, and extension to distributed algorithms. They indeed lead to meaningful contributions to the important topic of DRO and open new avenues for structured constrained optimization problems. The paper is written very clearly and the empirical results on realistic problems are much appreciated.
Although the problem studied in the paper is interesting, all the reviewers believe that the current draft has limited technical contributions. Moreover, there are serious issues with the writing and presentation of the work. Also the experiments are rather limited and their results are not significant. I strongly recommend the authors to take the reviewers  comments into account and improve different aspects of their work for future conferences.
This is a well done job which combines a few ideas to reach means to identify problematic cases and indicate this when classifying. It has raised doubts about the applicability, though I can see that a abstention rule can have multiple uses. While the work seems to be well done, it has not largely excited the committee members. It initially missed to be placed well wrt existing work to highlight the novelty, and the demonstration that the approach can be generally useful is not complete. Dealing with abstention rules always brings another facet to classification and comparisons are not trivial in many situations. Not surprisingly, this has landed as a borderline case, which I place on the inside (as I like the topic and I think it is interesting work) but it could become an outsider depending on the overall view of the selected papers for the conference and other constraints.
This paper proposes learning to make stylistic code edits (semantics remains similar) based on information from a few exemplars instead of one. The proposed method first parses the code into abstract syntax trees and then use the multi extent similarity ensemble. This was compared to a Graph2Edit baseline on C# fixer and pyfixer, which are datasets generated by rule based transcompilers. The proposed method got around 10% accuracy improvement due to a combination of the method and using more than one examplar. 

The reviewers find that any improvement due to more examplars to be expected and suggested that 1) one carefully chosen examplar is enough, and 2) that the need for multiple examplars means more practical difficulties in providing them in an application 3) the targets are all generated by rule based methods and the benefits may not extend to a realistic case where the edits are not so clear and the reviewers wondered about the application value and the potential need for human evaluations.  The authors argued that it is unexpectedly difficult to expand the base method to multiple examplars and users should be able to provide examplars in an application. The authors further provided additional results that addressed some of the reviewer s concerns but the reviewers did not change their evaluation.

Rejection is recommended based on reviewer consensus.
This paper proposes a number of improvements to the previously published transformer based MQ forecaster model for multi horizon forecasts on time series data. They show strong empirical improvements in terms of accuracy and excess forecast variability on a large proprietary dataset, as well as four public datasets. Concerns were raised about the relatively incremental changes to the MQ forecaster model this work is based on, lack of ablations on public data and, relatedly, inability to reproduce results on the proprietary data.
This paper proposes the distributed Skellam mechanism for differentially private federated learning that relies on secure aggregation. Since multi party computation protocols rely on finite precision, the Skellam distribution meets the criteria of closure under addition and discreteness. During the discussion, the reviewers raised a concern that the proposed idea is highly similar to a recently published NeurIPS paper (https://arxiv.org/abs/2110.04995). However, since the timelines of the two papers are close, they can be considered concurrent work. Both results advance the study of private federated learning that leverages secure aggregation techniques. Through a more in depth comparison, the current paper also provides sufficiently different proof techniques than the ones in the concurrent paper. The authors should provide an extensive comparison between their work and the NeurIPS paper in their next revision. 

While the paper provides new results, there are several concerns in the reviews. First, even though the proof techniques are different from concurrent work, the reviewers still think that the main technique of Skellam perturbation has limited novelty. Second, the presented experiments also appeared to be quite weak. For example, the accuracy on MNIST is much lower than simple baselines in earlier work. While the authors tried to justify this reduction of accuracy through their decentralized training setting, the argument is not fully convincing. In particular, even though the noise addition is done in a decentralized fashion, the proposed algorithm is still subject to the same (standard) differential privacy constraint (as opposed to local differential privacy). The authors could consider improving the experiments or providing a more principled justification for the reduction of accuracy in their algorithm. Due to these issues, the paper does not clear the bar for acceptance at ICLR.
The authors question the assumption that the epistemic uncertainty provided by Bayesian neural networks should be useful for out of distribution detection. They start their analysis in the infinite width limit so as to be able to understand how the induced kernels in a Gaussian process behave. The paper also discusses the potential tradeoffs between generalization and detection. Overall, the paper presents some facts that, while not surprising, (Reviewer fGuy), are helpful in questioning the default assumption. Overall, though, the combination of the lack of surprise with the multi part, somewhat loosely connected message reduces the quality of the submission.
The authors consider the problem of using expert data with unobserved confounders for
both imitation and reinforcement learning settings. They showed how latent confounders 
negatively affect the learning process and proposed a sampling algorithm that mitigates the 
impact and delivers good empirical results.

I agree with the reviewers, this is a borderline paper but with a preference to accept. 
The most salient concern was the lack of clear contribution. While the algorithm is interesting 
with good experimental results that attract interest, it lacks actual theoretical backbone.  
That being said, the authors put in solid effort and addressed concerns sufficiently in the rebuttal stage.
Thus I would prefer to see it accepted. The proposed research direction should be explored in the future.
This paper proposes a new autoregressive flow model with autoencoders to learn latent embeddings from time series. The authors conducted extensive comparative experiments, and the experimental results are very encouraging. However, the proposed method, as a combination of the encoder/decoder structure and autoregressive flows on the latent space, does not seem novel enough.
This paper proposes a novel method for the single shot domain adaptation with the help of Generative Adversarial Nets. The proposed method is interesting, novel, and versatile. Moreover, the performance is impressive and better than the existing methods. However, the writing needs some improvement for better readability. More quantitive results should be provided in the revision for completeness.
The authors  provide a discussion of Cover s Theorem in the setting of equivariance.  The reviewers consider the work well explained and interesting, especially after the revisions, and so I will vote to accept.
The paper proposes an entropic coding approach for sentence embedding. 

Reviewers have spent good efforts in reviewing. They generally feel the problem is important/interesting, but also found it difficult to understand the paper. Thus, the authors are encouraged to thoroughly revise the paper according to the reviews provided, and another round of review is needed to better determine the merits of this paper.
This is a borderline paper on the well researched theme of Topic models.
The strongest point of the paper is that it proposes a new topic modelling framework where both word and topic embeddings live in the same space.
 It then appeals to optimal transport theory to do the necessary training using SGD. However, this is not the first paper to examine
Topic models and Optimal Transport theory. Several papers[1,2,3] in the recent past have started investigating this line of research.  In the rebuttal phase, the author(s) justify the choice of state of the art methodologies and also discuss the key conceptual difference between 
existing literature and the submitted one. The major difference seems to 
they approach the problem differently leading to better quality topics(as measured by several metrics) and computational efficiency existing state of the art requires more complicated iterations whereas proposed approach works with SGD. 
The manuscript, if accepted, needs to be updated considerably to reflect some of these aspects. 




[1] Hongteng Xu, Wenlin Wang, Wei Liu, and Lawrence Carin. Distilled wasserstein learning for wordembedding and topic modeling. In NeurIPS2018.

[2] Viet Huynh, He Zhao, and Dinh Phung. OTLDA: A geometry aware optimal transport approach fortopic modeling. In NeurIPS2020.


[3] He Zhao, D Phung, V Huynh, T Le, and W Buntine. Neural topic model via optimal transport, 2020
The paper proposes a classification method that improves model calibration using variational information bottlenecks and a noise contrastive loss.

Unfortunately, the authors  were not able to participate in the discussion of the paper with the reviewers. Given this, the reviewers raised several unaddressed concerns: First, it was argued that the different components of the proposed method required additional justification, in particular with regards to novelty. Second, reviewers argued that the paper required additional empirical validation, for example by testing if it works well with different convolutional methods such as ResNets.

Given these concerns, a consensus was reached that this paper should be rejected which is also my recommendation,
This paper on  anarchic  federated learning (FL) envisions an FL framework where edge clients can act independently instead of their participation being controlled by a central server. The idea is certainly promising, however, the reviewers pointed out the following main issues:
1) Technical gaps in the theoretical analysis need to be addressed
2) Bounded delay assumptions are too strong and are mismatching with the  anarchic  goal of the framework
3) The linear speed up claim should be better explained and justified.
The paper generated lots of post rebuttal discussions. However, the concerns about the theoretical analysis still remain, and therefore I recommend rejection. I hope the authors will take these constructive comments into account when revising the paper.
The authors propose a rank coding scheme for recurrent neural networks (RNNs)   inspired by spiking neural networks   in order to improve inference times at the classification of sequential data. The basic idea is to train the RNN to classify the sequence early   even before the full sequence has been observed. They also introduce a regularisation term that allows for a speed accuracy trade off.

The method is tested on two toy tasks as well as on temporal MNIST and Google Speech Commands.

The results are very good, typically improving inference time with very little loss in accuracy.

Furthermore, the idea seems novel and the paper is well written.

An initial criticism was that experiments with spiking neural networks (SNNs) were missing. The authors added a proof of concept for SNNs, which satisfied the reviewer. 

The authors also added some control experiments in response to the initial reviews, which improved the manuscript.

In summary, the manuscript presents a valuable novel idea with good experimental verification and interesting aspects both for ANNs and SNNs. The reviewers consistently vote for acceptance.
The manuscript develops a new and simple graph neural network architecture. The proposal make use of only O(V) (number of vertices) rather than O(E) (number of edges, meaning that it may be useful for scaling to larger problems. The didactic figures are especially clear, and as is shown in Fig 1 the proposed architecture passes messages based only on the source vertex rather than based on source and target. This challenges common ideas in the field that passed messages ought to reflect a function of both source and target. In spite of this introduced simplification, the architecture performed better than or as well as a set of strong baselines on a set of 6 datasets. The manuscript also examines latency and memory consumption, showing that the methods comes out favourably in this regard.
One of the reviewers worries that the paper does not directly provide a solution to scaling network training to very large graphs; they note that several of the datasets that are examined do not contain large graphs. This is true, but the paper does not overclaim in this regard, and I agree with the majority of reviewers that the manuscript is worth publishing on the basis of having developed a well performing approach that challenges the accepted assumptions in the field. While it may not be a direct solution, the counterintuitive results may help point the direction toward development of simple, effective approaches that do scale up.
The submission introduces an algorithm for structured pruning of fully connected ReLU layers using ideas from tropical geometry. The paper begins with a very accessible overview of key concepts from tropical geometry, and shows how ReLU networks can be thought of as  tropical polynomials. It gives an efficient K means based algorithm for pruning units in a way that approximately minimizes the Hausdorff distance between certain polytopes. Experiments show that the method outperforms other methods based on tropical geometry and is competitive with SOTA methods from a few years ago.

I think the reviewers, authors and I all agree on the following points: tropical geometry is a mathematical topic not commonly used in our field and for which it is difficult to find expert reviewers (notice that most of the citations aren t from ML venues). The paper is well written, and the authors have taken pains to present the required concepts in an accessible way. Nobody has raised any concerns about correctness. While this isn t the first pruning method that uses tropical geometry, the algorithm is novel and interesting. It s expensive, but not unreasonably so. The experiments are a proof of concept: they use small networks by today s standards, and the baselines aren t the current SOTA. 

The average scores are slightly below the usual cutoff. The reviewers are concerned about whether this method is useful, given that is based on different principles from current methods and can t quite compete with current SOTA. But my own sense is that this is a paper that we d like to have at ICLR. It gives a clear, accessible introduction to tropical geometry and demonstrates its usefulness for practical deep learning. It demonstrates competitiveness with fairly strong baselines, which is all we should expect from methods that haven t benefited from years of hill climbing on the same handful of ideas. I recommend acceptance.
This paper show that in several different neural network  architectures, recurrent  
networks that share parameters over iterations have comparable 
performance and similar features to feed forward networks of the same
"effective depth".  

Reviewers initially had some reservations about novelty and 
generalizability to deeper SOTA networks.  These were successfully
addressed by the authors and all reviewers feel the paper is above the
bar due to the importance of the area, and that this paper brings
together many important insights that, while many may have been known 
before, had not previously been all brought together before.  The maze 
task was also considered a useful task for the field.  I agree that
the paper makes a worthwhile contribution and am in favor of 
acceptance.
This paper proposes a few shot learning method that learns task adaptive semantic features that can incorporate for both of the support and query sets. Two approaches for modality combination are developed. The additional experiments in the author response addressed some concerns of the reviewers. However, the technical novelty of the proposed method is high enough since the proposed method uses existing techniques.
Reviewers were almost unanimous in favor of this paper, with scores of 5,8,6,8.
I think it s a neat idea and am inclined to accept despite some issues w/ motivation / scalability. 
Science proceeds in increments, and it s OK to propose something with scalability issues that someone else later tries to fix, etc.
The paper studied an interesting yet challenging problem in active learning and provided an intuitive heuristic for selecting informative subset(s) of training examples. The reviewers generally find the paper well presented and highlight that the clarity of the exposition of the issues of existing query heuristics, especially for training deep models with class imbalance data.

However, there are shared concerns among all reviewers in whether the existing experiments sufficiently justify the practical significance of the proposed heuristic (Reviewer 4ATq: Missing comparison against important baselines such as Gal et al 2017; Reviewer Cp2k: ablations of class and boundary balancing; Reviewer yngU: lack of comparison to SOTA and ablation for important hyperparameters; Reviewer oEcZ: lack comparison against SOTA). Reviewers also point out that the approximation guarantee is against an algorithm that is optimal wrt a somewhat ad hoc objective, which makes the theoretical components of the paper not as significant. Given the above concerns, the paper does not appear to be ready for acceptance at the current stage.
This paper discusses an empirical scaling law in terms of samples needed for pretraining for effective downstream transfer. The reviewers liked the premise but had major concerns with the evaluation and some clarifications about empirical choices made. The paper initially received reviews tending towards rejection. The authors provided a thoughtful rebuttal that addressed some of the questions. The paper was discussed heavily and all the reviewers updated their reviews in the post rebuttal phase. In conclusion, all reviewers still believed that their concerns regarding empirical evaluation like why evaluate only sim2real transfer, etc. still stand. AC agrees with the reviewers  consensus and encourages the authors to take the feedback into account for future submissions.
The paper proposes a new approach for linked view clustering based on chained non negative matrix factorization. Reviewers highlighted that paper proposes a novel and interesting approach to an important problem. However, reviewers raised also significant concerns regarding clarity of presentation (motivation, general approach, contributions, scope) as well as the experimental evaluation. Reviewers raised also concerns regarding justification of the approach being a novel paradigm. After author response and discussion, all reviewers and the AC agree that the paper is not yet ready for publication due to the aforementioned issues.
This paper studies the expressivity, complexity and unpredictability of emergent languages in referential games. The authors defined measures of complexity and unpredictability and empirically showed that the expressivity of emergent languages is a trade off between the complexity and unpredictability of the context that the languages are used in. They introduced a contrastive loss based training method that alleviates the collapse of message types seen using standard referential loss functions.

The paper is controversial among the reviewers. On the positive side, most liked how the paper has a clearly stated hypothesis and extensive evaluations which makes a clear contribution to the field of emergent languages. On the negative side, the paper only shows the results in an artificial setting where the key variables are highly simplified (e.g. size of candidates). The main negative review argue the authors used an inappropriate definition of unpredictability and that the batch size is actually the key independent variable instead of what is claimed. The paper does somewhat equate batch size with the candidate size that is so important to their results (after eq (1)), but they seem to measure candidate size in the key figures. Perhaps an experiment controlling for batch size independently of the candidates size can address this issue. On the point of defining unpredictability, the other reviewers and I find the given definition to be reasonable and at least defensible. However, the reviewer remained unconvinced. More generally, the paper relies on one definition of the concepts measured in one setting to make a general claim, which is at risk of missing other important variables. Overall, most reviewers found the scope to be sufficient, and two improved their scores after the discussion.

Recommendation: accept
This work received borderline rates with slight preference to rejection. The main concerns range from writing, novelty to empirical evaluations. Given that no authorsâ€™ responses are submitted, we have decided to reject this work.
The paper proposes a method for decentralized learning of cooperative games by maximizing the mutual information between the agents. The paper is novel and interesting and well evaluated.

Prior to the rebuttal, most of the reviewers saw presentation as the biggest weakness. Specifically, it was not clear what InfoPG refers to, and how it is related to the mutual information. During the rebuttal the authors cleaned up the misunderstandings around the presentation and provided a detailed analysis in the Appendix.

While the author responses provided helpful clarification and analysis, the authors should revise the paper holistically to remove unnecessary terminology and connections, and bring the analysis in the main text.
The paper proposes to improve generated images via a post processing update procedure guided by gradients from a robust classifier.  After the author response and discussion, all reviewers agree that the paper is below the acceptance threshold.  Reviewer concerns include limited technical novelty and missing experimental comparison to relevant baselines.
The authors propose two new variants of (projected) gradient descent for attacking a classifier and a detector simultaneously. Using these two new variants they are able to break four recent detection methods for adversarial samples.

Strength:
  All the reviewer acknowledge that breaking these four defenses is a valuable contribution.

Weakness:
  From a technical perspective the paper is rather simple and no theoretical support for the suggested variants is provided. The justification is rather handwavy. From an optimization perspective it is unclear why not a simple penalty based approach would have given the same results or would even work better. The choices maded in this paper seem a bit arbitrary and are mainly justified by the fact that they work for the four detectors
  the honeypot defense was already broken in 
A Partial Break of the Honeypots Defense to Catch Adversarial Attacks, Nicholas Carlini, arXiv:2009.10975

Minor:
  the authors should update the references, several papers have appeared in the meantime

While I appreciate the contribution of the broken defenses, in terms of technical contribution and discussion of the methods this paper is borderline.
This paper studies the problem of learning single layer neural networks under Gaussian marginals in the presence of outliers. The authors give a recovery algorithm in this setting. The consensus among the reviewers was that the paper lacks technical depth. Specifically, the algorithm is a minor tweak of the one in Wu et al. 2019 for the case without outliers. Another concern was that the algorithm does not recover the prior result when the fraction of uncorrupted points goes to 0. Overall, the paper is below the acceptance threshold.
Strengths:
* Theoretical foundation provided to knowledge integration problem
* Findings from the empirical studies are interesting
* Authors dedicated significant time and energy to coordinating with reviewers in the rebuttal period

Weaknesses:
* It is not clear whether the GCS is a suitable approximation for measuring KI. For example, relation types are not supported in the GCS architecture making it unclear whether GCS adequately approximates knowledge integration. As reviewer 4qCM mentions, (X, born_in, Zurich) is very different knowledge from (X, died_in, Zurich). The current formulation only learns co occurrence between entities rather than relational knowledge. 
* Empirical study is limited to two knowledge integration methods (ERNIE & K Adapter) and only evaluated on entity typing datasets, which are likely to be well suited for their method which ignores relation information.
* The presentation and takeaways of the results could be clearer. Authors should explain in depth why experiments that drop knowledge randomly are not suitable baselines.

This paper is promising and the topic explored by the authors is interesting. I think it would benefit from integrating the comments from the reviewers and will make for a strong submission at a future venue.
The paper studies rate at which SGD escapes local minima and provides a potential justification for the "flat minima" observation. 
Reviewers agree that the paper studies an interesting problem and provides a nice result. But it seems like that paper in it s current shape is not ready for publication at ICLR. Issue is that the paper s writing is not up to bar, and requires a fair bit of work. In particular, the paper doesn t define the key quantities formally, doesn t provide all the assumptions in one place and justify why they might be reasonable. Finally, it would be great if the final result about escape rate is provided clearly with a self contained theorem/lemma that define/describe most of the key quantities in the rate.
This paper studies the problem of characterizing the optimal early stopping time in overparameterized learning as a function of model dimension and sample size. To do this the paper uses an explicit form of the gradient flow from prior work to present high probability bounds in the over parameterized setting and characterizes various properties of the optimal stopping time. The authors also conduct various experiments to verify the theory. The reviews though the paper was interesting and insightful. They also raised some concerns about the (1)restrictiveness of the distributional assumptions, (2) poor explanation of the theoretical results, and (3) novelty with respect to other work and (4) other technical issues. The discussion and response mitigated these concerns but the reviewers decided to mostly keep their original score. My own reading of the paper is that there are good ideas in this paper and I agree with the authors that some of the technical issues raised by the reviewers is incorrect. However, it is also clear that the paper needs a bit more work to put it into the right context and also the proof need to be more clearly and carefully written before this paper can be accepted. Therefore I recommend rejection but encourage the authors to submit to a future ML venue after a thorough revision.
#### Summary

The goal of this work is to reduce the costs of inference in ensembled models by ensembling sparse models. The paper also aims to reduce the costs of training these ensembles as well. The proposed techniques (DST and EDST) each these goals, respectively. 

#### Discussion

As noted by the reviewers, the paper is interesting and timely. The authors provided significant clarifications in the response that satisfied the reviewers  concerns. There is still significant room to revise the remaining points and polish the text of the paper for the camera ready (I highly recommend proofreading from an individual who is not an author on the paper; there are still typos in the revised edits)

#### Recommendation.
 
I recommend Accept, due to the strengths above and the reasonably scoped remaining work to do going into the camera ready.
This paper presents an empirical study of generalization in visual reinforcement learning. This study is carried out in the domain of video games and it addresses the benefits of techniques such as regularization, augmentation and training with auxiliary tasks. The reviewers for this submission were positive about the goal and setups in this paper. They agreed that understanding why present day methods that attempt to improve generalization continue to fall short, is an important problem. However, most reviewers were underwhelmed by the findings presented in the submission. As examples: Reviewer 185P mentions that "the paper does not seem to provide a clear and definite answer to the question" and " I am not convinced the experiments described in this paper support the claims made by the authors." and Reviewer SFef mentions that "Most of the conclusions are already known". Some reviewers also found a lack of clarity and several typos in the initial submission. The authors have provided detailed responses to the reviewers. In particular they have fixed most writing issues. They also detailed why certain algorithms and techniques were benchmarked in this submission and others were left out. I think this is reasonable. One cannot expect a paper to benchmark every algorithm out there, and choosing promising and representative ones is sufficient. My takeaway from detailed discussions about this paper are that: The paper is much improved from a writing point of view and the rebuttal addresses some concerns well. However, I do agree with the reviewers that the findings presented in the paper are for the most part expected. This reduces the value of the paper to readers. When this is the case, it may be beneficial to dig deeper into these findings and present a narrow but deep analysis. Please see Reviewer 185P s suggestions in this regard. Given the above, I am recommending rejection for this conference, but I encourage the authors to take into the reviewers suggestions and resubmit.
The paper talks about a novel setting in Federated Learning and argues that personalization methods may cause the personalized models to overfit on spurious features, thereby increasing the accuracy disparity compared to the global model. To this end the authors propose a debiasing strategy using a global model and adversarial tranferability. 

 There were some positive opinion about the problem being interesting .However reviewers had several concerns about the validity of assumption and hand wavy arguments used in the solutions for existence adversarial tranferability. Overall, the settings and the need for removing personalization bias needs to be validated more convincingly and rigorously, with concrete real scenarios and experiments.
I thank the authors for their submission and active participation in the discussions. All reviewers are unanimously leaning towards acceptance of this paper. Reviewers in particular liked that the paper is presenting an interesting and systematic study of reward hacking [GVMn] that is useful to the research community [bfGN] and targets an important problem [uYeb] in a rigorous way [16uL]. I thus recommend accepting the paper, but I strongly encourage the authors to further improve their paper based on the reviewer feedback, in particular in regards to improving positioning with respect to related work and a better formalization of their work.
In this manuacript, the authors develop feature fool attacks with feature level adversarial perturbations using deep image generators and a novel optimization objective. They further show that the feature fool attacks are versatile and can generate targeted feature level attacks at the ImageNet scale that are simultaneously interpretable, universal to any source image, and physically realizable.
The reviewers agree that the paper is well motivated and the authors have addressed some concerns.
However, the reviewers still do not satisfy with some concerns so as to keep the initial scores.
In comparison with the manuscripts I m handling, I have to recommend to reject it!
The paper proposes an approach to constructing steerable equivariant CNNs over arbitrary subgroups of E(3), by generalizing the Wigner Eckart theorem for steerable kernels in Lang & Weiler (2020). The intuitive idea is to use a steerable basis for a large group like O(3) to build a basis for a subgroup of interest like SO(3). Reviewers were generally happy with the author response, finding the paper makes a good contribution to steerable network design, with theoretically interesting ideas. However, there were still questions after the rebuttal about the practical utility of the approach, such as the relevance of subgroups of O(3). Reviewers also felt that much of the material was not written in an accessible way, such that it could only be appreciated by experts working on group equivariant CNNs. 

In a final version, the authors should try to make a much clearer case for practical relevance, introduce the key concepts assuming less prior knowledge, and present the material in a way that makes the high level story more clear, as detailed by reviewers.
The authors have proposed a new consistency loss for improving model robustness to common corruptions. With a student teacher training setup, only the student network uses batch normalization at training time. Improvements are shown on small scale corruption datasets (CIFAR C), a single domain generalization dataset (VLCS), and RobustPointSet.

Though, positive feedback were given on the quality of the story telling, and on an interesting motivation by a few toy examples, some concerns remained among the reviewers.
In particular applicability of the method as model and data sizes increases, e.g., on ImageNet C, was questioned.
After Additional results were provided by the authors, the method seems to break as scales increases.
The way relevant baselines from previous work was also judged light and should be improved.
Hence, the paper could be improved to include more comparisons and more convincingly showing advantages of the method.
The paper presents modifying latent optimization for representation disentanglement using contrastive learning, resulting in improved performance on disentanglement benchmarks. Despite the empirical success, the proposed algorithm has many moving parts and loss functions. Most reviewers agree that given the incremental and complex nature of the proposed technique, the empirical results are not sufficient for acceptance at ICLR, especially since the results do not present additional insights into the inner workings of the method. I encourage the authors to try to simplify the technique, or provide a convincing evidence that such complexity is necessary.

PS:
I didn t find much discussion of how the hyper parameters are chosen (temperature, lambda terms, etc.).
A discussion of recent self supervised disentanglement methods (e.g., https://arxiv.org/abs/2102.08850 and https://arxiv.org/abs/2007.00810) can be helpful.
The paper presents an interesting approach for defining conditional diffusion models. The core idea of this work is based on a new analysis of how class centers evolve in the forward diffusion process. On this positive side, this work builds on top of this analysis and introduces conditional diffusion processes that are guided towards class centers. This paper shows marginal improvements in small image datasets (MNIST and CIFAR 10) and auxiliary applications such as image inpainting and attribute based image synthesis (demonstrated through only qualitative experiments). On the negative side, the proposed approach has the fundamentally limiting assumption that a class can be represented by a cluster center in the RGB space. Unfortunately, this assumption does not hold for practical datasets such as ImageNet where samples in each class have high diversity, and the class centers in the pixel space are not very distinct for different categories. The reviewers have rated this paper slightly above the borderline. They have acknowledged the novelty of the proposed guided diffusion. But they have criticized the submission for the lack of experiments on more common and challenging benchmarks. They also have criticized this work for not providing quantitative results on the auxiliary tasks. I agree with the reviewers that these experiments would shed light on whether the class center idea would hold for more challenging scenarios.

In the rebuttal, the authors provided additional quantitative results for the text to image generation task. However, these results show that the proposed method is outperformed by prior works. Most other auxiliary tasks including image inpainting and attribute to face generation are still demonstrated through qualitative experiments without detailed quantitative results. 

In summary, given the limitation of the proposed approach, this submission currently lacks an in depth analysis of the proposed work on challenging benchmarks and a detailed quantitative comparison to relevant baselines for the auxiliary tasks. Because of these concerns, we believe that the paper in its current form is not ready for publication at ICLR at this point.
This is an interesting paper which further extends the duality theory of deep networks.  Unfortunately, reviewers had many concerns about, presentation, technical details, and missing prior work.  I will add that a large volume of relevant implicit bias work (e.g., in the setting of deep linear networks, mirroring Proposition 2) is completely uncited (e.g., works by Arora et al., Soudry et al., Ji et al.), despite being earlier than many of the works which are currently cited.  As such, I urge the authors to continue in their valuable line of work, taking into consideration all of these points and also the reviewer comments.

Separately, I note that there is a violation of the blind policy in the current revision: grant information was included.  The PC decide this was a minor violation and should not affect the review process, however their decision could have easily been otherwise.  I urge the authors to be exceptionally careful with such issues in the future.
This work proposes an approach to unify pre training based and meta learning based few shot learning, inspired by dropout.

None of the reviewers support the acceptance of this work, despite the authors  detailed rebuttals, with the majority of reviewers confirming their preference for rejection following the author response. 

I unfortunately could not find a good reason to dissent from the reviewers majority opinion, and therefore also recommend rejection at this time.
The authors propose a simple addition to adversarial training methods that improves model performance without significantly changing the complexity of training.  The initial reviews raised some questions about whether experiments were sufficiently extensive, but these issues were resolved during the rebuttal and discussion period, resulting in a strong consensus that the paper should be published.
This paper proposes a method to use Transformers with tabular data by sharing attention. Reviewers raise significant concerns about the motivation, writing and experimental results. Author s did not submit a response. Hence I recommend rejection.
This paper proposes to modify DETR, a recent Transformer based architecture for object detection.
More precisely,  they propose to sparsify input feature maps by learning an extra classifier to select which input features (few of them) will be used in the attention module. The supervision of this classifier is guided by second extra module  coming from the Transformer decoder attention weights.
The resulting framework, called Sparse DETR, is an efficient end to end object detection architecture that allows to overcome the main computational bottleneck of DETR. Sparse DETR can use  only 10% 50% of the original encoder query while achieving DETR comparable results.

Authors tried to answer to all the questions raised during the rebuttal. 
Even if the final scores are still contrasted, most of reviewers are very positive. 
Overall, this paper provides valuable insight into reducing computational complexity (by reducing the number of queries) for DETR like detectors. The proposed method is novel and technically sound. Even if there are some tricks to make the whole working well, this work is more effective and efficient than previous propositions to handle this complexity problem,  bringing us some new insight of how to sparsify queries without performance dropping.  
All these elements lead me to propose this paper for publication at ICLR.
There are numerous known methods for memory reduction used in CNNs.
This paper takes two such quantization (Q) and random projection (RP) and applies them to GNNs.  This is a novelty, but I agree with the reviewers: on its own this novelty would not be "surprising" enough to report at ICLR.  

The paper further goes to show empirically that these methods, when applied to a reasonable set of datasets, do indeed produce their predicted memory reductions (unsurprising) with a small ($\approx 0.5\%$) drop in accuracy (surprising, in the sense of not being something one could predict without doing the experiment).

All of the above is in one sense "just engineering", with only a small inventive step.  Any real world deployment of GNNs would, if an army of engineers were available, naturally implement quantization and RP in order to see what kind of improvements they might make.  This would be just two more hyperparameters (R,B) to add to the sweep, and the deployment would vary them until the required accuracy was achieved in the minimum time (OOM is a red herring   one would vary batch size, other compression, or ship values to CPU in order to make progress).

However, "simply adding two more hyperparameters" is a significant increase in the deployment burden, which is where the paper s third contribution comes in: the theoretical analysis of the effects of the two processes, with straightforward but nonobvious calculations of the effect on gradient variance of the two processes, and, usefully, their interaction.  The value of this theory is twofold: first, it gives us new tools to analyse such processes; and second, it allows us to be much more judicious in the selection of these hyperparameters.

In all, the reviewers  objection of no great novelty in porting ideas from CNN  to GNN is sustained; but the authors  claim as to the value of the theory is sustained, and no reviewer provides prior art to dispute the novelty of the theory calculations.

The revised paper has already expanded the key sections in Appendix E, and added welcome experiments which strengthen the paper.  I would encourage a final copy (and certainly the poster presentation) to emphasize some of the insights over the raw experimental numbers.  As the authors hint, those numbers are subject to vagaries of what PyTorch happens to implement, while the underlying analyses are a little longer lasting.

Some other comments: 

A lot of discussion time was spent on the question of whether 0.5% is negligible.  This is entirely application dependent, and is part of the hyperparameter/architecture tuning process.

    the extra time overhead of swapping "can go up to 80%, which is not feasible in practice".
  Not so: if choosing between OOM or 1.8x slowdown, I will of course choose the latter.
    "for a fair comparison, we do not change any hyperparameters"
  Again, not relevant: in a real application (which is where this paper contributes), we of course change the learning rate when batch size changes.
    "the accuracy drop of sampling may be greater than EXACT"
  Again, whether that drop is too much depends entirely on the actual application.  

And please do take a look at typos/grammar/English etc.
There appears to be to be a fundamental error in the paper, w.r.t. the application of the proposed approach to finite fields. As a result, the paper cannot be accepted in its current form.
The paper proposes a simple approach to quantizing neural network weights with encouraging empirical results. The authors did work hard to improve the paper and address reviewers  concerns during the discussion period. I believe the presentation of results can improve by adding a discussion of inference time. I am not sure if all of the baselines (e.g., in Figure 4) have the same inference cost.

PS1: The method does seem to unroll the iterative optimization process (ie. EM) of a Gaussian mixture model (GMM) and differentiates through the unrolled iterations. The paper makes the connection to attention, but does not seem to make a clear connection with GMM and EM. If this connection is correct, adding a discussion can be helpful.

PS2: I am not a big fan of using differentiable k means as the method name. Differentiable k means is confusing partly because k means is differentiable, i.e., one can optimize k means centers using gradient descent. The proposed approach seems more relevant to meta learning, where one differentiate though one optimization process to optimize a secondary objective.
As the reviewers say, the subject matter of this paper is important, and of interest to the ICLR audience (I discount tzHo s suggestion that the paper is more suited to other venues).

However, there are three primary reasons this paper should not be published as is:
1. A theoretical paper *must* be precise, accurate, and clear. 
  The reviewers universally consider the notation ambiguous, and the theorem unproved because of this ambiguity.
 2. The leap from solving a series of tasks optimally to having solved the composition optimally is indeed poorly argued in the paper, and is not resolved by the discussion.
 3. I would also strongly recommend showing a less trivial example.  It does not need to be "real world", but it should address numerically the specific doubt of BuW: the relationship of OTE of the composed model to OTE of the subtask models.

In summary: TaFL may be true, but this paper does not show it to be true; or conversely, TaFL may be false, in which case publishing this paper would be a grave error.  The authors should use the reviewers reports to clarify and strengthen the argument.  This does include showing numerical results, because inspection of the code generating such results can often aid reviewers and readers in judging the truth of the theoretical claims, and in finding subtle missteps in the derivations.
This paper received 3 rejections and 1 marginal accept. Reviewers were unanimous in that empirical evaluation is lacking. No rebuttal was submitted and I have no reason to overturn the reviewers  decisions. I recommendation this paper be rejected.
The authors propose UPSIDE, a method for improving state coverage in unsupervised skill learning. "Direct and diffuse" policies concatenate phases of directed behaviour followed by dithering to improve neighbourhood coverage in the region of state space visited, trained with a discriminability objective to ensure diversity in region coverage, and composing these skills into a tree structure to further improve coverage.

Reviewers generally found the submission highly novel and well written, were encouraged by the experimental results, and were unanimous on the importance of the problem addressed. Reviewers d1m1 and wdBX praised the visualizations as adding considerable insight. d1m1 raised the concern that a random walk in the second stage would not work in some environments (which the authors pointed out can be overcome by the tree method) and suggested DADS was a better baseline than DIAYN (also mentioned by zwRd, with which the authors agreed, and proceeded to implement). zwRd expressed concerns about gaps in the finetuning results (addressed by the authors in follow up) as well as details of the DIAYN setup (also addressed to zwRd s satisfaction). H3Jm s minor concerns around the text and equations, figures and DIAYN experimental results were fully addressed in discussion. wdBX was the reviewer with the most substantive concerns around the apparent complexity, ad hoc design, scalability and lack of theoretical grounding. After discussion (which resulted in the authors drafting a proof in the comments, an ICLR first for this AC) wdBX believed that the paper would be improved by additions proposed, but remained "borderline" in their evaluation.

The work presents a compelling, if somewhat complicated (and therefore perhaps unsatisfying, to some), method improving state coverage in unsupervised skill learning. Research in contemporary machine learning often takes the form of one piece of work pushing the boundaries significantly with a complex algorithm with further work dissecting and refining the ideas therein, reducing them to their essence. Viewed through this lens, the paper in question presents several ideas that all form parts of the overall method which appear likely to serve to inspire and motivate future work. With all reviewers leaning accept either strongly or weakly, there is little doubt in the AC s mind that this paper should be accepted, and widely read by researchers interested in unsupervised skill acquisition.
All reviewers agree on acceptance and I agree with them. I recommend a spotlight.
A number of suggestions have been given about the manuscript. The evaluation raised questions about clarify, placement with respect to other approaches, choices for the design, etc. There are no immediate replies from authors, so I hope the suggestions are useful for future work.
This paper presents a deep learning method that aims to address the curse of dimensionality problem of conventional convolutional neural networks (CNNs) by representing data and kernels with unconstrained â€˜mixturesâ€™ of Gaussians and exploiting the analytical form of the convolution of multidimensional Gaussian mixtures. Since the number of mixture components rapidly increases from layer to layer (after convolution) and common activation functions such as ReLU do not preserve the Gaussian Mixtures (GM), the paper proposes a fitting stage that fits a GM to the output of the transfer function and uses a heuristic to reduce the number of mixture components. Experiments are presented on MNIST (2d) and ModelNet10 (3D), which show competitive performance compared to other approaches such as classic CNNs, PointNet and PontNet++ methods.

There is somewhat an overall consensus on the novelty of the proposed approach and its potential to pave the way for further research. There were, however, several issues raised by the reviewers in terms of clarity, memory footprint and computational cost that limits the applicability of the method to more complex datasets. While the authors expanded on the dense fitting in their comments and in the revised version of the paper, it still remains unclear the role of the negative weights, as the dense fitting stage seems to constrain all the weights to be positive. In terms of memory footprint, the authors refer to the theoretical footprint and their implementation does not match this. Finally, it is acknowledged by the authors that the computational cost is a limitation that hinders the method from achieving competitive performance in more complex tasks.
This paper proposes a policy gradient algorithm based on the Bregman divergence and momentum method. While one reviewer was initially concerned about the technical novelty of the paper given some existing works, after the author s response and paper revision, the reviewers are all convinced and have reached a consensus to accept this paper. Thus I recommend acceptance.
This paper investigates the linear mode connectivity of the loss landscape of neural networks, i.e. whether a convex combination of two parameters of local optima on the SGD paths has low loss values (i.e. low barrier) up to some permutations. To probe this question, this paper empirically studies the loss gap, named as â€œbarrierâ€, between two local minima and their convex combinations or linear interpolation. Before permutations, such barriers are typically non zero; yet, after taking into account of permutation invariance of models, such barriers could be reduced along to zero with the width increasing, a main conjecture formulated in the paper. To support this conjecture, the authors proposed a simulated annealing algorithm to search for such permutations, demonstrating that the barrier reduces after such permutations.

The reviewers unanimously accept the paper, if the authors make the proposed improvement in the final version. In particular, a reader points out a paper by Singh and Jaggi,  Model Fusion via Optimal Transport , NeurIPS 2020, that supports the same conjecture with a constructive algorithm to find optimal permutations or matching using optimal transport. This should be included in the final version as the authors replied.
This paper presents an analytic approach for estimating the optimal reverse variance schedule given a pre trained score based model. The experimental results demonstrated the efficacy of the proposed method on several datasets across different sampling budgets. Given the recent interest in score based generative models, I believe that the paper will find applications in various domains. I am pleased to recommend it for acceptance.
This paper introduces a novel approach for out of distribution detection that generates scores from a trained DNN model by using the Fisher Rao distance between the feature distributions of a given input sample at the logit layer and the lower layers of the model and the  corresponding mean feature distributions over the training data.

The use of Fisher Rao distance is novel in the context of OOD, and the empirical evaluations are extensive.  The main concerns of the reviewers were the limitations of the Gaussianity assumption used in computing the Fisher Rao distance and the use of the sum of the Fisher Rao distances to the class conditional distributions of the target classes rather than the minimum distance. These concerns were addressed satisfactorily in a revision. In terms of technical novelty, experimental evaluation and novelty, the paper is above the bar of acceptance.
This paper proposes an idea to address the non IID issue that is well known in federated learning. After the discussion with the authors, there are still some concerns remained about the proposed approach. First and foremost, the training of local GAN at each client can be demanding computationally and statistically, which limits the practicality of their approach. Secondly, there has been other work that aims to study the non IID issues in federated learning, as suggested by the reviewers. The authors should consider citing some of the work in this literature and compare the prior approaches with their GAN based approach. Thirdly, there is a lack of a formal statement about the privacy guarantee in this paper. In particular, it seems that the privacy guarantee would only make sense in the cross silo setting, in which each client has many users  data. If each client corresponds to a single user, it does not make sense to train a local GAN. The authors should consider elaborating on the privacy guarantee in the next revision.
The paper shows that adversarial training can be fooled to have robust test accuracy < 1% with a new type of poisoning attack ADVIN on the CIFAR 10 dataset, even though the robust training accuracy > 90%. This requires 100% poisoning rate, though the claim is that the poisoned data is  semantically similar  to the original data. This is an interesting research direction. Questions were raised about novelty as well as whether these poisoned data could be detected. During the rebuttal phase the authors provide some evidence that with adaptive attacks detection could be evaded (as expected). The authors are encouraged to take all comments into account and update the paper as indicated in the rebuttal for any future revision.
The paper proposes a graph convolution operator (BankGCN) to be used in graph neural networks. The reviewers mainly raised concerns about the limited of novelty in the light of numerous previous works that are similar or address similar problems as well as lacking evaluation. While the rebuttal addressed some of the concerns, the overall impression is that the paper is not of sufficient methodological or experimental significance for the conference.
The paper describes a new model based RL technique for constrained MDPs based on Bayesian world models.  It improves sample efficiency and safety. The reviewers are unanimous in their recommendation for acceptance.  This represents an important advance in RL.  Great work!
The paper investigates the performance of low precision Stochastic Gradient Langevin Dynamics (SGLD). While similar low precision techniques have been widely used in optimization, much less is known for Markov Chain Monte Carlo (MCMC) methods. The paper develops a new quantization function to make SGLD suitable for low precision setups and argues for its use in deep learning. 

The main concerns among the reviewers were related to the paper presentation (separation and comparison between optimization and sampling), comparison to Dalalyan Karagulyan 19 and overview of this work, technical depth, and numerical experiments. The authors have adequately responded to the reviewers  comments and addressed them to the extent possible. However, there was ultimately not enough support to lead this paper to acceptance.

I find low precision sampling a worthy topic of study and the contributions of the paper are interesting. The authors are encouraged to revise the paper based on the reviewers  comments, more clearly highlight the contributions, and resubmit.
The authors introduce a method that improves the representation learned by RL agents, making them more robust to visual distractions. In particular, their approach proposes to use mutual information between two views as a proxy for that objective. This is clearly a borderline paper that required many discussions among the reviewers and the authors. The reviewers mention that the approach is novel, addresses an important problem of robustness in RL and some of the experiments provided are impressive. On the other hand, the reviewers point out that the baselines seem to achieve lower results than previously reported, writing could be improved and some of the results don t show significant improvement over baselines. 

Given that some of the results cause confusion around the evaluation protocol (it s still not 100% clear why the performance of baselines is lower than expected) and other doubts expressed by the reviewers, I encourage the authors to continue working on the paper and resubmit. I believe that with a little bit of extra work and clarifications this can be a very strong submission.
The reviewers had remarkably consistent feedback about this paper. They appreciated the formulation of the federated learning problem with architectures having both shared and private (personalized) components. On the other hand, they felt the experiments were insufficient to prove the effectiveness of the method, and had several suggestions in terms of tasks and datasets. They also felt that it s hard to assess whether the existence of private/personalized components is warranted without visualizing the difference between architectures. Overall, the reviewers had good feedback that could strengthen the paper.
In spite of some slightly mixed scores (with one borderline positive review), scores are ultimately lukewarm and tend toward negative (and furthermore, reviews are broadly in agreement as to the issues they raise). Main issues center around low significance of the results, and issues with the presentation that need to be addressed.
The idea of learning unstable features from source tasks to help learn stable features for a target task is interesting and well motivated. As the proposed method and its theoretical analysis of learning unstable features from tasks are an incremental extension of an existing work [Bao et al. 2021], the technical contributions line in applying the idea of stable and unstable features learning to the setting of transfer learning. Therefore, the evaluation of this work is focused on the effectiveness of the proposed method in the transfer learning setting. 

In transfer learning, one major goal is to make use of knowledge extracted from source tasks to help learn a precise target classifier even with a few or no labeled examples of the target task. It would be more convincing if experiments are conducted to show how the performance of the proposed method changes when the size of labeled data of the target task changes. This is to verify whether the exploitation of unstable features can help to learn a stable classifier for the target tasks more efficiently (i.e., with fewer labeled examples). In addition, as some baseline methods used for comparison do not need to access any labeled data of the target task (like unsupervised domain adaptation or domain generalization approaches), it is not fair to conduct comparison experiments in the setting where there are sufficient labeled examples of the target task since the original designs of such baselines may fail to fully exploit label information in the target task. 

Another concern is whether the proposed method is realistic for real world transfer learning problems. Though in the rebuttal, the authors provided experimental results on a natural environment (CelebA), the constructed transfer learning problem is more like a toy problem. Indeed, there are many transfer learning benchmark datasets that contain multiple domains/tasks. It would be more convincing if experiments are conducted on those datasets.

By considering the above two concerns, this paper is on the borderline. My recommendation is a weak rejection based on the current form of this paper. Note that as some references listed by reviewers RJhJ and J8M5 are not really related to the proposed research here, the novelty of the proposed method compared with those references is NOT taken into consideration to make this recommendation.
Perron Frobenius operator (P) is a well known tool which maps the density of a dynamical system at time t (p_t) to that at t+1 (p_{t+1}): p_{t+1}   P p_t. The idea has recently been extended (kernel Perron Frobenius operator (kPF); Klus et al. 2020) to map a probability measure p_Z to p_X via covariance operators (4) associated to a reproducing kernel; this corresponds to the transformation of the kernel mean embedding of Z to that of X as it is recalled in (5). The authors use the kPF technique in generative modelling to map the known prior (p_Z) to the data generating distribution (p_X), and illustrate the idea numerically.

While the focus of the paper (generative modelling) is relevant, the reviewers had several severe issues with the submission:
1) the manuscript lacks clarity of presentation at multiple points,
2) the reviewers had concerns with the scalability of the approach (which unfortunately has not been analyzed),
3) the submission is a straightforward application of a well established tool (kPF) in the literature; the paper lacks novelty.

Significantly more effort is required before publication.
In this paper, the author present a method for learning a shared latent space between the fMRI activity of multiple individuals processing the same stimulus. The method consists of an auto encoder with a single encoder and subject specific decoders which is specifically regularized to decouple common and shared representations. This paper generated a lot of discussion between the reviewers and the authors, as well as between the reviewers. In light of these discussion, I cannot recommend acceptance at this point, as the paper is not ready. The main concerns were (1) about how the results and improvement are evaluated statistically, (2) that the baselines chosen were not strong enough and did not include existing approaches (neural or non neural) and relatedly (3) that the paper was not framed correctly within the existing literature on finding shared spaces between participants, which would help with determining and understanding the novelty of the proposed approach. Some other smaller points were made by the reviewer can also strengthen the paper for a future submission in a neuroscience or machine learning venue.
The four reviewers believed the paper was below threshold for acceptance to ICLR. They raised concerns with the experimental evaluation and thought that the paper could benefit from another edit to help with the clarity.
PAPER: This paper presents analysis of cross modal interactions in multimodal models and propose a method to help balance the multimodal learning process. The cross modal analysis is based on measures related to conditional utilization rate and the proposed approach is related to conditional learning speed.
DISCUSSION: The reviewers showed support for this line of research, as a way to better understand the learning process for multimodal models. The discussion allowed to identify points that needed to be clarify and concerns about the experimental results. The authors addressed many of these issues in their response. All reviewers took the time to read these responses as well as other reviews. There are many reviewers who are still expressing concerns with the experimental results. 
SUMMARY: This is an important line of research, and the authors should continue this research endeavor. While the paper presents some interesting research hypotheses about multimodal learning, it seems that more experiments are needed to properly address these hypotheses. In its current form, the paper may not yet be ready for publication.
This paper proposes a new evaluating metric for assessing the quality of model generated images, that aims to correct some of the problems with the popular FID metric. The reviewers acknowledge the importance of this problem, but do not find the empirical evaluation convincing. In particular, they highlight the following issues
* Comparing FID and the new metric on examples that are adversarially selected against FID does not provide a fair comparison.
* The methods are compared on images of bad quality (FID > 25) that are therefore not informative.
* The comparison against existing techniques is incomplete
* The reviewers raise concerns about how the comparison is done quantitatively

The reviewers are not sufficiently convinced by the author response regarding these issues. I therefore recommend not accepting the paper.
This paper develops a novel continual meta reinforcement learning algorithm that focuses on learning sequential tasks without revisiting previous tasks. The setting is compelling, and the method is well developed with good empirical results. The initial version of the paper included a variety of issues, especially lack of clarity in some aspects and the contributions, that were remedied through discussion with the reviewers and subsequent revisions. The discussion among the reviewers seems to have settled on leaning toward a weak accept overall, with one low score that should be dismissed claiming lack of novelty (which isn t correct   the paper certainly is sufficiently novel).  There do remain some concerns by two reviewers that although "the paper has enough meat to be accepted, ... [it] needs more careful and well thought out ablations and analysis to be truly valuable." Although the authors have revised the paper to address this issue of a precise analysis, adding material into the appendices with some changes to the main text, they are encouraged to make certain that these aspects are integrated and clear throughout.
The reviewers are in consensus that this manuscript falls just short of the bar. I recommend that the authors take their recommendations into consideration in revising their manuscript, with a particular focus on comparison to the state of the art.
This well written paper introduces an improved exploration strategy by exploiting knowledge about sequences of actions that lead to the same state. The idea is straightforward and easy to understand and apply, which makes it potentially interesting. An important downside is the limited applicability of the method, as there mainly seems to be an advantage in (mostly deterministic) grid like MDPs. In addition, priors about action sequence equivalences have to be available. Overall, the contribution of the paper is not deemed significant enough for publication at a top tier conference like ICLR by the majority of the reviewers as well as myself. For these reasons, I recommend rejection.
After carefully reading the reviews and rebuttal, I believe this work is of sufficient quality for acceptance. Understanding continual learning from a theoretical stand point is a very important topic. I find that one of the main issue raised by reviewers was about the exact meaning of Continual learning, and whether what the authors studied was more akin to sequential learning. While I don t mind the term sequential learning, and is quite descriptive of the work as well, I disagree that the considered setup is not continual learning.
The paper proposes a symmetry informed neural network for modelling many body systems. The network is empirically evaluated in the tasks of predicting Newtonian trajectories and molecular conformations.

All four reviewers are critical of the paper and recommend rejection (one weak, three strong). The reviews have flagged weaknesses and quality issues with several aspects of the submission, including the proposed methodology, the novelty of the contribution, and the clarity of the presentation. Although detailed clarifications were provided by the authors, most of the reviewers  concerns remain, and the consensus among reviewers remains to reject the paper.

Consequently, the current version of the paper does not appear to meet the quality standards for acceptance to ICLR.
This paper analyzes the gradient behavior of RNNs in terms of the Lyaponuv exponents of its trajectory/orbit, showing that RNNs with cyclic or stable equilibrium dynamics have bounded gradients, but if the dynamics are chaotic the gradients will explode. From these insights, the authors propose an algorithmic remedy for this pathology, which is essentially a teacher forcing method that periodically projects the observation onto the hidden state during training. A thorough empirical investigation is performed showing the utility of the proposed approach for modeling chaotic data.

The reviewers had split opinions on this paper. Some reviewers found value in the theoretical contributions and the connection between Lyapunov exponents and behaviors of the dynamics of recurrent neural networks, while others thought the theoretical framework may have limited practical utility. Several reviewers found the initial experiments to be lacking, though many of their concerns were alleviated after the substantial additions the authors provided during the discussion phase.

I believe the observation that exploding gradients are unavoidable when modeling chaotic data is important and would be of significant interest to the broader ICLR community. However, the practical implications of this observation have not been thoroughly described or investigated, and without this perspective, the theoretical results by themselves are much less impactful. In practice, it is usually the case that the ground truth function is not learned exactly, the time horizon is finite, the gradients are noisy, the data generating process is opaque, etc. Do these caveats have any bearing on the conclusions? The experiments address some of these questions, but only indirectly, and a more explicit discussion of the practical implications would broaden the impact of the paper.

Along the same lines, the practical utility of the theoretical framework could be further supported if there were some analysis of more varied or additional RNN use cases. As one reviewer mentioned, I think the ICLR community in particular would appreciate any theoretical or algorithmic insights that might yield improvements on a standard baseline task like seqMNIST, which has served as a point of comparison for many alternative methods and which would facilitate comparisons to prior work.

Overall, this paper does make some nice and potentially important theoretical insights about training RNNs on chaotic data, and it does include an extensive battery of empirical evaluations, however the practical implications remain largely unconvincing, and I believe the paper falls just short of the bar for acceptance.
The paper investigates the use of flow models for out of distribution detection. The paper proposes to use a combination of random projections in the latent space of flow models and one sample / two sample statistical tests for detecting OOD inputs. The authors present results on image benchmarks as well as non image benchmarks. 

The reviewers found the approach well motivated and appreciated the ablations. The authors did a good job of addressing reviewer concerns during the rebuttal. During the discussion phase, the consensus decision leaned towards acceptance. I recommend accept and encourage the reviewers to address any remaining concerns in the final version.

It might be worth discussing this paper in the related work: Density of States Estimation for Out of Distribution Detection https://arxiv.org/abs/2006.09273
Scores ultimately point to accept. The one negative review is borderline and doesn t raise any red flags. Weaknesses in other reviews mainly point to minor improvements in the paper and are largely supportive. Rebuttal points are uncontroversial and seem to clarify several issues.
This paper presents a graph neural network model to predict the severity of depression symptoms from text. It proposes to construct a graph with word nodes and use schema structure to capture the context information in the text. A schema encoder is introduced for modeling the constructed graphs.

Strength:
  Interesting application domain and clear motivation
  Experiment results demonstrate the effectiveness of the method
  Paper writing is clear and easy to follow

Weakness:
  Technical novelty of the method is limited
  Experiment comparison with some recent work is missing
  More in depth analysis on the method are needed
  Some details of the method pipeline require further elaboration
This paper presents a technique for compositionally constructing embeddings for nodes in knowledge graphs, hence reducing the memory requirements as well as allowing inductive learning. The reviewers find the direction promising and the approach novel and well motivated. There were some concerns about the experiment results â€” Reviewer KuBz suggests including more baselines, Reviewer CpaB suggests trying NodePiece on single relation graphs and Reviewer 2qcD notes that NodePiece lags behind the other approaches on some tasks. Most of these concerns seem to have been addressed in the author response and I tend to agree with the authors that single relation graphs are out of the scope of this work. Reviewer X7aq also raised a concern about the claims made regarding (i) uniqueness of the hashes and (ii) sub linearity of the approach. It is good to see that claim (ii) has been removed, but (i) is still present in many places â€” it would be good to add a discussion about why the hashes are highly likely to be unique in the final version.
This paper studies the problem of using oracle information that s only available during training in RL. The key contributions are 1) a variational Bayesian approach that models the oracle observation as latent variables; and 2) a Mahjong environment for benchmarking RL with oracle guiding. The novelty of the proposed approach is limited, but reviewers find the problem intriguing and agreed that it s a reasonable application of Bayesian approach to RL with latent oracle information. In addition, the Mahjong environment could benefit the community and spur new work in this direction. Therefore, I recommend this paper to be accepted as a poster.
PAPER: This paper presents a multimodal auto encoder architecture built on the premise that unimodal variations can be best generated when taking advantage of a shared latent space. This is operationalized by defining a hierarchical model with two primary levels: a shared structure space and unimodal variations (which could be multi layer). 
DISCUSSION: The reviewers and follow up discussion brought many questions and issues. The authors submitted a significantly revised version of their paper which clarified many issues and added a few extra results. While many of the reviewersâ€™ questions were addressed by the authors, it seems that reviewers ended up not changing significantly their review scores. One fundamental concern is if the basic assumption about the shared structure is effectively the proper way to approach such generative modeling task. The experimental for image generation did not seem to support this hypothesis.
SUMMARY: While the revised version was an improvement over the original submission, improving clarity and adding some experimental measures, the experimental results did not seem to always support the main hypothesis. Human evaluation results may help in this direction.
The paper proposes a method to learning rate scheduling that uses information form the eigenvalues of the Hessian. It shows that this scheduler obtains the minimax optimal rate on the noisy quadratic problem; and, empirically, this scheduler demonstrates faster convergence on CIFAR 10 and ImageNet, when the number of epochs is small.  Using Hessian information in direct and indirect ways is of interest to the community, and the paper does a nice job illustrating that in a context of interest.
This paper studies the design and analysis of contextual bandits algorithms, combining the ideas of neural network models (Zhou et al, 2020 and Zhang et al, 2020) and reward perturbations (Kveton et al, 2019, 2020); this has the computational advantage of avoiding inverting large covariance matrices, as is done in the other neural contextual bandits algorithms. Although the reviewers think that the papers need to do a better job in highlighting differences and extra challenges in the current work compared to prior works, they also acknowledge that this paper is the first that combines the above two ideas. 

The reviewers also acknowledge that the additional experiments in the rebuttal period help clear the concern the reviewers have about why all regret curves look linear. However they also pointed out, that comparison with the FALCON+ algorithm (Foster et al, 2020) may be slightly unfair, as the algorithm retrains the neural network after every new iteration. Overall, the reviewers think that the pros outweight the cons, and they lean towards acceptance.
The paper provides a new geometric functional analysis perspective for the generalization bounds for neural networks. As the AC, I actually quite liked the twist the authors are providing for this particular work. Unfortunately, the current presentation is too crude to provide an elementary picture for the developments and I strongly encourage the authors to revise the paper for the next deadline based on the remarks from the reviewers.
Thank you for your submission to ICLR.  All the reviewers are in agreement that this paper presents a nice contribution to the field, highlighting a class of DEQ models that correspond to optimization problems.  This provides a nice perspective on what kinds of computations DEQ models may perform, and I think provides a valuable contribution to the field.  The comments provided by the authors in their responses were satisfactory, and all reviewers were ultimately in agreement  that the paper should be accepted.

One comment: the authors mention that monDEQ models are restricted by requiring that the activation be a prox function, but actually [Revay et al., 2021] (https://arxiv.org/abs/2010.01732) showed that any monotone Lipchitz <  1 function can be used there.  I believe this captures the settings the authors consider here, so it s not clear to me that the formulation indeed provides greater expressivity that monDEQs, and this point should be considered in the paper.  More broadly, however, it is true that the perspective of the monDEQ techniques are different, but I would try to be as precise in this point as possible.
This submission describes an approach to compressing the communication in federated learning. The key idea is using a set of random samples from a prior distribution and then performing importance weighed sampling. The work performs an analysis of the privacy guarantees of this process and experimental evaluation.
The main issue with this work is the authors appear to be unaware that the basic problem they pose is solved in a more comprehensive and lossless way in a recent work https://arxiv.org/abs/2102.12099 (Feldman and Talwar, ICML 2021). That work shows that any differentially private randomizer can be compressed via a simpler algorithm that performs rejection sampling using a PRG. The algorithm does not loose privacy or utility (under standard cryptographic assumptions) while guaranteeing low communication. In contrast this work loses significantly in utility and provides opaque privacy guarantees.
This submission analyzes  a randomized that adds Gaussian distribution and, in particular, the compression technique in (Feldman and Talwar) applies to it. The technique proposed in this work is very similar in spirit (with prior distribution corresponding to reference distribution in the earlier work.
In light of the earlier work I do not think the contributions in this submission are sufficient for publication.
The paper uses graph based neural networks to ensure constraint based simulation.  Even though the approach is a good one, it is only incremental w.r.t. the work published by Yang et al at NeurIPS in 2020; then, the experimental section is not convincing enough.

While the authors indicate their dissatisfaction with one of the reviewers  assessment, the overall reviews of the paper are not very positive.
The paper introduces a purely spike based method for training spiking neural networks with recurrence, by extending the recently published "implicit differentiation on the equilibration state (IDE)" technique. As a purely spike based method for both the forward pass and the gradient computation, the proposed technique potentially represents an important advance.

Based on the original submission, the reviewers had difficulties understanding the paper s contributions and verify its claims. I commend the authors for engaging with the reviewers by answering their questions and updating the paper to better explain the contributions. However, even after considerable back and forth, the most positive reviewer still expressed major concerns [[1](https://openreview.net/forum?id VQyHD2R3Aq&noteId Ubrdawfds5)], and the other reviewers appeared unmoved, based on their scores.

The reviewers  principal concerns were a little hard to distill. It is possible that their initial difficulties with understanding and validating the paper s contributions made it hard to fully appreciate the paper. One reviewer is unable to verify that the algorithm is purely spike based, and that the energy costs are appropriately calculated. They are also unsure if the method will scale to more complex settings [[1](https://openreview.net/forum?id VQyHD2R3Aq&noteId Ubrdawfds5)]. A second reviewer was also initially unable to verify the same claims, was unsure if the theoretical improvements were sufficiently significant, and whether the method could apply to non IF models of spiking neural networks [[2](https://openreview.net/forum?id VQyHD2R3Aq&noteId cc2tMpSst0t)]. The authors addressed this in their response by performing additional experiments with LIF neurons, but it wasn t clear if the main concerns were sufficiently addressed, since the reviewer did not change their score.

Based on the largely negative appraisal by all reviewers, I recommend that be paper be rejected. However, I strongly encourage the authors to revise and resubmit their paper to a future conference, focusing on making sure that their central claims can be more easily understood and verified.
The paper proposes a series of zeroth order optimization approaches to stabilize DARTS training. Although the reviewers think that zeroth order approach is novel to the NAS community, they also point out several weaknesses. In particular, the method will introduce extra computation time and the results are not really standing out comparing with other state of the art methods. Therefore, despite some interesting ideas are presented in the paper, we decide to reject the paper and encourage the authors to address those weaknesses in their future revision.
A conceptually and technically highly innovative paper which reinforces an existing powerful connection between the critical set of two layer ReLU networks and suitable convex programs with cone constraints. The reviewers are in strong consensus that the paper is sound and has merits for publication.
Thank you for your submission to NeurIPS.  The reviewers are quite split on this paper, but some remain substantially negative even after discussion.  I m a bit more optimistic about the paper: the observed increase then decrease in ER during fine tuning _does_ strike me as a fundamentally interesting phenomenon, and I believe that papers that present such phenomena can be valuable contributions even without more fundamental "explanations" of the observations.  My recommendation, therefore, ultimately rests largely on the fact that I think (as is honestly evidenced by the reviews to a large degree), the presentation and contextualization of these results can be substantially improved in a future revision of the paper.  Specifically, the fact that several reviewers found the results obvious and/or not sufficiently substantiated suggests that the basic premises here are still failing to land.  I would strongly suggest revisions that clarified these points in a resubmission.
The paper analyzes a 2 stage method for federated learning, first using FL with local steps, followed by a final phase of  always communicate  centralized SGD. For the convex case, the paper studies the influence of the data heterogeneity, a key parameter in FL, on the convergence of related schemes. Surprisingly the results of the 2 stage method seem to be basically identical to pure local training followed by the final centralized phase, and almost match the lower bound for communication.

Reviewers liked the interesting aspect of the heterogeneity induced error floor when the phases are switched, and its impact on the convergence rates, which can be substantial. Downsides are that the analysis only works for strongly convex setting, and the combination of the two methods and proof being relatively straight forward. Simplicity of the algorithm is a plus, while of the proof depends on novelty, about which reviewers are border line but positive. 

Deep learning experiments should be expanded, as there the very opposite order of the two phases https://arxiv.org/abs/1808.07217 is more commonly used (i.e. more communication in early phase can help), which should be discussed. Also, in the experiments the tuning of hyperparameters in the single stage baselines needs to be improved to be more fair, which the authors have started but not fully finished for the Cifar case.

We hope the authors will incorporate the open points as mentioned by the reviewers.
Multi objective learning is an increasingly important topic. This paper presents a method for better finding parts of the Pareto frontier through a new method to estimate the distance to the frontier and use this proxy to refine the state space partition.  The reviewers found this paper interesting and compelling and generally well written. The reviewers also thought the work could be further improved by better clarifying in the text where the proposed approach might fail, and what properties of the domain are needed, and also to better situate this paper within the related work, potentially including additional experimental comparisons. The authors provided detailed responses to the proposed questions and the authors are encouraged to ensure that these suggestions and discussions are well represented in the revised version.
This paper proposes a technique to perform data imputation with normalizing flow defining a joint density between observed and unobserved variables. This is achieved by introducing a variational posterior over the missing variables which is parametrized in terms of the original model by using the Schur complement of the model s Jacobian over the hidden variables.
The idea is interesting, but the proposed setup is quite complex and the experimental results are not conclusive. The quality of the results shown can likely be matched or surpassed with much simpler techniques. The paper would substantially benefit from more detailed experiments.
The paper addresses unsupervised conditional text generation extending emb2emb (Mai et al, 2020) with bag of vectors antoencoders.

Reviewers shared several concerns about the clarity of this paper and empirical results.
This paper studies augmentation based methods to improve GNN fairness. 
Specifically, based on upper bounds, they propose augmentation tricks to reduce such bounds, empirically validated from benchmark datasets.

Before rebuttal, there was a negative consensus that evaluation results are inconclusive and important state of the arts are not discussed. 
Authors have significantly revised to address the concerns of some reviewers (gszb and LHQM), though some concerns still remain that the scheme is rather ad hoc. 
Meanwhile, reviewer xMz4 did not find rebuttal sufficient, as some valid comments were not fully discussed.
First, datasets where sensitive attributes are the inherent attributes of instances are more suitable for evaluation, which has not been properly addressed by authors.
Second, important baselines were mentioned by xMz4, but they were only mentioned briefly in the new section of related work in the revised work.
For example, (Agarwal et al 2021) is mentioned as dealing with counterfactual fairness only, but statistic parity studied in this paper is highly related to this concept.
Similarly, this work can be viewed as an ad hoc extension for fairness of GCA, without in depth discussions to compare/contrast with these work, and to better highlight novelty/distinction.

Summing up reviewer discussions, we conclude this paper is not ready yet as an ICLR publication.
This work presents a proposal for increasing the compositionality of emergent languages that uses a measure of topographic similarity as an auxiliary loss function on the communication game. The authors find that in certain cases this loss indeed results in increased generalization but overall the authors do not find a strong relation between high weights for weighting loss and and generalization.
All reviewers agree that the of compositionality is very important and the idea of explicitly optimizing for compositionality (through the topographic similarity metric) is also novel. At the same time a number of concerns are raised by reviewers: 

a) B3Jo and WEY2 raise the point that more evidence is required to establish the robustness of the current findings, e.g., by controlling whether topsim is merely inducing a regularization behaviour and providing more confidence on the current presented results (e.g., Figure 2 currently provides a somewhat perplexed pictured as the additional loss doesn t seem to improve across the board).

b) The relation between compositionality and generalization is not a new one and it is not clear what exactly the current paper is adding on this discussion and, as zB6g and  B3Jo point, this makes it seem rather incremental.

c) the paper is currently somewhat hard to follow with numerous results reported in a somewhat raw format, little to no examples and important details being presented only in Appendix (e.g., the loss function is only given in p12 and the actual format of L_{C} is never provided)

As such, I cannot recommend acceptance at this time but, given the importance of the topic, I sincerely hope the authors will work on incorporating reviewers  feedback for a later resubmission.
This paper proposes an extension of CTC by considering the wild card to adjust the label missing issues during training. The authors propose to minimize the loss over all possible sub segments of the input to automatically align the one that matches the available transcript. It is empirically proved to significantly improve performance over CTC even if up to 40 70% label sequence is missed (overall performance similar to the complete label case) across different tasks. 

As agreed by the reviewers, the paper is well presented and the problem is interesting to a broad community. Dynamic time warping with unconstrained endpoints itself is not a new idea and a classical topic for speech recognition (e.g. word spotting). The contribution of the paper is the formal introduction of the approach to CTC and to give experimental results to confirm the effectiveness. Also the use of simulated data weakens the paper a bit.

The decision is mainly based on the clear presentation and fair experimental justification.
The paper explores self supervised learning on tabular data and proposes a novel augmentation method via corrupting a random subset of features. The idea is simple but effective. Experiments include 69 datasets and compare with a number of methods. The result shows its superiority. It would be inspiring more work for SSL on the tabular domain.
This paper recognizes that several common sub problems studied in RL, such as meta RL and generalization in RL, can be cast as POMDPs. Using this observation, the authors evaluate how a straightforward approach to deal with POMDPs using a recurrent neural network compares to more specialized approaches. The reviewers agree that the research question studied in this paper is very interesting. However, after careful deliberation, I share the view of reviewer 2WFY that the results insufficiently support the claims made in the paper. In particular, I view the main claim from the abstract "We find that a careful architecture and hyperparameter decisions yield a recurrent model free implementation that performs on par with (and occasionally substantially better than) more sophisticated recent techniques in their respective domains."  as insufficiently supported. The main issue with the experiments is that only a small number of simple domains are considered. As Luisa points out in the public comments, variBAD dominates recurrent baselines when more complex tasks are considered, while on simpler domains such as the Cheetah Vel domain considered in this paper, it performs similar to a recurrent model free baseline. In the rebuttal the authors have added a more complex domain to address this, showing that a recurrent model free baseline outperforms an off policy version of variBAD. However, I view these results as inconclusive, as only a single complex domain is considered and they appear to contradict previous results with on policy variBAD. For these reasons, I don t think the work in its current form is ready for publication at ICLR. But I want to encourage the authors to work out this direction further. In particular, adding more complex domains and also considering the on policy variBAD method, can make this work stronger.
This paper shows SGD enjoys linear convergence for shallow neural networks under certain assumptions. However, reviewers reach the consensus that this paper lacks technical novelty. The meta reviewer agrees and thus decides to reject the paper.
The paper revisits lossless compression using deep architecture. In contrast to main stream approaches, it suggests to make use of probabilistic circuits, introducing a novel class of tractable lossless compression models. Overall, the reviews agree that this is an interesting direction and a novel approach. I fully agree. Actually, I like that the paper is not just saying well, we could use a probabilistic circuit for ensure tractability but also shows that there is still a benefit of different variable orderings for encoding and decoding. In any case, adding probabilistic circuits to the "compression family" is valuable and also paves the way to novel hybrid approaches, combining neural networks and probabilistic circuits. I have enjoyed reading the paper, reviews, and discussion.
Meta Review of Learning from One and Only One Shot

The motivation of this work is to address the problem of learning from very few samples, which is of high relevance for many machine learning problems. The paper proposes an (interpretable) approach for one or few shot learning, which tries to simulate the human recognition ability for â€œdistortâ€ objects. To achieve few shot learning, they first model the topological distance with training data points while minimizing the distortions, to find neighbors that are conceptually similar to the input image. Their experimental results show that this simple method can achieve good performance when only very few samples are available and no pre training is allowed.

All reviewers, including myself, agree that this paper is well motivated, nicely written, and appreciated how they connected ideas from neuro psychology to their ML model, and the novelty is recognized. But there are issues raised by reviewers with the paper that prevent it from meeting the bar for me to recommend it for acceptance at ICLR 2022.

The main issues raised by all reviewers is that the proposed method is only experimentally verified on simple datasets such as MNIST, EMNIST and Omniglot (and to some extent, Quick, Draw!). The authors (to their credit) in the rebuttal noted that the narrative of the paper is to focus on abstract images, and the purpose is more from a scientific investigation perspective (rather than proposing an algorithm that is immediately useful for ML practitioners), and this is a fair point. However, I do believe the issue here is beyond the simple criticism of "it works on MNIST, how about ImageNet?" as I think some reviewers genuinely think there are fundamental aspects of the approach that might prevent it from scaling (as planned in future work, even by the authors in the last section). For instance, as gUCx noted:

1) The proposed approach is based on topological similarity. It seems that it only is suitable for images with simple topological structure, such as the character images. Maybe it is hardly used to classifier complex nature images since we need more information for natural image classification, not only use topological structure.

2) The authors did not provide the experimental comparison with enough training data, such as the whole training set in MNIST. The reviewer wonders about the upper performance of this approach with enough data.

I tend to agree with these points. Non topological similarity can be displayed in abstract images / datasets. Even in "abstract" images, the paper should describe limitations of the approach, and whether it breaks down (like in the "abstract" Quick, Draw! dataset, there are different types of distinct "yoga" poses in the yoga class. And likewise, in the cat or pig class, there are animals with only the heads, and animals with the head and the full body). Conveniently, Quick, Draw! had not been used in any of their classification experiments [1], and only for a simple clustering example.

And for the other points, reporting the terminal MNIST performance will be useful, even if it doesn t look good, so the readers have an idea of the limitations of the approach, where it is good, where it is not, and what needs to be improved. I would love to see improvements (either in the writing or in the experiments) in future work where the paper can effectively convince the readers that the direction has the promise of being able to scale to "real" or "complex" images. (Perhaps even performing the approach on the output of a pre trained self supervised autoencoder on ImageNet, as a method to get "abstract" versions of real photos, like a parallel of the giraffe experiment, though this may distract from the narrative of no pre training). All in all, I don t want to discourage the authors as we are all excited about the direction of this work. I hope to see an updated version of this work published in a future venue, good luck!

[1] https://www.kaggle.com/c/quickdraw doodle recognition
The paper integrates several dimensionality reduction and sparsity methods for improving the efficiency of large pre trained models. Overall, the paper is interesting and discusses an important topic. However, it seems that it is not ready to be published at the current stage. I would encourage the authors to take reviewers comments into account and further improve the paper 

The pros and cons of the papers are summarized in the following: 

Pros: 
+ Improving the efficiency of large pre trained models is an essential research issue. 
+ The idea is interesting although the technical novelty is a bit limited. 

Cons: 
  The key concern is that the technical and practical benefit of the proposed approach is not clear based on the results demonstrated in the experiments. 
  The writing of the paper can be further improved in general to make the motivation more clear.
This paper proposes an architecture of a policy network (WaveCorr) that is particularly effective for portfolio management tasks.  A key observation that leads to the design of WaveCorr is that the dependency across asset should be treated differently from the dependency across time.  The proposed WaveCorr has the property that it is "permutation invariant" with respect to assets, which means that the class of functions that can be represented by WaveCorr is invariant to permutation of assets.  WaveCorr is shown to achieve the state of the art performance in a portfolio management task.

A major point of discussion was the definition of "permutation invariance".  The reviewers and AC understood the difference between the permutation invariance defined in this paper and that studied in the prior work (the output of a network is insensitive to the permutation of the particular values of the input).  With the definition in this paper, however, a fully connected layer is permutation invariant, but the Corr layer proposed in the paper appears to have more structure.  It is unclear exactly what properties of the Corr layer leads to the performance improvement.
This paper explores the connections between reward maximization (RM) with REINFORCE and distribution matching (DM) with distributional policy gradients (DPG) for fine tuning language models. Based on this, the paper proposes to apply a baseline (an idea in reinforcement learning) in DM to reduce variance and improve sample efficiency. Reviewers have concerns on the technical novelty as claimed in the paper, since the application of baseline is a straightforward practice and the resulting method is a simple addition to the existing method. More analysis (such as on the tradeoff between prior and constraint satisfaction, etc) was also suggested.
All the reviewers agree that this paper made a solid contribution of understanding the algorithmic regularization of SGD noise  (in particular the label noise for regression) after reaching zero loss. The framework is novel and has the potential to extend to other settings.
This paper proposes to use evolutionary methods to learn auxiliary loss functions, demonstrating superior performance vs. typical auxiliary losses previously proposed in the RL literature.

Demonstrating that it is possible to learn auxiliary losses by evolution, both for pixel and state representations, that help train significantly faster (even on new environments) is definitely a meaningful contribution, as acknowledge by the majority of reviewers.

Although many of the original reviews  concerns were addressed by the authors during the discussion period, two major ones were only partially answered, both related to the limited empirical evaluation of the proposed approach (which is crucial for such a contribution that aims to demonstrate an improvement over existing related techniques):
1. The limited set of environments used for evaluation (and in particular the lack of partially observable environments)
2. The fact that the baseline being compared to was CURL, which the paper describes as "the state of the art pixel based RL algorithm", while reviewers mentioned DrQ and RAD as two more recent (and better) algorithms that were known well ahead of the ICLR submission deadline (note that the more recent DrQ v2 is now even better). Since the data augmentation techniques used by these algorithms help shape the internal representation, like auxiliary losses do, it would have been important to validate that the proposed technique could be useful when plugged on top of such baselines.

The authors did try their best to address these major concerns during the rebuttal period, but the discussion between reviewers and myself came to the conclusion that this wasn t quite convincing enough yet. I encourage the authors to investigate these points in more depth in a future version of this work so as to make the empirical validation stronger (NB: the links provided in the last comment by authors on Nov. 30th didn t work, but this wasn t the main factor in the decision).
The paper proposes a variational inference based on singular learning theory (SLT), where the resolution of singularity is learned by normalizing flow so that the latent distribution is factorized.

Pros:
  A unique idea to use SLT for variational inference.

Cons (only serious concerns):
  Goal is unclear.  The authors say that they propose variational inference based on SLT.  But apparently, they propose it not as an alternative to the state of the art variational inference for neural networks (if so the experiments shown are far from the acceptable level).  The authors must clearly say for what purpose they propose a new method.  I would guess the proposed method is for analyzing singular models to compute their RLCT.  In that case, the authors should compare with existing methods for evaluating RLCT, e.g., MCMC based methods:

K. Nagata and S. Watanabe, "Exchange Monte Carlo Sampling From Bayesian Posterior for Singular Learning Machines," in IEEE Transactions on Neural Networks, vol. 19, no. 7, pp. 1253 1266, July 2008, doi: 10.1109/TNN.2008.2000202.

and discuss pros and cons of the proposed method.  For DNN, you should use the state of the art MCMC sampling methods like

Wenzel, F., Roth, K., Veeling, B., Swiatkowski, J., Tran, L., Mandt, S., Snoek, J., Salimans, T., Jenatton, R. &amp; Nowozin, S.. (2020). How Good is the Bayes Posterior in Deep Neural Networks Really?. <i>Proceedings of the 37th International Conference on Machine Learning</i>, in <i>Proceedings of Machine Learning Research</i> 119:10248 10259 Available from https://proceedings.mlr.press/v119/wenzel20a.html.

as a baseline.  Approximating the posterior with normalizing flow can be another baseline.

  Large n issue.  SLT can be seen as a generalization of the asymptotic learning theory for the regular model, where the model complexity is represented by the parameter dimension d, and "asymptotic" means n >> d.  Watanabe revealed that the model complexity cannot be represented by d in singular models, and therefore the definition of "asymptotic" is not as clear as the regular case.  But it is known that typical neural networks are overparameterized and can achieve zero training error.  I have seen no work arguing that SLT holds in this regime.  If the authors insist that their method is applicable to deep neural networks, they should cite references where it would be proved that SLT holds in the overparameterized regime or prove it by themselves.

There are many more concerns including those pointed out by reviewers, and the paper is not ready for publication.
This paper develops a hybrid search space consisting of both multiplication based and multiplication free operators. It also presents a weight sharing mechanism for searching in the introduced search space.

Pros:
* A hybrid search space is developed.
* Strong empirical results are reported for both CV and NLP tasks.
* The paper is well written and is easy to follow.

Cons:  
* Incremental technical novelty.
* Missing baselines and competing methods.
* Missing information on the search cost.
* Lack of insights into the discovered architectures

The rebuttal has provided most missing information and comparisons, and it has provided additional insights into the searched architectures. However, the reviewers still rate this paper at borderline primarily due to the limited technical novelties. Unfortunately, given these concerns, this submission does not meet the bar for acceptance at ICLR.
The paper provides theoretical bounds for imitation learning with rewards (algorithm from Wang et al. (2019)). The bounds/proofs are highly novel and a very interesting contribution to the community, even though they are a lot more conservative than what is observed in practice. All reviewers agree on this point.
It is laudable that the authors also additionally provide an experimental evaluation. After the revision and the discussion, quite a few of the reviewers are still not 100% convinced about them, on the one hand as they would have liked to see more tasks, and on the other hand due to concerns about the reward relaxation (i.e., doesn t match the assumptions in the theorems any longer) which is required for experiments on standard benchmarks.
In the final answer the authors provide evidence that there is no big discrepancy, which is good enough (given that there don t seem to be any alternatives to get around this issue, except removing the experimental section altogether, which would be undesirable). Please clearly point out those limitations of the experiments in the paper and also incorporate this evidence.
This paper considers a generalized weighted least squares optimization method for the random Fourier feature model. Generalization error analysis is carried out under both the over parametrized and under parametrized schemes, and under both noise free and noisy scenarios.

Reviewers generally agree that this is a solid theoretical work that considerably extends previous work (Belkin et al. (2020), Xie et al. (2020)) in the literature and that the paper is ready for publication.

While some reviewers express reservation about the relevance of the work to ICLR, I believe this is a nice work that would be of interest to the machine learning community at large.
This paper studies the important problem of adding structured knowledge (in this case from Wikidata) to pretrained language models. The reviewers do not see this paper as ready for ICLR and recommend a number of revisions. Unfortunately the authors did not respond during the author response period. The area chair hence agrees with the reviewers.
This paper presents some interesting new ideas on training binary neural networks. However, as many reviewers point out the study is quite limited by their experimental section, and some technical issues were raised. These are criticisms that remain largely unaddressed after the author response, hence the paper is not ready for publication at its current form.
This paper considers that the model s training data may be not accessible when learning the attacking model, and thus a more practical blackbox attack scheme, Beyond ImageNet Attack (BIA) framework, is designed. All the reviewers agreed that the setting in this paper is important and helpful when designing attack methods. However, the method is not totally new. Nevertheless, considering the importance of the problem investigated in this paper, the nice design of the overall framework, and the extensive experiments, the AC recommends accept for this paper.
This paper introduces a defense method (gradient broadcast adaptation) against backdoor attacks on pretrained language models. It proposes to utilize prompt tuning to guide the perturbed weights back to a normal state and thus helps avoid the degradation of model s generalization ability. 

Strengths:
  Experiments are conducted across multiple datasets with different types of backdoor attacks, demonstrating the effectiveness of the proposed approach
  The proposed idea is well motivated and intuitive

Weakness:
  Improvement on experiment results seems marginal
  Some technical details of the attack setup are unclear
  Writing of the paper needs improvement
The paper presents a new way to train the prediction of implicit 3D scene representations from a single view. The main innovations are a novel numerically stable and memory efficient formulation of the derivatives of a loss function based on the spatial gradients of the implicit field, and focusing the training on regions near the surfaces of objects. The method leads to good performance, especially when training on imperfect ground truth scan data.

Concerns were raised about the novelty of the approach and its significance. These were adequately addressed in the author response and revisions. The experiments were found to be well described and executed, which increases the confidence in the approach and its potential impact. I recommend acceptance.
The paper presents a method for learning sequential decision making policies from a mix of demonstrations of varying quality. The reviewers agree, and I concur, that the method is relevant to the ICLR community. It is non trivial, the empirical evaluations and theoretical analysis are rigorous, resulting in a novel method that produces near optimal policies from more readily available demonstrations. The authors revised the manuscript to reflect the reviewers  comments.
This paper introduces a method to reduce the number of unique weights in a network. The motivation is that this reduces energy consumption, and can lead to speed ups.
The reviewers agree that the paper is generally well written. They also agree that the method presented is interesting and useful to reduce the number of unique weights of the network.
However, reviewers challenge the authors claim that fewer unique weights necessarily lead to lower energy consumption. Operations involved with this method may even lead to more memory transfers. The experimental section supports well the claim that the number of unique weights is decreased at very little cost in terms of accuracy, but does not provide much data in terms of actual energy consumption.
Generally, the reviewers agree that the fundamental idea of this paper is good and should be presented, but since this paper motivation is entirely focused on energy consumption, I suggest that the authors revise the paper and submit it in a future venue. I can see two possible ways to revise the paper: 1) Add actual energy consumption data to the experimental section; or 2) change the motivation of the paper to solely focus on reducing the number of unique weights. It may be useful for some niche applications, or as a starting point for future works.
The authors of this work introduced new metrics for node embedding that can measure the evolution of the embeddings, and compare them with existing graph embedding approaches, and experimented on real datasets.

All reviewers agreed that the work addresses interesting problem and that the proposed measures are nove, but there are too many flaws in the initial version of the paper, and despite the thorough responses of the authors, it is believed that there are still too many open questions for this paper to be accepted this year ICLR.
This paper presents an augmentation based training of autoencoders with stochastic latent space. The proposed method is examined on the representation learning task on several image datasets. While the reviewers found the submission interesting, simple, and easy to implement, they also raised serious concerns around the novelty of the proposed method and the impact of removing the KL term (which removes the generative interpretability of the model). Unfortunately, the experiments do not provide a convincing utility of the model compared to more popular representation learning methods (i.e., contrastive and non contrastive methods). Given these concerns, the paper is not ready for presentation at ICLR.
The paper proposes a novel protocol for examining the inductive biases in learning systems, by quantifying the exemplar rule trade off (as measured by the exemplar vs rule propensity (EVR) defined in Eq. (2)) while controlling for feature level bias. 

Reviewers mostly agree that the problem studied in this paper is practically relevant and that the two bias measures are potentially interesting and (jointly) more informative than existing measures such as spurious correlation. However, a shared concern among the reviewers (with confidences scores > 3) is the clarity of the exposition (e.g., many key concepts such as the data conditions are informally specified [Section 2 (Reviewer TPBn)], some key messages not clearly conveyed in the main paper [Section 3 (Reviewer RJtk)], and results inconclusive or not sufficiently supported by the experimental results [for both the synthetic setting (Reviewer RJtk) and the real world setting (Reviewer yoH5)]. Based on the above concerns, the reviewers were not convinced that this work is well supported in its current state to merit acceptance for publication.
The manuscript proposes a framework for imposing priors on the feature extraction in deep visual processing models. The core contribution of this manuscript is the systematic formulation and investigation of how different, distinct feature priors leads to complementary feature representations that can be combined to provide more robust data representations. The manuscript uses early work on co training and also the more recent work on self supervision and self training. Experiments are performed with classical shape  and texture biased models, and show that diverse feature priors are able to robustly create a set of complementary data views.

Positive aspects of the manuscript includes:
1. The topic of this paper, creating and combining robust, generalizable and diverse feature representations, is of high relevance;
2. Positive results from co training of groups image classification models designed to focus on shape but not texture or vice versa.

There are also several major concerns, including:
1. The ensemble results presented in section 3.2 are generated using very primitive ensembling techniques;
2. The absence of spurious correlation in unlabelled data assumption be presented more cautiously;
3. Definition of feature prior;
4. Analysis on another domain aside from image classification.

During the rebuttal period, the Authors provided additional experiments using a more sophisticated method (â€œstackingâ€), and additional discussion of where spurious correlations are likely to occur. The manuscript has high rating variance. Some reviewers think that the manuscript lacks the technical novelty, and the results presented are the results of an empirical study. The focus of this manuscript is on two natural feature priors (i.e., shape and texture). It would strengthen the manuscript if the Authors can provide further analysis to emphasise the generality of the proposed framework that it could accommodate any two feature priors as long as they are sufficiently diverse.
This paper presents a new method for solving the problem of inverting image classifier models. The authors introduce three new augmentation based techniques to do this. The techniques are validated using Vision Transformer and MLP models and compared against previous methods. The reviewers appreciate the problem that the paper aims to solve. However, the reviewers are not satisfied with the presentation and evaluation of the proposed approach. The main contribution of the paper is not presented clearly enough, according to the reviewers, and it remains unclear to them what aspect of model inversion the authors most want to improve on, and whether their proposed technique indeed achieves such an improvement. In their response, the authors do provide Inception scores that show that their inversion method improves the perceptual quality of generated images compared to previous approaches. The reviewers acknowledge the author response, but indicate that it does not fully resolve their concerns. I recommend that the authors update their paper to more clearly present their main contributions and conclusions, and to provide a more thorough comparison against previous methods, before submitting to another conference.
The authors propose a novel framework for Distributing Black Box Optimization (DiBB) which can encapsulate any Black Box Optimization (BBO) method. DiBB overcomes some of the limitations of existing methods by leveraging expert knowledge in the problem. The reviewers raised a variety of important technical concerns. The authors seem to agree that they need to substantially rewrite the paper. Therefore I recommend a rejection.
This paper introduces a method for classifying corrupted data and quantifying uncertainty by training semi supervised autoencoders only on clean (uncorrupted) data.

Pro:  The approach is novel utilizing metric Gaussian variational inference.

Cons: More thorough experiments are needed:  (1) extensive experiments on more complex data, (2) ablation study, (3) comparison to additional baselines.

Summary:  The paper introduces a novel method, however experiments are limited.
This paper receives positive reviews. The authors provide additional results and justifications during the rebuttal phase. All reviewers find this paper interesting and the contributions are sufficient for this conference. The area chair agrees with the reviewers and recommends it be accepted for presentation.
The manuscript proposes a meta attention based mechanism for improving off policy actor critic algorithms. Instead of introducing attention into networks at the level of pixels or multiple sources of information, this work focuses on using attention between features from the actor network (which become queries and values) and features from the critic network (which act as keys). Attention produces new features that are given as input to the action net, enabling it to potentially improve it s action selection. The attention is trained using a meta learning objective that encourages outputting features that help other parts of the architecture to learn.
Reviewer note that there are some positive features of the paper. It is relatively well written and the figures are useful for spelling out the approach. In addition, so felt that the basic idea is an interesting one. However, there was general agreement that the manuscript is not ready for publication. 
Most reviewers noted that, the newly proposed architecture and learning rules were not well motivated by the manuscript. Why was this particular approach pursued? Is there any better theoretical justification than can be offered? These questions on their own are not problematic. However, the empirical work does not robustly demonstrate that the algorithm yields a clear performance gain over baseline actor critic methods in the literature. Most of the tasks are relatively simple and those gains that are observed are marginal. This is especially difficult for the manuscript given the increased compute and complexity of implementation required by the method. Finally, several reviewers were concerned about the presentation of some of the technical aspects of the works, potentially making it difficult to replicate important aspects of the work. In sum, the manuscript is not ready for publication.
This is a borderline paper with 2 marginally above and a marginally below acceptance recommendations. While the authors provided valid responses to some of the criticism, I still find some of the motivation and assumptions not sufficiently clear, theoretical and practical issues are mixed, and the validation on only synthetic data raises practical questions.
The topic of this paper is timely and important.  However, ultimately the reviewers remained unconvinced that this paper provides a sufficiently clear and sufficiently significant advance to lifelong RL.

As an additional note, the setting under investigation here is not the full lifelong learning setting.  E.g., several of the challenges outlined by Schaul et al. [1] are not treated, and this work is, instead, situated in a somewhat typical multi task setting with substantlal structure.  That is not bad, but it would be good if this is reflected clearly in all the statements, and, e.g., in the title of the work.

The authors are encouraged to carefully take the provided feedback and see how they can use it to improve their work.  This is an important research direction.  It was just felt the current submission was not quite ready for publication yet.

[1] https://arxiv.org/abs/1811.07004
The paper proposed a new kind of activation function called matrix activation function that can be learnt jointly with the weights and biases. The paper got 2 strong rejects and 3 rejects. The major challenges include unclear motivation, limited novelty, incomplete related work, weak experiments, and poor paper writing. The author rebuttals did not convince the reviewers. The AC also read through the paper and agreed that the paper is below the bar of ICLR. In particular, the authors neglected a large literature of learning activation functions in the original version, 

(two more examples:

[*] Xiaojie Jin, Chunyan Xu, Jiashi Feng, Yunchao Wei, Junjun Xiong, Shuicheng Yan: Deep Learning with S Shaped Rectified Linear Activation Units. AAAI 2016: 1737 1743.

[#] Yan Yang, Jian Sun, Huibin Li, Zongben Xu: ADMM Net: A Deep Learning Approach for Compressive Sensing MRI. NIPS 2017.
)

making them unable to compare with existing learnable activation functions thoroughly in the revised version in order to justify the necessity of using matrix activation functions. So the AC recommended rejection.
Thank you for your submission to ICLR.  While all reviewers felt that there were some interesting aspects to the proposed work, the consensus was also that the work didn t properly situate itself within the existing literature on related methods.  In particular, I agree with Reviewer kLFD that a numerical comparison to Pfaff et al., is notably missing here; while the authors did provide qualitative comparisons in their discussion, it s not clear to me that these differences are ultimately that significant, and the methods need to be compared directly if a case is to be made for the advantages of the proposed approach.
Most reviewers were positive about the paper, seeing that the proposed method is practical and has convincing experimental performances. One reviewer was a bit negative and raised questions about clarity. After the authors responded, the negative reviewer didn t respond further. After reviewing all the comments, the AC feels that there is enough support from reviewers to accept this paper.
This paper describes Flashlight, a tool for ML researchers with specific design considerations for conducting systems research. The needs for such tool are significant, and recent advances in this topic have been relatively slow, so this research is timely and important. 

Reviewers are positive about the importance of the problem and the nice design of Flashlight. It seems the tool has been used by researchers with positive feedback. At the time of the original submission, reviewers expressed some concerns about the novelty and the weak arguments for convincingly showing the advantages over other similar tools. 

Authors provided nice replies including specific case studies, but with the short time period to reassess the proposed changes and additions, some reviewers remain hesitant, and thus this paper cannot be accepted at this time. I strongly encourage the authors to incorporate all of the proposed revisions and resubmit to a future venue.
This paper presents a method for producing a mixture of (disjoint) predictive distributions for deep learning models rather than a single predictive distribution.  The reviewers in general found that the idea had strong potential, was well motivated and addresses an important and under appreciated problem in deep learning.  They seemed to find the proposed approach of using mixture density networks to be sensible.  However, the reviewers seemed to find that the paper was unclear in presentation and grammatically, as if hastily written.  One reviewer noted that they would not be able to reproduce the method given the confusing presentation.  The reviewers also found that the experiments didn t adequately evaluate their method empirically.  Unfortunately, the reviewers all agreed that the paper is not quite ready for publication (5, 3, 5).  Careful rewriting of the paper and the technical contributions and strengthening the experiments would go a long way towards improving this paper for a future submission.
The paper investigates a very interesting problem of the connections between adversarial detection and adversarial classification. Theoretically, the authors show that one can always (ideally) construct a robust classifier from a robust detector that has equivalent robustness, and vice versa. This theorem is only correct without considering the computational complexity. However, the authors did not provide any approximate results of the reduction steps to verify the feasibility of the theorems in practice, which is the main concern of all reviewers. So we can say the paper is a reminder to the community we need to be careful about the detection results but did not provide any evidence to say they are overclaimed (only a conjecture based on the theorem in the paper) which greatly limits the contribution of the paper. Due to the competitiveness of ICLR, I cannot recommend accepting it.
This is a nice paper which shows that KL regularized natural policy gradient (assuming exact access to the MDP, meaning no noise in the reward and Q function estimates), which achieves linear convergence, can use ideas from quasi newton methods and recover their quadratic convergence.  Given the excitement surrounding policy gradient methods and their convergence rates, this is a valuable direction and family of ideas.  Unfortunately, the reviewers had many concerns about presentation, and also of the exact meaning and relationship of the results to prior work; I ll add to this and note that one issue with quasi newton methods is that it is unclear how long the "burn in" phase is, meaning the phase before their quadratic convergence kicks in, and this is still an issue in the present work s theory; another issue, as raised by reviewers, is the difference between the regularized and unregularized optimal policies.  As such, it makes sense for this paper to receive more time and polish.
### Summary

The paper investigates the relation between pruning and splines. The results show that the prunable nodes of networks correspond to splines that do not affect the ultimate decision boundary of the network. The results also show that splines stabilize early in training, in correspondence with the claims of early bird tickets. Finally, the results show the connection between similarity based pruning and splines, with an evaluation of the quality of similarity based pruning.

### Discussion

The paper demonstrates a very interesting connection with splines and promising results with the multiple analyses. However, there could be more precision in the stated connections between splines and pruning. Also, reviewers requested more elaboration of the base technical definitions (e.g., splines).

### Recommendation

I recommend Reject. While the first part of this paper is quite strong in that the spline view of networks provides an interesting analysis framework that draws out interesting connections to pruning. The latter part that develops the concrete pruning algorithm needs reenvisioning. Specifically, to the best of my reading, using the similarity of filters for pruning is not new [1,2,3]. That means the proposed spline pruning policy approach cannot be claimed as new. Though it is reasonable to claim the connection to splines as new.

Second, while the evaluation is extensive in its comparison to other techniques, it is quite murky. The results claim victory by including both spline and EB spline, neither of which strictly dominates the other. My suggestion is to remove the EB spline and, in the worst case,
claim that similarity based pruning is (1) competitive with other approaches and (2) rests on a foundational theory that is a new alternative to typical importance based techniques. That latter observation is an important one for the community that can lead everyone into new productive directions. 

Additionally, in line with some requests from the reviews for more analysis, simplifying the evaluation (with fewer and less ambitious claims) and, even perhaps, eliminating the EB analysis altogether, would make room available for additional analysis and elaboration.

[1] BCAP: An Artificial Neural Network Pruning Technique to Reduce Overfitting
Kiante Brantley. UMD Masters Thesis, 2016

[2] A dynamic CNN pruning method based on matrix similarity
Mingwen Shao, Junhui Dai, Jiandong Kuang, Deyu Meng. Signal, Image and Video Processing volume 15, pages 381â€“389 (2021). Published August, 2020

[3] Similarity Based Filter Pruning for Efficient Super Resolution Models.
Chu Chu; Li Chen; Zhiyong Gao. IEEE International Symposium on Broadband Multimedia Systems and Broadcasting, 2020
The reviewers have the following remain concerns:
1. The bounded function value assumption is strong. Note that the previous works for SGD and SGD M for other LR schemes do not necessarily need this assumption, hence it may be unfair to compare with existing results and say that this work has improvements for non monotonic schemes. The authors also agree that it is not easy to prove and remove this assumption. 
2. The novelty is limited, and the contributions are somewhat incremental. The bandwidth step size scheme was already introduced in a previous work with a very similar setting. The convergence rate for the proposed LR scheme is the same as previous works for other schemes (or only better by a logarithmic term), which makes the results incremental. 
3. Some of the claims are not well supported. For example, the reviewers comment that it is not clear how the proposed bandwidth step size can help to escape local minima. Although the authors aim to show this empirically, the toy setting is not strong enough to conclude the superior performance of the proposed scheme.

We encourage the authors to improve their paper and resubmit to another venue. Here are the related suggestions:
1. The authors might try to investigate and provide a rigorous proof of how the non monotonic step size can help to escape local minima. It also helps to characterize the effectiveness of each cyclic rule (cosine/ triangular or any other) and make clear what property (cosine/linear rules or bandwidth or non monotonicity) contributes most in the good performance of a LR scheme.
2. It is better if the assumption on the bounded function value can be removed. In addition, a theoretical/empirical analysis on the generalization performance of the proposed scheme might also be helpful.
The paper introduces the notion of interventional consistency of a representation learned using autoencoders, which is claimed to be a desirable property for disentanglement. The reviewers agree that the contributions are novel and relevant, but they also found the paper hard to follow due to a lack of clarity and motivation. Further, they considered the underlying assumptions very strong and possibly hard to find practical instances where they may hold (e.g., the assumption that statistical dependencies in the prior are preserved by the response map). The reviewers also noted that some real world examples showing the interventional consistency would be helpful. 

After all, the paper contains interesting ideas and we would like to encourage the authors to pursue this line of work. Still, the paper in its current form is not ready for publication. We encourage the authors to address the reviewers  comments explicitly in a future version of the manuscript.
This paper aims to address the imbalanced class problem in unsupervised domain adaptation. The challenge lies in how to handle the difficulties introduced by imbalanced classes. To this end, this work proposes a new data augmentation strategy by taking the interpolation of two samples from the same class but from different domains as the augmented samples. The experiments demonstrate promising performance on the class imbalanced domain adaptation datasets.

However, there are several concerns raised by the reviewers. 1) The interpolation between a source and target sample of the same class can potentially be unreliable as the pseudo label methods. 2) Some statements are based on intuition but not well supported by either theoretical analysis or experimental evaluations. 3) The proposed method is inferior to baseline methods on some datasets, it would be helpful to have further analysis of the advantages and limitations of the proposed method. 

Overall, the paper provides some new and interesting ideas. However, given the above concerns, the novelty and significance of the paper will degenerate. More discussions on the principles behind the proposed method and more experimental studies are needed. Addressing the concerns needs a significant amount of work. Although we think the paper is not ready for ICLR in this round, we believe that the paper would be a strong one if the concerns can be well addressed.
The topic of learning reward functions from preferences and how to do this efficiently is of high interest to the ML/RL community. All reviewers appreciate the suggested technical approach and the thorough evaluations that demonstrate clear improvements. While the technical novelty of the paper is not entirely compelling, all reviewers recommend acceptance of the paper.
This work has generated a lot of discussion between authors and reviewers and among reviewers.
Overall it is reported that the results on EEG are not conclusive and directly relevant for this field.
Besides the theoretical contribution is not reported as a strong point of the work and the
comparison with alternative baseline methods is judged too limited.

For all these reasons the paper cannot be endorsed for publication at ICLR this year.
The reviewers are in consensus. I recommend that the authors take their recommendations into consideration in revising their manuscript.
This paper presents a method to improve search engines; the method is designed based on the BM25 retrieval method and is evaluated on NQ open dataset. The reviewers agree that the motivation is interesting and implementation is reasonable, but the authors have only showed the impact of their approach over one retrieval method and one dataset, which is limited and does not show if the method is general enough or not.
This paper introduces a new RL benchmark that is a simplified 2D version of Minecraft   it is designed to support complex behaviors but reduce the training complexity. It is very well written and clear, positioned well with respect to other benchmarks, and is likely to improve the speed of development/testing of some RL algorithms. It is likely to appeal to a subset of the community and drive research in some cases, while others may prefer to stick with full 3D Minecraft. As such, there are some mixed reviews on the paper, with open questions as to whether it would be welcomed by people who work on Minecraft style domains, whether behaviors learned in the simplified 2D environment would generalize to other settings/domains, and the potential for agents to game the environment. The authors are encouraged to take these aspects and perspectives into consideration when revising the paper.
While some of the reviewers find that the paper proposes a solid contribution to a problem, I will tend
to agree with other ones that the proposed approach has limited novelty and limited potential for improvement over baselines. In addition,  simulations are pretty weak due to lack of comparisons to strong baselines and to lack of clarity.
This paper tackles a relatively novel problem that is the result of recent work on prefix tuning   specifically the need to be robust to adversarial perturbation in the context of prefix tuning and they show a method for achieving this without requiring more storage and obtain good results.

There were some clarity issues that were addressed by the reviewers during the rebuttal. The main issue that was pointed out was the effect of batch size on the success of the model. The authors gave experiments with batch size 1 where results are less impressive but still outperform the baseline. Also the authors say that for now they are not considering the case where only some of the elements in the batch are adversarial, which I think is ok for a research paper on such a cutting edge topic. 

Thus, the result of the discussion is to lean to accept this paper given that it is now more clear, has experiments that make it clear what the benefits are in realistic settings and obtains improvements.
This paper explores geometric properties of image perturbations (e.g. frequency content and local consistency) and their impact on the adversarial response of networks.  The reviewers feel that the paper is at times unclear about the meaning of terminology (e.g. â€œlocal consistencyâ€) that is not clearly defined.  Also, while the reviewers acknowledge that the paper contains a number of interesting ideas, it is not always clear how the paperâ€™s discussions and contributions differ from existing papers (e.g. Dong 2019, Yin et al., 2019, Wang et al., 2020a, Tsuzuku and Sato 2019) that also discuss the frequency content and smoothness properties of adversarial perturbations.
The paper introduces a technique for randomised dynamic programming and uses it to scale a latent variable model that enables interpreting the hidden states of large pre trained models for text representation and generation.

The current version needs to be improved with regards to scope, which can be seen by the various confusions that it triggered, and which the authors tried to address in the rebuttal phase. It is somewhat unclear to all of us (myself included) whether the paper is about i) randomised dynamic programming (RDP), or ii) RDP s role in a particular LVM (with a CRF posterior approximation), or iii) RDP+LVM s ability to interpret deep Transformer models? Empirically, the paper is much more about (iii), somewhat about (ii, e.g. Table 1), very little about (i, e.g. Figure 2).

*Because the scope is now confusing*, the current version sometimes comes across as relatively incremental or even incomplete:

* Should the authors embrace interpretation. The overall strategy is *very interesting*, and it scales a neat model precisely in the way it needs to be scaled to do what it s meant to do, but this would change the focus of the paper, RDP would be all but a means to an end, and perhaps other techniques for interpretation would be needed.

* Should the authors embrace RDP itself (disentangled from its application to model interpretation). Some of us felt like the randomisation technique on its own is not too surprising (given the work of [Liu et al](http://proceedings.mlr.press/v97/liu19c/liu19c.pdf), for example), and, regardless of that, to push for RDP s significance, the paper would need more comparisons. The only alternative to RDP investigated in the paper is a heuristic top K gradient. There are deterministic gradients that are less heuristic, and which may become unbiased eventually as training progresses, see for example [[1]](https://aclanthology.org/D18 1108/) and [[2]](https://papers.nips.cc/paper/2020/hash/887caadc3642e304ede659b734f79b00 Abstract.html).

In the first round of reviews there were some comments that questioned the paper s fitness to ICLR, I would like to remark that this has been clarified, and the paper targets a problem of clear relevance to the conference.

I would personally like to add a minor comment: it would be nice to acknowledge some older literature on randomised DPs (see for example [[3]](https://papers.nips.cc/paper/2009/hash/e515df0d202ae52fcebb14295743063b Abstract.html) and [[4]](https://aclanthology.org/N10 1028/)).
This paper provides a new differentially private training method. The key idea is sparse gradient updates that is, their variant of differentially private SGD (DP SGD) only updates on a random subset of the parameters in each iteration. The authors argued that their method has a benefit in terms of memory and communication efficiency. The reviews suggested that the paper may require further evidence to motivate and justify the novelty of the proposed method. First, the reviewers are not fully convinced that the proposed method reduced both memory and communication. In particular, would the technique of random freeze require running DP SGD for more iterations? Even though the authors added a new theoretical result (mostly adapted from Chen et al.), the newly added Theorem 2 does not explain the benefits of the freezing technique. Thus, the paper can benefit from more extensive theoretical analyses or justification. The authors should also consider including the additional related work brought up by the reviewers. In summary, the paper is not ready for publication at ICLR.
The paper describes a novel learning scenario where there are many related tasks, some seen at test time, and some seen only at training time, where additionally the task labels can be hidden or present.  This approach generalizes both a "relational setting" (where auxiliary task labels could be used as features) and a "meta setting" (where new tasks need to be solved in a zero shot setting using data from related tasks only).  The idea behind the method is to do MTL with a common representation and a set of task specific heads, and build a graph where (1) tasks are nodes associated with the parameters of their task specific "heads" and (2) edges link examples to tasks with known labels.  A GNN method is then used to find regularities in the graph.

Pros
   The setting is innovative and the approach is novel
   The experimental results are strong

Cons
   Some of the terminology seems awkward and/or strained (eg "knowledge graph" for the task example graph)
The paper proposes a new recurrent architecture based on discretization of ODEs which allow for learning multi scale representations and help with the vanishing gradient problem.
The reviewers all agree this architecture is novel and provide substantial theoretical and empirical evidence.
A strong accept.
This paper explores strategies for scaling vision transformers that can be transferable across hardware devices and ViT variants. While it presents some interesting observations as well as a useful practical guide, multiple reviewers expressed major concerns over the novelty and significance of the methods and findings. Besides novelty and significance, there are also some concerns about comparison with existing work as well as clarity of the presentation.
Thank you for your submission to ICLR.  There is some disagreement about this paper, and several of the reviews are of relatively low confidence.  While I appreciate the effort that the authors have put into addressing the concerns of the reviewers, after going through the paper and the responses myself, I m ultimately coming down on the side of the less positive reviews.  My reasoning, honestly, is that I think the authors are vastly overestimating the knowledge that the ICLR audience will have about numerical methods for PDE solutions.  Reading through the paper, I honestly have very little idea about how the actual numerical techniques are carried out, and it s unclear to me precisely where this method falls in between a traditional numerical solver an actual neural network.  Reading through the reviews, even the more positive ones, I don t think I m alone in this perception (and the authors will hopefully believe me that these reviewers _are_ indeed emblematic of the subgroup of ICLR that is most experience with differential equations).  I really feel like either a substantial rewrite of the paper is needed, to make clear the full extent of the numerical methods being applied; or alternatively, the work may really be better suited for a numerical methods venue.
The most positive reviewers have not decided to step forward to champion the paper. Others have a negative impression which has not sufficiently changed after the answers from authors. Actually, it is acknowledge that there have been many modifications, but they are not happy enough with this situation: modifications (some significant ones) cannot always be fully checked again and even with the efforts that were made by reviewers, strong concerns remained. It has been pointed out that the direction has potential. My recommendation is based on the data that I have available.
### Description
The authors note that the most recent and successful in terms of accuracy binary networks are in fact combining binary and floating point computations, in particular have residual full precision paths, with their parameters, that connect all the way to the output. Although such paths are made lightweight, they can be a bottleneck with respect to the energy consumption, memory and latency. The paper proposes a novel binary neural network model that uses only binary operations (except in the first and last layers). It is proposed to estimate the energy efficiency of binary networks more accurately, using hardware design compilers.

### Decision
Reviewers came to a consensus that the proposed BNN architecture claimed in the paper to be the main novelty, while it is indeed quite distinct from the mainstream and best performing BNNs, does not propose novel solutions with respect to the total state of the art. This is our main reason for rejection. The paper makes a great effort in steering the development of BNNs towards more energy efficient models, by carefully estimating the potential energy consumption of different models, using specialized software for design and simulation of hardware needed to run particular models. This is proposed not as a practical solution for industry but rather as a way to measure the potential efficiency of different models. While this was recognized as a great effort, some questions remained regarding fairness of comparison and possibility to reproduce the results by non experts in order that other developers could estimate and compare potential efficiency of their models. Additionally, it is unclear how power efficiency of a model with $k$ slices (BoolNet) differs from that of a $k$ bit quantized network.

### Details
First of all, I very much like the motivation for the work: to aim at a design of BNN that would be more efficient in terms of speed and energy and to measure that efficiency more precisely. I am not an expert in the hardware, however I do share the concern in this paper that full precision residual paths and blocks would incur a larger latency and higher amount of computation (more chip area, more energy,...). The fact that residual connections in BiRealNet make it necessary to read and write full precision feature maps to the global memory was non obvious to me. The current state of the art reports binary and floating point operations and largely ignores the necessary memory operations, which as authors argue are the real performance bottleneck.

The main claimed contribution of the paper is the design of a novel model. This was the main point of concert. It is indeed innovative with respect to the current trend of the best performing binary networks to make a "pure" binary network. However it is hard to agree that removing some of the components from BiRealNet or models based on it and going back to simpler architectures which were used before can be a contribution. Specifically, plain non residual BNNs were the very well known pioneering works [r1, r2]. The fact that BN in front of binarization can be simplified is trivial and well known, e.g. [r3]. Using multiple binary activations through power of two coding or uniform thresholds has been considered multiple times [r2, r6, ABCNet]. Same for group wise convolutions. I have not seen residual connections in the form of shuffle net so far, but it can also be considered as a rather standard trick. Using stride instead of max pooling or average pooling is another well known trick. Many of these solutions are in fact used in recent works, e.g. [r4] has no skip connections, combines BN with sign at the inference time and uses strided convolutions for downsampling. To summarize, the submission does not appear to propose novel modelling solutions relative to the total state of the art.
Furthermore, [r3] is closely related in that they investigated the energy efficiency (however looking at the energy consumption of individual operations only) and with a similar motivation developed binary networks without 32 bit residual connections. Please see also other references pointed out by reviewers.

### Unclear in the paper:
 What is the meaning of $F$ column in Fig4a for BaseNet / Boolnet?
 Why grouped convolution with input channels 256k and k groups has output of 256 channels in fig 3a. 
If the outputs from each group are summed, isn t it equivalent to a full convolution 256k x 256?
 What do the authors mean by 3x3 depth wise convolution?
  The local adaptive shifting module is discussed inside the paragraph describing MS BConv. Is it a part of MS BConv or not?
  It seems that with $k$ slices, there is $k$ times more bits per activation used and $k$ times more bits per weight used (because the channel width is multiplied by $k$).
It must be therefore equivalent in terms of the power consumption to a network that uses $k$ bits per activation and weight. 
Using these $k$ bits to represent uniform slices rather than powers of two slices appears inferior in terms of quantization error and accuracy. Indeed, many works have successfully quantization different models down to $4$ bits without a loss in accuracy. 

### Related work: 

Rather than reviewing different methods for making networks more efficient, a deeper review of BNNs would be more helpful, in particular 
looking at the works that are closer to the hardware, such as "in memory computing", "neuromorphic computing", [r5, r6].

### Discussion

A well justified way to measure the potential energy efficiency of BNNs would be an excellent contribution that could standardize comparison and drive the development of BNNs in the energy efficient direction. Unfortunately,
it does not appear at the moment that non experts in hardware and design compilers could repeat the compilation and simulation of accelerators as the authors proposed. A simplified estimation method is needed that can be used in python for any model composed of some standard blocks. The authors seem to be in a good position to propose and validate such a simplified estimator. To start with may I suggest to clarify the following questions:
  Do we need power for the operation pipelines for different operation types (cache, global memory) that are not currently used?
  Are the arithmetic operations implemented in hardware to optimize energy or the throughput?
  Can we assume that all latencies can be masked by parallelism?
  Is it a good approximation to assume that a convolution (with an efficient implementation and large enough cache) needs to read the input only once?
  Can any coordinate wise transform be appended to the preceding transform on the fly, canceling a read write in between?
  What is a reasonable estimate of a cost for float32 operations? It seems from the quantization literature that all such operations can be safely quantized to e.g. 8 bit representations without a loss of accuracy.

[r1] Courbariaux et al. 2016, Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or âˆ’1] 

[r2 Hubara 2018: Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations, JMLR] 

[r3 Ding et al. 2019: Regularizing Activation Distribution for Training Binarized Deep Networks] 

[r4 Livochka et al. Initialization and Transfer Learning of Stochastic Binary Networks From Real Valued Ones]

[r5 Baskin et al. Streaming Architecture for Large Scale Quantized Neural Networks on an FPGA Based Dataflow Platform]

[r6 Umuroglu et al. FINN: A Framework for Fast, Scalable Binarized Neural Network Inference]
This submission received a diverging set of the final ratings: 6, 3, 6, 5. On the positive side, reviewers appreciated practicality of the approach and supporting empirical results. At the same time, all of them expressed concerns with the presentation (typos, unfinished sentences, inconsistent notations). Additional requests for clarifications and ablation studies have been mainly addressed in the rebuttal. The most skeptical reviewer did not participate in the post rebuttal discussion, thus the final decision took that into account..

The AC has read the paper and verified that the minor technical issues pointed out by the reviewers have been fixed in the updated version (there are still a couple of typos remaining). This submission was further discussed between AC and SAC, as well as in the PC calibration meeting. Both AC and SAC agreed with the comment of Reviewer aAcK who pointed out that generating adversarial samples for mining hard examples has been explored in more general but related contexts before, which limits the novelty of this work to an application of a known idea to a particular domain (3D). At the same time, performance gains on the ModelNet40 dataset are marginal compared to the point cloud based baselines, while the proposed method still uses point clouds for generating adversarial views. In combination with other minor issues pointed out by the reviewers, and given that none of the reviewers was championing the paper, AC and SAC believe that the weaknesses of this paper at the end outweigh its strengths and do not recommend acceptance at this stage.
This paper presents a method for target side data augmentation for sequence to sequence models.  The authors of the paper use a relatively straightforward method to generate pseudo tokens that are used for enhanced training.  The authors present results on dialog generation, MT and summarization where automatic metrics show improvements.  For really robust results, I would have preferred to see more human evaluations since BLEU and ROUGE are metrics that the NLP community is moving away from.  Overall, the majority of the reviewers are happy with the paper and there is significant back and forth between the reviewers and authors that have improved the paper;  I think the authors went to significant lengths to allay all concerns from the reviewers and the paper should be accepted.
The paper proposes a new approach to inductive rule prediction for knowledge graph completion. Reviewers highlighted as strengths that the paper proposes an interesting approach to an important problem that is relevant for the ICLR community. However, reviewers raised also concerns regarding model design and correctness as well as clarity of presentation (e.g., motivation, analysis, comparison to related work, evaluation). After author response and discussion, all reviewers and the AC agree that the paper is not yet ready for publication at ICLR due to the aforementioned issues.
This paper proposes mixed distributions over convex polytopes, and provides theory for mixed distributions that is relevant to the machine learning community. All of the reviewers were positive, and agree that this is a solid contribution. I agree, and I believe that this paper stands a chance of being a foundational paper for future work in probabilistic ML and structured learning.
This paper presents a new method for performing Bayesian optimization for hyperparameter tuning that uses learning curve trajectories to reason about how long to train a model for (thus "grey box" optimization) and whether to continue training a model.  The reviewers seem to find the paper clear, well motivated and the presented methodology sensible.  However, the reviews were quite mixed and leaning towards reject with 3, 6, 5, 3, 6.  A challenge for the authors is that there is already significant related literature on the subject of multi fidelity optimization and even specific formulations for hyperparameter optimization that reason about learning curves.  A common criticism raised by the reviewers is that while there are extensive experiments, they don t seem to be the right choice of experiments to help understand the advantages of this method (e.g. epochs instead of wall clock on the x axis, choice of baselines, demonstration that early results are used to forecast later success, etc.).  Unfortunately, because there is significant related literature, the bar is raised somewhat in terms of empirical evidence (although theoretical evidence of the performance of this method would also help).  It seems clear that some of the reviewers are not convinced by the experiments that were presented.  Thus the recommendation is to reject the paper but encourage the authors to submit to a future venue.  It looks like the authors have gone a long way to address these concerns in their author responses.  Incorporating these new results and the reviewer feedback would go a long way to improving the paper for a future submission.
The paper investigates what we can learn from _suboptimal_ demonstrations for imitation learning. It suggests that we can learn about the structure of the environment by finding a factored dynamics model including a latent action space. It demonstrates both theoretically and empirically that this information can reduce sample requirements for downstream IL.

The reviewers praised the simplicity of the method (including its minimal assumptions), the theoretical analysis, and the breadth of the experimental validation. The authors were helpful during the discussion period, and addressed any questions or concerns the reviewers raised.

Overall, this is an interesting idea and a well executed paper.
This paper argues that the widely adopted graph attention networks (GAT) have a shortcoming that with the static nature of the attention mechanism, they may fail to represent certain graphs. This paper presents an alternative, GATv2, a simple variant with the same time complexity as GAT but with more expressivity, able to represent the graphs that GAT fails to. This is shown both empirically and theoretically, with various tasks on synthetic as well as standard benchmark graphs. 

GATs are of high interest to the ICLR community, and this paper makes fundamental progress in how attention works in GNNs. This is one of the few papers that present both empirical and theoretical analyses, and these findings will motivate others in the community to make further advances in this field.
This paper builds upon existing works to prove that learning (correlated) equilibrium can be fast, i.e., faster than \sqrt{n} even in extensive form games.

Three reviewers are rather lukewarm, and one reviewer is more positive (but seems less confident in his score). The two major criticisms is that this paper is very difficult to read and that the results might seem rather incremental with respect to the literature.

I tend to agree with both points but the paper still as merits: the reason is that extensive form games are intrinsically way harder than normal form games and they more or less all have a burden of notations. We agreed  that the authors actually did some efforts to make it fit within the page limit. but another a conference or a journal would have been better suited than ICLR.

Our final conclusion is that the result is interesting yet maybe not breathtaking for the ICLR community; we are fairly certain that another venue for this paper will be more appropriate and that it will be accepted in the near future (I can only suggest journals based on the large amount of content and notations, such as OR, MOR, or GEB   yet, conferences such as EC should be more scoped too) . It does not, unfortunately, reach the ICLR bar.
The paper brings the "supermask" idea used in neural architecture search to the application of federated learning, here represented by a single mask of a larger network. The method can be seen as pruning before training, or more precisely pruning instead of training. It is a simplified version of the related works LotteryFL, PruneFL or FedMask, with the difference that here no personalization and no training of the weights is performed, only learning of a global mask. Related work discussion should be improved. While the communication efficiency impact of the method seems minor but positive, the interesting point is that authors here argue that masking will improve robustness to adversarial participants during training. 

Unfortunately no theoretical evidence is provided for success of training, in the sense of Byzantine robustness. It is known that robust training can be attacked with small perturbations correlated over time (e.g.  little is enough ), so also over layers, two important aspects which are ignored here   as voting here is only analysed static at a single time point. As pointed out by reviewer JJjz, the considered attack (inverting ranking) is far from being formally proven to be the strongest one, and we would have wished for a more precise discussion of these issues as the target of the paper seems to be mainly robustness.

Concerns on the paper also remained on the level of novelty, as it only uses existing building blocks which are more or less directly applicable from the centralized setting, and on the limited contributions towards formal robustness, and on the limited discussion of related work mentioned by several reviewers, only some of which we were able to address in the discussion phase.

We hope the detailed feedback helps to strengthen the paper in the future.
Unfortunately, the reviewers have unanimously voted to reject this paper. 
There was some discussion of whether the paper was out of scope for ICLR; 
I don t think that it is, necessarily, but I think that we can kind of screen off that topic because the reviewers had plenty of non scope related concerns that seem disqualifying to me, including both issues of novelty and issues related to the experimental validation.
Therefore, I am also recommending rejection in this case.
The paper propose a value aware transformer for sparse multivariate time series data. While to approach is well motivated and the problem well motivated from a clinical viewpoint, the comparison with related work brought up by reviewer qFRi and  reviewer ph4X would really make it clear where this paper stands. The authors attempt to diffuse this issue in their replies, but empirical comparisons in the paper would guide practitioners more. This is especially important as the paper is motivated by a real world problem.
All reviewers have substantial concerns regarding this work including novelty and experimental validation. The authors do not provide a rebuttal for the raised concerns. As such, the area chair agrees with the reviewers and does not recommend it be accepted at this conference.
This work tackles an important clinical application. It is experimentally solid and investigates
novel deep learning methodologies in a convincing way.

For these reasons, this work is endorsed for publication at ICLR 2022.
While the reviewers place this manuscript right at the threshold of acceptance, I find the revisions that they have made to address the majority of the reviewers concerns. That, combined with some of the reviewers  scores being slightly miscalibrated with their (largely positive) reaction to the author feedback, I am advocating for this paper to be accepted.
The reviews are of good quality. The responses by the authors are commendable, but ICLR is selective and reviewers continue to feel that important choices in the research are not sufficiently clear and fully justified.
This work proposes an interesting approach for learning the relational constraints of a dataset and then generating according to those constraints. Learning the constraints via a constrained optimization problem is an interesting contribution. The application of constrained generation is also interesting and can be applied to several domains though only music and poetry is examined in this work. However, the music evaluation is unconvincing and the paper lacks clarity in the description of the approach such as building the GCN. Music evaluation could be improved with human evaluation since loss metrics don t paint a full picture. Finally, a more diverse set of experiments and datasets (rather than just one poetry collection and one folk song corpus) and more analysis on the learnt constraints could give a more complete story for this approach s effectiveness on sequence data.
This paper studies the average convergence rate for first order methods on random quadratic optimization problems. Specifically it is a follow up to work of Pedregosa and Scieur. They study the expected spectral distribution (e.s.d.) of the objective s Hessian and show asymptotic guarantees that work under some assumptions. In comparison to Pedregosa and Scieur, the main takeaway is that you only need to know the distribution at the edges as opposed to the entire spectrum in order to get the same improved convergence. However some reviewers felt that the contributions were oversold, and for example that Assumption 1 is quite restrictive.
This paper proposes a new approach called CoAE MLSim as a faster alternative to PDE solvers.  According to the authors, the main strength of the method is that it needs fewer training data, however, as pointed out by the reviewers, the factor is not significant enough in the current results. Another concern raised by the reviewers is that the iterative algorithm is much more expensive than only one forward pass of other ML methods. Furthermore, the reviewers criticized that the comparisons with baselines are not sufficient, and some claims in the paper were not backed up. The authors provided their rebuttals and had a long discussion with most of the reviewers. Some clarifications have been made, and some reviewers increased their scores accordingly. However, overall speaking, the major problems with the paper still remain, and the rebuttal has not successfully convinced the reviewers to turn to the positive side. Therefore, we cannot give a green light to this paper yet.
This paper presents work on open world object detection.  The main idea is to use fixed per category semantic anchors.  These can be incrementally added to when new data appear.  The reviewers engaged in significant discussion around the paper with many iterations of improvements to the paper.  Initial concerns regarding zero shot learning were addressed, as were remarks on presentation and claims.

In the end the reviewers were split on this paper.  I recommend to accept the paper on the basis of the semantic topology ideas and the thorough experimental results.

The remaining concern centered around the evaluation protocol used in the paper, which follows that in the literature (e.g. Joseph et al. CVPR 21).  While this is not a fatal flaw, it is an issue with how this genre of methods is evaluated.  It would be good to add discussion to the final paper to highlight this as an opportunity for future work in the field to address.  Specifically, as a reviewer noted "after detecting "unknown" objects in T1, the (hypothetical) annotation process provides boxes for ALL objects of some new classes instead of only for those that have been correctly detected (localized and marked "unknown")."
*Summary:* 
 Study the location of local minima for quantum generative models. 

*Strengths:* 
  Rigorous analysis of an important question. 
  Clear writing with important conclusions. 

*Weaknesses:* 
  Technical writing might not be very accessible.

*Discussion:* 

Reviewers were mostly favorable about this submission. They found the topic important and the contribution significant. A main concern was that the writing might not be sufficiently self contained and the writing might not be accessible to a broad audience. Authors worked on the accessibility. In the initial review, zxWF expressed concerns about concepts, proposed methods, numerical experiments. zxWF found that the author responses carefully covered most of their comments and raised score as a consequence. zxWF still finds that some aspects could be improved, particularly in regard to experiments. F6sD found the question well motivated, the techniques impressive, and the claims important. 

*Conclusion:* 

Three reviewers are favorable about this work. Two of them find it good and one marginally above the acceptance threshold. I find the topic and the nature of the claims important. Considering the unanimously positive reactions from the reviewers I am recommending this article to be accepted. I ask the authors to take the comments from the reviewers carefully into account when preparing the final version of the paper.
In this paper, the authors introduce a simple mixture of experts model, by greatly simplifying the routing mechanism: experts are randomly activated both at train and inference time. A consistency loss function is added for training the proposed models, enforcing all experts to make consistent predictions. The proposed method, called THOR, is evaluated on machine translation tasks, including multi lingual MT, and outperforms the recently proposed Switch Transformer MoE.

The reviews note that the paper is well written and easy to follow, and that the proposed method is simple. While the results look promising, the reviewers also raised concerns regarding comparisons to previous work, some of which were addressed in the rebuttal. Finally, a reviewer raised the concern that this method is related to ensembles, which work well for machine translation, but are not discussed or compared to. For these reasons, I believe that the paper is borderline, leaning toward acceptance.
The paper proposes an adversarial data augmentation technique searching for adversarial weight perturbations of a corruption network (e.g. a pretrained image to image model). The goal is to achieve common corruption robustness as well as a non trivial level of adversarial robustness. The authors claim state of the art performance on CIFAR10 C.

Most reviewers had initial concerns which the authors could clarify in most cases. Finally, all reviewers argue for acceptance.

Strengths:
  no pre defined corruption model necessary
  extensive experiments on CIFAR10 and ImageNet with SOTA results
  all reviewers agree that this paper would be valuable as a future reference

Weaknesses:
  the theoretical part is in my point of view rather misleading and should be completely rewritten as the authors lack here rigor concerning the setting they are working in (as also Reviewer 31sh criticizes). In particular the corruptions are not just a covariate shift (p(y|x) is invariant, only p(x) changes) as the corruptions mix the conditional distributions of different points. Thus under the given assumptions (which should be summarized at one point rather than being scattered over the text) the Bayes optimal classifer is invariant but not the Bayes optimal predictive probability distribution (which is clearly important for assessing uncertainty of the prediction). 
However, when training with the cross entropy loss we are estimating the predictive probability distribution and thus the given statement
about convergence of the risks makes no sense for me and the optimal parameters of the classifier need not be equal even if their classifications agree everywhere. Thus the required changes to fix this theoretical part go significantly beyond what the authors suggest in their rebuttal. 
  the improvements over AugMix+DeepAugment (7.83 mCE vs 7.99 mCE) are negligible and most likely not statistically significant
While AdA has significantly higher adversarial robustness than AugMix+DeepAugment, it remains unclear if using AugMix+DeepAugment together with adversarial training for l_2 as done in
 [3] Kireev, Klim, Maksym Andriushchenko, and Nicolas Flammarion. "On the effectiveness of adversarial training against common corruptions." arXiv preprint arXiv:2103.02325 (2021).
could have led to a similar result 

The paper provides an interesting approach to achieve robustness against common corruptions and all reviewers recommend acceptance. The theoretical part as written currently is misleading as discussed above   the authors have to make it completely rigorous (including proofs, formal statements/defininitions etc) or get rid of it.

Minor weak points:
  According to the leaderboard of RobustBench the SOTA for CIFAR10 C is  by now taken by the NeurIPS 2021 paper
Diffenderfer et al,  A Winning Hand: Compressing Deep Networks Can Improve Out Of Distribution Robustness
which achieves 96.56% standard accuracy and 92.78% mCE. This is 1.64% and 0.61% better than in the present paper and needs
to be discussed as prior work.
  for the adversarial robustness evaluation regarding "AutoAttack & MultiTargeted" you refer to Gowal et al (2020) but the robustness
evaluation has to be properly discussed in this paper
This paper presents an approach based on conditional denoising diffusion models for point cloud completion. The reviewers have recognized the significance of contributions, the clarity of presentation, and the comprehensivity of experiments. I am happy to recommend this paper for presentation at ICLR.
This paper proposes a design of GNNs that are amenable to operating on heterogeneous graphs. The proposed model introduces numerous operations over the adjacency matrix and combines them as a final aggregation result. Experiments are conducted to show that the proposed method outperforms some baseline methods.

The submission suffers from, an incremental novelty, missing important references, and unconvinced experiments. 

All reviewers tend to reject this submission before and after the rebuttal.
Nice paper, providing a thorough investigation of a simple idea that may be useful to a wide range of practitioners. All reviewers are positive, and the discussion has led to significant improvements in exposition and overall in the quality of the submission.
Implicit neural representations are a new and promising method to represent images and scenes. Implicit neural representations enable good performance on task like view synthesis. Those networks generate an image of scene pixel by pixel and are therefore computationally expensive. The paper proposes a method to accelerate inference with an MLP by learning each dimension of the input (e.g., x, y, and z coordinates) separately. The paper reports speedups by a factor of up to three.

The four reviewers all agree that this paper should be accepted. The reviewer s highlight that the speedup is a solid technical contribution, and agree that the idea of splitting the coordinates is well motivated and well evaluated, and leads to the advertised speedups. 
During the review process, the reviewers also raised some issues, such as a slight performance loss at that cost of a slight increase in efficiency, but were convinced by the response of the reviewers. 

I recommend accepting the paper. Like the reviewers, I think that the proposed idea is interesting and technically sound, however, I m a bit concerned about the slight drop in performance: If a slight drop of performance is allowed, then a speedup is possible by simply making the networks smaller, reducing the layers, or through other more immediate means than the proposed one. The contribution would be even more convincing if the paper also compares the speedup in a fair setup where the performance is kept constant.
The reviewers unanimously recommend rejecting this submission, and I concur with that recommendation. This submission is not appropriate for a machine learning conference like ICLR. It does not display a thorough understanding of the literature nor does it make a sufficiently valuable contribution. There is no need to "generalize" MLPs, the community knows quite well that we can use dropout, skip connections, and batch norm with them. Even the original dropout paper applies dropout on fully connected ReLU MLPs.

As another example, the submission attributes skip connections to He et al. 2016, but skip connections (also known as "shortcut connections" were in common use in the late 1980s and throughout the 1990s in the Connectionist community, including for non convolutional simple feedforward neural networks or "MLPs". They were a well known technique throughout neural network history, although the advent of deeper layered neural network architectures perhaps gave them new importance. He et al. certainly popularized them for modern neural network architectures and popularized their residual formulation. The earliest reference I could find easily for skip connections was "Learning to Tell Two Spirals Apart" which was published in 1988 by Kevin J. Lang and Michael. J. Witbrock, but in general such architectural tricks were not viewed as particularly remarkable in the 1990s neural networks literature.
This work addresses the issue of learning reward functions that overfit less/are invariant to irrelevant features of expert demonstrations. 
The proposed algorithm builds on top of adversarial imitation learning (AIRL) and proposes to include a regularization principle that is based on invariant risk minimization. The proposed algorithm is evaluated both in grid worlds as well as continuous control tasks. Both zero shot policy transfer, as well as transfer of the reward function to learn out of distribution tasks from scratch.

**Strenghts**
This work is well motivated and addresses an important problem
The proposed method is well motivated, and provides theoretical foundations 

**Weaknesses**
The manuscript had many missing details/no appendix 
only one baseline is provided, while many relevant IRL algorithms exist
The evaluation is very limited in actually evaluating the invariance properties of the learned reward function 
poor alignment between how the proposed algorithm is motivated (learning invariant reward functions), and on what most of the experimental evaluation is focussed (zero shot transfer of policy)**(more details on this below). 


**Rebuttal**
the authors have updated the manuscript to include an appendix and were able to address most structural issues and provided many of the missing details. 
No additional baselines were provided, and the experimental evaluation remains limited/poorly aligned with the initial motivation

**Summary**
This manuscript addresses an important problem and proposes a promising algorithm. My major remaining concern is the  experimental evaluation that seems not well aligned with the main contribution of this paper. As the authors state in their rebuttal the main supporting evidence for their claim is provided in Section 5.3, with only one set of experiments on using the reward function to learn policies on OOO tasks and very little analysis (< quarter of a page). While the majority of the evaluation (Section 5.2) is focussed on zero shot transfer of the learned policy (which is trained during the IRL training phase). These zero shot transfer experiments are not motivated in the context of the "learning invariant reward functions", so it s unclear what these results show. If these results are still relevant in showing that the proposed algorithm learns "invariant rewards", then this needs to be explained. Furthermore, more baselines would have been required (e.g algorithms that are focussed on learning a good policy by learning a "pseudo" reward   such as GAIL). 
Because of this, my recommendation is that this manuscript is not quite ready yet for publication.
This paper proposes to use longstanding statistical learning techniques to identify the nationality of the author of a text.

Reviewers agreed that this work is a poor fit for ICLR, as there is nothing here that advances our understanding of representation learning. Reviewers were further concerned about the soundness of the claims, raising issues about data contamination and comparison with prior work. 

Finally, reviewers pointed out (correctly in my view) that work that aims to infer protected identity characteristics of non user human subjects should be held to an especially high ethical standard, and needs a highly persuasive cost benefit analysis that defends why the problem is ethical to study at all. The available discussion of ethics is not up to this standard.
Thank you for your submission. The reviewers agree that this paper provides new contributions to data privacy. In particular, the proposed definition interpolates between the local differential privacy and shuffled differential privacy definitions. As argued in the paper, mechanisms under this framework can prevent certain inferential attacks based on the relationships across the individuals (e.g., which individuals belong to the same household). The paper also provides good evidence that their mechanism guards against a specific type of inferential attacks and provides stronger utility than mechanisms based on uniform shuffling.
The authors consider the problem of unconditional image generation in the low data regime, such as learning from frames of a single video or even from a single image. The main idea is to apply GANs with a specific two branch discriminator architecture such that the content features and layout features are handled independently. Secondly, to improve the variability of generated images the authors apply diversity regularization. The authors show that the proposed model is able to, to a certain extent, generate diverse high quality samples.

The paper is well written, the authors described their method and the evaluation protocol thoroughly and clearly. The reviewers felt that this submission was borderline, with questionable novelty and significance. In an extensive rebuttal and discussion phase the authors addressed several raised challenges and improved their paper. However, two points remain:
  **Technical novelty**: content and layout separation as well as diversity regularisation previously appeared in many contexts and papers.
  **Motivation and practicality**: One of the main arguments for the utility of the proposed method is to use it for data augmentation. While it may indeed result in content based augmentations, it nevertheless necessitates training of a GAN for every single image, which is severely limiting in practice.

After reading the manuscript, reviews, and the rebuttals, my view is that the paper is below the acceptance bar and I agree with the points on novelty and significance. In particular, the main application to data augmentation seems to be "unexciting" and the proposed method impractical. At the same time the proposed method is a combination of already known techniques, albeit in a different setting. I suggest the authors condense the arguments in the extensive rebuttal to improve the points raised above and resubmit.
One of the major challenges in model based RL is the learning of accurate world models. This work proposes a method that can learn to decompose the dynamics of the world into several sub dynamics, which is postulated to lead to better prediction accuracy (which in turn should lead to better policies/downstream performance). The proposed method clusters the sub dynamics in latent space, and can be combined with any existing MBRL method. Here it is specifically combined with Dreamer and MBPO and evaluated on the deepmind control suite/mujoco.

**Strengths**
This work addresses an important problem, and on a high level the problem/approach is well motivated
The proposed algorithm is "simple" (which can be a really good thing) and very general in that it can be combined with seemingly any MBRL method

**Weaknesses**
The manuscript was lacking in clarity on a technical level (partially addressed during rebuttal)
On average the experimental results are not very convincing (yet)

**Rebuttal**
The authors clarified a few misunderstandings the reviewers and also updated the manuscript accordingly

**Summary**
I agree with the reviewers that the experimental results are not fully convincing. When looking at the model error plots, the y scale is very small, and it looks like there is no significant improvement. Also in the "downstream tasks" only for the humanoid do we seem to see a significant improvement. Overall this seems promising, but the authors should investigate why there seems to be a clearer benefit for the humanoid, and show more such results. Furthermore, there are some concerns/clarity issues with respect to what your approach learns   I would recommend you take a small ish toy example to introduce the intuition of your approach, and maybe visualize learned sub dynamics.
Overall, while promising, in it s current state this manuscript is not quite ready yet for publication
The authors are strongly encouraged to elaborate further about the novelty of their method, as well as to give detailed (either theoretical or experimental) justifications for the design choices they make within the paper. Finally, the paper could benefit from additional experiments, as outlined in the reviews.
All reviewers agreed that the contribution is too limited for the paper to be published. I encourage the authors to take the reviews into account when improving their work.
This paper proposes Adam and Momentum optimizers, where the optimizer state variables are quantized to 8bit using block dynamics quantization. These modifications significantly improve the memory requirements of training models with many parameters (mainly, NLP models). These are useful contributions which will enable training even larger models than possible today. All reviewers were positive.
This paper studies physical "adversarial programs" that allow an attacker to control a machine learning model by placing transparent patches on top of an image. The reviewers are split on this paper: while some reviewers like the work, others are concerned about the practicality, novelty, or utility of the attack.

Starting with novelty, reviewers raise valid concerns about how this approach is similar to prior attacks that generate programs. The authors respond here, but the overall question remains unanswered and it is not clear which of the new pieces this paper introduces are responsible for the success. (Would prior techniques have sufficed? If not what part of prior methods makes this not the case?)

For utility, the paper does not make a clear case of why it would be easier for an adversary to place N~ 5 patches on top of an image as compared to other physical attacks (see especially Li et al. 2019 as a paper that deserves more than a sentence of comparison why is this approach easier?).

One final comment raised by many reviewers is the fact that the title and setup to this paper heavily lean on the "physical" component of the evaluation, and yet the paper does not demonstrate anything physical. The authors rebuttal that the word "towards" absolves them of responsibility for trying an attack in the real world does not convince me; either the paper should attempt this attack in the physical world (and say if it works or if it doesn t) or make it clear from the top that the attack is going to be digital from the start, but motivated by the physical world. Prior accepted papers that include physical world in the title (e.g., Kurakin et al., Athalye et al., Li et al.) don t solve the problem completely, but at least run experiments in the physical world.
The idea to adapt the noise variance in the certification of a base classifier sounds natural and interesting, but unfortunately fundamentally flawed, as correctly pointed out by Reviewer viFi (also acknowledged in the authors  response): the author s main algorithm does not lead to any theoretical certification while the empirical fix (based on memory), however successful in one s experiment, does not rule out the possibility of failure when future test samples flood in. Incidentally, I believe this fallacy may have also answered Reviewer Xsdx s question (why this has not been done before). I agree with Reviewer viFi that the writing of this work is a bit deceptive and will require significant change. In particular, one cannot wave hands at claims on certification: you need to formally prove the memory based empirical fix will provably certify a region for what classifier and under what assumption. Therefore, the current draft cannot be accepted. Please consider rethinking about the idea and rewriting the paper according to the reviewers  comments.
This work identifies an interesting bias that can occur when applying occlusion based interpretability methods to debug image classifiers. For context, the motivation behind many of these methods is that by occluding various parts of the image, one can ask counterfactuals such as "what would the model have predicted if this object were not present in the image"? However, the authors note that when occluding pixels, classifiers are still functions of the occlusions themselves, so this process may introduce a bias as a result. This is most clearly demonstrated in Figure 2 where a convolutional architecture classifies various occluded images as "jigsaw" or "crossword puzzle", arguably due to the fact that scattered patch based occlusions resemble crossword puzzles. The authors then demonstrate that ViT models can be modified in a way to ask the above counter factual in a more principled manner namely by dropping image tokens within the transformer model, the resulting function doesn t take any occluded pixels as input. Reviewers all found the analysis quite insightful, and did not find any significant flaws in the experiments. During the rebuttal, the authors added numerous experiments to address concerns raised by reviewers regarding lack of datasets for which the method was run on. Unfortunately, only one of the reviewers acknowledged the rebuttal and did not raise their score citing doubts that the method may not work well on datasets with differing image statistics (e.g. medical imaging). After reading all of the reviews and rebuttal, the AC feels the authors have adequately addressed the most pressing reviewer concerns, and finds the presented analysis sufficient to warrant acceptance.
Addressing the problem of catastrophic forgetting in continual learning, this paper extends OML to use experience replay (ER) during training, instead of the original approach which uses ER during test phase only. The paper proposes a policy for samples replacement from the reservoir. Experiments show the superiority of the approach in three standard benchmarks compared with several baselines. 

Reviewers were unanimously concerned that the technical contribution of the paper is not sufficient. The authors addressed several issues, including experiments to compare with additional baselines, but the technical novelty remains limited for an ICLR publication. 
The paper cannot be accepted at its current form.
All reviewers agree that the presented ADA Nets approach is very interesting and sufficiently novel, addressing the degradation problem in face clustering. The reviewers are satisfied with the presented experimental studies in most cases. The rebuttal addressed a large majority of additionally raised questions. I disagree with one reviewerâ€™s comment â€“ that the focus of the paper is too narrow â€“ because clustering techniques are of great interest to the ICLR community. I believe that the paper will be of interest to the audience attending ICLR and would recommend a presentation of the work as a poster.
The paper proposes to learn embeddings into complex hyperbolic space. This is an extension of the popular hyperbolic space embeddings which have shown success on graph like and tree like data. Reviews and discussion mostly centered around the lack of clear motivation for the work (why complex hyperbolic spaces?) and the lack of a clear advantage over other manifold embedding methods that have varying curvature. The reviewers mentioned many questions and points that they thought the work should cover. There was also concern about the baselines against which the method was compared. There was not a consensus that the paper should be accepted, and no reviewer argued strongly for acceptance, even after the author response. As a result, I recommend that this paper not be accepted at this time. I expect a new version of this paper, incorporating this reviewer feedback and especially improving the explanation of the motivation, will be a good submission for a future conference.
The paper presents a gradient based hyperparameter optimization method, wherein a differentiable reparameterization is proposed for various popular CNN hyperparameters including kernel size, number of channels and hidden layer size.

All reviewers have pointed out the lack of novelty (such reparameterizations are standard) and lack of convincing experiments.

The authors didn t write any rebuttal.

Overall, there is a large consensus among the reviewers that this paper is not ready for publication at ICLR.
The authors did not respond to the concerns raised by all the reviewers. As the recommendation were on the edge, this lack of engagement seems odd, and it left the reviewers with little material to discuss and revise their recommendation. We recommend the authors carefully consider the reviews if they plan to resubmit.
This paper studies the problem of dealing with long contexts within a Transformer architecture.
The key contribution is a  kNN memory module that works in concert with  a Transformer by integrating upper layers with additional retrieved context.

The idea is simple  but the execution is good.  While the  idea is reminiscent of other recent work on this topic, and novelty is somewhat borderline, it is practically useful.
Overall, though ambivalent, my recommendation is that the paper should probably be accepted
The paper presents some efficiency improvements over existing methods to compute matrix square root and its gradient.  Reviewers find that the novelty over existing methods is sufficient, and that the improvements are valuable.

I propose a poster despite the relatively high numerical scores, because the group of practitioners who will use the result is somewhat niche   the reviewers are of course selected from this group and hence value the paper more highly.

In addition the real world speedups are modest, but it is nevertheless important to document this approach.
This paper argues that the existing approaches for reducing power consumption do not model the precise power usage of each model. To remedy this an approximate power usage model is proposed using bit flips and a simple approach called PANN is introduced that relies on tricks such as unsigned arithmetics and implementation of multiplications with addition. The reviewers have found the overall direction of this paper in modeling power consumption important and have acknowledged the clarity of presentation. However, they have also raised serious concerns regarding (i) the efficacy of modeling power consumption with bit flips and ignoring memory power, (ii) its relevance to modern hardware, and (ii) the efficacy of replacing multipliers with repeated additions. Unfortunately, the paper in it is current form does not provide a compelling answer to these concerns. Given these criticisms, we don t believe that the paper is ready for publication at ICLR.
The paper proposes a manifold distance based detection based against adversarial samples, i.e., using the difference between the highest and second highest softmax outputs from a model to detect adversarial examples. All the reviewers gave negative scores. The main concerns lie in 1) poor quality of writing; 2) contributions of the paper are not clearly stated; and 3) limited engagement with well known recommendations from the research community on the evaluation of defenses/detection methods for adversarial examples. No rebuttals are provided. Thus, I cannot recommend accepting the paper to ICLR.
This paper studies whether the Bellman error is a good metric to reflect the quality of value function estimation, focusing on finite sample off policy data sets. Both theoretical analyses and empirical experiments have been provided, showing that the Bellman error is often not the right metric to consider. However, while I appreciate the authors  theoretical attempts, the current theoretical contributions are not deep/significant enough. As the reviewers mentioned, the failure of the direct use of BRM is not surprising given the insufficiency of data (namely, no algorithm can make predictions on completely unseen regions unless further modeling structure is present). The authors might want to further strengthen their theory along this important direction.
This paper proposes a personalized federated learning method using a hyper network to encode unlabeled data from new clients. At inference time, new clients can use unlabeled data as input to this hyper network in order to obtain a personalized version of the model. The key strength of the paper is that the idea is interesting and timely. Personalization has been studied for clients that participate from the beginning of training, but personalization of models for new clients that join later on has not been considered in most previous works. The experimental results also show a reasonable improvement over the baselines. However, the following concerns remain:
1) Novelty in comparison with reference [1]. Please add a detailed comparison when you revise the paper.
2) Explanation of the experimental results and comparison with baselines was deemed insufficient by some of the reviewers.
3) The generalization bound and the DP results seem standard extensions of existing works and do not add much novelty to the paper.

There wasn t much post rebuttal discussion and the reviewers decided to stick to their original scores. Therefore, I recommend rejection of the paper. I hope that the authors will take the reviewers  constructive comments into account when revising the paper for a future resubmission.
This paper proves a global convergence rate of a newly proposed algorithm finite sum problem under some assumptions. While the proposed algorithm provides some interesting ideas to solving the finite sum problem using intermediate proxy solver, the current assumptions are too strong and I m afraid that this can make the result essentially trivial:

For example, assumption 3 assumes that 
$ H_i * v \approx \nabla_z\phi_i(h(w, x_i))$ for every i. This simply implies that 
$ \|\nabla_w\phi_i(h(w, x_i)) \|_2 $ is as large as $\|\nabla_z\phi_i(h(w, x_i))\|_2^2 $ as long as the norm of v is small, since 
$\nabla_w\phi_i(h(w, x_i))    \nabla_z\phi_i(h(w, x_i)) H_i $.



Hence the assumption simply assumes that "If the loss is not small, then the gradient of the objective is not small (using the convexity of $\phi$, so $|\nabla_z\phi_i(h(w, x_i))$ has to be large)"   This would imply that gradient descent can also work (and arguably having the same convergence rate) under this assumption.  Note that "the smallest movement that can decrease the objective the most" is indeed following gradient descent direction   So gradient descent would not move the weights more than this algorithm as well. 

Therefore, I am not sure that there is a clear benefit to using this algorithm compared to the standard (stochastic) gradient descent. In particular, I would suggest the authors at least show one example where under the current set of assumptions, gradient descent does not work as efficiently compared to the proposed algorithm   This will make the proposed algorithm much more justified.
The paper proposed a speech to speech translation (S2ST) model. The model is trained end to end from speech to speech, along with an auxiliary speech to phoneme task. Experiments firmly support multiple claimed improvements to the previous model.

However, most reviewers argue the novelty and clarity of this paper, making this paper cannot be accepted by ICLR 2022. We hope the authors can modify this paper accordingly.
This paper proposes a new architecture for point cloud processing, with good empirical results. All reviewers recommended accept. AC does not see a reason to overturn the consensus.
This paper proposes a new method for generating synthetic environments and reward networks for reinforcement learning tasks. This happens as a nested process: policies are learned in an inner loop, and environments are evolved in an outer loop. The environment representation is quite simple: the parameters of an MDP. Similarly, the reward networks are simply neural networks. Results show that the the learned environments and reward networks are reasonably good at decreasing policy training time by RL. The proposed method appears to be simple and quite general, and it would be interesting to see how it scales up to more complex environment representations.

The discussion around the paper centered on understanding various details of the method, and on the quality of the results. The reviewers generally agree that the paper is easy to read, and vary in their assessment of the significance of the results. It was pointed out that the generated environments are not necessarily similar to the base tasks, but it was nowhere claimed in the paper that they were. (In fact, it could be argued that the dissimilarity makes the method more interesting, given the good results of policy training.)

I m happy to recommend the paper for poster acceptance. If the results would have been more impressive, it could have been accepted for a more prominent presentation form; however, I believe that the method can yield better results in the future with more sophisticated environment representations.
The paper proposes a method for compressing unconditional generative models by leveraging a knowledge distillation framework. Two reviewers consider the paper slightly above the acceptance threshold for the interesting topic studied in the paper and the simplicity of the method. However, the other three reviewers consider the paper below the acceptance threshold with two reviewers rating the paper slighting below the acceptance threshold and one reviewer rating the paper as not good enough. Several issues were raised, including that the paper only contains results from one unconditional model (StyleGAN2) and that the presented results are not convincing enough. Consolidating the reviews and the rebuttal, the meta reviewer found the concern raised by the reviewers justified. It would be more ideal if the paper can present results on different unconditional models and more datasets. The authors are encouraged to incorporate the reviewers  feedback to make the paper stronger for a future venue.
This work receives mostly positive rates. Most reviewers agree that the use of Bayesian attention to neural processes is novel, and its interpretation is interesting. Since the reviewer TBTA requests a substantial revision of the submission and fortunately authorsâ€™ feedback is thoroughly satisfactory, we highly recommend the authors to prepare for a significantly improved camera ready version that clarifies most of reviewersâ€™ concerns.
In order to evaluate the evidence lower bound (ELBO), VAEs typically use a parametric distribution based decoder $p(x|z)$. If the data is continuous, one often considers a Gaussian VAE, where the canonical setting is to assume a diagonal covariance matrix $p(x|z)   N(x; \mu(z), \sigma^2 \mathbf{I})$. In this paper, the authors suggest replacing the diagonal covariance matrix with a structured covariance matrix (low rank + diagonal). As this only amounts to a minor change to a canonical Gaussian VAE, strong empirical results are expected to justify its acceptance. However, the image generation results presented in the paper are not comparable to the state of the art VAE results (e.g., Arash Vahdat, and Jan Kautz. "NVAE: A Deep Hierarchical Variational Autoencoder." Neural Information Processing Systems (NeurIPS), 2020).
This paper discusses an issue with decomposing a conditional generative model into an unconditional model and a separate classifier using Bayes  theorem, which is an approach that has recently received increased attention in the context of score based generative models. It explores several alternatives for mitigating this issue, including a novel one, which is to use a different loss function to train the classifier.

Reviewers praised the writing and the way this work draws attention to an issue that is underappreciated in the community. Although several weaknesses (clarity, scale of experiments, appropriateness of baselines, missing experiments) were also highlighted in the original reviewers, all reviewers agree after discussion that the authors have adequately addressed these for the paper to be considered for acceptance. I will follow their recommendation and recommend acceptance as well.
This paper proposes to use self supervised learning in the context of "imbalanced regression", where some values of the outcome variables are rare, such as in long tailed regression. The author s proposal can be interpreted as a Monte Carlo approximation of a density smoothing technique, akin to Yang et al. 2021. They test their approach on three datasets. Overall, it provides marginal improvements, whose statistical significance are not assessed. All reviewers agreed that the paper has merits but that it should be further improved to demonstrate that the proposed method is indeed a step forward  in solving the problem of imbalanced regression. The authors should also provide stronger motivation for their pipeline details and experimental setup choices. I therefore recommend rejection, with encouragement for improvement in two directions: strengthening the experimental section, in particular by assessing statistical significance, and by improving the writing of the paper by developing a more rigorous exposition.
This paper applies a metalearning strategy to point cloud registration, which refines 3D registration networks to improve performance on specific datasets/settings.  Reviews for this paper recognized its potential interest but uniformly highlighted that the work is lacking in polish both from an expository perspective and in terms of experiments.  Questions included whether the experiments truly support the claim of generalization, and whether the work would be better considered as a method for scene flow.  Authors did not rebut these points, so I am recommending rejection.
The paper proposes Diversity Regularized Training (DRT), a new training method for an ensemble classifier to improve its certified robustness when randomized smoothing is applied. Specifically, it trains a set of base classifiers to diversify their input gradients while maximizing the confidence margin of each. The method is backed up with a theoretical observation on robustness of ensembles of smooth classifiers.

After the discussion phase, the reviewers unanimously ended up with supporting acceptance of this paper, and the authors were quite responsive to address the reviewers  concerns. Overall, the reviewers appreciated its strong empirical results with a theoretical support   AC also agrees on that, and thinks the paper presents a promising and under explored direction to boost certified robustness of randomized smoothing.
This paper studies how recurrent neural networks, and more specifically GRUs, store and access information. The authors analyze the solution obtained by gradient descent to the variable delay copy memory task for discrete sequences. They use concepts from dynamical systems, such as slow manifold, to understand the behavior of the learned model. Finally, based on this analysis, the authors propose a synthetic solution to a simplified version of the delay copy memory task.

Overall, while the scores for the paper are rather positive, I still have concerns about the paper, based on the reviews and discussion. I do not believe that these concerned were well addressed by the authors in their rebuttal. First, I tend to agree that the paper is somewhat lacking novelty and insightful findings (reviewers TN5R, afLb, MToe). For example, I think that tools from dynamical systems are mostly useful to analyze RNNs when the input is constant (Jordan et al., 2019). In the case of the copy task, this corresponds to the "delay" period, where in practice the hidden state is almost constant. This behavior is easily explained by the value of the update gate, close to 1. I thus agree that other hypotheses than slow manifold should be discussed to explain how GRUs store and access information, and that the benefits of using dynamical systems is not obvious. Moreover, I believe that previous solutions to the copy task (eg, from Henaff et al.) could be extended to the variable setting by adding a gating mechanism to these solutions. In particular, Henaff et al. claimed that LSTM could solve this task empirically, while the authors claim otherwise.

Second, after reading the revised version a couple of times, I still find the paper hard to follow (MToe, afLb, TN5R). For example, I think that the concept of slow manifold is not introduced properly, and in particular, how it applies to the learned solution is not clear. More generally, I found the sections regarding how information is stored and accessed a bit confusing. Finally, I think that the studied task is simple, and probably does not provide strong insight about the working of recurrent networks. Specifically, LSTMs tend to perform similarly or slightly better than GRUs on many tasks, while the authors claim that this architecture cannot solve the studied task.
### Summary

The paper proposes a technique that enables inference directly on a compressed model without decompressing the model.

### Discussion

  Strengths
    An important problem as well as a compelling direction, namely inference without decompression.
 
  Weaknesses: 
    The reviewers provided a number of both broad and specific criticisms of the work. 
  The most salient point is the lack of comparison to modern baselines.  Notably, the primary comparison is to a 2015 technique that, while seminal, has since been followed by significant related work (e.g, that identified by Reviewer eHWE, R8Un, and G6tm). In concert, the evaluation should consider at least one more contemporary network in the domain, such as a ResNet.

### Recommendation

I recommend Reject.  At current, this work is the first step in a strong, compelling direction. However, the work needs to be contextualized within a more modern context of contemporary results
This paper analyzes the behavior of score function as $t \rightarrow$ 0 and proposes simple approaches to mitigate issues around estimating an unbounded data score. The reviewers have acknowledged the importance of the problem and its relevance to current efforts on denoising diffusion models. However, they have also raised serious concerns regarding the clarity of the presentation and missing information across different sections. Additionally, the unbounded data score is only shown on a toy example, and it is not clear if it exists in real world datasets. Finally, e3pu pointed out that in the commonly used score parameterization from Song et al. the score parameterization can in fact grow to infinity when needed. Given these criticisms and without a response from the authors, we don t believe that the paper is ready for publication at ICLR.
This paper studies the Lottery Ticket hypothesis in reinforcement learning for identifying good sparse representations for low dimensional tasks. The paper received initial reviews tended towards acceptance. However, the reviewers had some clarification questions and concerns. The authors provided a thoughtful rebuttal. The paper was discussed and most reviewers updated their reviews in the post rebuttal phase. Reviewers generally agree that the paper should be accepted but still have good feedback. AC agrees with the reviewers and suggests acceptance. However, the authors are urged to look at reviewers  feedback and incorporate their comments in the camera ready.
In this paper, the authors consider two algorithms for solving (strongly) monotone variational inequalities with compressed communication guarantees, MASHA1 and MASHA2. MASHA1 is a variant of a recent algorithm proposed by Alacaoglu and Malitsky, while MASHA2 is a variant of MASHA1 that relies on contractive compressors (by contrast, MASHA1 only involves unbiased compressors). The authors then show that
  MASHA1 converges at a linear rate (in terms of distance to a solution squared), and at a $1/k$ rate when taking its ergodic averge (in terms of the standard VI gap function).
  MASHA2 converges at a linear rate (in terms of distance to a solution squared).

Even though the paper s premise is interesting, the reviewers raised several concerns which were only partially addressed by the authors  rebuttal. One such concern is that the improvement over existing methods is a multiplicative factor of the order of $\mathcal{O}(\sqrt{1/q + 1/M})$ in terms of communication complexity (number of transmitted bits) for the RandK compressor, which was not deemed sufficiently substantive in a VI setting (relative to e.g., wall clock time, which is not discussed).

After the discussion with the reviewers during the rebuttal phase, the paper was not championed and it was decided to make a borderline "reject" recommendation. At the same time, I would strongly urge the authors to resubmit a properly revised version of their paper at the next opportunity (describing in more detail the innovations from the template method of Alacaoglu and Malitsky, as well as including a more comprehensive cost benefit discussion of the stated improvements for the RandK/TopK compressors).
This work tries to tackle the problem of anomaly detection across different tasks. To do so, authors employ energy based models (EBMs) and define an outlier score in terms of the EBM energies, having a shared sparse code for different tasks. This pipeline is tested on some image and video anomaly datasets for industrial inspection.

The reviewers highlighted some concerns that need to be addressed before the paper is ready for publication. 

First, a revision could benefit from a rewriting the clearly formalizes the learning problem from page 2 and then discusses about the possible modeling options given i) the task at hand and ii) some efficiency requirements.

Second, concerning the modeling choices of the proposed pipeline, the motivation behind the choice of EBMs should be strengthened. For example, it is not clear why the proposed sparse coding could be used for any other latent variable probabilistic model. As observed by one reviewer, the pros of having energies instead of probabilities (or just reconstructions from a deterministic autoencoder) is not discussed sufficiently. Additionally, the heuristics of running Langevin dynamics for only 5 steps should be backed up by stronger empirical evidence as it lacks theory, and it should be discussed how much you should run the Markov chain to obtain sensible negative samples.

Third, conclusions over the experiments on the provided benchmarks seem preliminary. For instance, a new revision could benefit from adding a statistical significance analysis to the reported accuracies. I appreciate that authors added further ablation studies including experiments on contaminated data in the latest revision. I suggest them to extend the experimental suite to more benchmarks including the commonly used for anomaly detection.
All reviewers believe that the paper is not ready for publication and clarity issue remain. All reviewers read the rebuttal responses, but they found that the paper wasn t revised during rebuttal, thus they retain their decisions.
The paper introduces an cross layer attention mechanism for image restoration. To reduce the computational complexity, the framework uses deformable convolutions and an adaptive selection for reducing the number of keys, as well as a neural architecture search. The paper received three borderline reject recommendations and a clear accept. After reading the reviews, responses, and the paper in details, the area chair agrees with Reviewer 6N93 that the paper has some merit. Unfortunately, he/she also agrees with the fact that the proposed framework is quite complicated with many components for a marginal improvement (something that also Reviewer 6N93 has mentioned in the discussion between reviewers). Overall, this points towards rejection, which is the final recommendation of the area chair.

Another point that would be helpful, in case this paper is resubmitted elsewhere, is to release the code for the method, given its complexity.
The manuscript brings up an important issue: that current methods and datasets don t generally highlight interactions when it comes to trajectory prediction. This is despite the fact that it would seem that current methods incorporate agent interactions and that datasets appear to require reasoning about agent interactions. This qualitative and quantitative observation should lead to better datasets in the future as well as more refined metrics pushing the field forward. Reviewers were in agreement that this is a strong submission. The authors responded with substantive new experiments that cleared up any lingering issues.
The paper considers learning classifiers under a fairness constraint which enforces the loss to be equal on certain subgroups. Reviewers found the work to be well motivated, but raised concerns on the lack of discussion and comparison to relevant prior work. Notable examples in the fairness literature are Donini et al., "Empirical Risk Minimization under Fairness Constraints", Celis et al., "Classification with Fairness Constraints: A Meta Algorithm with Provable Guarantees", while in the more broader constrained optimization literature, Kumar et al. "Implicit Rate Constrained Optimization of Non decomposable Objectives". The authors are encouraged to incorporate reviewers  detailed comments for a revised version of this work.
The paper explores the application of generative adversarial networks as posterior models in simulation based inference. A new method is proposed, and its connections with related work are studied. The proposed method is empirically evaluated on joint inference of up to 784 parameters.

The reviews are borderline, with one weak reject, two weak accepts, and one strong accept. Overall, the paper is well written and well executed. Its main strength is the promising performance of the proposed method in high dimensional parameter spaces, which are out of reach for many existing approaches. The main weakness of the paper is its lack of novelty: the proposed method is only marginally different from already existing ones, while the paper could have explored the differences to a greater extent.

On balance, I m leaning towards recommending the paper for acceptance. Despite the lack of novelty, the paper is well executed with potential impact in high dimensional simulation based inference.
This manuscript proposes an information fusion approach to improve adversarial robustness. Reviewers agree that the problem studied is timely and the approach is interesting. However, note concerns about the novelty compared to closely related work, the quality of the presentation, the strength of the evaluated attacks compared to the state of the art, among other concerns. There is no rebuttal.
The paper presents a reinforcement learning technique for problems with continuous actions. The proposed approach consists in learning  a discretization of continuous action spaces from human demonstrations. This discretization returns a fixed number of actions for each input state. By discretizing the action space, any discrete action deep RL technique can be readily applied to the continuous control problem. Experiments reported in the paper show that the proposed approach outperforms several RL baselines such as SAC.

The key criticism from the reviewers relates to the incremental nature of this paper s contribution. While the precise equation proposed by in this paper for learning discrete actions from demonstrations may be novel, there have been several very similar techniques in the literature. For example, Gaussian Mixture Models (GMMs), a closely related model, have been widely studied in the context of learning policies from demonstrations. 

In summary, the reviewers are not convinced that the paper contains sufficiently novel ideas for an ICLR publication.
This paper proposes two new sets of conditions under which we can identify temporally causal latent processes. In this sense, this work makes valuable contributions to the theories of identifiability in this topic. The authors also propose LEAP, extending the VAE, to estimate temporally causal latent processes.

The reviewers had many constructive comments, and the authors strived to address them. In the end, the reviewers were satisfied with the final version of the paper. 

Given that the theoretical identifiability theorems are major parts of the paper, I encourage the authors to elaborate more on the two sets of assumptions. They should discuss when these assumptions will hold and provide examples in which they will be violated.
This paper considers helping to decide whether behavior cloning or offline RL is likely to be more effective given a particular offline dataset. The reviewers initially appreciated the importance of insights into this question around how to best leverage an existing dataset. They also had some initial concerns, due in part because the theory is restricted to tabular settings, whereas many challenges typically arise when function approximators are used, the realisticness of the assumptions over the data collection process, and a number of places where further details or clarifications would better situate and strengthen the work. The authors gave very extensive responses to the feedback which made reviewers feel much more confident about the revised paper and resulted in significantly higher scores. Though there remains many interesting areas for future work, this paper makes an interesting contribution that may be of interest to many using batch decision making data.
The paper analyses the frequency filtering properties of self attention in vision architectures, shows that it mainly acts as a low pass filter, and proposes fixes that allow to better preserve the higher frequencies. These fixes yield moderate classification accuracy gains (~0.5 1%) for several existing attention based architectures.

The reviewers are quite borderline about the paper, but after considering the authors  responses lean towards acceptance. Pros include interesting and novel analysis and sound model improvements leading to non trivial empirical gains. The main con is that the experimental results are fine, but not outstanding.

Overall, I recommend acceptance. Empirical results are indeed good but not outstanding, but the theoretical analysis is interesting and it is good to see that it leads to actionable insights on the model design side that actually help in practice   even is not by a huge amount. One part that in my opinion is confusing (and might have been confusing to the reviewers too) is that the title seems to suggest the paper will present very deep vision transformers while it does not. Adding deeper models or adjusting the title would help here.
The paper introduces a method called DeepTLF that handles heterogeneous tabular data by using GBDT as an encoder for a DNN.

The paper is clearly written and the method works as intended.

There is however the issue of novelty (raised by Q6we). The method indeed relies of the capacity of GBDT to represent the data, the internal node values are used as features to train a downstream neural network. This process is straightforward, which is good from an application perspective, though the paper offers limited insights to the community from a scientific perspective.

Another reviewer concern was that of incompleteness of experiments and lack of certain details (reviewers vaip and gWeP). This was answered in the rebuttal, which the reviewers acknowledged, however, the authors did not provide a revised version of the manuscript, when ICLR in fact allowed (and actually encouraged) revised versions to be submitted by Nov 22. Without a revised version, it is difficult for the reviewers to assess whether the text in the final manuscript will actually accurately reflect the changes they suggested. This justifiably caused two of the reviewers to keep their original scores (they explicitly stated the lack of an updated manuscript as the reason).

Given lack of an update, coupled with the issue of novelty, I conclude the paper is not ready to be accepted in its current form.
This paper presents a comparison and analysis of continual learning methods for pretrained language models. The authors categorise continual learning methods into three categories, those that use cross task regularisation, those that employ some form of experience replay of previous training examples, and those that dynamically alter the network architecture for each task. Evaluation results from representative examples of these three paradigms are then presented and analysed. In general methods that incorporate experience reply appear to perform the best, while analysis of the predictive power of individual layers of the pretrained models suggests that some network layers are more robust to catastrophic forgetting than others, and that this also varies across architectures (BERT, ALBERT, etc.).

In general the reviewers agree that this is a well conducted study that provides an interesting contribution to an important area of research. They also generally agree that the many of the results are unsurprising given the properties of the algorithms explored and prior work in this area. The main point of difference then becomes how valuable it is to present a thorough study of existing algorithms that confirms our assumptions. I believe that the current work raises enough interesting questions to make it a useful contribution to researchers working in continual learning. In particular the results analysing the relative differences in catastrophic forgetting across different layers in models suggests interesting avenues for follow on work.
The manuscript proposes a method for addressing spurious correlations and sub population (group) shift problem by modelling intergroup interactions. Past work (GroupDRO) focuses on the worst group which is subject to failure when groups have heterogeneous levels of noise and transfer. This work focuses on the group whose gradient leads to largest decrease in average training loss over all groups. The manuscript presents insights on why the proposed method called CGD may perform better than GroupDRO by studying simple synthetic settings. The manuscript also provides empirical evaluation on seven real world datasetsâ€“which include two text and five image tasks with a mix of sub population and domain shifts.

There are several positive aspects of the manuscript, including:
1. The idea of training on the group which leads to largest overall decrease in loss is natural and interesting;
2. The synthetic examples presented in the manuscript clearly bring out the use cases of the method proposed and comparison with GroupDRO;
3. The empirical results presented lead to improved results on a variety of benchmark tasks.

There are also several major concerns, including:
1. More discussion on why the proposed method works for the chosen real world datasets by connecting them to the synthetic setups presented in the manuscript;
2. The proposed algorithm does not minimize a specific loss function;
3. The standard benchmarks are altered. For example, the CivilComments dataset is shown as a 2 group when it is originally a 8 group task (the groups being the demographics of the users) as shown in the WILDS dataset paper.

Authors clarified, among others, that the proposed approach optimizes the macro average loss function, and the standard benchmarks are not modified and the experiment setup is exactly like GroupDRO evaluation on the CivilComments WILDS dataset. Reviewers noted that the generative model has not added anything new since it is essentially the synthetic example and it just shows what every robust machine learning method is supposed to do, i.e., don t rely on e_s (group specific components) but on e_c (common components) while making predictions. It doesn t justify the procedure of choosing to focus on the group that minimizes the error for the group that decreases all other groups  errors. 

The revised manuscript includes a clearer motivation, and more discussion on how the synthetic examples connect to the real world datasets. Based on that, I put an accept recommendation.
This work introduces an autoregressive flow model that generates molecular geometries by placing one atom at the time. 
In order to preserve the E(3) invariance of the density, successive atom locations are sampled relative to already placed atoms (in a coordinate system described by distance, angle and torsion).
The paper is overall well written and experimental results are compelling.
The paper proposes a new approach for weakly supervised learning, based on conditional normalizing flows. Reviewers generally found the paper to have an interesting, novel proposal with empirical promise. However, some concerns were raised: to name a few,

(1) _Clarity._ Several reviewers found portions of the technical content hard to follow, e.g., the description of constraints in Sec 4.

(2) _Scalability compared to data programming._ One reviewer was unsure of how the present approach compares in terms of inference time and/or accuracy to a two stage data programming approach.

(3) _Infeasibility of sampling from Equation 2._ One reviewer suggested the paper discuss and compare to a simpler baseline, which is to perform rejection sampling from the constraint set.

(4) _Suitability of point cloud problem._ One reviewer was unsure of whether the point cloud problem, considered as an experimental setting in this paper, is reflective of weakly supervised learning.

(5) _Practicality of knowing weak labeler error rates._ The paper assumes knowledge of the weak labeler error rates in constructing constraints. Some reviewers raised concerns on the practical viability of this assumption.

For point (2), the relevant reviewer was not convinced following the discussion. The suggestion is to treat LLF as a label model, which serves as input to a non MC predictor. The question then is what the predictive performance of this combined approach looks like, as opposed to the LLF s themselves.

For point (3), the response clarified that the number of constraints might make rejection sampling infeasible. This appears to be true, but it is suggested that the paper at a minimum discuss this, and ideally also clarify claims about the general purpose need for the proposed approach (since in some cases one might be able to do rejection sampling).

For point (4), the discussion was somewhat inconclusive. It is suggested that the authors explicitly discuss some of the points brought up in the response.

For point (5), while the assumption not wholly uncommon in the literature, it would be better for the authors to perform some sensitivity analysis against misspecification of the error rates.

Overall, the paper has some interesting ideas that are well worth exploring. The present execution appears to have some scope for improvement, with the reviews providing a range of suggestions of areas of the paper that could be made clearer or strengthened. The paper would be best served by incorporating these comments and undergoing a fresh review.
This paper provide an explanation why contrastive learning methods like SimSiam avoid collapse without negative samples. As the authors claimed, this is indeed a timely work for understanding the recent success in self supervised learning (SSL). The key idea in this submission is to decomposes the gradient into a center vector and residual vector which respectively correspond to de centering and de correlation. Such an explanation is interesting and novel. The empirical results are solid and convincing. During the rebuttal stage, the concerns from the reviewers are well resolved, and the writing of the new version is significantly better than the original one.
The paper provides an interesting analysis of aligned GAN models. The paper shows that when a model is obtained (fine tuned) from another, then the corresponding hidden semantic spaces are aligned. The paper uses this property to show that without any additional architecture or training, the models can perform diverse tasks such as image translation and morphing. The paper also demonstrates that zero shot tasks can be performed by learning in the parent domain and transferring to the child domain.

All reviewers agree that the paper presents an interesting analysis and findings and will make a valuable contribution to the field. The reviewers raised some particular concerns, which were addressed by the authors in their response.
In this paper, data augmentation for graph contrastive learning (GraphCL) is studied. Most reviewers agree that the problems addressed in this paper are interesting and important for unsupervised graph representation learning literature. However, many reviewers were not fully satisfied with the novelty and the claim of the main contribution of this paper, a theoretical analysis of the conditions under which data augmentation works in GraphCL, due to the lack of clear explanation and evidence. Unfortunately, no reviewer has suggested acceptance of this paper at this time.
This work was the subject of significant back and forth (between authors and reviewers, but also between reviewers & myself) due to the wide range of opinions. Two of the reviewers have found this work below the bar: they have provided multiple reasonings that I would rather not repeat here. The third reviewer found this work more compelling and argued for its acceptance. My attempts at reaching a consensus have yielding the following conclusions:

  * There s agreement that one shot generation is indeed a challenging task
  * Some of the results are indeed impressive, but many results are not compelling.
  * The rebuttal addressed some of the concerns (e.g. visualization of latents), but some issues are unaddressed (e.g. more motivation, explanation of why the proposed method works better)
  * One of the reviewers has argued rather forcefully that the work doesn t quite do domain adaptation in the typically understood sense. Moving beyond definitions of domain adaptation, the same reviewer was not very convinced by the quality of the results themselves.
  * The reviewer most positive about this work agrees that this work only explores a limited form of domain transfer. They argued that some of the potential applications of this work do make the submission interesting. 

Fundamentally, the discussion did not necessarily resolve the differences in opinion one way or another. Ultimately, all 3 reviewers believe that it would fine if this work was not accepted to ICLR at this time, despite some of the interesting results and promise. Given the discussion and this mildest consensus, I am inclined to recommend rejection too. I do think there s a substantial amount of constructive feedback in the reviews that would make a subsequent revision of this work quite a bit better.
This paper explores why adversarial examples do not transfer well in
adversarial examples on automatic speech recognition systems. The authors
propose a number of potential causes that are then quickly evaluated in
turn.

This could be an excellent paper, but in its current form, it is borderline.
The main problem with the paper is that it proposes a number of causes for the
limited transferability, and then evaluates each of them with one quick
experiment and just a paragraph of text. In particular, none of the results
actually convince me that the claim is definitely correct, and many of the
experimental setups are confusing or would have other explanations other than
the one variable that is aiming to be controlled for.

That said, even with these weaknesses, this paper raises interesting and new
questions with an approach I have not seen previosuly. So while I don t believe
the paper has done much to actually demystify transferability, it does take
steps towards performing scientific experiments to understand why it is so
hard. And these experiments, while not perfect, can serve as the basis for
future work to extend and understand which factors are most important.
All reviewers appreciate the suggested EM approach to goal conditioned long horizon reinforcement learning, and the technical contributions of the paper. While there is a mix in ratings, even the most critical reviewers feels that the paper has clear merits and is acceptable, and there are two solid acceptance recommendations. Overall, the papers significantly meets the standards of an ICLR paper acceptance.
The paper proposes a deep learning pipeline to extract lemmas from proofs and how to (re)use them to compress a proof library. The The topic is highly relevant, the motivation behind the work is clear but the current state of the manuscript is yet not ready for publication.

In fact, while all reviewers somehow recognized the potential impact of this work, they also highlighted some critical aspects that should be addressed before full acceptance. Namely:

    the role of GNNs in achieving the achieved performance shall be discussed more in detail (what if they are replaced in the pipeline? an ablation study would help)
    the broader literature of automated theorem proving should be addressed more precisely in the related works
    comparisons against other ML methods for theorem proving should be extended (the comparison with MetaGen added in the discussion is a good starting point that can turn into a full experiment)
The paper gives high probability bounds on excess risk for differentially private learning algorithms, in the setting where the loss is assumed to be Lipschitz, smooth, and assumed to satisfy the Polyak Åojasiewicz (PL) condition. The key idea in the paper is to leverage the curvature in the loss (PL condition) and the generalized Bernstein condition. 

Authors show that they get sharper bounds of the order \sqrt{p}/(n\epsilon) when the loss is assumed to satisfy the PL condition besides being convex Lipschitz/smooth. Without using some curvature information about the loss function, the best upper bounds we can get are in the order of \sqrt{p}/(n\epsilon) + 1/\sqrt{n} â€” and this is tight at least in terms of the dependence on n given the nearly matching lower bounds â€” in fact, the dependence on n is tight as it matches the non private settings.  

So, I find it a bit misleading when authors say that they improve over the existing results. That statement is not true in its generality â€” it is true that we can leverage the PL condition to give faster rates but that is not the setting of prior work. Again, the bounds that authors compare against are for smooth/Lipschitz convex loss functions and without any assumption on the curvature of the loss. 

If we do look at the literature for when and/or how can curvature help, we can compare against the existing bounds for strongly convex losses. The best known result in the setting that is most closely related is that of Feldman et al. (STOC 2020): https://dl.acm.org/doi/pdf/10.1145/3357713.3384335. As we can check from Theorem 4.9 in that paper, the bounds we get are in the order of 1/n + d/n^2 which is actually better â€” not surprising since PL condition is a weaker condition. There is merit to the results in this paper but the current narrative is quite misleading and a more careful comparison with the existing literature is needed. The bounds are hard to parse â€” for example, what is the dependence on the strong convexity parameter (\mu)? It would also help to instantiate specific loss functions so that we can fix some of the parameters in the bound to have a clear comparison with the existing bounds.
This paper describes a few practically relevant extensions of the conformal prediction framework, that has recently become popular in the ML community for providing (marginally valid) prediction sets without making distributional assumptions. The conceptual contributions are not major, given existing work   without recalibration, the main idea of optimizing over two parameters was explored by Yang and Kuchibhotla (and is well understood even before YK, albeit not fleshed out). The current paper generalizes YK, and with the additional recalibration dataset, it is again simply an instance of standard conformal prediction. The optimization via Lagrangians is a nice addition, but it is ultimately a heuristic that performs well in practice. Nevertheless, the paper is well written, and the experiments are well done, making this a good contribution for practitioners. I recommend acceptance, and congratulate the authors on a nice work. 

As a minor note, Remark 1 should not be attributed to [AB21], since it is a well known fact and deserves an earlier reference.
the aim of this work is to produce an open vocabulary detector.  The approach is via knowledge distillation from existing large scale V+L models, and the evaluation is based on novel classes with LVIS.  The reviewers were generally happy with the work (approach and results), but there were substantial points of clarification during discussion that need to be properly integrated into the final manuscript.
The paper is about a topic that has been extensively studied for more than a decade, hence a very precise discussion of prior work as well as the new insights is absolutely necessary. Unfortunately both are lacking at this stage, thus the paper cannot be accepted.
The paper revisits representation learning for extreme settings (large number of class categories) in a federated learning setup. The authors show how each client can sample a set of negative classes and optimize only the corresponding model parameters with respect to a sampled softmax objective that approximates the global full softmax objective. The authors investigate the interest of the approach for image classification and image retrieval. 

The reviewers appreciated the interest of the approach to reduce communication and the experimental evaluation on several datasets. The reviewers also expressed concerns about privacy, a central concern in federated learning. One reviewer noted for instance that â€˜since every sampled set of each client has to include the classes that the client has, the central server can infer the classes the client hasâ€™. The reviewers would also have liked to see a more comprehensive evaluation, in the absence of the theoretical guarantees. Finally, the reviewers expressed regarding accuracy/efficiency trade offs, one reviewer commenting that â€œthe proposed method degrades the accuracyâ€. 

The authors submitted responses to the reviewers  comments. The authors discussed the challenges related to privacy. The authors also commented on other gradient sparsification communication reducing competing approaches (FedAwS) and the choice of datasets. After reading the response, updating the reviews, and discussion, the reviewers found that â€˜the current good results are only obtained on smaller scale datasets with fewer classes [while] in machine learning, the phenomenon could be quite different at different scalesâ€™ and that â€˜it is not clear if the proposed method can outperform TernGrad at the same amount of transferred data [and] TernGrad also has a better convergence proof compared to the proposed methodâ€™. 

We encourage the paper to pursue their approach further taking into account the reviewers  comments, encouragements, and suggestions. Recent progress in privacy protection theoretical frameworks in FL (secure multi party computation, etc.), see the recent survey by Kairouz et al. in FnT in ML, should help the authors develop guarantees for their approach. Moreover the reviewers suggested a clear path towards further improvements of the experimental evaluation. 

The revision of the paper will generate a stronger submission to a future venue.

Reject.
The results reported in this paper and the model checkpoints released are of interest and broad utility to the community in the opinion of the NLP.  While one reviewer was somewhat negative, most reviewers were in favor of acceptance of this paper, which expands the results from [1] to downstream tasks. The AC therefore recommends acceptance.
This paper establishes high probability generalization bounds of the order O(1/n) for a range of stochastic minimax problems. The reviewers agreed that results are of broad interest and the techniques are non trivial. I recommend acceptance.
The paper presents attacks against sentiment recognition NLP systems using specially crafted homoglyphs. The novelty of the proposed method is, however, marginal; contributions over some of the related work are unclear. Furthermore, the quality of writing is insufficient. The authors provided no response to reviewers  comments.
In an attempt to understand generalization, this paper aims at understanding the dynamics of functions presented by the network for different images in the training set. Authors look at activation patterns (whether a ReLU activation is on or off) as a way of characterizing the active paths in the network and approximating the function presented by the network for each image. Authors study different related statics (eg. correlation) and how they evolve during training including.

Pros: 
  Understanding the dynamics of training, how diversity is encouraged by the training procedure and its relationship to generalization is an important problem.
  This paper takes an empirical approach and tries to make interesting empirical observations about the dynamics of the training.

Cons:
  The paper is poorly written in terms of structure, making clear arguments with enough evidence, notation, etc.
  Some empirical trends are shown but their connections to the main claim of the paper about generalization is very weak. The main attempt to connect the observations to generalization is Fig. 7 which shows model accuracy correlated with the ratio of early to mid overlap. This is problematic both because it only has 6 data points and also because a simple correlation analysis is not enough to establish this claim which is more about the cause of generalization.

Reviewers have pointed to various concerns including but not limited to clarity of the paper, lack of rigorous arguments, not providing enough evidence for the arguments, etc. Unfortunately, authors did not participate in the discussion period.

Given the above concerns, I recommend rejecting the paper.
### Summary

This paper builds on previous work on sparse training that shows the many modern sparse training techniques do no better than a random pruning technique that selects layer wise rations, but otherwise randomly selects which weights within a layer to remove.  The key difference in this work is to take these existing results and scale the size of the network to show that as the size of the network increases, the smaller   as measured in pruning ratio   a matching subnetwork becomes.

### Discussion 

#### Strengths

Places an emphasis on simple techniques

#### Weakness

Significant overlap with previous work. Prior already demonstrated the equivalence of random pruning and contemporary pruning at initialization techniques.

### Recommendation

I recommend Accept (poster). However, I do want to stress that there is a significant overlap with previous work. The paper does appropriately attribute observations to previous work. However, there is some risk that readers may misinterpret the title and claim results as a wholly new observation about random pruning, where the reality is instead much more nuanced. Given that the work points to new methodological directions on considering scaling the network as an additional parameter to consider in pruning observations, I do believe these results   even if narrower in scope that can be interpreted   provide value to the community.
The paper describes an interesting approach to predicting continuous closed surface segmentations from discretized image data using a wavelet output representation. This is an interesting idea with a lot of potential. Unfortunately, the paper currently suffers from major weaknesses which we encourage the authors to address.

1. While the idea of generating a continuous output representation of a segmentation is technically interesting, what are some applications where this is actually useful?
2. The ground truth annotations in the datasets evaluated are implicitly quite variable. The annotations are not made to sub pixel precision and there is likely to be large multi pixel variability across different annotations of the same image. This makes a poor problem to demonstrate the need and potential of a sub pixel accurate segmentation algorithm.

I encourage the authors to find applications and datasets where reliable sub pixel ground truth annotations exist, and to demonstrate that their approach to generating sub pixel segmentations is superior to appropriate baselines which also predict sub pixel segmentations.
This work studies the approximation and estimation errors of using neural networks (NNs) to fit functions on infinite dimensional inputs that admit smoothness constraints. By considering a certain notion of anisotropic smoothness, the authors show that convolutional neural networks avoid the curse of dimensionality. 
 
Reviewers all agreed that this is a strong submission, tackling a core question in the mathematics of DL, namely developing functional spaces that are compatible with efficient learning in high dimensional structured data. The AC thus recommends acceptance.
The paper investigates attacks against time series analysis methods such as GNN and DNN for anomaly and intrusion detection. Standard attacks such as FGSM and PGD are extended for the time series domain and evaluated on several datasets including automotive, aerospace and resource utilization datasets. While the authors claim to be the first to investigate such attacks, some related work was not considered in the paper, which was pointed out by reviewers. Also some other weaknesses of the proposed method, e.g., its focus on feature space perturbations were pointed out. Hence, while acknowledging the importance and the novelty of this paper s contributions, the reviewers agree that the paper must be better positioned in the context of the related work in order to be accepted.
All reviewers are very critical about the submitted paper regarding novelty of results, insufficient placement with respect to existing results, and clarity of presentation. The authors also did not submit a rebuttal. Hence I am recommending rejection of the paper.
This paper proposes a method of multi agent reinforcement learning that separately deals with the risk associated with uncertainties of the other agents and the risk associated with the uncertainties of the environment. This allows for example to be agent wise risk seeking and environment wise risk averse.  The proposed approach is largely heuristic with little theoretical justifications.  The experimental results are promising but not sufficiently convincing, given the lack of formalism.  Further improvement on clarity might complement the lack of formalism or theoretical justifications.
The paper studies the setting of group based fairness under the so called demographic shift, where the marginal distribution of the data remains the same conditional on the subgroup but the subgroup distribution can change. It provides a class of algorithms which give high confidence guarantees under demographic shift in both the known and unknown shift setting.

Overall the paper is a worthwhile contribution: it provides a new angle to the important problem of group based fairness with good theoretical and empirical results.
Meta Review for Learning Representations for Pixel based Control: What Matters and Why?

In this work, the authors presented large scale empirical evaluations and ablation studies to analyze various components (e.g. contrastive objectives, model based approaches, data augmentation) for pixel based control with distractors. Reviewer 7euW wrote a great summary for this paper:

This paper presents an approach for learning representations from pixel data that are amenable for control tasks. The proposed approach is a simple baseline that does not require data augmentation, world models, contrastive losses etc. but only contains two simple sub tasks that are supposed to contribute heavily towards an effective representation: reward prediction and state transition prediction. Along with evaluating this proposed baseline, the paper also compares it to several prior works on representation learning: i.e., several approaches such as data augmentation, distance metric losses, contrastive losses, relevant reconstruction etc. It is shown that the proposed simple baseline either outperforms several of these methods or at least is very close in performance. Finally, the paper presents an interesting discussion about how evaluating an algorithm is not just about the dataset and the chosen benchmark task, but requires a more nuanced point of view of several factors such as reward sparsity, action continuity/discreteness, relevance and irrelevance of features to the task, and so on. The findings of the paper are not just about the effectiveness of the proposed method, but a more overarching view of which types of representation learning methods work in what conditions. 

Along with myself, most reviewers (including the critical 61FY) agree that there is great value in the large scale studies presented in this paper. Furthermore, I personally like how it links a large body of recent work in this topic together in one study. The reviews were mixed (6, 6, 3, 3), and the negative reviews (the 3 s) generally had issues with not the study or experiments, but the conclusions the authors drew from them. In the words of 61FY (who managed to have a good discussion with the authors):

*I m not convinced by conclusions as the authors try to generalize behavior of specific implementation to a family of methods. If I were to implement a new agent, I don t feel like I can believe these conclusions so that makes me question what knowledge this paper can add to the community. Furthermore, many details are either missing or not made clear, and the main story isn t very strong. Therefore, I don t think this paper is ready for publication in the current status.*

Although I really appreciate the effort and detail that went into this nice work, based on the current assessments from the 4 reviewers, I can t recommend it for acceptance in its current state. I feel though, that with a change of narrative, or even with a re examination of the experimental results, the authors can turn the paper around into a highly impactful paper. The description of all of the methods explored, and experiments performed alone makes a wonderful survey of the field with sufficient impact, so I think the authors are *almost* there in publishing a highly impactful work that can make the community look deeper into pixel based control methods (with distractors). I hope to read an updated version of this work in the future published at a journal or presented at a future conference. Good luck!
In this paper, the authors generalized the Fenchel duality formulation of the maximum likelihood for F1 EBM, which leads to a min max optimization formulation. Meanwhile, the optimization reveals a new connection between primal dual MLE with score matching. These contribution is significant and make the paper interesting to the community. 

However, there are several issues need to be addressed. 

  *Experiments*: most of the reviewers concern about the empirical study, which are conducted on synthetic data. The paper will be much stronger if the comparison on real world dataset, e.g., MNIST and CIFAR10, can be conducted. 

  *Clarification of paper*: I totally understand that due to this is a theoretical oriented paper, it must be notation and derivation heavy. However, the paper will be much easier for reader, if more discussion is added, e.g., the implementation of the proposed algorithms and more explanation about the comparison between primal dual algorithm vs. score matching and the experimental results for broader audiences.  


In fact, I personally like the paper very much, which provides a promising solid algorithm for EBM estimation, and the connection to score matching is also novel and different from the current understanding. However, unfortunately, the authors gave up the rebuttal and did not successfully address the concerns from the reviewers. I strongly encourage the authors to revise the draft according to the reviewers  suggestion and resubmit to another venue.
The paper proposes a novel meta algorithm, called Self Imitation Policy Learning through Iterative Distillation (SPLID) , which relies on the concept of  distilled policy to iteratively level up the quality of the target data and agent mimics from the relabeled target data.
Several aspects of the paper can be improved. The reviewers are concerned in particular about the experimental section which might not exhaust the core set of tasks, where the method should be compared with baselines. Furthermore the presentation can be significantly improved (lots of grammatical errors). Another major point is the novelty of the presented algorithm.

In the rebuttal the authors tried to address some of the remarks, in particular by adding additional experiments to the empirical section of the paper. Those experiments still do not convince some of the reviewers. Furthermore, one of the biggest concerns is still a limited novelty of the approach. The presentation of the paper still needs to be substantially improved. Thus the paper still requires nontrivial work.
This manuscript proposes and analyses a bucketing method for Byzantine robustness in non iid federated learning. The manuscript shows how existing Byzantine robust methods suffer vulnerabilities when the devices are non iid, and describe a simple coordinated attack that defeats many existing defenses. In response, the primary algorithmic contribution is a bucketing approach that aggregates subgroups of devices before robust aggregation. This approach is also easily composed with existing Byzantine robust methods. The manuscript includes an analysis of the performance of the proposed approach, including an information theoretic lower bound for certain settings.

During the review, the main concerns are related to the clarity of the technical contributions, and unclear technical statements. The authors respond to these concerns and have satisfied the reviewers. After discussion, reviewers are generally strongly positive about the strength of the manuscript contributions. The authors are reminded to make the final changes agreed in the public discussion e.g., discussion of the reduction to SGD when  $\delta 0$
The reviewers are in consensus. I recommend that the authors take their recommendations into consideration in revising their manuscript.
This paper proposes a personalized federated learning framework based on neural architecture search, in which the local clients perform NAS to search for a better architecture for the private local data. Specifically, the authors extend MiLeNAS, which is an existing NAS algorithm, to be run in the federated learning setting, and use FedAvg for model aggregation. The proposed FedNAS framework is validated against personalized federated learning methods with predefined architectures, such as perFedAvg, Ditto, and local fine tuning, and is shown to largely outperform them on non IID settings with label skew and LDA distribution. FedNASâ€™s collaborative search for the optimal architecture also yields a better performing global model than FedAvg.

The paper received borderline ratings. Three out of four reviewers are learning negative, while one is leaning negative. The below is the summary of pros and cons of the paper mentioned by the reviewers:

Pros
  The idea of using NAS for personalized federated learning seems novel and interesting. 
  The proposed FedNAS framework is shown to be effective in tackling the data heterogeneity problem, which is a fundamental problem with federated learning.
  The authors have released the code for reproducibility.

Cons
  The technical contribution of the work seems limited, since the proposed FedNAS straightforwardly combines an existing NAS method (MiLeNAS) with federated averaging, and there is no challenge mentioned for this new problem of federated NAS. 
  The choice of a specific NAS method (MiLeNAS) is not well justified, and other NAS methods should be also considered.
  The motivation is unclear: It is not clear whether the authors aim to perform collaborative automotive design or solve personalized federated learning.
  There is no convergence analysis.

While some of the concerns have been addressed away in the authorsâ€™ responses during the rebuttal period, the reviewers did not change their ratings, and the final consensus was to reject the paper. 

I agree with the authors that combining federated learning with NAS, and applying it for personalized federated learning is a novel idea that intuitively makes sense. However, I agree with the reviewers that the current method is a straightforward combination of an existing NAS method and an existing FL algorithm, the authors should identify new challenges posed by the combination of the two methods, and identify them. 

Further, performing NAS on edge devices may be possible, but not the best solution, since it could result in large computational overhead. While the authors mention that MiLeNAS is computational suitable in such settings, there should be a proper investigation of the accuracy efficiency tradeoff, showing how well FedNAS performs against others with the same computational budget (or training time / energy consumption).  

Overall, this is a paper that proposes a novel and interesting idea that seems to work, but the paper does not sufficiently examine challenges posed by the new problem. I suggest the authors identify the new challenges and examine the efficiency issue mentioned, and further develop their method, if necessary.
The authors present a method for learning disentangled skill representation that uses weak supervision. The reviewers mentioned that the paper tackles an important problem, delivers interesting and novel visualizations of the learned skills, and positions the paper well in the context of related work. The reviewers also point out several points of criticism: the complexity of the method, lack of convincing comparisons to baselines that utilize the same amount of data and the quality of writing, among others. I encourage the authors to address these points in the future version of the paper.
All reviewers eventually agreed on rejection. The highest scoring reviewer agreed their interpretation of the framing of the paper caused their initial high score, where as the other reviewers took a totally different view on the papers contribution. The authors agreed that the text of the paper was not clear in this regard. And the high scoring reviewer downgraded their score and suggested a different pitch.

Much of the reviews focused on how the paper includes a single handcrafted environment for empirical evaluation, and missing related work on reward shaping. In the AC s view (and several of the reviewers said this too) the simple observation "non obvious shaped rewards bias language" indeed begs of a broader study across a variety of environments.

Whether more experiments are needed or if this work can be reshaped such that one existence proof experiment is enough does not need to be resolved here; the paper in its current form needs significant changes.
This work proposes to define densities via the pushforward of a base density through the gradient field of a convex potential as studied in OT theory and, in particular, inspired by Brenier s theorem.

More concretely, it proposes to use ICNNs to parametrize the convex potentials and considers two mechanisms to match a target density: 1) with a known (normalized) target approximately solve the Monge Ampere equation via optimization; 2) with only samples available, they propose to use the maximum likelihood approach.

While the paper is overall well written, the idea is very close to existing work that was not mentioned or discussed in the paper. The paper would benefit from a substantial revision to incorporate the missing references and emphasize the relative novelty.
The reviewers and AC all find the presented approach interesting and promising. 

However, as pointed out in the reviews and as the authors recognized, the strongly convex + smooth objective setting considered is limited. Given the prevalence of non convex settings in many practical applications and the rich related literature on the analysis of SGD and variants in the non convex setting,  it would be highly desirable to (i) consider experiments on small NN architectures (since the method cannot accommodate larger architectures)  to gain some understanding of the value of the approach and (ii)  to try and extend the present analysis to the non convex case. 

It would also be valuable to perform experiments illustrating the impact of theta indicated by the theory.
This paper explores the representations that are learned by the same network, on the same data, but with different objectives/tasks (RL, supervised, unsupervised). Though the reviewers were positive about some aspects of the paper, the reviews were generally low (3,3,3,6) and indicated rejection. The principle recurring theme in the reviews as to why this was a rejection was the lack of clear motivations/implications. The authors decided not to submit an updated version of the paper. As such, this was a reject decision.
The paper proposes Reasoning Modulated Representations (RMR). That is, it incorporates how to incorporate (structure) prior knowledge (such as a law in physics) into a pre trained reasoning modules, and investigates how doing so shapes the discovered representations in a number of self supervised learning settings from pixels. The reviews and (short) discussion have presented salient arguments about the suitability of the paper for publication at this stage. One review argues that the "methodological contribution is minimal," another one is asking for "systematic evaluation" of the main claims made. Moreover, while we all agree that the direction is interesting, the RMR approach presented is not shown to "scale well" (yet), as pointed out by one review. This, however, is important since the general idea that prior knowledge shapes the representation learned is common wisdom in the literature. Indeed, one may now argue that the paper is much more about "how best to combine pixel based deep learning and neural algorithmic reasoning algorithms" as one reviewer puts it. From this perspective the ATARI experiments are more interesting but here the benefit compared to C SWM seems to be marginal and one should compare to other deep baseline conditions on the RAM; the significance is not looking at the difference in score and degree in freedoms but just the number of wins. Additionally, there should be other baselines that directly make use of more structured models (structure   prior knowledge, e.g., HMMs or some other way to have bit of a memory), other datasets (where no access to RAM exists) as well as a discussion of other approaches that combines (combinatorial) reasoning with pixel based deep learning. That is, while pushing for a more high level contributions is fine, this also requires some more illustrations and discussion of the broader context. Therefore, my overall recommendation is reject at this stage of the paper.
The paper presents a method for compositional task learning in the continual RL setting, by composing and reconfiguring neural modules. The method is evaluated on mini grids and simulated robot manipulation tasks.

The reviewers agree, and I concur, that the paper proposes an interesting solution to a difficult and important problem. The paper is well presented and would make a good addition to the multi task continual learning. The reviewers appreciate the authors  responses and the improvement to the manuscript, and in particular the extra experiments with the wrong number of modules.

The final version of the paper should:

  Clarify the explanation of functional modularity
  Move the relevant pieces to the main text.
  See Gur et al., NeurIPS 2021, https://openreview.net/forum?id CeByDMy0YTL for a definition of learnable compositional tasks via Petri Net formalism.  

Reviewers appreciate the extra experiments with the wrong number of modules.
The authors provide a theory for training feed forward spiking neural networks (SNNs) on input to output spike train mappings. They utilise for this heterogenous neurone and skip connections.

The resulting method is tested on DVS Gesture, N Caltech 101 and sequential MNIST.
It achieved very good performance. The reviewers agreed that the results are interesting and significant.

In the initial reviews, the reviewers pointed out some doubts about the theory and clarity of writing.
These doubts and objections were addressed in the revision and the reviewers were quite satisfied with that.

In conclusion, the manuscript presents interesting results for SNNs with a solid theory and very good experimental results. All reviewers vote for acceptance.
This paper introduces a new structured bandit problem called congested bandits, where the expected reward for an arm is a decaying function of how many times it has been played recently. This model aims to address problems such as route recommendation, in which recommended routes tend to get congested (hence yield lower rewards). Different from prior work on bandits with non stationary reward distributions, the effect of congestion in this model resets after Delta time steps. The authors show that this problem can be formulated as a structured MDP and propose a variant of UCRL2 that learns to recommend the optimal arm for each congestion state. They show that the proposed algorithm achieves a policy regret bound that significantly improves upon UCRL2. They also propose a variant of their algorithm tailored for the linear stochastic contextual bandit setup with the associated analysis.

Unfortunately, this is a rather niche problem formulation and it fails to truly capture congestion models for traffic routing platforms (or other practical routing problems) which serve as the main motivation for the paper.  Moreover, the novelty is limited: the setting is very close to existing non stationary bandit models and the proposed algorithms are straightforward extensions of existing strategies. A possible way for supporting the novelty of the setting could be to improve its theoretical understanding through a lower bounds analysis, which is currently lacking from the paper. Although this paper contains interesting and well articulated ideas, contributions are not sufficient.
The reviewers were generally split on this paper. On the one hand, reviewers generally appreciated the clear presentation, discussion, and explanations, and the experiments. On the other hand, most reviewers commented on the lack of comparative evaluation to other works, including works that are related conceptually. While the authors have a potentially reasonable argument for omitting such comparisons, in the balance I do not believe that the reviewers were actually convinced by this. Particularly when the novelty of the contribution is not crystal clear, such comparisons are important, so I am inclined to not recommend acceptance at this point (though I acknowledge that the paper is clear borderline and could be accepted).
The paper provides a method to accelerate training by choosing a subset of points. After the initial submission, the reviewers raised a major concern about the practicality of the method. In the rebuttal phase the authors provided additional experiments on a large datasets that addressed this concern. That being said, the reviews are still quite borderline. The biggest remaining concern is about the quality of writing. Specifically, there are still requests to â€œfix the narrativeâ€ (NdhY, DHeZ). In addition, some details seem to remain vague regarding the positioning of the paper when compared to the active learning literature (BBTj).
Overall, the paper seems to have potential, especially with the new experiments. However, the changes it required when compared to the originally submitted version are simply too extensive to be thoroughly reviewed in a rebuttal phase.
This paper introduces a new method for diffusion based generative modeling through a Brownian bridge formulation, where the data and latent variable can be coupled. They extend their method to mixtures of diffusion bridges and spatially correlated processes that go beyond the factorial diffusion processes used in prior work.

We thank the authors for engaging with the reviewers and addressing many of their detailed concerns. While reviewers agreed that the proposed theory and methodology were novel and interesting, there are no small or large scale experiments or empirical comparisons to the relevant prior work. In the absence of theoretical justification (bound or proof) as to why the proposed diffusion bridge mixture transport method would result in better performance, more empirical comparisons and evaluations are needed. Additionally, several reviewers found the presentation confusing and overly complex, including the notation, writing, and figures. Given the lack of experimental results and concerns over presentation, Iâ€™m inclined to reject this paper.
The paper proposes a metric to measure the difficulty of training examples. The main thesis is that hard training examples lead to bad test adversarial error. There are theoretical results on simple models establishing such claims. The paper also proposes a method to adaptively weight training examples to improve training which gives improvement for adversarial error. 

The reviewers have raised a number of questions and the rebuttal period has been useful. In particularly, I agree with the reviewers that  model agnostic  is misleading in this context and the authors have agreed to remove this in the future. It is felt that more experiments, comparison to adversarial training, etc. is needed and I think the paper will need to go through a proper review process again before acceptance.
This paper presents a reinforcement learning algorithm to target variable in every time step.  Although the paper proposes an important problem in many real world applications, there were various major criticisms raised by reviewers.  Most importantly, technical novelty is not well motivated or justified.  There is also a significant lack of a specific description of the proposed method, discussion of computational complexity, clarity and presentation, and evaluation metrics, which decreased the enthusiasm of the reviewers.
In general, the reviewers appreciated the elegant concept behind the paper and the good results. However, they also raised considerable reservations about the significance of a method that decreases the parameter count but not necessarily computational efficiency (FLOPS) or memory. While the additional analysis that the authors provided definitely helps to understand the limitations of the method, the reviewers were in the end quite divided on the significance of the results. In addition, all reviewers agreed that the writing was in somewhat rough shape and needed improvement.

In summary, this is definitely a borderline paper, but given the current reviewer assessment, I would recommend that it is not quite ready for publication.
This paper experiments with what is required for a deep neural network to be similar to the visual activity in the ventral stream (as judged by the brainscore benchmark). The authors have several interesting contributions, such as showing that a small number of supervised updates are required to predict most of the variance in the brain activity, or that models with randomized synaptic weights can also predict a significant portion of the variance. These different points serve to better connect deep learning to important questions in neuroscience and the presence of the paper at ICLR would create good questions. The discussion between authors and reviewers resulted in a unanimous vote for acceptance, and the authors already made clarifications to the paper. I recommend acceptance.
This paper develops a technique to provide both privacy and robustness
at the same time using differential privacy.

Unfortunately the paper in its current form does not have meaningfully
interpretable security or privacy claims. The reviewers point at a number
of these flaws that the authors do not address to the satisfaction of
the reviewers, but there are a few others as well.
  What is actually private, at the end of this whole procedure? If the
  actual "pretrained classifier" is not made private, then what s the
  purpose of the entire privacy setup in this paper? Why does the denoiser
  need to be private if the classifier isn t?
  The proof of Lemma 1 appears incorrect. The proof in Appendix E says that
  Equation 10 is true, but this sweeps all of the remaining Taylor series
  terms under the rug and doesn t deal with them. How are they handled?
  In Figure 4(a), what does it even mean to have a "FGSM privacy budget
  epsilon"? Or a "MIM privacy budget epsilon"? A privacy budget is almost
  always something defined with respect to the *training data privacy*,
  how does this relate to the attack in this paper?
  How does this paper compare prior *canonical* defenses, both on the
  robustness and privacy side? In particular, comparisons to adversarial
  training on the robustness side, and some recent DPSGD result on the
  privacy side?
The paper proposes using a mixture of Gaussian models for transformer keys (MGK) so that the posterior distribution of key given query matches the attention scores in the transformer architecture under some assumption. Similarly but in reverse, the query given key under a MoG also matches transformer attention score. The paper proposes that this formulation can learn more diverse attention heads and reduce the computation by replacing some heads with the Gaussian mixture heads which are easier to compute. Moreover, the authors show that it is straightforward to transfer their formulation to linear transformers. The authors perform experiments on the Long Range Arena benchmark and on Wikitext 103 where they show some improvements with less attention heads, while using up to 20% less FLOPs for softmax transformers and around mostly 20% but up to 80% less parameters for the worse linear transformer.

Reviewers generally find that the interpretation of transformers as mixture of Gaussian interesting, although it is based on normalization assumptions. The main objections from both positive and negative reviewers is the weakness of the empirical results. In summary, all the improvements in perplexity are less than 0.5, and accuracy improvements are less than 1. In the discussions, the authors claim that their comparisons are using fewer heads vs baselines with more heads, which is how the tables are presented, but upon closer look I find this unconvincing since the same head count comparisons can be reconstructed and still show very weak results. For example in table 1, acc(MGK4) 61.85 vs. acc(baseline4) 61.23, or in table 2 acc(LinearMG4) 55.7 vs acc(LK4 baseline) 55.61. To begin with, it would be better to compare the same number of heads for more head to head comparisons, so I find the whole argument to be misleading. The claims about FLOPS and memory has a similar flavor where the differences are small but the claims were big. A visual evaluation of figure 3 show that most reductions are around 10% in FLOPs or parameters for the better softmax model (note the axis does not start at 0). The retrieval task for the worse linear transformer is the only case where the FLOPs reduction seems significant. Given that EM is now required during training, I d interpret the results as a negative for MGK given the marginal improvements achieved.

Several reviewers are unhappy with the strength of the empirical results while most reviewers gave favorable scores after the discussion where the authors insisted that some valid points are not valid. The authors requested the AC to look further into the one negative review due to the lack of a reviewer s response. After taking some time to look at the substance of the paper and the review, I recommend rejection due to the weakness of the results, the misleading presentation and discussions.
The paper proposes a hierarchical policy architecture with two substituent policies, "go" and "stop", and a controller mechanism for switching between them on every step (either a rule or a learned network), taking inspiration from neuroscience concepts of inhibition. Both are trained via Soft Actor Critic on the subset of states assigned to them and comparisons are made against a baseline. The use case targeted is the repurposing of pre trained agents to new or updated environments.

Reviewers regarded the method as sound, technically correct, and involving illustrative experiments (although perhaps picking problems too carefully adapted to the solution being presented), and were positive on the general direction of taking inspiration from neuroscience. Reviewer y7rr found the details unclear, recommended more focus on a concrete realization of the general method, and questioned the differences with more traditional hierarchical RL; while many specific inquiries were addressed the reviewer s broad concerns about contextualization remained. Reviewer dDqD had similar concerns around confusing presentation and positioning within the broader literature on "multi task RL, non stationary environments, online/continual learning, etc.", and the discussion unfolded similarly   many specific concerns addressed but fundamental issues remaining. 6ibM, like y7rr, raised the question of why one should stop at 2 policies rather than N policies, noted the under discussed relationship to options, and questioned the starting point of SAC, and while this was clarified to be about value functions rather than policies, the reviewer still thought this was an ill justified choice that rendered the system "brittle", and remained unhappy with baseline choices not extending beyond SAC based agents. tm8g had similar concerns about clarity and in particular that reward engineering seemed central; the authors clarified that this was not the case.

There is wide consensus among qualified reviewers that the presentation (and in particular situating the method with respect to prior work) is inadequate for publication, and I am inclined to agree. As y7rr put it, "evaluating its importance and correctness is hard" without adequate context on the relationships to in particular existing work on hierarchical, multi task and continual learning. While the direction appears interesting, unfortunately the hard work of contextualizing one s contribution is an utterly essential part of the scientific enterprise; without it we risk retreading well explored terrain while merely wearing slightly different boots. I encourage the authors to further clarify their presentation incorporating the valuable feedback from the reviewers on this aspect.
The reviewers all agree that this paper proposes a very interesting approach of finding useful information encoded inside a generative model. They show how foreground/background semantics learnt in a generative model are useful for tasks like segmentation.
This is a general approach that can be applied to other models in the future.
It is an accept.
This paper proposes two methods to learn the architecture of normalizing flows models; Their framework is inspired by (Liu et al., 2019) which uses ensembles/mixtures with learnable weights for architecture search. The application of these ideas to NFs requires a trivial modification to respect the invertibility constraint; which consists in building a mixture model over all possible sequences of compositions of transformations from a fixed set.

The paper proposes to use an upper bound to the forward KL instead of the fKL directly. The reasoning is that this will lead to a "pure" model after optimization, that is, the mixture weights will be in {0, 1}. Mathematically, this simply corresponds to treating the mixture as a latent variable model and performing MAP inference over discrete latent variables, assuming that all mixture components have the same prior weights in the mixture.

The experimental results across various datasets are very mixed, and the family of transformations considered in the experiments is quite restricted.
This paper proposes an improved mean field analysis for multi player residual networks. Compared with prior works, the proposed analysis removes a full support assumption needed in prior works. The authors have addressed some of the reviewersâ€™ concerns by adding comparisons with the existing analysis of ResNet in the NTK regime, and a more detailed comparison with Ding et al. 2021. While this paper gathers some support from a reviewer, there is still concern that the novelty of this paper is not significant, especially given that the analysis is heavily built upon prior works. I think this paper can benefit from providing a proof sketch to highlight the key difference between the new analysis and existing analyses,  or explicitly demonstrating the key proof technique/technical lemmas that enable the removal of the full support assumption. This paper might be a strong work after careful revision.
The paper discusses an approach for privacy preservation in the context of multi task classification. All reviewers struggled to follow the paper and had fundamental questions about the motivation, methods and technical contributions. Unfortunately there was no feedback from the authors to help support the submission.
The paper studies the nearest neighbor search problem for objects embedded in hyperbolic space. It develops a graph based approach to NNS in hyperbolic space, showing (interestingly) that the time complexity of graph based NNS can be lower in hyperbolic space than in Euclidean space under some assumptions. This nice theoretical analysis feels like an intuitive result, as hyperbolic space is in some sense more graph like/tree like then Euclidean space, so it is not too surprising for graph based methods to perform better here. What s really cool about this theoretical result is to see something that is _easier_ in hyperbolic spaceâ€”normally things are harder in hyperbolic space due to its great volumes and significant curvature. The discussion among reviewers was mostly positive, with the majority of reviewers leaning towards acceptance and multiple reviewers expressing that the author response and revision addressed their concerns and raising their scores accordingly. There is some question as to whether the revision is too large and should go through peer review again, but on balance I think it is close enough to the original submission that it can be reasonably accepted here. Therefore, following the majority opinion of the reviewers, I will recommend acceptance.
The authors proposed an algorithm for sampling DAGs that is suited for continuous optimization. The sampling algorithm has two main steps: In the first step, a causal order over the variables is selected. In the second step, edges are sampled based on the selected order. Moreover, based on this algorithm, they proposed a method in order to learn the causal structure from the observational data. The causal structure learning algorithm is guaranteed to output a DAG at any time and it is not required any pre  or post processing unlike previous work.

There were concerns by two reviewers on the slight lack of novelty ("the proposed method of this paper is only a combination of well developed techniques") but I believe the proposed method is still worthwhile. In addition, the paper is overall well written and its experiment evaluation is thorough. It will be a nice addition to the field of differentiable causal discovery.

My recommendation is to accept the paper as a poster.
This paper aims at raising awareness of climate change by GAN projecting flooding images of popular places. This is an interesting case. While all reviews agree that this is an interesting direction, they also value the contributions differently. Two of them would like to see more methodological contributions, two focus more on the contribution to the (psychological) fight of climate change. Nevertheless, the rolling discussion helped to clarify several of the issues raised by the reviewers, and (in my opinion) "combining" existing methods to realize an important model that is one little step towards making people more aware how climate change may impact their own lifes is highly creative and useful. I therefor overall suggest strongly to accept the paper. As the ICLR CfP reads, ICLR is not just about methodological contributions. Societal considerations of representation learning are explicitly mentioned.
Dear authors,

I have read the reviewers and your careful rebuttals. I would have liked to see much more engagement from the reviewers. However, even after your rebuttal, no reviewer suggested acceptance, with two reviewers proposing reject (3) and two proposing weak reject (5).

The reviewers found the paper well written. I concur. The reviewers also notice that the contributions are very marginal compared to prior literature. Personalized FL formulation studied here was in a simpler form first proposed by Hanzely and Richtarik (Federated learning of a mixture of global and local models, 2020) and later generalized by Hanzely et al   a paper the authors cite. That work performed an in depth analysis, also including the nonconvex case, which the authors (claim) did not notice. Compared to that work, the authors perform an analysis in the partial participation regime. However, partial participation is by now a standard technique which can usually be combined with other techniques without much difficulty. The authors tried to argue that their analysis approach is unique, but the reviewers remained unconvinced. 

In summary, I think this is a solid piece of work which is perhaps judged, looking at the raw scores, a bit too harshly. However, most verbal comments are indeed fair. I am also of the opinion that the paper in its current form does not reach the necessary bar for acceptance. I would encourage the authors to carefully revise the manuscript, taking into account all feedback that they find useful. I think the paper can be improved, with not too much effort perhaps, to a state in which the bar could be reached. 

Kind regards,

Area chair
This paper presents a zero shot incremental learning approach that does not store past samples for experience replay. The idea is novel and well motivated, and the paper is well written. Reviewers  comments were mainly about missing baselines, missing ablation studies, and  clarifications about the proposed method. In the revised paper, the authors provided more justifications and added new experimental results on large benchmark datasets as well as ablation studies. After discussion, all the reviewers are positive about this submission. 

Thus, I recommend to accept this paper. I encourage the authors to take the review feedback into account in the final version.
This paper aims to improve performance on edge devices by utilizing a large capacity network in the cloud. To this end, the authors suggest using the routing network that decides whether to use the base model (on the edge device) or the global model (on the cloud). They also propose an overall training scheme for learning not only model parameters, but also network architectures. After the discussion period, 3 reviewers are on the negative side, and 1 reviewer is positive. AC thinks that the authorsâ€™ response was not enough to convince the negative reviewers. In particular, AC agrees with the negative comments of reviewers on limited novelty, unclear motivation for the proposed method, and unclear presentations. Overall, AC recommends rejection.
All three reviewers viewed this paper as marginally above the acceptance threshold (6).

Most of the initial concerns of reviewers were around (a) the applicability of the theory to actual practical use cases and networks, and (b) the presentation and framing of the work, and scope of its results. There were fairly detailed responses from the authors: two of the three reviewers increased their scores after the author response. There s still some lingering questions as to how "real world" relevant the theory is, but the consensus at this point is to accept the paper.

My primary concern for acceptance would be that the proofs techniques are based on Boolean circuits, and none of the reviewers (nor the AC) are particularly familiar with this, and thus the proofs in the appendix have been only lightly reviewed. The "impression" of all reviewers is of correctness.
The main contribution of the paper is to perform a systematic and large study of self training as a method to deal with distribution shifts. Reviewers have appreciated the clarity in the overall writing of the paper, and rigor in the empirical analysis. However the main concern from two of the reviewers is that the technical contributions of the paper are only marginal and incremental in nature. The premise that self learning improves robustness is already somewhat well established (Reviewer PUq6 has pointed out papers that focus on how self training / self learning improves distribution shift and how self training and pre training stack together), and the main contribution of the paper is a systematic application to different datasets. Given the existing work on the relevance of self training in distribution shift, the paper falls below the acceptance bar for ICLR in my opinion.
The paper proposes PipeGCN, a system that uses pipeline parallelism to accelerate distributed training of large scale graph convolutional neural networks. Like some pipeline parallel methods (but unlike others), PipeGCN involves asynchrony in the sense that its features and feature gradients can be stale. The paper provides theoretical guarantees on the convergence of PipeGCN in the presence of this staleness, which is a nice contribution in itself. In discussion, the reviewers found the work to be well executed and sound. All reviewers recommended acceptance, and I concur with this consensus.
The paper contributes a theoretical understanding of training over parametrized deep neural networks with rectified linear unit (ReLU) activations using gradient descent with respect to square loss in the neural tangent kernel (NTK) regime. Authors consider a non parametric regression framework wherein the labels are generated using a ground truth function, which is assumed to be in the RKHS associated with the NTK, perturbed with noise. Authors show that gradient descent based training without early stopping fails whereas \ell 2 regularized gradient descent achieved minimax optimal convergence rate. The paper is clearly written and the results are solid. Overall, a good paper.
This paper proposes a method for class imbalanced data based on meta learning. The technical contribution of the proposed method is limited as it is a reasonable but straightforward extension of the existing method. In addition, as commented by the reviewers,
the comparison with existing methods is not enough, it is unclear why it is meta learned with balanced test data, and hyperparameter tuning details are not given.
This paper proposed a long term object based memory system for robots.  The proposed method builds on existing ideas of data association filters and neural net attention mechanisms to learn transition and observation models of objects from labelled trajectories.  The proposed method was compared with baseline algorithms in a set of experiments.

The initial reviews raised multiple concerns about the paper. Reviewers nrGQ and  V7qP commented on the conceptual gap between the problem proposed in the introduction and the extent of the experiments. Reviewer qPet understood the paper to be a form of object re identification and was concerned about the limited comparisons with related work.  The author response clarified their goal of estimating the states of the objects in the world, which they state is different from the goals of long term tracking and object reidentification mentioned by the reviewers.  The authors also clarified the relationship to other work in slot attention and data association filters.  

The ensuing discussion among the reviewers indicated that the paper s contribution remained unclear even after the author response. Two reviewers noted the paper did not clearly communicate the problem being solved (all reviewers had a different view of the problem in the paper).  These reviewers wanted a better motivation for the problem being addressed in this paper.  The third reviewer remained unconvinced that the problem in the paper was different from long term object tracking.

Three knowledgeable reviewers indicate reject as the contributions of the paper were unclear to all of them. The paper is therefore rejected.
Summary: 
Paper addresses the cross domain few shot learning scenario, where meta learning data is unavailable, and approaches are evaluated directly on novel settings. Authors propose a 3 step approach: 1) self supervised pretraining, 2) feature selection, 3) fine tuning, and demonstrate gains over state of art. 

Pros:
  Approach is novel for this setting
  Paper is clear and easy to understand
  Performance beats several prior methods
  Experiments are thorough
  Fundamental problem is worthwhile of investigation 

Cons:
  Some concerns among multiple reviewers on how hyperparameters are selected. Authors have provided more information and tables in the paper.
  Training process is multi step and not unified. Authors provided additional information about unified training results, which yielded poorer results, likely due to overfitting from training many parameters at once. 

Overall recommendation based on the consensus of reviewers and AC expertise: accept.
This paper offers a disentangled pose and identity representation for image to image translation.  The reviewers are borderline, but the AC finds the discussion by reviewer SuTz compelling, and agrees that the authors missed key references in the submitted manuscript.  In their rebuttal, the authors acknowledged the references were relevant, but believed their paper is not in the same area.  Overall this paper is borderline, but just below the threshold for acceptance in the opinion of this AC.
The paper presents miniF2F, a dataset of 488 highschool and college level math problems. The problems are fully formalized and include proofs in the Metamath, Lean and Isabelle theorem provers (as the reviewers pointed out, the support for Isabelle is limited, and that should be made clearer in the abstract). This multi platform support is the main selling point of the benchmark, because it will make it possible to make direct comparisons among systems targeting different theorem provers. 

The paper also does a good job discussing the benchmark selection and formalization process. This is important since some of the problems were translated from word problems. 

As part of the rebuttal, the authors added extra information on the performance of the baselines and some qualitative details on how they fail. 

Overall, there is agreement among the reviewers that this is a valuable benchmark that will enable comparisons among systems that today are very hard to compare.
The submission initially received mixed reviews. The authors presented convincing answers during the author response period, after which all reviewers recommended weak accepts. The AC has carefully read the reviews, responses, and discussions, and agreed with the reviewers  recommendation. Despite the marginal performance gains, the submission has presented a useful and inspiring way of learning shape representations. The AC, therefore, recommends acceptance.

The authors are encouraged to further revise the paper based on the reviews. In addition, the authors should use $\citep$ for all citations that are not used as a pronoun, including all citations in the tables. Please find more information here: https://journals.aas.org/natbib/
This paper addresses the scale issue in Graph Neural Networks by proposing a â€œcondensationâ€ approach that produces a small synthetic graph from a large original graph such that GNNs trained on both graphs have comparable performance. 

Reviewer cTj2 had concerns with novelty: they claimed the proposed method was close to gradient matching. However, they admitted that the graph setting was new. They suggested some clarity and experimental improvements. 

Reviewer R5cV made a similar comment w.r.t. the similarity to gradient matching. Though overall they were more positive than R5cV and thought the idea was interesting and results were compelling.

Reviewer XqrK like the others, argued that the paper â€œlacked technical innovation in terms of technical contributionâ€. They pressed for a complexity or runtime analysis. 

Reviewer peGb found the idea and problem â€œintriguingâ€ though felt the solution fell short. They offered many suggestions for improving the quality of the experiments and the analysis. 

In the discussion period, reviewer peGb raised their score, thanking the authors for answering their questions. They felt that the additional experiment for NAS was relevant and cleared up a key doubt. Reviewer cTj2 updated their score as well but stated it was critical that the author release the code for reproducing the new experimental results. I think that with most reviewers now on the â€œacceptâ€ side of the fence, I am more inclined to recommend acceptance because I do not see any critical flaws. I think that cTj2â€™s request for code is reasonable and strongly suggest that the authors do so.
This paper presents a debiasing technique that modifies a model s attention mechanism by equalizing attention across social groups.  The authors show that their approach (which is perhaps the first of its kind to look at transformer based models and debiasing instead of fixed word representations) work well in debiasing across certain social group indicators while maintaining overall performance.  However, there is disagreement between reviewers in terms of acceptance of the paper (especially Reviewer 7L6Q wants the paper to be rejected and points to recent critiques such as https://aclanthology.org/2021.acl long.81.pdf that point out pitfalls with the benchmarks used in this paper).  I agree with said reviewer that a lot of these benchmarks are toy ish and finding real impact of bias in NLP models is quite elusive.  Hence, I am recommending the paper be rejected for ICLR 2022 and the suggestions below be incorporated towards a better draft for the future.
This paper proposes a Graph Neural Network model to estimate latent dynamics in the human brain using functional Magnetic Resonance Imaging (fMRI) and Diffusion Weighted Imaging (DWI). The representation is tested on a classification task. While reviewers acknowledge the importance of this application, various concerns have been raised and partially addressed. The work focuses on graph deep learning and offers limited evidence of its superiority over more traditional ML or non graph based deep learning. Besides the methodological novelty is unclearly argued, which is not ideal for the audience of a conference like ICLR.

For all these reasons, this work cannot be endorsed for publication at ICLR 2022.
This paper tackles the problem of how to utilize a network from the source domain to benefit target domain training in terms of sample/training efficiency. In contrast to prior methods (e.g. that perform fine tuning or distillation), this paper poses it as a bandit problem that decides how to wire intermediate representations of the source model into the target model as well as what aggregation function to use. An alternating/mixed discrete continuous optimization is proposed to perform this decision making, and results are shown across a mix of source target pairs and network architectures. 

  The reviewers overall found the method interesting and paper topic both interesting and extremely practically useful, presenting an opportunity to save significant energy, compute, and labeling requirements when training on target domains. The results also show very significant improvements, on the conditions tried. However, a number of concerns were raised including comparison to simple same architecture fine tuning (3TiT), comparison benchmarks e.g. VTAB and newer methods in the area (u325, 7soo, 3TiT), need for the adversarial bandit formulation (3TiT, 7soo, 8d9w), and the added storage/inference costs required (3TiT). 

  Based on these reviews, the authors provided a thorough rebuttal, additional baselines and experiments demonstrating the efficacy of the method (especially the full version) over both reasonable simple baselines (same architecture fine tuning) as well as simpler versions (fixed aggregation), and time/inference time matched performances. Importantly, the advantage of the full method comes out a lot more in the new experiments. Overall, through the rebuttal process the paper has been made much stronger. 

  Given that the paper provides a nice principled approach to the problem and now has strong compelling results, I recommend acceptance.
This paper introduces the Filtered CoPhy method, an approach for learning counterfactual reasoning of physical processes in pixel space. The approach enables forecasting raw videos over long horizons, without requiring strong supervision, e.g. object positions or scene properties. 

The paper initially received one strong accept, one weak accept, and one weak reject recommendations. The main reviewers  concerns relate to clarifications and consolidations in experiments, including stronger baselines, experiments on real data, or more diversity on the datasets. The rebuttal did a good job in answering reviewers  concerns, especially by providing new experimental results and analysis. Eventually, all reviewers recommended a clear acceptance after authors  feedback. 

The AC s own readings confirmed the reviewers  recommendations. The proposed approach is a meaningful extension of CoPhy for the unsupervised prediction at the pixel level. The proposed approach is solid, clearly described, and overcomes important limitations of previous methods. The dataset is also an important outcome for the community. Causality and counterfactual reasoning are of primary importance for the design of effective and explainable AI prediction models: this paper brings therefore an important contribution to the ICLR community.
This paper studies the method to achieve the batch size invariant for policy gradient algorithms (PPO, PPG). The paper achieves this by decoupling the proximal policy from the behavior policy. Empirical results show that the methods are somewhat effective at providing batch size invariance.

After reading the authors  feedback, the reviewer discussed the paper and they did not reach a consensus. On the one hand, the rebuttal made some reviewers change their minds who appreciated the explanations provided by the authors and the new Figure that better highlights the batch size invariance property.
On the other hand, some reviewers think that there is still significant work to be done to get this paper ready for publication. In particular, it is necessary to improve the theoretical analysis and the evaluation of the empirical results.

I encourage the authors to follow the reviewers  suggestions while they will update their paper for a new submission.
The paper addresses an interesting problem, namely how to evolve effective weight and activation update rules for online learning of a recurrent network. The work focuses on two specific tasks: character sequence memorisation and prediction. Two approaches based on meta gradients and evolutionary strategies are explored. Unfortunately the paper is missing some important related works. Moreover, presentation needs to be improved, as well as experimental assessment should be expanded both in terms of tasks and in terms of comparable models presented in the literature.
I am recommending a poster for this paper.  There was considerable discussion and much author response.  The reviews were good (after taking author response and paper revision into account) with one out of three being enthusiastic. There was a concern that the basic idea was technically mis represented as the inductive bias is being placed in the decoder rather than prior. But I am convinced that it is a reasonable idea to place bias in the decoder and that idea is worth publication.

Personally I think the paper would be much stronger with better empirical evaluation.  I find a focus on MNIST (or fashion MNIST) unconvincing. Results on CelebA should be accompanied by sample image generations.  I would rather see downstream task metrics based on learned features.  This paper cannot be put in the same class as recent results on unsupervised learning of image features for downstream tasks. It remains an open question as to whether this paper provides any contribution in that arena.
This paper adopts the recently developed MLP based architectures for image classification to Automatic Speech Recognition with 3 different modifications to handle variable length sequences. The three architectural modifications are: C MLP (w/ depthwise convolution), TS MLP (w/ shift operator), and F MLP (w/ Fourier transform and w/o gating). The approaches are then evaluated on the Librispeech 100h and Tedlium 2 corpora and are compared to baselines from the literature. The proposed approaches are shown to yield better performance than Transformer based models.

As pointed by the reviewers, there are 3 major concerns:
clarity: the initial version of the paper needed more improvement in writing, the authors did improve the writing a lot, which led to increased ratings by the reviewers;
reproduction: many experimental details were missing in the initial version, but the authors added those in the revision and shared the code
novelty: as agreed by all the reviewers that The novelty of the paper is somewhat weak. It is mainly an application of a known technique to a new use case and the modifications are commonly used in ASR. 

The decision is mainly due to the limited novelty.
This paper introduces a technique to measure the *expected* robustness of a
neural network by measuring the probability random input perturbations will
cause the model to make a mistake.

The reviewers are not convinced by the results in this paper. The methods
are not carefully evaluated against prior work, and it is not exactly
clear what lesson one can draw from the resulting statistical evaluation.
The experimental setup is not clearly explained in several places, making the
paper difficult to fully follow.

Since the authors do not respond to the reviewer concerns, there was no
opportunity to address these concerns.
The paper looks at the worst class adversarial error for multi class classification problems. The question is given a certain level of adversarial error on average, is it possible that some classes have adversarial error significantly worse than average? And if so, is this a problem? I agree with the authors that there are applications where such an imbalance could be problematic; other than the examples provided by the authors I can also think of this being important from a point of view of fairness, depending on what exactly the class labels represent. The reviewers have raised the question of low accuracies reported in the empirical results compared to the state of the art on those datasets for adversarial learning. I share these concerns   especially it s worth understanding whether more accurate models also have such an imbalance, or whether this imbalance is a result of incomplete training or models that are not representationally powerful enough. While I agree with the authors that  state of the art  results  are not required for ICLR submissions, especially those making conceptual contriubtions, in this case I think further experiments may be needed in addition to addressing the other questions raised in the reviews. The authors acknowledge that they have made significant revisions in response to the reviews, but I think that would require a fresh review cycle.
This paper introduces a graph neural network (GNN) based on the finite element method (FEM) for learning partial differential equations from data. The proposed finite element network is based on a piecewise linear function approximation and a message passing GNN for dynamics  prediction. The authors also propose a method to incorporate inductive bias when learning the dynamical model, e.g. including a convection component.

The paper received three clear accept and one weak accept recommendations. The reviewers discussed the possible extensions of the method, and also raise several concerns regarding experiments, e.g. the added value of a synthetic dataset, implementation tricks or hyper parameter settings. The rebuttal did a good job in answering reviewers  concerns: after rebuttal, there was a consensus among reviewers to accept the paper.

The AC s own readings confirmed the reviewers  recommendations. The paper is well written and introduces solid contribution at the frontier of GNNs and finite elements methods, especially a pioneer graph based model for spatio temporal forecasting derived from FEM. Therefore, the AC recommends acceptance.
This paper investigates the role of BPE and vocabulary sizes in memorization in transformer models. Through a series of experiments on random label prediction, training data recovery and membership inference attacks, the paper shows that larger vocabulary sizes lead to improved memorization. The Reviewers all agree that the paper investigates an important question and does so thoroughly. The main concerns were about: (1) the validity of the conclusion that it is sequence length indeed which affects memorization; and (2) the lack of more tasks to validate the findings. For (1) the authors added another set of experiments which further rule out frequency effects as a factor, but I agree with Reviewer KAZC that more evidence is needed which directly shows that sequence length is responsible (e.g. are shorter PAQ questions memorized better?). For (2) the authors shared a google drive link with additional results on NMT after the deadline, which the reviewers appreciated. Overall, however, the paper needs more work in order to unify all these results in a single draft.
This paper presents a novel method for class incremental learning (CIL) with the help of placebo data chosen from a free image stream. Such placebo data are unlabeled and easy to obtain in practice. To adaptively generate phase specific functions as the accurate estimation of placebos  quality for KD, this paper applies reinforcement learning based on the constraints of the CIL. The effectiveness of the method has been verified on multiple datasets, including ImageNet 1k and ImageNet Subset with both lower memory and higher accuracy than baselines. The major concern is about the novelty that the unlabeled auxiliary data is not quite new for CIL despite the minor difference in settings and methods. Moreover, the improvements over the baselines are not significant enough, which is a minor concern.
This paper shows that images synthesized to match adversarially robust
representations are similar to original images to humans when viewed 
peripherally.  This was not true for adversarially non robust
representations.  Additionally the adversarially robust
representations were similar to the texform model image from a model
of human peripheral vision.


Reviewers increased their score a lot during the rebuttal period as  
the authors provided more details on the experiments and agreed to 
tone down some of the claims (especially the strong claim that the
robust representations capture peripheral computation similar to
current SOA texture peripheral vision models).  As well stated by 
reviewer s6dV, two representations with the same null space are not
necessarily the same.  

With reviewer scores of 8 across the board, reviewers agree that this
is interesting work that should be presented at the conference.  I agree.
This paper develops a new method, named Augmented Intermediate Level Attack (Aug ILA), to improve the transferability of black box attacks. Specifically, the proposed Aug ILA contains three key modules: image transformations, reverse adversarial update, and attack interpolation. 

Overall, the reviewers think it is an interesting paper, but are concerned that the original ablations are not enough to support the effectiveness of the proposed method, including missing strong baseline attacks and defense methods, and only one dataset is considered. During the discussion period, the authors actively provide new results. However, the Reviewer TcRw and the Reviewer 2dLg are not fully convinced by the rebuttal, especially regarding 1) in these additional experiments, no comparison is provided with other SOTA attacks beyond ILA based approaches; 2) Table 11 shows the proposed method even degrades (rather than improves) the performance of VNI CT FGSM on defense models;  3) the attack rate of the proposed method is sensitive to the selection of layers, therefore, need to be carefully tuned in experiments (which could lead to unfair comparisons to other attacks). These concerns are indeed legitimate, and should be addressed carefully before publication.

I encourage the authors to incorporate all the reviewers  comments and make a stronger submission next time.
This work deals with training generators of aligned pairs of images and segmentation maps. It is based on the recent DatasetGAN approach, which generates images and maps, but requires human annotations on a handful of generated images. This paper is addressing this problem by learning the annotation model over annotated real images instead of generated ones. To this end, the paper proposes a meta learning that uses the Gradient Matching Loss.

Overall, the rebuttal provides valuable insight and many issues raised by reviewers have been convincingly answered by the authors.
On the whole, the reviewers converged positively, the novelty and the interest of the proposal stand out clearly, and this despite the lack of very convincing experiments, at least before the rebuttal. Authors are strongly encouraged to take all comments into account for their final version.
This paper has a deep analysis of the over smoothing phenomenon in BERT from the perspective of graph. Over smoothing refers to token uniformity problem in BERT, different input patches mapping to similar latent representation in ViT and the problem of shallower representation better than deeper (overthinking). The authors build a relationship between Transformer blocks and graphs. Namely, self attention matrix can be regarded as a normalized adjacency matrix of a weighted graph. They prove that if the standard deviation in layer normalization is sufficiently large, the outputs of the transformer stack will converge to a low rank subspace, resulting in over smoothing.

In this paper, they also provide theoretical proof why higher layers can lead to over smoothing. Empirically , they investigate the effects of the magnitude of the two standard deviations between two consecutive layers on possible over smoothing in diverse tasks.

In order to overcome over smoothing, they propose a series of hierarchical fusion strategy that adaptively fuses presentation from different layers, including concatenation fusion, max fusion and self gate fusion into post normalization. These strategies reduce similarities between tokens and outperforms BERT baseline on a few datasets (GLUE, SWAG and SQuAD).

Overall I agree with reviewers that this is a good contribution.
The paper proposes a technique to efficiently retrain a model when a small number of classes are required to be removed. 
Reviewers in general like the paper, but the key issue is motivation for the problem. The motivating examples in the rebuttal are not very good because a. authors do not provide any evidence that such situations are critical or commonplace, b. the data points that are available for retraining might be very biased. 
A more careful grounding of the work would be important to motivate the ICLR community and the ML community in general to further study this problem. But for now, unfortunately the paper does not seem ready for publication at ICLR.
This manuscript presents a method built on top of CLIP which transforms language embeddings to understand new phrases while maintaining the original abilities of CLIP.

1. The technical novelty of this work is limited. That being said, if the task is of wide interest, a straightforward approach that performs well is not just good, it s even preferable to one that is complex. The authors bring up the fact that reviewers are asked to rate both technical and empirical novelty. This is true. Yet, reviewers were unconvinced both by the method and the setting.

  The manuscript does not explain why this setting provides additional challenges or value compared to the many other continual learning or zero shot settings that exist in the literature. What the authors say "we are not aware of any previous work that learns a direct transformation of the representations themselves to accomplish both continual learning and retained zero shot use of those representations with the same model" seems to be undisputed by the reviewers. But is it critically important to future ML research that a single model does both? Or that a model learns a direct transformation to do so? Overall, this task seems very constrained and tailored to this one approach, while usually the more general a setting is, the more it is valued by the community because it will be more likely to stand the test of time and lead to new advances. Reviewers also could not point to a compelling immediate practical need for such a model, which would be another reason for considering a novel setting.

  While in the responses the authors acknowledge that they do not consider their method to be the ultimate solution, that the method has significant limitations, and that this really should be considered a strong baseline, this is not how the work is presented. Relatively little is said in the manuscript about any of these topics.

2. In response to requests for experiments (such as exploring the space of transformations and exploring alternate models to CLIP) the authors put forward that space limitations preclude such experiments. I would encourage authors not to rely on this argument going forward as it does not serve their cause well. Between the unlimited appendix and the possibility of linking to an anonymized website space cannot be a constraint. Science is complex these days and it s not unusual to have to report extensive additional experiments outside the main body of the manuscript. I encourage authors to consider that these requests by multiple reviewers are likely going to be the first questions that the readers of their work will also want answers to. Exploring other transformations and models is critical to understanding the value and impact of the work.

Minor point: I did not see this in the reviews but Figure 1(a) has the labels for CLIP text and image flipped.

If the authors round out the experiments and demonstrate either that their idea is more general purpose, i.e., that it can be applied to other settings and problems, or that this setting is of great value on its own, this could be a strong contribution.
Existing methods for graph clustering usually use node/edge information, but ignore graph level information. This paper proposes incorporating graph level labels into graph clustering and formulating the new problem as weakly supervised graph clustering.  The paper further proposes Gaussian Mixture Graph Convolutional Network (GMGCN) framework for the task.  Experimental results on several datasets demonstrate the effectiveness of the method.

The authors are very active in answering questions by the reviewers.  They have successfully addressed some of the issues. However, there are still questions that remain unaddressed. The submission is not of the quality of ICLR papers.

Strength
* A new method is proposed.
* The proposed methods outperform baseline models on the given datasets and synthetic datasets.

Weakness
* The explanations are not clear enough.  Although the authors provide detailed responses to the reviews,  the problems indicated by the reviewers are still not well addressed.
* The proposed method seems to be too complicated.
* It is not clear why the proposed method works.
* The problem studied might not be realistic.

 
Here is a summary of the reviewers  final comments.

* Reviewer oDis slightly increased the scoreã€‚

* Reviewer r2ym saysâ€œI read responses to my concerns and others, but except for some clarifying statements and notations, authors  responses are not convincing enough. Also, while I now understand the concept of proposed work better than before, I do not think that it is explained and presented well enough.â€

* Reviewer inpd says â€œwould like to keep my original scoreâ€.
This paper presents an approach "ImpressLearn" to continual learning using the idea of task specific masks. The idea builds upon another idea   SupSup (Wortzman 2020)   which uses a backbone network shared by all the tasks and binary task specific masks. However, the number of parameters for an approach like SupSup can become excessively large when the number of tasks is very large. This paper presents a solution by having a small number of basis masks and learning a weighted combination of these basis masks to use as the task specific mask for each task. The experimental results show that ImpressLearn yields significant parameter savings as compared to SupSup.

There were several concerns shared by all the reviewers, such as (1) Limited novelty as compared to SupSup, and (2) Limited experimental evaluation and not having enough baselines. From my own reading of the paper, I largely agree with the assessment of the other reviewers.

The authors responded to the original reviews and acknowledged some of the concerns raised by the reviewers. The reviewers read the authors  response but their assessment has remained unchanged.

The basic motivation and the idea is nice but offers limited novelty (especially as compared to SupSup). If the authors could improve the experimental evaluation (more baselines, larger datasets/networks, etc), it will be a much stronger paper. However, in its current shape, I as well as the other reviewers do not think that the paper is ready for publication.
The paper discusses the auditing of AI systems which is clearly a very important topic. The approach targets mainly image classification systems where the main idea is to do the certification not in pixel space but in a latent space of a generative model. For the certification the authors use the existing CROWN IBP for cubes around test points in latent space. The authors report results on several datasets using existing methods for the generative models.

Most of the reviewers liked this work even though all of them also raised quite similar concerns. The positive reviews were to some extent based on the fact that auditing of AI systems is a very important problem   and I completely agree on this. On the other hand auditing of AI systems, in particular of safety critical systems as discussed with the chest x ray application in this paper, is a serious problem where a paper has to fulfill very high standards. In particular, being transparent about its limitations is crucial which in my opinion does not hold for the current version of this paper.

My main conerns are:
  the motivating example (varying the angle from which a face is seen by less than 30 degress) is misleading as such a precise threat model is not feasible with the generative models discussed later on (see last bullet point)
  there is no discussion on the potential trade off regarding test accuracy training the classifier i) in latent space, ii) with CROWN IBP using a rather shallow model. This is important to judge the utility of this approach   it has to be transparent what the trade off is regarding test accuracy and what kind of price one has to pay in terms of test accuracy for the certificates.
  the deployment check discussed in Section 2.4 is nowhere implemented as later on only boxes in latent space around individual test points are checked but not a fixed box
  the main problem connected to what I mentioned above with the motivating example is that the latent dimensions have to be interpreted by humans. I doubt that this can be made fully transparent to humans what such a certificate in latent space meand and thus to give very precise labels to the latent dimensions like "brightness" is in my point of view misleading. It suggests a much more finegrained semantic control than what is available and to use such inprecise and potentially wrong ``labels  is highly problematic for an auditing process as e.g. this paper does not contain any guarantee against "brightness" changes. For safety critical systems this is an important distinction.

As a minor point there is little technical novelty (certification with CROWN IBP is done with an existing library) but this plays no role in the decision.

In total the paper is not ready yet for publication. The authors should revise their manuscript by addressing the points above, being fully transparent about what the paper can do and what it cannot do. I suggest to do less examples but provide more details on how the authors envision the auditing/certification process for the given application, in particular also with the discussion about rejection of inputs not lying in certified areas (which has not been done in the present paper).
This papers presents a method for solving symbolic mathematic tasks. It first pretrains a transformer model with language translation, and then fine tunes the pretrained model to the downstream mathematic tasks. It contains interesting points but our reviewers have serious concerns which are not fully resolved in the rebuttal. For the integration task, the proposed method achieves good results comparing with Lample & Charleston 2019 (LC) with much less training data. However, as the authors also noted (see the rebuttal), the higher accuracies in LC are achieved with more data. If the authors could also at least show how much data the proposed method needs to achieve the best result in LC, it will be very helpful for understanding the value of this work. In addition, the proposed method did not show similar improvements on the ODE task. So it is hard to see how this proposed method can be generally useful. Our reviewers also have big concerns on writing. Many sentences are really confusing.
This paper studies constructing text2text transformer models that are good at zero shot task generalization via multi task learning over a diverse set of NLP tasks. One main contribution of the work is to create prompt templates for various NLP tasks (that are of different task formats) such that all tasks can be framed into text2text learning format and that is "natural" to the pretrained T6 model. The paper conducts extensive experiments to demonstrate the promising zero shot generalization ability of such multi task learner.

Strength:
  Important problem setup that has broad applications
  Extensive experiments to validate the claims
  Useful resources are developed for the problem

Weakness:
  Good to study the effect of using different combination of training tasks on the downstream zero shot generalization, which can shed some light on the usefulness of upstream tasks
  Justification of "true zero shot learning" capability would require further experiments on analyzing the data overlap between MTL datasets (and also T5 pertaining task data) and the unseen task data.
  Some more discussion on the task split and categorization will be helpful.
A new sampling strategy for experience is proposed and compared with alternative sampling strategies. The main weakness of the paper is the limited applicability of the strategy as it only works well goal oriented tasks, and stochasticity reduces the effectiveness. And within this setting, only good performance is shown on two gridworld like: MiniGrid and Sokoban. In the rebuttal phase, the authors have added additional experiments that suggest applicability of the approach beyond just goal oriented tasks, which have let several reviewers to raise their score. While general applicability of the approach is still somewhat of a concern, the authors have done enough to show the potential of their approach. Hence, I recommend acceptance.
This paper proposes a categorization of out of distribution examples by texture and semantics, and proposed a model that extracts the texture and semantic information separately before combining them via a normalizing flow based method to obtain good results. While the categorization provides some interesting perspectives, most reviewers found the assumptions too strong, and there are some issues with the derivation. Reviewers have some positive feedbacks on the proposed algorithm for OOD, but also expressed concerns about the fair comparison with more recent baselines. The paper, in its current form, is not ready for the publication, but the authors are encouraged to improve the paper with reviewers  suggestions and resubmit.
This paper presents a numerical approach to solving the multi body Schrodinger equation.  Three reviews give low confidence scores and the one review with high confidence, and high score, is very brief and the reviewer appears to have a weak background in this area.  My feeling is that the ICLR reviewer pool does not contain reviewers who are really competent to review this paper.  There is a large literature in the physics community on this problem and the paper should be reviewed in an appropriate venue.  This is especially true for evaluating the empirical results. If the mathematical techniques are relevant to general machine learning, and the authors want to have an impact on machine learning community, then it should be possible to give empirical results on a problem commonly used to evaluate machine learning methods at machine learning venues. Whether or not this is important for physics should be judged by physicists.  In any case, the reviews are for the most part not enthusiastic.
The reviewers all consider the paper to be below the acceptance bar. While the revision addressed some concerns, several critical ones remain open. This includes empirical concerns with regard to the extremely simple grid world environments used, and with regard to the vague distinction between instructions and goal specifications. To improve the submission, the authors should seek stronger empirical foundations, and either refine or remove vague distinctions with regard to the phenomena they aim to study.

Special thanks to the reviewers for an extremely productive discussion.
The paper proposes a neural network compression technique based on sparse and low rank approximations. The paper received mixed reviews, with one accept, one reject, and two borderline accepts. Most reviewers have appreciated the effort conducted for the evaluation. Three reviewers are nevertheless worried about the limited novelty and two of them found the positioning in the literature unclear with many missing references. In particular, one reviewer makes a strong case against the accceptance of the paper.

The authors have made a significant effort to address the issues raised by the reviewers with a very long rebuttal. The area chair has read in details the responses, the points raised by the reviewers, and the paper itself. He/she tend to agree with the issues raised by the reviewers about the positioning of the paper in the literature and the missing baselines. The rebuttal was very helpful and addresses some of the concerns. There are still some remaining issues
   the discussion about related work is relegated to an appendix. Yet, it is critical for positioning the paper and a discussion within the main paper would be more appropriate.
   there is no assessment of the statistical significance of the results. Hyper parameters are fixed to some ad hoc values and it is unclear what the effect of different hyper parameter choices is upon the method and other baselines.
   for reproductibility purposes, providing code with the submission would be very helpful, especially given the empirical nature of the contribution.

Overall, this is a borderline case, which, unfortunately, would require additional work before being ready for acceptance.
The paper addresses a problem encountered in many real world applications, i.e. the treatment of tabular data, composed of heterogeneous feature types, where samples are not i.i.d. In this case, learning is more effective if the typically successful approach for i.i.d. data (boosted decision trees + committee techniques) is combined with GNN to take into account the dependencies between samples. The main contribution of the paper with respect to previous work in the field is the introduction of a principled approach to pursue such integration. One important component of the proposed approach is played by the definition of a specific bi level loss (efficient bilevel boosted smoothing) that allows for convergence guarantees under mild assumptions. Both theoretical and experimental contributions are sound and convincing, justifying the claimed merits of the proposed approach. Another strong point is the fact that the proposed approach is general and amenable to support a broad family of propagation rules. One weakness with the original submission was presentation, mainly because some key information was confined into the supplementary material. The revised version addressed this problem and added some more empirical results that confirmed the superiority of the proposed approach.
Finally, the fact that learning over tabular graph data is very important in Industry, the proposed approach may be of interest for a wide audience.
This paper at first used the name NAS Bench 360 for the benchmark, which confused several reviewers (who expected a tabular benchmark behind this name). The authors renamed the benchmark, which removed this issue, emphasizing that the contribution does not lie in proposed a new tabular NAS benchmark, but a new performance evaluation of NAS on different data sets.
One reviewer recommended acceptance, but 3 reviewers stuck with their rejection scores, the reasons being that 
  there are by now several papers applying NAS outside of computer vision, with a seemingly more comprehensive analysis 
  more analysis would be useful
  it is unclear how general the conclusions are that can be drawn from performance on the included datasets.
(Low technical novelty was also mentioned, but I do not believe that this type of paper can be very impactful even if it has no technical novelty.)

Overall, although I agree with the accepting reviewer that this type of work can be very useful to the community, the rejecting reviewers have too many criticisms to accept the paper in its current form. I encourage the authors to address them and to resubmit.
One note (which did not affect the decision, but which I d like to notify the authors about) is that a reviewer found that the author identity was revealed in the anonymous codes provided by the authors (https://anonymous.4open.science/r/NAS Bench 360 26D1).
The paper focuses on the Catastrophic Overfitting problem of adversarial training of FGSM. One reviewer gave a score of 6 and the other three reviewers gave negative scores. The authors failed to address or clarify (no rebuttal provided) how perturbation distribution and robustness are linked (four reviewers all agree on this). Other issues include unclear motivation, limited experiments validation, and lack of theoretical analysis. Thus, the current version of the paper cannot be accepted to ICLR.
The paper worked on an important problem (robustness concerning spurious correlations) and proposed a useful method (achieving SOTA worst group performance without the true group labels). However, the motivation part is weak so that it is unclear why to go from the theoretical/empirical observations to the proposed method (more specifically the contrastive learning part). The novelty is also not very strong as argued by reviewers (the real novelty was not highlighted by the authors and thus cannot easily be appreciated by readers). It is indeed a borderline case but seems to be below the bar of acceptance and the two reviewers staying on the positive sides would not like to fight for it. Since there is still room for improvement, we hope the paper would benefit from a cycle of revisions for a re submission and the improved version would be accepted in the near future.

By the way, what GgTx suggested is not really an out of scope study, as far as I understood. The authors certainly think that the paper/method has been clearly motivated. This reviewer was asking for a strong motivation, namely, what is missing or what is wrong in existing methods or the SOTA method so that we need/have to apply the proposed method? Without clarifying this point, the paper/method is partially but not fully motivated and the method may look like another alternative though it should be a better one. Instead of showing the better performance, the reviewer would like to see the conceptual advantage of the proposal by understanding what is missing/wrong in the current SOTA method. Therefore, I think this is a great question for the authors to maximize the impact of their work in the end.
This paper introduces a model free RL algorithm claiming SOTA performance. All but one reviewer agreed on rejection.

#1 The empirical results are based on only 5 seeds (too low) and the plots across 5 domains show no clear evidence of improved performance due to overlapping error bars. The paper s poor empirical practice does not support the main contribution.

#2 The proposed method builds on REDQ, but the authors maintained in the response that their method performed better than REDQ (failing to articulate significant algorithmic novelty) . Even the most positive reviewer (iNq8) did not agree when the authors claimed "our performance improvements are achieved by the innovations we introduced in our algorithm". iNq8 responded "it is unclear whether this performance improvement is really meaningful". The authors never responded to iNq8 s followup questions about overlapping error bars and differences in the behaviours produced by the new method.

Points #1 and #2 combine to form the clear conclusion that this work is not ready in its current from for publication.
The paper proposes a unified framework for point cloud upsampling, denoising, and completion through a two stage approach. It receives three reviews with three leaning to accept and one leaning to reject. Most of the reviewers like the proposed two stage approach for its simplicity and demonstrated strong performance. The reviewer recommending marginally below the acceptance threshold expresses concerns about missing comparison to neural shape implicit representation and a lack of insights on what is learned by individual layers in the network. While the meta reviewer agrees that having both would make the paper stronger, the meta reviewer feels the paper has enough merit and would like to recommend its acceptance.
A multi scale hierarchical variational autoencoder based technique is developed for unsupervised image denoising and artefact removal. The method is shown to achieve state of the art performance on several datasets. Further, the multi scale latent representation leads to an interpretable visualization of the denoising process.

The reviewers unanimously recommend acceptance.
This paper tackles the difficult problem of learning to segment objects from an image using no supervision during training. The paper is clearly written and a new synthetic dataset is made available. Unfortunately, the reviewers raised a number of issues with the submission (missing citations and comparison to relevant related work / additional baselines  + ablation studies / missing empirical evaluation of the proposed method on standard dataset beyond the toy dataset proposed by the authors). The paper received 1 reject, 2 marginal rejects and 1 accept but even the positive reviewer agreed that these were limitations. The authors also conceded to these limitations and initiated experiments that are starting to address the reviewers  comments. At this time, the results of these experiments remain incomplete and hence most reviewers agree that the paper should go through another round of reviews before it is publishable. I thus recommend this paper be rejected in the hope that a subsequent revision will make it a much stronger contribution.
The paper considers the setting of distributed optimization and proposes an adaptive gradient averaging and compression scheme to reduce the communication cost. The proposed scheme is shown to achieve the same convergence rate as full gradient AMSGrad algorithm, but due to the reduced cost, it exhibits linear speedup as the number of workers grows.

The reviews appreciated the clear presentation of the results, technical soundness, and convincing numerical experiments. The paper is a solid contribution to distributed optimization. Thus, I recommend acceptance.
The paper proposes to apply graph neural networks to predict battery state of charge. The main concern is the lack of technical novelty, since the main work is a straightforward application of existing works. The work could be better suited for a more application oriented venue.
Inspired by dendritic nonlinearity, this paper extends previous work on PLRNN/PWL dynamical system modeling by Durstewitz s group. The extension replaces the ReLU nonlinearity with a linear combination of ReLUs. This preserves the theoretical properties of PLRNN, however, the dimensionality of the latent dynamics remains the same, increasing the expressive power of prior PLRNNs. I (area chair) actually read this paper since not all reviewers provided high quality reviews and one key reviewer is having a personal emergency. Though I appreciate the premise, detailed numerical evaluations, and the inference approach, the novelty is marginal and I do not buy the theoretical advantage of this class of models as presented (see below). Therefore I cannot recommend this paper to appear at ICLR at this time.

Some additional weaknesses that reviewers did not point out:
1. Dendritic nonlinearity is summarized as a point nonlinearity; It lacks the interesting phenomena of dendrites such as nonlinear summation and calcium spikes with its own internal dynamics.
2. The many analytical properties of PLRNN may sound nice on paper, but very impractical. To search for the fixed points and cycles, the amount of required computation exponentially increases as the number of neurons and cycle length increases. In addition the boundary effects cannot always be ignored. In general detailed analysis can become quite non trivial quickly, e.g., https://arxiv.org/abs/2109.03198
3. High dimensional PLRNN that approximates a low dimensional dynamical system due to model mismatch won t have the same topological stability structures. Theoretical analysis of higher dimensional DS may be very misleading.
This paper revisits the problem of influence maximization and suggests using graph neural networks to estimate an upper bound on the influence, which can then be used to find good seed sets. The paper gives a variety of experimental evidence that the methods improve on various algorithms in the literature. There was a wide variation in opinions. Some reviewers felt that the overall idea was not particularly novel, as methods that combine graph embeddings and reinforcement learning to solve influence maximization have already been proposed in the literature. Additionally some reviewers felt that the experiments were missing important comparisons, particularly to learning based methods, without which it is difficult to argue that these methods really do advance the state of the art.
This work combines steerable MLPs with equivariant message passing layers to form Steerable E(3) Equivariant GNNs (SEGNNs). It extends previous work such as Schnet and EGNNs, by allowing equivariant tensor messages (in contrast to scalar or vector messages). The paper also provides a unifying view of related work which is a nice overview for the ML community. It is overall well written, but would benefit from further revision to improve readability in some parts (in particular section 3, cf. reviews).
It shows strong empirical results on the IS2RE task of the OC20 dataset and mixed results on the QM9 dataset.
Unfortunately, I feel the paper is not quite ready for ICLR, even if the reviews seem in general quite positive (though of low confidence). 
After reading the reviews and rebuttal, and going over the paper I have to make the following comments:
 * The paper do two modification to the backbone architecture, that have an impact on the ability of these systems to continually learn; these changes are adding layer normalization and a mask
  * The paper is mostly empirical in nature; while there are some intuitions presented clearly about these ideas, their efficiency is proved empirically, which is completely fine

 However:
  * The empirical validation seems not sufficient; the main results are permuted MNIST, incremental CIFAR 10, incremental CUB200; the results on permuted MNIST in terms of final accuracy seem surprisingly low (particularly when involving CL solutions, like EWC, ER, HAT .. see table 2; e.g. FA1 < 80% seems very surprising). This seems strange to me and adds a bit of shade on the results 
  * The proposed methods are simple; There is a strong message behind them, namely that the choice of the backbone (architecture size, normalization layers) has a huge impact on learning. But being a purely empirical result, this really needs to be backed up with analysis and an attempt at understanding of what is going on. E.g. looking at the masks over time .. to they converge to be task specific? Anything that would give a bit of depth to the results. Discussing the Figures (e.g. I m looking at Fig 3 and I was grep ing the text to see a discussion of how one would interpret those results). Why is FashionMNIST used to produced Fig 3, and why is not something like this done for one of the CL benchmark considered. Providing additional typical measures for CL (e.g. showing learning curves). 
  * Just overall does not seem that the work provides sufficient insight, or analysis. 

I do think there is something really interesting in this work, and I do hope the authors will resubmit this work after some modification. And I do agree that there are many aspects of the backbone or architecture that have big impacts on CL and this is an understudied and not well understood topic. So in that sense I think the idea of this work is good. But I just feel it fails short in terms of results, analysis. I feel in the current format, the work will not have the impact it deserves.
In this paper, the stopping condition of Bayesian Optimization (BO) is discussed. This problem is very important when BO is applied to the Hyper parameter optimization (HPO) task. All the reviewers agree that the proposed approach based on high probability confidence bound on the regret is interesting and reasonable.  An important issue raised by a reviewer is that many existing BO works discussed how to achieve efficiency and saving budget in BO although they did not explicitly mention the stopping condition. Due to the lack of discussion regarding the relationship with these highly related studies, we have to conclude that the paper cannot be accepted in its current form.
This paper decouples the adversarial training of a domain adaptation model with the detector learning process, and is able to disentangle the features of foreground and background when performing adaptation.  State of the art results on four different domains/tasks are presented with significant improvement. Reviewers are unanimous that the submission is acceptable.  Reviewer PJVS is the most authoritative and experienced reviewer, and notes the paper clarity is impaired, and that the paper is immodest in various places and overclaims what is otherwise supported by the results therein.  The AC concurs with this view.  However the paper is acceptable in present form for ICLR, and the AC advises the authors to revise as discussed in the rebuttal and response to the rebuttal.
The reviewers initially struggled to position this contribution in terms of usefulness. During the discussion phase, it became (more) clear that the proposed method is best used to reduce the communication overhead of ZeRO3. While the integration of this work and ZeRO hasn t been attempted yet, the authors claim that this work "clears the theoretical barrier". From that point of view, the reviewers were not satisfied with the guarantees of the method, arguing that the resulting algorithm is slower than standard EF and could suffer in terms of runtime (when one factors the cost of compression) even when compared to standard uncompressed SGD. Overall, the discussion greatly improved the paper, although directly integrating ConEF with ZeRO could be even more convincing.
This paper presents a method, called Zest, to measure the similarity between two supervised machine learning models based on their model explanations computed by the LIME feature attribution method.  The technical novelty and significant are high, and results are strong.  Reviewers had clarifying questions regarding experiments and suggestions to add experiments, which involve additional domains (text and audio) and different families of classifiers, and more contexts based on prior literatures. These were adequately addressed by the authors. Overall, this paper deserves borderline acceptance.
The authors present a method for creating a curriculum for goal conditioned reinforcement learning. In particular, they propose to use reachability traces to define a sequence of sub goals that aid learning. During the review process, the reviewers mentioned the novelty of the proposed approach and the intuitive explanations provided by the authors. However, the reviewers also pointed out that the experiments could be more thorough, errors in the theoretical justification of the method as well as simplicity of the evaluation environments, among others. Some of the reviewers increased their score after the authors  rebuttal but it was not enough to advocate for acceptance of the paper. I encourage the authors to incorporate reviewers  feedback in the next version of the paper.
This paper proposes a neural architecture for tasks involving user interfaces. Tasks involve detecting objects on screen, writing captions about UI components, attribute recognition, etc. The reviewers for this submission found the proposed model to be reasonable and effective. They also found the paper to be well written and easy to understand. However, they did have one major concern, before and after rebuttal: While the model and design choices were reasonable, they questioned if the insights gained from this paper were of interest and relevance to the broader vision community. They also had other concerns/suggestions including adding inference costs and adding more detail, which were addressed in the rebuttal. Another concern was the fact that multi task did not provide large gains over single task. I agree with the authors in this regard. I think the goal here was to produce a multi task model that attained at least parity with respect to single task, because a single model would provide large benefits when running on a device, and hence I think this concern was well addressed.

My takeaway from the paper, reviews and discussion, however, continues to focus on the major concern of the reviewers. I think this paper would have benefited from answering at least one (if not more) of the following questions to the reader: (1) Why should the broader community work on this task ? (2) If this task is of limited interest, are there instead, aspects of this task that serve as a useful testbed for multimodal research ? (3) If the task and testbed are not directly applicable, are there new techniques developed in this paper that are broadly applicable to other problems or domains ?

Unfortunately, I think that this paper does not presently address either of these questions strongly to the reader. The paper proposes a method for their task, but readers who aren t directly interested in that end task may find this submission less interesting, in terms of insights for their own work. Given the above, I encourage the authors to address this concern and resubmit. I recommend rejection.
Most of the reviewers think this paper is clearly a valuable addition to ICLR based on the convincing theoretical analysis and extensive experimental results. Please refer to reviewers s review for more detailed discussions of the pros and cons of the paper.
This paper proposes a method for incorporating inductive biases into the model architecture of normalizing flows through a suitable probabilistic program. All reviewers agree the paper makes an interesting contribution to the growing normalizing flow literature. The paper is well written and the idea is novel. Additionally, the experimental results are promising and the additional experiments and baselines added during the rebuttal further strengthen the paper. I recommend acceptance.
Although by now there are several approaches for comparing probability distribution, the paper innovates by making their measure take into account the decision space and loss functions directly. The paper also frames its contribution within the literature at large. Reviewers were unanimous that the result is of major interest to the ICLR audience.
This paper proposes a voice conversion framework, ClsVC, which is based on disentanglement of speaker and content information in some latent space.  The authors introduce two classification constraints (a common speaker classifier and an adversarial classifier) to improve the separation of the two embeddings.  Experimental results are reported on a few voice conversion tasks with objective and subjective scores.  Reviewers have reservation about the novelty of the work which is not considered overwhelmingly significant given existing techniques. The theory and arguments on the claimed effectiveness of the disentanglement of speaker and content also raise concerns, which need to be further verified.  The experimental results need to be more convincing.  Lastly, the exposition needs significant improvements.  The authors  rebuttal answers some of the comments but a few major concerns still stand.  This paper can not be accepted given its current form.
The paper presents a methodology for modeling, and learning, trust in a multi agent reinforcement learning system. The reviewers considered this to be an interesting and important question to answer. Nevertheless, they maintained concerns on multiple fronts. The paper could benefit from being more focused. Authors are strongly encouraged to further scale down the claims in the introduction, and ensure that claims made there and later in the paper are matched with experiments that quantify, and validate, the notions introduced. Model choices made, as well assumptions introduced should be clearly motivated/mapped to reality, in light of their strength. Extending experiments to broader example settings, as outlined in the reviews, would also strengthen the work.
A nice paper and very close to being good.  But the focus on hyperparameter tuning of the optimisation method is really not novel, and the experimental validation is not strong enough.  With both theory and experimental just being marginal improvements, the paper is not considered quite ready yet.  Strong suggestion to improve on the weaknesses of the paper and resubmit â€“ next time you ll have a clear acceptance.
This submission receives mixed reviews. One reviewer leans positively while two reviewers are negative. They raise several issues upon improper evaluations, insufficient experimental analysis, baseline and sota network comparisons, presentation unclarity, and technical motivations. In the rebuttal and discussion phases, the authors do not make any response to these reviews. After checking the whole submission, the AC agrees with these two reviewers that there are several drawbacks to the aspects of the technical presentation and experimental configurations. The authors shall take these suggestions into consideration and make further improvements upon the current submission.
The paper proposes two new generalized additive models (GAM) based on neural networks and referred to as NODE GAM and NODE GA2M. An empirical analysis shows that the proposed and carefully designed architectures perform comparably to several baselines on medium sized datasets while outperforming them on larger datasets. Moreover, it is shown that the differentiability of the proposed models allows them to benefit from self supervised learning. 

Reviewers agreed on the technical significance and novelty of the proposed models and valued the clever design of the new architectures. Most concerns and open questions could be answered in the rebuttal and by changes in the revised manuscript. Based one the suggestions of one reviewer new experiments comparing the proposed models to NAM were added, which improved the paper further.  The paper should be accepted.
The paper addresses semi supervised learning with unbalanced class distribution, a.k.a long tail. The main idea is to alternate learning of the representation and the classifier. 
Reviewers pointed out that several papers already addressed this learning setup, often under the name "imbalanced semi supervised learning". No rebuttal was submitted. 

The paper should make direct comparison to recent papers listed by reviewers, both in terms of the technical approach and in terms of empirical experiments. It cannot be accepted tot ICLR.
This paper proposes a model to predict the spatiotemporal dynamics of physical simulations on irregular meshes. The observations are modeled as a sequence of graph representations, each graph corresponding to a snapshot of the observation sequence at time t. This model uses two components, a graph encoder decoder to compress the observations and an autoregressive transformer to model the dynamics. The two components are trained sequentially. At inference time, given an initial hidden state infered from the data and some additional conditional information, a sequence of states is predicted in an auto regressive manner in the hidden space, each state of the sequence is then decoded to produce a prediction in the original observation space. The originality of the model lies in the encoder decoder graph and in the use of a transformer for the prediction. Tests are performed on three fluid dynamics simulation data sets.

All reviewers pointed out some original contributions in the proposed method, in particular the use of transformers in the learned hidden space. In the rebuttal, the authors provided substantial additional results and further details and explanations. Their responses led two reviewers to increase their scores. All reviewers ultimately agree that the paper presents interesting results and conclusions.
This work analyzes the ability of pre trained language models to maintain entity coherence and consistency in long narrative generation. Along with new automatic metrics for analyzing narrative generation, it proposes a memory augmented model that allows tracking entities to improve narrative generation.  Although all the reviewers appreciated the importance of the problem, the novelty of the proposed approach, as well as empirical improvements in a subset of experiments, they also acknowledge several major weaknesses including the lack of rigor in defining the method, the lack of clarity in writing (especially in the experiments section), insufficiently strong baselines, and an issue of reproducibility since the code cannot be released. These concerns were in part addressed during rebuttal, but not enough to accept the paper.
This work studies the impact of distribution shift via a collection of datasets MetaShift. Reviewers all agreed that this work is simple, effective, and well motivated, and has key implications, and will be quite useful to the community. There were some concerns about the lack of analysis of MetaShift, and the binary classification setting, which was addressed by the authorsâ€™ responses. Thus, I recommend an acceptance.
This paper receives positive reviews. The authors provide additional results and justifications during the rebuttal phase. All reviewers find this paper interesting and the contributions are sufficient for this conference. The area chair agrees with the reviewers and recommends it be accepted for presentation.
This paper adopts ViT in the VQ GAN framework replacing CNN, and achieves SOTA FID and IS scores. The empirical results are pretty impressive. It could benefit some practical applications. 

The technical novelty is limited, but the tricks such as l2 normalization of codes are interesting.
The reviewers were in general lukewarm about the paper, not convinced by why realistic augmentation mean more robust features in SSL, had concerns over the szie of the datasets (up to ~100k), and the success depends on the relevance of color for classification. The AC agrees with the reviewers. While the paper sounds interesting, there are many questions remain unanswered   it s unclear that the rebuttal addressed the concerns shared by the reviewers.

In addition to the comments by the reviewers, the AC also feels that the overall design is adhoc and it s unclear that the proposed augmentation can generalize to larger, more practical problems.
This paper introduces a "model agnostic" attack to evaluate the robustness
of machine learning models, specifically focusing on adversarial training
style defenses. The attack uses a RNN optimizer and meta learning to achieve
this goal.

The reviews are mixed. The two more positive reviewers appreciated the
technical ideas of the attack, found the paper well written, and noted
the results were stronger than prior attacks.

However the more negative reviewer raises valid concerns around (a) the
magnitude of the contribution compared to prior attacks, and (b) to what
extent this attack will be useful more generally.

Starting with the first point (also raised by the other reviewers) the
total contribution of this paper is to improve attack success rates by
~0.1% compared to the best prior attacks. This is a fairly limited total
gain, especially because this technique requires much more sophisticated
attack techniques.

More fundamental is the question if this attack is useful to the community.
I tend to agree with reviewer 8AXD here that this contribution is rather
limited for two reasons:
1. As the authors acknowledge, the attack is most effective for adversarial
  training techniques, or others that don t make the gradient hard to
  optimize. This limits the attack to a smaller subset of defenses.
2. Complexity complicates attack evaluations. It s hard enough to get an
  attack working in the first place, and this paper has to use some fairly
  sophisticated tools to just get the attack marginally better than
  prior attacks that are (much) simpler. It s not clear that we would
  expect authors of future defenses to be able to get as good results,
  when prior methods are much simpler to apply.

And so on the whole, this paper s main contribution is improving attack
success rate by a small amount, with a fairly complex method, that only
applies to a class of defenses that don t make gradient descent difficult.
So while there is nothing outright wrong with this paper, in its current
form it seems to be adding unnecessary complexity to achieve something
that can already be done with existing techniques.
The paper conducts a series of empirical studies to evaluate the robustness of smoothed attribution methods. Although the reviewers think this is an important direction, there are several concerns about the experimental settings, such as the sample size and the models to be tested. Also, one of the main finding that Lp based smoothing methods are non robust to non Lp norm perturbations is well known and is not that surprising.
Reviewers raised various concerns and authors sent in no rebuttal. In view of the negative consensus, this paper then made a clear rejection case.
The paper proposes a strategy for multiple learning agents to explore a large RL problem s state space, via the divide and conqeuer principle. It prescribes a design for each agent s reward function, which when optimized enables the agents to  carve out  and cover different parts of the state space yielding efficient exploratory behavior. The argument for efficacy of the proposed method is purely experimental, with numerical benchmarking on complex simulated environments.

The reviewers have raised several concerns that persist even after receiving detailed responses from the author(s). These include the lack of discussion about comparisons with seemingly closely related and applicable work, the perception that the comparisons of this method with others are not fair ("not apples to apples"), and the assessment that the ablation studies and investigation of the sensitivity to hyperparameters may not be comprehensive to make a compelling argument. Thus, keeping in mind the unanimous impression of the reviewers, I am of the view that while the paper contributes an interesting principle, more work is needed to argue for its acceptance in a clear way.
Summary (from reviewer uzT5): This paper analyzes adversarial domain learning (DAL) from a game theoretical perspective, where the optimal condition is defined as obtaining the local Nash equilibrium. From this view, the authors show that the standard optimization method in DAL can violate the asymptotic guarantees of the gradient play dynamics, thus requiring careful tuning and small learning rates. Based on these analyses, this paper proposed to replace the existing optimization method with higher order ordinary differential equation solvers. Both theoretical and experimental results show that the latter ODE method is more stable and allows for higher learning rates, leading to noticeable improvements in transfer performance and the number of training iterations.

All reviewers appreciated the contributions of this paper and recommended acceptance. While the methods themselves are not novel, the game perspective applied to this problem appears to be and the use of higher order solves yield interesting theoretical and empirical improvements.

  Additional comments  

1) For the comparison vs. game optimization algorithms (Figure 3), it would be nice to normalize the x axis so that one "epoch" yields comparable computational cost among the different methods (as RK4 and RK2 is much more expensive than EG or GD per mini batch). Given that EG had such bad performance there, it would not change the conclusions; but the current scaling is still quite misleading. Same comments for Figure 2.

2) Note that modern approaches for stochastic extragradient recommend to use different step sizes for the extrapolation step and the update step (see e.g. Hsieh et al. NeurIPS 2020 "Explore Aggressively, Update Conservatively: Stochastic Extragradient Methods with Variable Stepsize Scaling") I suspect that much bigger step sizes could be used in this case while maintaining convergence, and this version should be added to Figure 3.

3) In "Related Work | Two Player Zero Sum Games"  > note that Gidel et al. 2019a provided all their convergence theory and methods for stochastic variational inequalities and thus it also applies to three player games, unlike seems to be implied by this paragraph. In particular, all the algorithms they investigated (Extra Adam amongst others) could also be applied to DAL. While I can see that the specifics of the objective in DAL might be different than for GAN optimization, it would be worthwhile to acknowledge these alternative approaches more clearly, and I encourage the DAL community to investigate their performance more exhaustively for DAL than what was done in this paper.
Three experts reviewed the paper and gave mixed reviews. Reviewer BBZL raised their score to 6 in the discussion phase. Reviewer dv5k was not fully convinced by the rebuttal and remained negative. Reviewer oUrr also remained negative. The reviewers were not excited by the proposed method in general and raised questions about both experiments and theoretical results. AC found clear merits in the paper, but the reviewers  comments suggested the work could be strengthened in both experiments and presentation. Hence, the decision is *not* to recommend acceptance at this time. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
This paper studies multivariate time series forecasting by making relational inference in a latent space. It attempts to address the important issue of reducing the computational complexity of the inferred graph. This motivation is well articulated.

Despite its merits, concerns have been raised regarding the relatively weak evaluation without using datasets involving more many nodes to demonstrate the scalability of the proposed method, which is a major selling point of the paper. As such, while the motivation of the work is clear, its experimental evaluation is not thorough enough to demonstrate the scalability of the proposed method.

The authors made the remark in their response that they are not aware of any public time series dataset of this size (which is not agreed by another reviewer who pointed out that some much larger datasets were used in other papers). Note that it is not uncommon in other work to use synthetic datasets to evaluate the scalability as well as other properties of the proposed methods.

Moreover, clarity of the presentation also has room for improvement.

The paper has potential for publication in a top venue if the comments and suggestions are incorporated to revise the paper.
The paper aims to improve complex reasoning. In this regard, authors identify that acquisition of data for symbolic reasoning domains is a challenge and propose generating the data by GANs. A transformer based architecture is proposed and trained for LTL and Symbolic mathematics. Experiments show samples generated are of good quality (e.g., correct syntax). We thank the reviewers and authors for engaging in an active discussion. However, the reviewers did not find the task of such data on its own not to be particularly interesting. Also, neither the architecture nor the training algorithm is very novel. If authors could provide a complete story i.e., show the augmented data can improve the performance of neural models that compute solutions, etc., it would make the paper much stronger. Thus, unfortunately I cannot recommend an acceptance of the paper in its current form.
The submission cannot be accepted as there seems to be a mistake in the proof of the main contribution (Theorem 2).
This paper studies the role of positional and relational embedding s for multi task reinforcement learning with transformer based policies, The paper is well motivated, the experiment shows its effectiveness against other competitive methods. In the rebuttal period, the authors solved most of the reviewsâ€™ questions such as novelty and ablation studies. There are still some concerns about the generalizability of this approach for other tasks and more experiments are needed.
This paper presents a light weight hybrid model using both convolutions and Transformer layers resulting in models with lower computational cost and good performance. Reviewers find the paper interesting and agree that the paper did a good job in presenting convincing experimental results. There were questions about role of different components in the proposed model, which author s addressed in the response with additional ablation studies. One reviewer expressed concerns about lack of theoretical foundations for the proposed approach. However they also agree that the paper presents a good and useful experimental study. I think overall the paper has good contributions that others can build on in the future and recommend acceptance.
This paper shows that SLGD can be non private (in the sense of differential privacy) even when a single step satisfies DP and also when sampling from the true posterior distribution is DP. I believe that it is useful to understand the behavior of SLGD in the intermediate regime. At the same time the primary question is whether SLGD is DP when the parameters are chosen so as to achieve some meaningful approximation guarantees after some fixed number of steps T and the algorithm achieves them while satisfying DP (but at the same does not satisfy DP for some number of step T  >T). Otherwise the setting is somewhat artificial and I find the result to be less interesting and surprising. So while I think the overall direction of this work is interesting I believe it needs to be strengthened to be sufficiently compelling.
The paper combines discriminative and generative positive unlabeled learning into a single framework. The reviewers argued the novelty and contributions are not enough for ICLR and unfortunately we cannot accept it for publication.
This paper develops an approach to modular lifelong learning over hierarchical tasks, proving the learnability of certain task classes under different modular architectures, with empirical evaluations on toy supervised tasks. The authors are to be commended for being one of the few works that develop lifelong learning theory. However, the reviewers found the theoretical contributions to be relatively minimal and that the empirical work needs to provide more substantial insight before it is ready for publication. Moreover, the reviewers had substantial concerns with the paper s overall presentation, in many cases finding the paper s organization confusing with many asides and critical details relegated to the appendices. The confusing presentation especially needs to be remedied, and the authors are advised to take the reviewers  concerns into consideration when preparing future versions of their manuscript.

On a minor point, the reviewers identified several places where the paper didn t cite or develop connections to relevant current literature. The authors might also be interested in connections to some much earlier work by Utgoff and Stracuzzi on many layered learning (Neural Computation 14.10, 2002), which shares some high level similarities to ideas explored in this paper.
All reviewers recommended accept after discussion. I am happy to accept this paper.
This paper introduces a new type of language model, the GNN LM, which uses a graph neural network to allow a language model to reference similar contexts in the training corpus in addition to the input context.  The empirical results are good, and the model sets a new SOTA on the benchmark Wikitext 103 corpus, as well as improving over strong baselines on two other language modeling datasets (enwiki8 and Billion Word Benchmark).  The main drawback, as noted by one reviewer, is the computational expense of the method with significant slowdowns compared to the baseline.

Two reviewers voted strong accept, with a third raising several concerns.  The largest concern was the lack of comparison to prior work, especially prior retrieval based methods on two datasets.  The authors responded with an ablation study comparing their method to KNN LM and showed their proposed GNN LM performs better.  Other concerns raised by the reviewer were the paper s lack of clarity (the authors should address the reviewers questions during the next revision) and incremental technical contribution.  Another reviewer highlighted the paper s novelty, and this AC agrees it is sufficient for publication.

Overall, the method is an interesting, if expensive, extension of retrieval based language models, and the empirical results support its effectiveness.
This paper offers an alternative formulation of demographic parity, named GDP, which makes it amenable to easier computation when the sensitive attribute is continuous. Analytically, the paper relates GDP to other notions, offers ways to estimate GDP from data, and establishes the convergence of these estimators. Experimentally, the paper adds the estimated GDP as a learning regularizer and establishes the accuracy fairness tradeoff that results by using this method versus others.

The need to handle continuous sensitive attributes is well motivated since they are ubiquitous. The direction of the paper is thus very pertinent. The experimental exploration of the paper is also strong, though reviewers initially raised questions of clarity of the relationship of GDP with adversarial debiasing. These are mostly addressed by the authors. One weakness of the paper that largely remains is whether the paper offers new conceptual insights. Indeed, demographic parity is simply a notion of independence between an algorithmâ€™s output and sensitive attributes. Other independence metrics are dismissed in the paper as unreliable to compute. However, one reviewer correctly raises the concern that *under similar regularity conditions* to the ones establishing the convergence of the kernel GDP estimator, it is also possible to establish convergence of other independence metrics, that would equally capture demographic parity. Another reviewer also points out that such convergence would follow using standard non parametric statistics techniques. Smoothed estimators of mutual information are indeed available in the literature, with convergence guarantees even in the high dimensional regime. The authors do not satisfactorily address this, casting doubt on the overall significance of the contribution.

That said, given the strong motivation behind the paper and the overall promise of the methodology, it may be worth sharing with the community. The authors are urged to address the above. Additionally, they are urged to be transparent about what the theory offers and what it doesnâ€™t. For instance, the convergence results of GDP only tell us that we can use these estimators to audit the fairness of existing models. In other words, although the paper is touted as showing that GDP can be successfully used for learning, the evidence there is purely empirical: there is no learning guarantee simultaneously on the accuracy and fairness of GDP penalized risk minimization.
The paper tackles the important problem of spurious feature detection in deep neural networks. Specifically, it proposes a framework to identify core and spurious features by investigating the activation maps with human supervision. Then, it produces an annotated version of the ImageNet dataset with core and spurious features, called Salient ImageNet, which is then used to empirically assess the robustness of the method against spurious training signals in comparison with current SOTA models.

As pointed out by the reviewers, this work is not about causality and the definitions of causal and spurious features were originally vague and inaccurate. During the revision and discussion,  the authors changed the terms "causal" features/accuracy to "core" features/accuracy. They also called the provided dataset "Salient Imagenet", instead of "Causal Imagenet", and changed the title to "Salient ImageNet: How to Discover Spurious Features in Deep Learning?". Following the prior discussion, we strongly recommend the authors discard any discussion about causality in the camera ready version of the paper to avoid confusion. Further, we encourage the authors to consider the reviewersâ€™ thoughts and comments in preparing the camera ready version of their manuscript.
This paper studies a Markov game with a single leader and multiple followers, where the state transitions are independent of the actions of the followers.  The paper studies online and offline RL methods for this subclass of the Markov game, and establishes a sublinear regret bound for the online RL method and sublinear suboptimality of the offline RL methods.

Although the paper appears to contain interesting ideas and contributions, the responses and revisions have not sufficiently addressed some of the major concerns of reviewers.  The reviewers and AC thus agree that the paper is not ready for publication.  For example, the issue of motivation and the myopic follower setting has not been resolved yet.  Also, the discussion on the issues studied in Balcan et al. (2015) is not provided.  Please also note that (Jin et al.,  20b) also proposes an optimistic variant of LSVI, and the exact algorithmic contributions still have some unclarity.
This paper a distillation framework where a light weight student model is trained to handle easy (frequent) instances, while the large teacher model is still used to handle the more difficult (rare) inputs. The models are trained to perform well in this two stage inference setting. Experiments are conducted on computer vision and NLP tasks. While the idea is potentially interesting, the experimental results are fairly weak and not very convincing.
The main contribution of the paper is in providing an additional layer to standard certainty equivalent control for LQR dynamics, that essentially prevents the state from exploding exponentially, via forcing a "descent" to a small bounded state space in growing epoch sizes. Theoretically, this is shown to ensure a notion of boundedness which is termed as "bounded cost safety".

Overall, the reviewers raised several concerns in their initial reviews. These included the relevance of the proposed new approach to modeling "safety" here, whether it is actually novel given the assumptions about open loop stability of the original plant, the fact that the learning algorithm could take the state to arbitrarily large states durign the learning process, the significance of the contribution of the paper wherein a "kill switch" is effectively being designed depending on the state norm, whether least squares is an arguably simpler system estimation method compared to the impulse response estimator here, whether existing approaches based on an input failure probability parameter can be used to yield the same result by iteratively taking it down to zero, and other concerns about the technical exposition.

The author(s) provided detailed responses to the reviewers  concerns. Specifically, the safety/stability notion was clarified as being distinct from standard input to state stability, that the learning algorithm could, during its operation, drive the system state to arbitrarily large sizes (although asymptotically almost surely this is supposed to not happen), and how this paper s assumptions are different (and lighter) than other related work that achieves almost sure guarantees.

In the discussion that ensued after the author response period, it was clear that the author responses had helped to address many of the reviewers  concerns. However, the overall impression has still not been convincing enough to recommend acceptance. This is primarily on two fronts, which I hope that the author(s) can address in the future to strengthen the submission: (1) The attempt at expressing "safety of learning" is found to be not satisfactory, in view of the observation that the proposed scheme does not guarantee classical notions of stability while in the process of learning. It also makes the main message of the paper confusing   the reviewers noted that the words "safety" and "safe learning" still recur in the revised manuscript, creating scope for misinterpretation. (2) The algorithmic contribution appears to be incremental   its essence is to "apply the brakes when the car runs too fast". This is not to take away from the hard work put in to analyzing the algorithm and deriving guarantees. (3) The experiments benchmarking the proposed strategy are not comprehensive
  more relevant baselines drawn from existing work, such as the ones
that have emerged from the author response discussion, could be compared against. In fact, it would be compellingly in favor of this submission for the author(s) to show that other approaches fail to offer the same kind of "safe" performance that is expected.

Upon a more careful reading by myself in the recent past, I would also like to bring up a fundamental technical criticism about Theorem 1 and Defn. 2 ("bounded cost safety"), which I believe must be fixed before the paper s conclusions can be accepted. Defn. 2 states that a learning process if bounded cost safe if for all times $k$, $\pi_k$ is not destabilizing in the sense that its value function is finite. However, the sequence of policies $\pi_k$, $k 1, 2, \ldots$ is a sequence of
*random* objects. So in what sense is Defn. 2 to be interpreted? If
the author(s) mean(s) to say that the event {$\forall k 1, 2, ...: \pi_k$ is not destabilizing} occurs w.p. 1, then this is a very strong requirement and cannot be guaranteed (random noise can cause a  bad  controller to be learnt and applied at some time t with positive probability). Basically my contention is that Defn. 2 is incomplete for a theory oriented paper like this, and as a consequence I do not see why Theorem 1 should hold (or more precisely, in what sense it should hold). The proof of Theorem 1 is not clear as well: The expectation in the third sentence of Sec A.2 ought to be taken for a
*fixed* controller $\hat{K}_k$ *always applied* to a system from time
zero until infinity. I do not see why a fixed controller s (standard infinite horizon average) cost should always be finite, leading me to suspect an irregularity in the proof argument. I wish this point could be discussed and resolved earlier in the author response phase, but it is unfortunately too late.
This paper has been independently assessed by four expert reviewers. Two of them recommend acceptance (one straight, one marginal), and two rejection (both marginal). Among the main limitations of the presented work, found by the reviewers, was the limited reproducibility of the results due to the use of private data. The authors attempted to defend their experimental design choice to use only one private dataset by stating difficulty in obtaining public data that would be suitable for their method. I am personally in a strong disagreement with that statement, and with the sub statement of the authors that some publicly available ICU data may not be suitable. That either signals limited practical utility of the presented approach to non ICU settings only, or is simply incorrect. The presented approach nonetheless has an intriguing potential and I would be inclined to recommends its acceptance. Alas, I find the lack of reproducibility to be a significant drawback of the way this work is currently presented and this limitation could not be easily resolved. Therefore I am leaning towards recommending a rejection.
This paper proposes an algorithm for achieving disentangled representations by encouraging low mutual information between features at each layer, rather than only at the encoder output, and proposes a neural architecture for learning. Empirically, the proposed method achieves good disentanglement metric and likelihood (reconstruction error) in comparison to prior methods. The reviewers think that the methodology is natural and novel to their knowledge, and are happy with the detailed execution. The authors are encouraged to improve the presentation of the paper, by providing rigorous formulation of the "Markov chains" to avoid confusions, justification of the independence assumptions behind them, and more in depth discussions of the learning objectives.
The paper proposes a simple modification to how data augmentation is done in image based RL. This results in some improvements on benchmark tasks. The change essentially amounts to adapting data augmentation strategies that are already understood in other fields to deep RL. However, the effect of data augmentation in simple image based deep RL tasks is already known. As such, I think the contribution in this paper is quite incremental   the notion that data augmentation in deep RL helps is already known, and the particular augmentation strategy proposed here is not especially novel. So while it s good in terms of producing improved results on some benchmark tasks, it doesn t seem to be of high significance to the study of reinforcement learning or machine learning more broadly. As such, I think it could be a valuable contribution to a more narrow venue, or as a technical report, but is too incremental and narrow in scope for ICLR.

A note to the authors (this did not impact the paper decision): due to the unfortunately lackluster quality of the reviews, I read and reviewed the paper myself as well to be able to produce a more accurate meta review. In the balance, I see the point the authors make in the response that some of the results in prior work (e.g., CURL) are unfortunately unreliable. That s not the fault of the authors, it s the fault of the prior works. I took this into account in my assessment. In this sense, I do think the comparison to prior work is sensible. On the other hand, I think the practice of reporting only very specific checkpoints (e.g., 100k and 500k), though borrowed from prior work, is not a good way to report results, as it hides the real performance of the methods.
The Authors propose a neural network based approach for the phase retrieval problem. Solving the phase retrieval problem is key for important application areas such as crystallography or radioastronomy.

After adding more baselines and other changes, 3 out of 4 reviewers recommended acceptance. Reviewer kQWk recommended rejection mostly based on the fact that the paper is quite narrow in scope.

Reviewer kQWk is right that the topic might not appeal to most of the ICLR community. It is worth noting that the main contribution of the paper is not about neural networks but rather about connecting phase retrieval with Blaschke products. As it stands, it seems that after making this connection, any non linear approximator could do well.

Having said that, this is an important application area and the progress is welcomed. Hopefully, it will draw inspire more research in this area.

Currently, the key issue of the paper is that it is very challenging to understand for people without background in the phase retrieval area or complex analysis. To make this paper more valuable for the ICLR community, I would strongly encourage the Authors to devote at least a page to explanation of what is the phase retrieval problem, and the intuition behind the solution. Perhaps [1] could serve as an inspiration.

[1] Phase retrieval in crystallography and optics, R. P. Millane
This paper proposes to generate 3D molecules using a step by step approach. The reviewers raised major concerns on the experiments, novelty, writing and technical details. The authors also were not aware of many of the important references, part (but not all) of which have been included during discussions. It is clear that this work is not ready to be accepted by ICLR.
Improving the expressiveness of GNN is an important problem in the current graph learning community. Its key idea is to generate subgraphs from the original graph, then encode the subgraphs into the message passing process of GNN. The proposed method is proven to be strictly more powerful than 1 WL. The authors also quantize how design choices such as the subgraph selection policy and equivariant neural architecture can affect the architectureâ€™s expressive power.

After the rebuttal, all reviewers are glad to accept this submission.

During the discussion, while reviewer B3oK has shown some concerns on the concurrent works in NeurIPS 2021, it should not affect the decision of the submission once the authors have discussed them in the main text. The authors have done this in their revision, thus an acceptance (spotlight) is suggested.
This paper proposes guiding principles with which to design objective functions for proposal distributions for MCMC. They design one such objective based on GSM (Titsias and Dellaportas, 2019). The two concerns raised by reviewers that resonated the most with me were:

  it was not clear that the actual proposed objective was the best way to implement these guiding principles
  a weak empirical evaluation that did not consider online tuning and high dim, highly non Gaussian targets. 

After rebuttal, revision, and discussion, reviewers felt that the authors did a reasonable job of addressing the issue of online tuning, but very highly non Gaussian targets were not addressed. There was still a sense that the ultimate instantiation of the design principles was a somewhat adhoc loss. Ultimately, I think that this work is just below the bar for acceptance and it can be improved by clarifying the choices made in implementing the objective and some more ambitious experiments.
The paper proposes an approach that allows online finetuning of an offline RL policy by adaptively changing a BC regularization term.

Even after discussions with the authors, the reviewers had several concerns. First, the paper seems to be limited in novelty as the "REDQ+AdaptiveBC seems incremental on top of TD3+BC". Second, there were concerns that the adaptive regularization term was insufficient as a contribution given its heuristic nature.

Given the consensus among reviewers of this paper, I recommend rejecting this paper.
This work proposes a new embedding for sets of features. A set is represented by the output means of an EM algorithm for fitting the input set with a mixture of Gaussians. The authors draw a new connection to an existing method for set embedding (OTKE). Moreover, their method achieves good experimental results.

There is general consensus among the reviewers that the paper is sound, well written and provides new insights for set representation, with convincing experiments.

The authors have answered to most comments raised by the reviewers and have revised the paper accordingly.

I recommend acceptance as a poster.
This paper presents a method for inference in state space models with non linear dynamics and linear Gaussian observations. Instead of parameterizing a generative model, the paper proposes to parameterize the conditional distribution of current latent states given previous latent states and observations using locally linear transitions, where the parameters of the linear mappings are given by neural networks. Under fairly standard conditionally independence assumptions, the paper uses known Bayesian filtering/smoothing tricks to derive a recursive estimation algorithm and a parameter estimation method based on a simple maximum likelihood objective.  

Overall, the reviewers found the idea to be novel and interesting and I agree.  They also found the relation to the noise2noise objective worth highlighting. Several concerns were raised during the discussion period, which I believe the authors addressed satisfactorily. However, I think the authors should bring the assumed distinction between â€˜supervisedâ€™, â€˜self supervisedâ€™ and â€˜unsupervisedâ€™ upfront, as usually these types of models are trained using the noisy data (to which the authors refer to as unsupervised). 

Given the large body of literature on dynamical systems, filters and smoothers, I believe the paper will benefit significantly from more comparisons across a wider range of (and more realistic) datasets.
This paper proposes a meta learning method with a latent feature space with a special structure of orthogonality and low rankness. This paper is well written, and the use of the orthogonal low rank embedding for meta learning is interesting. The experimental results (including additional experiments in the author response) demonstrate the effectiveness of the proposed method. The author response addressed some concerns of the reviewers. However, the novelty of the proposed method is not high enough.
There was a healthy discussion with all the reviewers with a consensus that the results are somewhat expected and unlikely to shed light beyond the ntk  regime, yet within the confine of ntk there is a solid and nicely written technical contribution.
I agree that reviewer Vxer was confrontational and abusive, especially in the response to the author s rebuttal, and believe that some form of sanction or reprimand is appropriate. That said, I do think that "performance" should be evaluated on both convergence rate and generalization.  Figure 3 does suggest some improvement on generalization for deep versions of resnet without batch normalization.

The three less offensive reviewers all indicated weak acceptance.  One reviewer pointed out the weakness of only getting positive results on deep versions of resnet with batch normalization removed.  Results on transformers, where Adam is typically used, would be more compelling.  This is my primary issue with the paper.  It has not demonstrated improvement in the standard practice of resnet (with batch normalization) and has not presented experiments on transformers.  The theoretical analysis is not aimed at explaining why the improvement is only observed on deep resnets with batch normalization removed or why L2 regularization seems to be of no value when batch normalization is present.  I understand that these are very difficult questions.

The paper has no champion and I am personally concerned about the significance of the contribution.
This paper proposes a â€œMixupâ€ type of data augmentation for graphs that accounts for the difficulty of mixing graphs of different number of nodes. The authors show that the mixed graphs are invertible functions of the original graphs.

Reviewer d3Ri liked the simplicity and effectiveness of the technique. They called it a â€œhealthy and useful contribution for the fieldâ€. Reviewer n1Dk thought that the paper explored an important problem and thought the paper was clear, though some of the math could have been simplified. This reviewer was concerned that a central claim of the paper, that the method avoids â€œManifold Intrusionâ€ was unsubstantiated. Specifically that it could not be deduced from the fact that edge connectivity could be recovered from the mixed graphs. The reviewer claimed that node features of the individual graphs were unrecoverable. The authors responded in detail to the reviewerâ€™s criticism, adding two new lemmas which purportedly guaranteed node feature vectors could be uniquely recovered. The authors admitted to some conversion between â€œManifold intrusionâ€ and invertibility and added a Theorem and its proof that invertibility guarantees no manifold intrusion. The authors also responded to reviewer n1Dkâ€™s concern about the significance of the reported improvements. Reviewer n1Dk responded to the author rebuttal with concerns about the strong and unrealistic assumption of linear independence of the feature matrix. They had further concerns that for the case of weighted edges the â€œIntrusion freeâ€ property could not be enforced. Discussion ensued, with the authors arguing that the independence assumption was not as strong as the reviewer claimed and that the â€œIntrusion freeâ€ property was only every for graphs with binary edge weights. 

Reviewer 7hBS and q8bs were both on the fence. 7hBS also raised some concern with the case of non binary weighted edges. They also raised the same issue with respect to the connection b/w invertibility and the â€œIntrusion freeâ€ property, which the authors addressed. Reviewer q8bs also thought the problem was interesting, the paper was clear, yet like n1Dk thought the performance improvement was marginal and had concerns with technical novelty of the work.

This was a tough call, so I engaged the reviewers in further discussion. 7hBS agreed with n1Dkâ€™s opinion that the central claim of the paper (the method being intrusion free) was not presented with strong evidence. They also raised another concern, which was that the paper didnâ€™t evaluate on node classification like most other graph mixup style models. Q8bs agreed with n1Dkâ€™s concerns and felt that post discussion the technical novelty of the work was limited. Without strong support from the reviewers, I think that this paper could use further development, either lightening the â€œintrusion freeâ€ claim or presenting evidence for it in other settings.
This work presents a novel and clever experiment for interpretable vision.  Reviewers all agreed that it tackles an important and interesting research question via a user study design.  There are some concerns around the generalization and transfer to large scale real world settings, as well as dataset construction. With the authorsâ€™ responses and discussion, I think the pros seem to outweigh the cons of this work a bit.
The paper proposes a general method to enhance the performance of first order optimizers. The main idea is to use a memory buffer to maintain a limited set of critical gradients from recent history. Namely, gradients with large l2 norm. The paper includes a convergence proof on strongly convex smooth objectives. Experimental results are reported for several architectures in vision and language tasks. When integrated with several commonly used optimizers (SGD, SGDM, RMSProp, Adam), the method shows an improvement in terms of learning speed as well as improved performance (in almost all cases). Several ablations are performed to show strong robustness to hyperparameters introduced.

The paper is very well written and easy to follow. The proposed method is simple and effective. The empirical evaluation is strong and the ablation studies exhaustive and convincing, as pointed out by all four reviewers.

The authors provided a solid rebuttal addressing many questions raised by the reviewers.

Reviewer KE2X is the only reviewer recommending to reject the paper, pointing out that the paper lacks a clear motivation on why the critical gradients are selected based on the l2 norm. In their response the authors provided a connection with recent methods designed to find important examples when training neural networks. In the discussion (not visible to the authors). Reviewer v5GJ stated that this criterion is intuitive and acceptable given the strong empirical performance reported. The AC agrees with this view.

Reviewer KE2X also points out that the theoretical results do not provide a convincing improvement over vanilla SGD. The authors acknowledge this point, adding that the convergence results apply to a wide variety of critical gradient methods and aggregation strategies, so it is natural to expect that improvements will depend on the specific conditions. While the AC agrees that having stronger theoretical results would improve the paper, as it stands it is certainly above acceptance threshold.

Overall the paper makes a solid contribution with a method that is simple and effective. It will likely inspire other alternative methods in the future. The AC recommends accepting the work.
This paper proposes an efficient method for message passing that can incorporate structural information that is provably stronger than 1 WL. As compared to three strands of provably powerful (more than 1 WL) GNNs, the method has limited additional computational overhead, and can also show encouraging results on the over smoothing problem. Overall speaking, all the reviewers like this paper quite a lot, although the also raised some minor concerns. The paper also attracted some unofficial reviewers who provided quite a few related works. The authors did a good job in interacting with the reviewers and addressing their minor concerns. So, we believe the paper is worth accepting, and could be a significant work in the field of graph neural networks.
This paper proposes a new method for domain generalization by adopting a single test example. Authors formulate the problem using a variational bayesian framework which ends up in an adaptation technique requiring a single feed forward computation. The provided empirical results indicate that the proposed method has comparable performance to techniques which require more data.

Reviewers all acknowledge the novelty and significance of this work. The paper is well written and the related work is adequately discussed. Moreover, the proposed method is computationally efficient and empirical results provide strong evidence in its favor. While I am recommending acceptance, I tend to agree with reviewer xA1m about the main weaknesses of this work and I recommend authors to improve them for the final version:

  Lack of proper discussion or intuition about under what conditions the proposed method works well. This may be using theoretical analysis, using toy examples, trying to break the method, motivate using prior work or just simply providing intuitive arguments. Also, as reviewers pointed, Figure 1 is currently very confusing.
  Lack of analysis or ablation study allows a better understanding of the proposed method
This paper proposes a metric for the safety and interpretability of supervised learning models based on the maximum deviation from interpretable white box models. The safety and interpretability of black box models is an important topic, and many reviewers agree that the approach proposed by the authors is interesting. However, the maximum deviation from popular models such as decision trees, generalized linear and additive models have been intensively studied in the context of robust statistics/learning. Without explicit discussion on the connections with these existing studies, the novelty of the proposed approach cannot be properly evaluated. We thus have to conclude that the paper cannot be accepted in its current form.
This manuscript expands the range of recent work in reinforcement learning for language games to much larger and more realistic datasets. A timely and relevant contribution and one that is well evaluated. Further work in stabilizing RL approaches for such large scale problems is likely to have other far reaching consequences. Reviewers were unanimous that this is a strong submission after the author discussion period.
This paper proposes a decomposition based explanation method for graph neural networks. The motivation of this paper is that existing works based on approximation and perturbation suffer from various drawbacks. To address the challenges of existing works, the authors directly decompose the influence of node groups in the forward pass. The decomposition rules are designed for GCN and GAT. Further, to efficiently select subgraph groups from all possible combinations, the authors propose a greedy approach to search for maximally influential node sets. Experiments on synthetic and real world datasets verify the improvements over existing works. During their initial responses, reviewers suggested that the authors experiment with more baselines and also clarify some of the technical details. The authors revised their manuscript to address several of these comments. So, I am tentatively assigning an accept to this paper.
The paper proposes a new approach to target propagation that performs well when used in RNNs on sequence modelling. The paper falls into something of an uncanny valley, where it is different enough from the original TP to lose some of its motivation ("biological plausibility"), and is now directly competing with backprop. Claims about outperforming backprop require EXTREMELY thorough and rigorous experimental evidence. Without meaning to cast any doubt on the authors work, there have simply been a lot of papers over the years that saw some improvements over backprop in some setting, that have not generalised or even been reproducible.
In this paper, the authors introduce an exploration method for RL according to experimental design perspective via designing an acquisition function, which quantifies how much information a state action pair would provide about the optimal solution to a Markov decision process, and the state action that maximizes such acquisition function will be used for sampling for policy update. The empirical evidences show the proposed method is promising. 

Since most of the reviewers support the paper, I recommend acceptance of this submission. 

However, besides the questions raised by the reviewers, e.g., computation cost and planning quality from CEM, there is a major issue need to be clarified in the paper:

>The algorithm designed for RL with generative model, which makes the state action reset can be conducted (this is sometimes impossible in practice where the agent must start from initial state). This is different from the common RL setting, and thus reduce the complexity of RL. This should be emphasized in the paper. Meanwhile, for a fair comparison, this should be explicitly specified in experiment setting.
The paper empirically benchmarks multiple sample selection strategies for offline RL based on the prioritized experience replay framework, including TD errors, N step return, Generalized SIL, Pseudo count, Uncertainty, and Likelihood. These are all benchmarked for the base algorithm TD3BC. The experiments study the performance and bootstrapping errors. Among other things, it is shown that non uniform sampling strategies are also interesting in a batch RL setting. The authors show that non uniform sampling can be helpful in offline RL compared to uniform sampling but they fail to avoid bootstrap error. They also found that there is no one outperforming metric for prioritized sampling in offline RL settings.

The reviewers are in agreement that the question studied is a sensible and interesting one   Are PER strategies which are effective in online RL also useful for batch RL? The overall study conducted by the paper is clear and well presented. 

While the study/benchmark and the results presented is clear, the reviewers point out the following shortcomings
1. The study is not comprehensive for this work to become a definitive exploration of this space of ideas. Only algorithm has been tested with these ideas. 
2. The results of the study are unfortunately inconclusive   while there are benefits these are achieved via different strategies and as mentioned by the paper no clear conclusions can be drawn. 

Since the paper is targeted purely as a benchmark, the originality aspect of the paper is naturally low. For benchmark papers in that case the impact factor squarely falls on comprehensiveness of the study and the emergence of some clear conclusions to further research in that area. The reviewers unanimously believe the paper falls short in both respects and therefore the decision. 

Hopefully the authors can consider the feedback provided and incorporate it to improve the paper.
This paper recieves extensive discussions among SAC, two ACs and PCs. The decision was not made lightly. We hope that you will find the comments below  from two ACs useful for future publication. 
 
This paper is concerned with the feature attribution framework, which distributes the prediction made by a Graph Neural Network (GNN) to its input features, such as edges or nodes, and identifies an influential subgraph as the explanation. The currently prevailing feature removal strategy, which feeds only the considered subgraph into the target predictor and then measures the importance of the subgraph, will encounter the so called Out Of Distribution (OOD) problem the new subgraphs may not appear in the training dataset.

This paper proposes to use the causal inference framework to deal with this OOD problem. The proposal seems interesting: it considered the considered subgraph as the cause of the prediction and treats domain shift as a (hidden) common cause for both of them. It proposes to estimate a surrogate graph G_s^* to satisfy the front door criterion and then estimate the causal effect of the subgraph on the prediction.

While the proposed method seems novel and interesting and the reported empirical results seem encouraging, I have some basic concerns about the proposal.
1. The authors didn t justify why the feature attribution evaluation problem can/should be considered as a causal effect identification problem. In feature attribution evaluation, one is essentially interested in evaluating the prediction given the subgraph. The distribution shift variable, D, contains information that is helpful for this purpose.  Why should one go with the causal effect identification formulation, in which D is made independent from the subgraph and then integrated out? I think this justification is essential.

2. It is not justified why the constructed variable, G_s^*, satisfies the front door criterion. In Section 3.1, the authors claimed that "G_s^* should follow the data distribution and respect the inherent knowledge of graph properties, thus no link exists between D and G_s^*."  I failed to see why this implies that there is no link from D to G_s^* (or that D and G_s^* are conditionally independent given G_s) note that D is part of the data distribution. Without this justification, I am not sure whether the application of front door adjustment is sensible.

Overall, the paper contains interesting ideas and the results look encouraging. It would be highly appreciated if the authors managed to make this work convincing, by properly addressing the issues above.  I hope to see those components in an updated paper.
 

This work considers the task of debiasing GNN explainer subgraph importance scores by generating surrogate subgraphs to correct distribution shift problems. Through a causal model, the authors argue that a model prediction based on the explanatory subgraphs suffers from a distribution shift (e.g., induced subgraph degree distributions are different than those of the full graph). Hence, the associations between the important induced subgraphs and the model prediction may be spurious. In particular, the casting of induced subgraph explanations as a front door hidden variable should be definitively valuable to the community (but maybe a little too strong of a condition, seems unnecessary for the proposed goal).

Overall the reviewers believe the goal and observations are novel and valuable. Most of the reviewers  concerns were addressed in the rebuttal. As a pure data driven domain adaptation work, I think the work is good. However, even after discussing with the authors, I am still concerned with the soundness of the causal theory behind the work.

A Conditional Variational Graph Auto Encoder (CVGAE) are tasked to produce subgraphs that act as a front door adjustment variable. The argument is that this front door adjustment solves the challenges of the distribution shift. Front door adjustment is generally performed over observed variables. If performed with a model, the model is likely mechanistic. CVGAE are extremely flexible models for a front door adjustment. And while the generated subgraphs are constrained to reproduce a data driven distribution of induced subgraphs (and that those themselves have constraints), that in itself is not enough to guarantee these generated graphs give a proper front door adjustment. The work also describes "generate counterfactual edges" without a clear causal model for how these edges are generated or why they are counterfactual. The causal theory needs a lot of work.

The work is very promising and may become a cornerstone contribution to graph explanations. However, as it stands now, the causal theory needs to be more formal (the work offers no proofs of the various claims). I am excited to see the causal theory fully developed in the future.
This paper studies the effect of importance weighting in three model classes: linear models, linearized networks, and wide fully connected networks, and show that under certain assumptions, gradient descent for training an overparameterized model converges to the same ERM interpolator regardless of the reweighting scheme. The reviewers acknowledge that this paper had good exposition and writing in general, but they were in consensus that the initial version of this paper includes many inaccurate overclaiming statements. In summary, after discussions, the reviewers would like the authors to:

  revise the abstract and the introduction, specifically, adding appropriate qualifiers on the neural networks, losses, full gradient descent training, etc (Reviewers hLzT, M2hT, fZgx)
  address the discrepancy between theory and experiments, e.g. the inconsistency of the loss chosen in theory and experiments, the requirement that the widths of the neural networks need to large (Reviewers wG4N, fZgx)
  add experimental results for early stopping (Reviewer hLzT)
  empirically verify that the final solutions of ERM, DRO, and IW initialized at the same \theta^0 are very similar (Reviewer M2hT)
  discuss the novelty compared to Sagawa et al in the paper (Reviewer wG4N)

thus, this submission needs a major revision, and is not ready for acceptance in its current form. We encourage the authors to revise accordingly, and resubmit in the future.
The paper builds upon previous work on neural temporal point processes. It mainly proposes the replacement of the LSTMs with Transformers as transformers are widely considered as a more powerful sequence modeling tool and the three advantages listed in the end of section 1 in this paper.

However, on the modeling side, it is not straight forward how to apply the transformer (the attention architecture) on to the continuous time sequence problem using NDTT. I think because I read a revised version of this paper, it is actually more understandable to me as compared to the reviewers who read the first draft of the paper. I think A NDTT is a natural and principled way of introducing the attention mechanism into the continuous time neural symbolic framework, although I agree it unfortunately does not leading to a significant improvement in every experimental setting.

To summarize the discussions, I think the authors did a good job in resolving the concerns on the related work and made the paper easier to understand. I appreciate these efforts from the authors even though I also understand there are concerns left still from the reviewers.

In summary, I am leaning to accept this paper because I think it is an interesting contribution. However, I do agree with the reviewers that the writing of the paper needs to be improved and the experiment section is relatively weak in this paper.
This paper presents a reinforcement learning inspired algorithm to train task specific adapters to adapt pretrained language models for downstream tasks. The paper attempts to tackle an important problem. All reviewers have concerns about whether the results are strong enough to justify claims made in the paper. I appreciate revisions that have been done by the authors during the rebuttal period. However, I believe that the paper is still below the bar for ICLR. I recommend rejecting this paper.
The paper considers test time adaptation to distribution shift which is a very important and impactful problem. The authors propose an empirical method that has different pieces, the most important ones being input transformation and confidence maximization and using likelihood ratio loss.

There were various concerns that got addressed during the rebuttal period such as, novelty of the proposed method, ablation study of different parts of the model, novelty and importance of diversity regularizer, choice of optimization. However there are still three remaining concerns that addressing them will improve the paper significantly: First, clear motivation behind the method for the cases when the model is certain but we have data imbalance. Second, analysis in the online setting of batch by batch prediction and adaptation. Third,
establishing the claim regarding data subset experiment that it enable the model to adapt on a subset of data and later switch to complete execution mode without adaptation for efficient run time and improved throughput. How is the method to know the data distribution has changed, or that it has sufficiently adapted to it when the data distribution is not changing?
This paper develops a new large language model trained on 25TB of (simplified) HTML text data. The HTML tags provide valuable information about the document structure. The training adapted the BART denoising objectives (to inject noisy size hint to control generation length during training). The paper also studies various prompting methods for the model. The model achieves state of the art performance on zero shot summarization and several text classification tasks. Reviewers have found the motivation of pretraining with structured text convincing, and the results are good.
This paper proposes a novel neural voice camouflage method that learns predictive attacks without any constraints about input and output. It is general, robust, and real time that could be used in a real world scenario. The experiments are solid, the in depth analyses are convincing.
The paper aims to improve generalization and sample efficiency in robotic control. In this regard authors note that modular robot systems, which provide building blocks for a task specific morphology, can be considered just another domain where transformers can be used. The authors propose to learn a universal controller over this modular design space and leverage successful â€œlarge scale pre training and fine tuningâ€ scheme for transformer. We thank the reviewers and authors for engaging in an active discussion. Reviewers found that methodological novelty is limited ("The contributions on algorithmic and network designs are marginal. Their approach is a direct application of Transformers and Reinforcement Learning approaches.") and there are very similar approaches in the literature (e.g., AMORPHEUS approach (AM) by Kurin et al., 2021.) Despite these shortcomings, overall the reviewers found the experimental results to be strong and analysis to be thorough, which will be valuable to the community.
Overall, the paper provides interesting counter examples for the SGD with constant step size (that relies on a relative noise model that diminishes at the critical points), which provide critical (counter) insights into what we consider as good convergence metrics, such as expected norm of the gradient. 

The initial submission took a controversial position between the mathematical statements and the presentation of the statements on the behavior of the SGD method in non convex optimization problems. While the mathematical is sufficient for acceptance at ICLR, the presentation was inferring conclusions that could have been misread by the community.

I am really happy to state that the review as well as the rebuttal processes helped improved the presentation of the results that I am excited to recommend acceptance.
The paper proposes to extend mirror descent to sampling with stein operator when the density is defined on a constrained domain and non euclidean geometry. All reviewers agreed on the novelty and the merits of the paper. Accept
The paper proposes using the intermediate representation learned in a denoising diffusion model for the label efficient semantic segmentation task. The reviewers are generally positive with the submission. They like the simplicity of the proposed algorithm. They also like the effort of the paper in verifying the intermediate representation learned by a diffusion model is semantically meaningful and can be used for segmentation. Initially, there was some concern about the size of the validation set, which is addressed by the rebuttal. Consolidating the reviews and rebuttals, the meta reviewer agrees with the assessment of the reviewers and would like to recommend acceptance of the paper.
The paper studies an important newly identified problem in continual learning of rapid adaptation, and proposes the use of a generate and test method to continually inject random features alongside SGD, enabling better learning on non stationary data streams.
Unfortunately the paper remained borderline in the discussions. While reviewers liked the overall research direction and contributions, they also agreed the paper in current form still would benefit from deeper insights into the proposed method, stronger empirical evidence.
Experiments cover broad applications including RL, but don t seem to give very clear advantages over other weight regularization schemes, and other metrics of quality could be added. We appreciate the authors have added additional experiments testing it both for the two important regimes of under  and over parameterized networks, though those can be expanded.
We are sorry that this good paper remained narrowly below the bar in this case, and hope the detailed feedback helps to strengthen the paper for a future occasion.
This review paper presents a way of comparative assessment of continual learning. Reviewers all agreed that this work is interesting, unique with comprehensive coverage of the CL space. The proposed categorization, CLEVA Compass, and its GUI have great potential to facilitate future CL work.
This paper studies the problem of how to collect demonstrations via crowd sourcing for imitation and offline learning. The paper received mixed reviews initially. The reviewers had difficulty understanding empirical results, asked for some more ablations, and were little unconvinced by the proposed usefulness of the collected data. The authors provided a strong thoughtful rebuttal that addressed many of those concerns. The paper was discussed extensively with one of the reviews who increased their score from 3 to 5. Reviewers generally agree that the paper is good but not all reviewers are on board with acceptance. AC recommends accept but agrees with the reviewers and the authors are urged to look at reviewers  feedback and incorporate their comments in the camera ready.
This paper makes use of cross attention transformers to extract invariant features for unsupervised domain adaptation. Combined with pseudo label approaches, the proposed method achieves state of the art performance, possibly because the transformer features are more robust to the noise. In addition, a two way centre aware labeling method is proposed to produce more reliable pseudo labels. 

However, there are some concerns raised by reviewers. After the discussion period, there is still a concern that is not completely unresolved. The comparison with existing methods might not be fair. It is possible that the performance gain is caused by the generally better representation of transformers, which has been shown in supervised image classification.

Overall, the paper is novel and interesting.  I would recommend acceptance of this paper given its impressive performance, but I highly suggest the authors add more ablation studies, for example, compare the proposed transformer and ResNet on a supervised classification task, to further confirm that the performance gain is solely because the transformer is more robust to label noise. The results can be updated in the supplementary. Also, as promised in the discussion, I hope the authors could release their code as soon as possible. This is because the backbone in this paper is totally new, it will be hard for other researchers to achieve SOTA results if they still use CNNs.
This paper considers the exploding gradient problem in RNNs. The proposed network SGORNN can be seen as an extension to the FastRNN model by adding orthogonal weight matrices. 

I recommend rejection for this paper mainly for two reasons. 

First, as mentioned in the review of Reviewer 815o and Reviewer W7nS, adding orthogonal constraints into FastRNN should not be considered as a significant technical contribution. 

Second, more importantly, the experiments of the paper are not that convincing. All reviewers raise concerns about this issue. I also do not see the point of comparing the proposed model with a baseline LSTM model of much larger parameter size. I canâ€™t think of a reason to do so. Also I think the small datasets will not give you a lot of meaningful insights in comparing the models â€“ PTB for example, is a rather small dataset for language modeling and the results presented there are far from well. The numbers look really bad, reflecting the quality of how these experiments are done ( https://arxiv.org/pdf/1707.05589.pdf ).
There were concerns that the paper has a fairly limited novelty, being based on the combination of two known ideas: bucketing and 2 party secure median for distributed learning. Also, the scale of experiments is quite limited. Other issues include the lack of comparison to relevant related work, some doubts on correctness, and issues with independence and scalability that weren t fully resolved. Overall the reviewers felt that the paper shoud not be accepted in its current form.
The three reviewers all felt the paper was above threshold for acceptance to ICLR. To improve the final version, they suggest some additional discussion and experiments may help the paper.
This paper addresses the identification of physical systems defined on graphs. The authors introduce the Adversarial Twin Neural Network (ATN), which consists in augmenting a simple linear model (PNN) with a virtual neural network (VNN). Some regularization terms are used to enforce maximum prediction from the PNN, and to enforce diverse outputs between PNN and VNN. 

The paper initially received tree rejection recommendations. The main limitations pointed out by reviewers relate to the limited contributions, the limiting assumption of using a linear mode for PNN, the lack of positioning with respect to related works, and clarifications on experiments. The authors  rebuttal answered to some reviewers concerned: Rdem1 increased its grade from 3 to 5, and Rdem1 from 5 to 6   although not willing to champion the paper. R8dT9, which provided a very detailed review and feedback after rebuttal still voted for rejection, especially because he was not convinced by the positioning with recent related works and the answers on experiments.  

The AC s own readings confirmed the issues essentially raised by R8dT9 and other reviewers. Especially, the AC considers that: 
  The contributions for driving a proper cooperation between the PNN and VNN models are weak, since it reduces to using simple skip connection and adversarial training. 
  The importance of these aspects have not been analysed in depth in the revised version of the paper, neither theoretically nor experimentally: for example, the difference with respect to [Yin+ 2021] for a proper augmentation, the discussion to alternative methods for representing diversity as done in [Rame & Cord 2021], or the positioning with respect to  Wasserstein distance based objectives.  
  There remains ambiguities in the cross validation process, which have not been addressed in the rebuttal.

Therefore, the AC recommends rejection.
The paper shows that the transfer attack is query efficient and the success rate can be kept high with the zeroth order score based attack as a backup.  Experiments show state of the art results.

Pros: 
  Simple method based on a simple idea.
  State of the art performance.

Cons: 
  Proposal is a straightforward combination of two methods, and therefore technical contribution is marginal.
  The threat model is easy (surrogate can be trained on the same datasets and use the same loss function) and questionable.  Most of the experimental evidence shows that the research for this threat model is almost saturated (and the problem seems almost solved).

This paper got a borderline score with reviewer s concerns above.  I agree with the authors that the simplest method is best among those performing similarly, but the threat setting considered might be not very realistic as the authors admitted.  I see the proposed method a kind of egg of Columbus in a negative sense.  Namely, the authors found a shortcut to win a game that was created and adopted by the community.  Perhaps this paper would give an impact on the small community and would make the community change the game.  But to give an impact to a general audience, the authors should convince that there are some situations where the analyzed thread model is realistic and therefore the proposed method is really useful.  Or, the authors could adjust the thread model to be more realistic.  Serious discussion on the thread model would be a big plus to the marginal technical contributions.

After discussion with SAC, and PC, our conclusion is that this paper effectively tells the community that the benchmark they are using is too simple, which alone is worthwhile publishing because this may move the community forward (even if the community is small).
This paper proposes a novel approach to include graph information into Transformers. Reviewers expressed concerns on 2 main issues  

1) The exact architecture proposed in the paper is not well motivated. In words of one of the reviewers  I still do not understand why the authors learn the spectral GCN filter weights from the attention matrix of the transformer, which can have a completely different sparsity pattern than the input graph, instead of learning the filter weights from the graph itself, e.g., by using a GNN.  . Authors tried to provide an explanation in the response however I think it needs to be made much more rigorous for it to be well motivated.

2) The interplay between existing position encoding schemes and the proposed method. This point also confused couple of reviewers as the empirical results seem to be strongly influenced by the choices of position encoding. Authors, I think did a great job in addressing this concern by providing additional results during the response period. 

Given the weak experimental results and lack of clear motivations I think the paper is not currently ready for acceptance.
This paper presents a way of using multigrid techniques to parallelize GRU networks across the time dimension. Reviewers are uniformly in favor of accepting the paper.  The main strength is that the paper provides a new perspective on dealing with long input sequences by parallelizing RNNs across time. The main weaknesses are around the experiments: only CPU experiments are run, and sequences are not very long (max 128 length). All in all, though, it provides an interesting perspective that should be valuable to the community.
We appreciate the authors for addressing the comments raised by the reviewers during the discussion period, which includes providing more experimental results to address the concerns. We believe the publication of this paper can contribute to the important topic of data augmentation.

The authors are highly recommended to consider all the comments and suggestions made by the reviewers when further revising their paper for publication.
This paper experimentally shows that the commonly used standard solvers such as Nesterov momentum or Adam can achieve the same performance as optimizers such as LARS and LAMB specially proposed for large batches training.

Large batch training is a very important topic, and if the author s argument is true, it might be an interesting discovery.

However, all reviewers were concerned about its limited technical contribution. Not only that, but above all, when tuning the optimizer using the same computational resource (for a new task), it seems unclear whether the standard optimizer can achieve as much performance as large batch optimizers (currently they tune the standard optimizers, fixing the hyperparameter for large batch solvers to match their performances). The authors did not answer the reviewer s questions about it, and they did not answer the reviewer s other questions in great detail. Through discussion among reviewers, all reviews agreed on this concern and agreed to reject this paper.

The quality of the paper will be greatly improved if this concern is resolved.
All reviewers recommended reject, and there were no responses from authors.
This paper proposes a method for time series imputation modelling the spatial dependencies with graphs, focusing on spatio temporal data, where the spatial dimensions are represented by a graph.
The reviewers find the approach novel. The paper is well written and clear. Related work is adequately discussed.
The experiments are convincing.
The reviewers agree that the paper should be accepted.
This work aims at giving a systematic evaluation of different unsupervised domain adaptation methods on time series classification tasks under a fair setting. By providing extensive experiments on various datasets, competitive baselines, and model selection approaches, this paper has the potential to facilitate future research on this topic if the mentioned concerns are well addressed.

After rebuttal and discussion, the final scores were 3/5/5/5/5. AC considered all reviews, author responses, and the discussions, as well as reading through the paper as a neutral referee, and reject the paper based on the following concerns:
+ *Model Selection Criterion*: As stated by the authors, employing labeled target data for model selection will violate the fundamental assumption of unsupervised domain adaptation. However, the proposed Few Shot Target Risk (FST Risk) also requires labeling a few target domain samples. If it is possible, why not directly conduct semi supervised domain adaptation?
+ *Experiment Details*: As a benchmark paper, it is extremely important to carefully design the experiment details to attain promising results. Among these details, a suitable network backbone for time series classification (Is CNN or ResNet 18 the best choice?  Or TCN mentioned by Reviewer f7Xp), large scale datasets with considerable domain gap, and evaluation metrics are the first consideration to attain insightful findings.
+ *Novelty or Interesting Findings*: As pointed out by reviewers, it is obvious that the technical novelty is limited but it may be okay for a benchmark paper if solid/interesting experimental results are observed. However, some of the findings are also fragile and the experiments should be carefully conducted to make them more solid.

In summary, this paper studies a promising research direction of domain adaptation, but the work cannot be accepted before addressing the reviewers  comments. The weaknesses mentioned above will have a high probability of being asked by the reviewers of the next conference. So the authors need to make sure that they substantially revise their work before submitting it to another venue.
This paper proposes a method for inspecting and interpreting the visual representations learned by self supervised methods. 
The method is conceptually simple and intuititive, the authors assume that concept labels for the images are available, and then go on to learn a mapping between the learned image vectors and the human provided descriptions of the images. The key insight is to learn a reverse mapping, i.e., to map label vectors to representation vectors. Specifically, feature vectors are quantized using k means to obtain clusters;  images are labeled (automatically) with a diverse set of concepts from expert models trained with supervision on
external data sources, and  a linear model is trained  to map concepts to clusters, measuring the mutual information between the representation and human interpretable concepts.

Reviewers raised some questions regarding the relation of the approach to topic models, the difference between reverse probing and linear probing, implementation details and computation. The authors addressed reviewers comments convincingly with additional experiments and/or explanations.
The authors study the limiting dynamics of a simple linear regression model. They use an underdamped Langevin equation which is quite common in the literature. Although the reviewers welcome the direction and the attempt to understand the dynamics of a simple model, the novelty of the paper is limited. As an example, the paper shows that the key ingredient driving these dynamics is not the original training loss, but a modified loss. As pointed out by the reviewers, this has already been pointed out in multiple papers. One important problem is the tendency of the paper to oversell the results (including the title), which makes it difficult to clearly separate the contributions made in the paper. After a discussion with the reviewers, the overall feeling did not change.

I can therefore not recommend acceptance. I strongly recommend the authors do a significant rewrite of the paper in order to clearly separate what contributions are truly novel and also improve the discussion of prior work.
In this paper, the authors motivate the paper well by the gap between the upper bound of the popular offline RL algorithm and the lower bound of the offline RL. By exploiting the special linear structure, the authors designed a variance aware pessimistic value iteration, in which the variance estimation is used for reweighting the Bellman loss. Finally, the upper bound of the proposed algorithm in terms of the algorithm quantity is proposed, which is more refined to reflexing the problem dependency. These results are interesting to the offline RL theoretical community. 

As the reviewers suggested, several improvements can be made to further refinement, e.g., 

  The intuition about the self normalization in the algorithm exploited to improve the upper bound should be introduced. 
  The discussion in Sec 3.3t about the insight of the improvement of the upper bound is not sufficient. 
  The extra computational cost about the variance should be discussed.
This paper addresses the reward free exploration problem with function approximation under linear mixture MDP assumption. The analysis shows that the proposed algorithm is (nearly) minimax optimal.  The proposed approach can work with any planning solver to provide an ($\epsilon + \epsilon_{opt}$) optimal policy for any reward function.

After reading the authors  feedback and discussing their concerns, the reviewers agree that the contributions in this paper are valuable and that this paper deserves publication.
I encourage the authors to follow the reviewers  suggestions as they will prepare the camera ready version.
This paper proposes a novel method for improving domain generalization based on the idea of learning different subspaces for each domain. Authors provide theoretical analysis related to their proposal and further evaluate their proposed method on a subset of DomainBed benchmark.
 
 
**Strong Points:**
 
  The paper is well written.
 
  The proposed method is novel.
 
  Authors provide theoretical analysis in support of their proposal.
 
  The theoretical results seem to be correct.
 
  Empirical evaluation shows that the proposed method improves over baselines on a subset of datasets included in the DomainBed benchmark.
 
**Weak Points:**
 
  The complexity of the theoretical results makes it very difficult for the reader to get any intuition about the underlying mechanisms at play.
 
  The theoretical analysis is disconnected from the proposed algorithm. It is hard to see how one could end up proposing such an algorithm following the theoretical results. I suggest that authors would consider reorganizing the paper with less emphasis on the theoretical part, perhaps simplifying the theoretical results and pushing the rest to appendix.
 
  The empirical evaluation can be improved significantly. Domain generalization is a very well established area at this point. WILDS is a carefully designed and well known benchmark and showing improvement in that benchmark would be very convincing but unfortunately authors do not discuss or even refer to it. They instead report their results on a subset of datasets used in DomainBed benchmark. The DomainBed benchmark is less challenging than WILDS but even following DomainBed closely and reporting the 3 evaluation metrics on all 7 datasets would have been satisfying. However, authors only report the results on 3 datasets. Reporting the results on a diverse group of datasets is particularly important in the case of Domain Generalization because we know that many methods are able to show improvements on a few datasets but it is challenging to beat the baselines on a significant majority of datasets.
 
**Final Decision Rationale**: 
 
This is a borderline paper. On one hand, the proposed method is interesting and novel. On the other hand, the theoretical contributions are very limited and the empirical evaluation is not strong enough for acceptance. Given that all weak points mentioned above can be addressed, I recommend rejection and I sincerely hope that authors would strengthen their paper by addressing them before resubmitting their work.
The authors propose two new benchmark datasets CIFAR 10 N and CIFAR 100 N, variants of CIFAR 10 and CIFAR 100 with real world human annotation noise. The benchmark datasets are more realistic (e.g. instance dependent noise) than some existing synthetic benchmarks for label noise. The authors also benchmark several popular baselines on the proposed benchmark

All the reviewers thought that this is an useful contribution to the community and appreciated the detailed author response. The consensus decision leaned towards accept. I recommend acceptance & encourage the authors to address any remaining concerns in the final version. 
Please clarify the license (e.g. MIT license) when you release the dataset.
This paper investigated online safe reinforcement learning problem in the constraint MDP setting. By introducing Safety Agent and Task Agent, the authors translate the RL problem into a Markov game. The AC agrees with all reviewers that there is a lack of theoretical analysis and experimental comparisons with existing benchmarks. It has not reached the bar of ICLR papers.
This paper tackles the contextual bandit problem with general function classes and introduces a novel algorithm called regularized optimism in face of uncertainty (ROFU). Although this is an important and relevant problem, the theoretical contribution is rather weak due to the strong assumptions, which also results in a lack of consistency with the motivation and the empirical settings. Moreover, although experimental results suggest that the proposed ROFU method may have potential, the empirical contribution is unclear as the paper currently lacks a comparison with appropriate previous work. All these concerns were raised in the reviews, but unfortunately, none were addressed in the rebuttal phase.
The paper tackles a very interesting problem in the context of diffusion based generative models and provides empirical improvements. Pre rebuttal, reviewers  main concerns lie in the motivation and clarification of the method, while after rebuttal, all reviewers satisfied the response and gave positive scores. The authors should include the additional results to well address the reviewers  concerns in the final version.
The paper proposes a novel method for (diverse) client selection at each round of a federated learning procedure with the aim of improving performance in terms of convergence, learning efficiency and fairness. The main idea is to  introduce a facility location objective to quantify how representative/informative is the gradient information of a given set of clients is, and then choose a subset that maximizes this objective.  Given the monotonicity and submodularity of the proposed facility location objective, the authors have been able to provide theoretical guarantees. Experimental results on two data sets (FEMNIST and CelebA) show the effectiveness to the proposed approach and algorithm. 

The reviewers had a number of concerns most of which were addressed in the authors response. The reviewers believe that the theoretical results of the paper are incremental given the prior work (see the reviews for more details); however, the reviewers (as well as myself) agree that the proposed method is novel and can provide significant practical advantage. Utilizing sub modular objectives for diverse selection is a well known (and effective approach), but I am seeing it in the context of federated learning for the first time. 

My suggestion to the authors: (i) Improve the experimental section by adding a few more common data sets (such as CIFAR when data is distributed in a heterogeneous manner). CelebA and FEMNIST are not really the best data sets to try in FL (although they are commonly used). (ii) One of the reviewers had several critical comments about the theoretical results, please address those in the updated version. (iii) Please clarify in more detail how the theoretical and algorithmic contributions of there paper go beyond the recent work of (mirzasoleiman et el. 2020); (iv) iIt seems to me that the paper is missing some references on client selection in federated learning. Please revise the related work accordingly.
This paper studies the combination between model uncertainty and data uncertainty based on the spectral normalized Gaussian process. Empirical results show the effectiveness of the proposed method. Overall, the paper is well motivated and well written. However, there are several concerns about the paper. (1) The novelty is marginal. The contribution of combining SNGP and heteroscedastic models into a single model may not be enough. (2) More analyses and insights are needed on why the mentioned two types of uncertainty are complementary. (3) More recent state of the art methods on classification with noisy labels are suggested to be included to interest the readers. There are diverse scores. However, no one wants to champion the paper. We believe that the paper will be a strong one by addressing the concerns.
The submission proposes a model to handle uncertainties in an irregularly sampling time series setting (HetVAE), built on the VAE framework and the previous work on mTAN (multi time attention networks), and introduces components to encode sparsity information and heteroscedastic output uncertainty. The paper is clear, well motivated, and contains extensive ablation studies showing the effect of eaach added components.
I recommend this submission for acceptance.
Summary: Authors present an approach to improve the robustness of vision transformers by mapping standard tokens into discrete tokens that are invariant to small perturbations. Method is applied to a variety of backbone architectures and evaluated on a range of out of distribution forms of ImageNet test set. Significant performance gains are measured across many of these tasks.

Pros:
  Novel, simple, effective approach
  General approach applicable across model variants, complimentary to other methods to improve robustness.
  Comprehensive study, evaluated on many ImageNet robustness benchmarks
  Well written overall

Cons:
  Biggest issue: 3 reviewers point out concerns about validity of claims that ViT architecture is more reliant on local patterns and less on global context. This seems mostly a semantic issue around conjectures about why the method works â€“ it does not invalidate the value of the new approach or its solid results.  Authors have responded to reviewer concerns by changing wording in paper to relax the claims, specifying â€œshape informationâ€ rather than â€œglobal informationâ€.  They have also added experiments to measure shape bias, as defined in prior art, to backup these claims. 
  Paper missing baselines of data augmentation strategies. Authors have responded by including such comparative experiments. 
  Paper is missing ablation studies on changing the type of codebook. Authors have responded by including multiple variations of codebooks, and varying the codebook size.

This paper was a close call based on the reviews. However, in AC opinion, the critiques have been adequately addressed by the authors. This is confirmed by adding an extra expert reviewer to the pool, who agreed with some earlier critiques, and was satisfied with the changes and additional experiments presented by the authors. AC recommendation is to accept.
This paper proposes to transfer the image pretrained model to a point cloud model by inflating 2D convolutional filters to 3D convolutional filters and finetuning the inflated image pretrained model, so that 3D point cloud tasks can benefit from 2D image pretraining. Extensive experiments are conducted to validate the effectiveness of the proposed method. Even though the performance gain using the 2D pre training is notable, the novelty of the paper is limited since inflating 2D model to 3D video action recognition has been studied, and theoretical understanding of the proposed model is lacking. During the rebuttal period, the authors addressed most of the reviewersâ€™ concerns by conducting additional experiments. Even though the performance is compelling, all reviewers agree that the novelty for the paper is limited and the discussion on why this method work is not convincing. Meanwhile, one reviewer points out that some claims made by authors are not well supported. Besides, one reviewer points out that the paper might have a broader impact in a computer vision conference but only provide a limited contribution to the ICLR community. After an internal discussion with reviewers. the AC agrees with the reviewers on their judgments and recommends rejecting the paper because of the limited novelty of the paper.
This paper proposes to repeatedly apply the classifier two sample tests (proposed by Kim, Ramdas, Singh, Wasserman, in 2016, and developed further by Lopez Paz, Oquab, in 2017) for the purpose of detecting covariate shift. The authors propose methods to extend the aforementioned tests to a sequential setting. Overall, the reviewers do not lean towards acceptance, and neither do I. Several constructive suggestions are provided by reviewers, some are summarized below.

The authors claim that sequential tests are not desirable in such a setting, and thus choose to pay a multiple testing price by repeatedly applying a batch test. However, sequential tests are in fact applicable (they will control type 1 error) but may have a worse power if the alternative is not true at the very start   but these were entirely dropped from the simulations; in fact, comparing the increased type 1 error of the authors  approach to the increased type 2 error of sequential approaches may be worth clarifying. 

Perhaps the "right" solution that the authors are looking for could be gotten by converting a sequential test into a sequential changepoint detection algorithm (via repeated application of a sequential test, each started at a new time). Also see "Conformal test martingales for change point detection" and "Inductive Conformal Martingales for Change Point Detection" by Vovk et al., which are currently not cited.
The reviewers identified missing comparisons to existing baselines (Deep RL and other tree based RL methods) as well as simplistic experiments as the main limitations of the paper. While the authors could address some of the issues raised by the reviewers, the missing comparisons and too simple experiments remain. I therefore agree with (most of) the reviewers that the paper can not be published at its current state.
The paper proposes a rotationally equivariant transformer architecture for predicting molecular properties. The proposed architecture demonstrates good computational efficiency and good results on three benchmarks.

All four reviewers recommend acceptance (two weak, two strong), citing the novelty of the architecture, the good computational efficiency of the model and the good empirical results as the main strengths of the paper. The reviewers expressed minor criticisms and recommendations for improvement, some of which were addressed by the authors during the reviewing process, which led to an increase in scores.

Overall, this is a nice contribution of machine learning to science, and I m happy to recommend acceptance to ICLR.
The authors propose a communication efficient distributed LAMB optimizer using a 1 bit compression. This work is similar in spirit to other prior work, eg 1 bit Adam. Although the algorithm works reasonably well it is a bit unclear how much compression is achieved. Overall, the algorithmic novelty is limited, given the prior work, and the benefits of the algorithm don t shine through as the experiments are quite limited in their data sets and models. The theoretical results are also of unclear usefulness due to the assumptions made.
This paper goes beyond the NTK setting in analyzing optimization and generalization in ReLU networks. It nicely generalizes NTK by showing that generalization depends on a family of kernels rather than the single NTK. The reviewers appreciated the results. One thing that is missing is a clear separation between NTK results and the ones proposed here. Although it is ok to defer this to future work, a discussion of this point in the paper would be helpful.
The paper proposes a new goal conditioned hierarchical RL method aimed at improving performance on sparse reward tasks. Compared to prior work the novelty lies in a new way of improving the stability of goal representation learning and in an improved exploration strategy for proposing goals while taking reachability into account.

The paper does a good job of motivating the main ideas around stability and combining novelty with reachability. Reviewers found the quantitative evaluation and the choice of baselines to be good with the exception of not including Feudal Networks which the authors explained was due to poor performance on the hard exploration tasks (something that has been observed in prior work). Reviewers also found the thoroughness of the ablations and insightful visualizations to be highlights. Overall, reviewers were unanimous in recommending acceptance, which I support.
This paper proposes a new  time varying convolutional architecture (ST GNN) for dynamic graphs. The reviewers were positive about the presentation and detailed theory, especially on the stability analysis. The shared criticism was on experimental validation synthetic datasets that the reviewers did not find appealing. The AC believes that while the lacking validation concerns are legit, there is a lack of sophisticated dynamic graph benchmarks in the community yet, so the authors did their best effort to test their method. We thus recommend to accept the paper.
This paper looks into growing neural networks, and finds an improved approach to the initialisations of new layers, viz by maximising the gradient norm.  Simple, straightforward, neat, and no good reason to reject.  It will benefit those who are using growing NNs.
The paper is exceptionally well summarized by Reviewer QC5G which is difficult to improve up on. I will save the readers the effort of reading more text (without adding more substance). The reviewers unanimously rated this paper highly. The discussion has been robust,  enlightening and also has improved the revised paper.
The paper presents an approach to predict relations between node pairs in heterogeneous graphs, with application to recommendation and knowledge base completion. 

The author s approach is to compute similarities between subgraphs that are neighborhoods of nodes where the relation holds or not to score a relation. The authors use graph neural networks to scores these subgraphs. The type of subgraphs that are considered are pairs of nodes, 3  and 4  cyles to make inference and training tractable. The paper lies in the stream of work that combines logical reasoning and neural network, even though in that particular instance it mostly combines graph mining techniques and neural networks. 

The reviewers unanimously liked the presentation of the paper and the high empirical performance. The rebuttal addressed most of the remaining concerns.
This paper uses chemical reaction data as a means to help train molecule embeddings, by requiring embeddings to satisfy known reaction equations. The idea is nice and clear, and the paper includes strong empirical evaluation. All four reviewers agreed the paper could be accepted, with two of them raising their scores after a detailed author rebuttal and discussion, which included additional experiments.
This paper is proposed to address a novel but practical setting that the test set consists of both seen and unseen classes of the training set. To tackle the crucial challenge of distribution mismatch between the inlier and outlier features, the authors proposed a new method named ORCA by grouping similar instances to enlarge the class wise margin for de biasing. The experimental results on ImageNet have shown the proposed ORCA has significantly outperformed baselines in both inlier classification and outlier detection. The whole paper is written with clear logic and is easy to follow. Moreover, such a new setting may bring more inspiration to the community.
In this paper, the authors propose a non parametric approach for learning a two layer neural net. I agree with the authors and reviewers that this is a timely problem. However, the solution in this paper come short of achieving this goal. In particular, the assumtions are very strong and cannot be generalized (e.g., non negativity). The authors also need to better spell out the sample complexity.
While a lot of previous work on emergent communications studies discrete protocols, this work explores a continuous and audio based channel for learning multi agent communication. Reviewers have commented positively on the novelty of the topic. At the same time, there are a number of concerns raised with respect to experimental design and implementation (6auy) and general approach of the topic which, as reviewers t576 and 42Xh point,  doesn t really go deep into the analysis and understanding of the particular experimental setup and findings. So, unfortunately as the papers stands I cannot recommend acceptance at this time. However, given that continuous communication in emergent communication is a somewhat overlooked topic, I would encourage the authors to use the reviewers  feedback and strengthen their manuscript.
This work starts from the observation that maximum likelihood estimation, while consistent, has a bias on a finite sample which is likely to hurt for small sample sizes. From this, they apply Firth bias reduction to the few shot learning setting and demonstrate its empirical benefits, notably relatively to L2 regularization or label smoothing alternatives.

After some discussion with the authors, all reviewers are supportive of this work being accepted. Two are also suggesting this work be featured as a spotlight.

The proposed method is simple, well motivated, and appears to be effective. Therefore, I m happy to recommend this work be accepted and receive a spotlight presentation.
The authors propose a new algorithm for clustering direct networks. The key idea behind the paper is to introduce a new flow imbalance measures and a new self supervised GNN model to solve the task.

Overall, the paper is interesting and it introduces some new ideas although it needs additional work before being published. 

In particular,
  the experiments could be improved by emphasizing more the evaluation on vol_sum/vol_max/etc metrics and by adding additional results on them
  the clarity of the experimental results should also be improved(for example, metrics / claims around Figure 4 still a bit hard to parse)
  finally, the paper would benefit by some theoretical results on the guarantees of the algorithm(most previous work in the area present interesting theoretical guarantees)
The paper proposes a framework for training autoregressive flows based on proper scoring rules. The proposed framework is shown to be a computationally appealing alternative to maximum likelihood training, and is empirically validated in a wide variety of applications.

All three reviewers are positive about the paper and recommend acceptance (one weak, two strong). The reviewers describe the paper as well written and well motivated, and recognize the paper s contribution as significant.

Overall, this is a nice and promising methodological exploration of flow model training that is worth communicating to the ICLR community.
The paper considers the difference between GD and ADAM in terms of implicit bias. It considers a specific distribution and architecture where the two algorithms converge to different solutions while perfectly fitting the training data. The authors highlight the fact that this happens while adding regularization, which does not happen in the linear case.
The reviewers found some of the insights and analysis interesting. However, they also had reservations about the impact of the results given that it is known that GD and ADAM have different implicit biases, and that the distribution appears specifically crafted towards  showing this effect for the architecture studied.
In future versions, the authors are encouraged to better motivate the chosen distribution, use more standard neural architecture (e.g., standard relu), and provide more explanation about the role of regularization in their result.
The paper considers the problem of distributed optimization in the Federated Learning setting in particular when the data in the clients is non i.i.d. The paper points to the problem of catastrophic forgetting during the local update stages to be a cause for the bad training of models and proposes to fix it via introducing a pseudo loss, which are based on adversarial examples of the global model in the previous step and the adversarial examples of the local model at current step. 

The paper s core idea was generally appreciated by the reviewers as well as the favorable evaluation of the proposed method when compared with other existing algorithms. The authors also provided further additional information during the rebuttal period and addressed author comments adding enough justification to the paper to resolve issues in the submission. One lingering issue that remains is the hyper parameter tuning on test set results that was performed. The authors have promised to redo experiments based on validation sets. 

Overall the paper is borderline but I am recommending accept based on novelty of the idea and strong experimental results.
This paper presents an adaptive gradient method for neural net training inspired by L BFGS. All of the reviewers recommend rejection. They raise concerns about the amount of novelty, the clarity of the writing, and the experimental comparisons. I encourage the authors to take the reviewers  comments into account and improve the submission for the next cycle.
This paper proposes to use Anderson Acceleration on min max problems, provides some theoretical convergence rates and presents numerical results on toy bilinear problems and GANs.

After the discussion, the reviewers agreed that this paper makes a nice contribution to ICLR. Some concerns were originally expressed in terms of incrementality of the theoretical results with respect to previous work (KCYs, gBHU), but the authors have well clarified their contributions in the discussion, and have updated their manuscript accordingly. There were also initial concerns about the related work coverage, but this was also properly addressed in the rebuttal, with additional experimental comparisons as well as extended related work section, as well as an additional convergence result for convex nonconcave problems.
This paper is proposed to address the few shot image classification with the help of two newly designed losses. The first loss function is the Proto Triplet Loss based on the revision of conventional triplet loss. Another loss function is based on an inter  and intra class nearest neighbors score to estimate the quality of embeddings. The proposed method has shown its superiority over the baselines on the miniImagenNet benchmark. The major concern of this paper is the novelty that both proposed techniques are not new. Moreover, the baselines are not the SOTAs, and the evaluations on miniImagenNet only are not comprehensive enough. In addition, the authors have not provided any rebuttal to address the reviewers  concerns.
The reviewers were mostly concerned about the practical impact/implications of the proposed methods. There was a long discussion across multiple threads of the benefits of the approach proposed in CNNs vs larger language models, dissecting the benefits in terms of training time (as opposed to memory or FLOPs, which may have a non linear impact on running time). Overall, the authors did a good job of putting their contribution into context and addressing the reviewer concerns.
The papers makes progress on the important question of implicit bias in gradient based neural learning. Remarkably they derive reasonable conditions for global optimality.
The reviewers had a number of concerns which seem to remain after the authors response. In particular, the reviewers were concerned about the validity of the paper s assumptions in real world applications and lack of experimental results. Also, while the reviewers acknowledge the novelty in technical contributions, they suggested that the authors explain more clearly how the results of this paper are distinguishable from prior art.
This paper proposes a simple approach to improve the robustness of training a sparsely gated mixture of experts model, which at a high level simply consists in training initially as a dense gated model, to better warm start a final phase of sparse training. Results are presented to highlight the potential benefits of this approach.

The authors have provided a detailed response and updated results, in response to the reviews. Each reviewer has also responded at least once to the author response. Despite that engagement, all reviewers are leaning towards rejection (though there is one reviewer with a rating of 6, they regardless state that "I m confident this will make a great resubmission at a future venue", indicating they actually support rejection).

The reviewers point out that the proposed method is not really novel, pointing to an existing recent paper. Even without that prior work, I would also argue that the proposed approach is conceptually straightforward and has benefits that were fairly predictable and not particularly surprising. Given the generally lukewarm reception from the reviewers, I think there is a legitimate concern to be had here about this work s potential for impact.

Though the review process has definitely improved the paper s manuscript since its submission, I unfortunately could not find a reason to dissent from the reviewers  consensus that this submission is not ready to be published. Therefore recommend it be rejected at this time.
A GNN model is developed for the supervised, real time learning of optimal solutions for an order fulfillment problem. GNNs with fast forward computations are naturally one good choice given the real time nature of the problem.

While the complexity of the problem and formulation were generally appreciated by the referees, there were major concerns about the experimental setup, datasets, technical claims, sample complexity, and suitability for ICLR. Overall, the paper does not seem ready for publication in ICLR, and the authors are encouraged to consider and work on the reviews carefully.
This paper extends the symbolic representation learning work of Konidaris et al. (2018) to be object centric, and generalize in this respect. All the reviewers agreed that this is an interesting problem, and that the approach is novel.
Two reviewers gave positive evaluations (6,8), and one reviewer gave a mildly negative review (5), where the main critique is that the method still requires some human effort in designing the planning domain.
While completely alleviating human efforts is definitely a good goal to pursue, I believe that it s too high a bar to ask for in the setting of limited data, and I believe that there are many real world problems where requiring some human effort is not too limiting. 
Therefore I recommend acceptance. 
Please take all reviewer comments into account when preparing the final version.
This paper presents a query embedding approach for answering multi hop queries over a hyper relational knowledge graph (KG). The main contributions are a new dataset (WD50K QE) for this task and a simple but sensible extension to an existing model for query embeddings to also handle relation qualifiers. Reviewers wJVm and Bute note that the reification and StarQE models perform similarly. While this is not a negative result, as the authors note, it does raise the question of the relative pros and cons of the two methods. I hope the authors can add a discussion of when one might prefer StarQE over the conceptually simpler reification method in the final version. The authors addressed Reviewer frRtâ€™s concerns about faithfulness and backwards compatibility (though more evidence on purely triple based tasks would be nice here). Reviewer GQAR also raised some concerns about writing, but the other reviewers mostly found the paper to be well written and well motivated and I tend to agree. Overall, while there are some very good suggestions on how the paper can be extended and improved, I find the current contributions to be substantial enough to warrant a publication.
This paper proposes a new bilinear decomposition for universal value functions.  The bilinear network has one component dependent on state and goal and another component that depends on state and action.  The experiments with the DDPG algorithm in robot simulations show that the proposed architecture improves performance data efficiency and task transfer over several baseline algorithms, including improvements on earlier bilinear decompositions.

The reviews noted several aspects of the paper could be improved, and the author response addressed several of these concerns. Multiple reviewers appreciated the insights from the experiment added in section 4.5 on a simple grid environment, which enabled a direct interpretation of the vector fields used in the method.  Several aspects of the presentation were clarified based on the reviewers comments. Additional details were also provided on the problem specification and the solution methods. During the discussion, the reviewers agreed that the revised paper presented a useful addition to the literature.

Four knowledgeable reviewers indicate to accept the paper for its contribution of an effective network architecture for a goal conditioned universal value function approximator.  The paper is therefore accepted.
This paper received 2 marginally below and 1 marginally above ratings. We discussed the paper with the reviewers and there was broad consensus that 1) the paper lacked clarity; 2) multiple modeling choices were debatable (e.g., ordering or embedding of neurons and convolution over neurons!!) and not sufficiently justified (and these choices will critically impact the conclusions drawn from the analysis); 3) we were not convinced by the relevance of the synthetic data to reflect a meaningful biological process; 4) we did not see any meaningful knowledge gained for biology from this whole analysis. My recommendation is thus to reject this paper.
The paper studies the convergence of a generalized gradient descent ascent (GDA) flow in certain classes of nonconvex nonconcave min max setups. While the nonconvex nonconcave setups are computationally intractable and GDA is known to converge even in some convex concave setups, the paper argues that (generalized) GDA can converge on what is dubbed "Hidden Convex Concave Games" in the paper and argued that it contains GANs as a special case.

The reviewers all found the paper interesting and a worthy contribution to the literature on nonconvex nonconcave zero sum games. Main concerns expressed by the reviewers were w.r.t. the lack of convergence rate established for the considered dynamics, novelty compared to existing work, and practical usefulness of the considered scheme, as it involves preconditioning/matrix inversion. The authors made an effort to address all the concerns, to the extent possible.

Given the complexity of nonconvex nonconcave min max setups, their importance in GAN training, and the insightful perspective on hidden convexity/concavity in typical problem instances, I would like to see this paper published at ICLR. I would, however, strongly advise the authors to take all of the reviewers  comments into account when preparing a revision.
The paper proposes a MLP based architecture that makes extensive use of the shift operation on the feature maps. The model performs well on several vision tasks and datasets.

The reviews are mixed even after the authors  response. Main pros are that the proposed architecture is elegant and reasonable, and the experimental evaluation is thorough and strong. The main con is that the novelty is somewhat limited to some prior papers.

Overall, I recommend acceptance. The reviewers point out that the architecture is good and the results are strong. Similarities to prior works do not seem serious enough to warrant rejection   even an author of arguably the most related (concurrent) works   S2 MLP and S2 MLPv2   confirms that there is sufficient difference. Moreover, this is one of the first papers to show very strong results on detection and segmentation.
This paper presented a domain transportation perspective on optimizing recommender systems. The basic motivation is to view recommendation as applying some form of intervention, implying a distributional shift after the recommendation/intervention. Distribution shift brings tremendous difficulty to traditional causal inference or missing data theory perspective of recommender systems as it violates the distributional overlapping assumption: in simple terms, if the model recommends radically different set of items, there isn t much you can say about its generalization ability; on the other hand, if the model only recommends items that it already observed during training (no distribution shift at all), it would inherent all the biases which already exist in the data. To that end, this paper proposed a domain transportation perspective by introducing a Wasserstein distance constrained risk minimization to find interventions that can best transport the patterns it learns from the observed domain to the post intervention domain.

The paper received overall borderline scores. All the reviewers acknowledged that the proposed perspective is novel and has the potential to spark a new direction for future work. The reviewers raised concerns, ranging from the bounds in the paper, sensitivity of the optimization w.r.t. the hyperparameter, to some relevant but missing baselines. The authors provided very detailed response and revised the paper quite substantially to address most of the feedback. I also read the paper myself given the borderline scores, and I think the authors did a reasonably good job improving the paper and I agree this paper provides an interesting and novel perspective on viewing recommendation, though I also agree with one reviewer that the idea of "partially extrapolation" can be further explored. 

My major complaint is around experimental evaluation. It seems to me that only the semi synthetic experiment actually makes sense in this context (where the measure is based on the unobserved relevance as opposed to observed click), as the traditional random split on clicks evaluation would inevitably favor models with little distributional shift (the training and test data essentially come from the same distribution, maybe not so with a sequential setting but still close). Furthermore, the inclusion of Yahoo R3 and Coat dataset is even more confusing, as the associated test set implies random exposure which is certainly not what this paper aims to address, unless I am missing something in which case more clarification would be nice. 

My overall assessment of the paper is still leaning towards positive but I also wouldn t be too upset if this paper doesn t end up making it. However, if accepted, I do want the authors to carefully revise the presentation of the experimental results for the final version.
This paper has been independently assessed by three expert reviewers. The results place it at the borderline of acceptance decision: while one of the reviewers gave it a straight accept evaluation, two others assessed it as marginally rejectable, even after discussion with the authors. All of the reviewers agreed that the theoretical results provided should help promote the use of MLE estimators over perhaps more prevalently used in current practice TMO, and that is the main contribution of this work. The reviewers were concerned with the clarity of the presentation and with a confusing notation used. Some of these issues have been addressed in the authors  responses. All things considered, I conclude that this work can be of some interest to the ICLR audience, and as such it can be assessed as marginally acceptable for this conference: "accept if needed". I will recommend it as such for consideration by the Senior Area Chair and the Program Committee.
The paper proposes a new framework to express and analyze embedding methods based on the stable coloring problem. Reviewers highlighted as strengths that the paper provides an interesting perspective for understanding one of the central approaches in NLP, graph learning, and other fields   and as such could inspire promising research directions. However, reviewers raised concerns regarding the significance of contributions (theoretical insights and analysis, relation to prior work, missing empirical evaluation etc.) as well as the clarity of presentation (also with regard to correctness and scope). All reviewers and the AC agree that the paper is not yet ready for publication at ICLR and would require an additional revision to address the aforementioned issues.
This paper surveys a collection of existing works that the author frames as evidential deep learning.

While the paper has been recognized as a nicely written survey, all reviewers have raised the major concern that the paper does not have a sufficient academic contribution compared to the surveyed papers. In particular, novelty appears to be limited as the paper does not offer novel views into the surveyed subfield.

Given the strong consensus among reviewers, I recommend rejecting this paper.
The reviewers all generally appreciated the idea in the paper. However, the nature of this contribution necessitates an empirical evaluation, and the reviewers generally found this to not be sufficient convincing. My assessment is that this idea can likely result in a successful publication, but will require additional empirical evaluation and analysis as suggested by reviewers. While the authors did add some additional results during the response period, they do not seem to be sufficient to fully address reviewer concerns.
This paper explores prototype vs linear classifiers for few shot learning. It has been found that pre training a classifier network, followed by training a linear head can produce competitive results to meta trained prototypical networks. A natural question therefore is whether one can directly derive prototypical classifiers from pre trained classifiers. Naively applying this idea doesnâ€™t work well in practice though, and this paper performs a theoretical investigation to determine why. The theory is a generalization of Cao et al., 2020, that doesnâ€™t require assumptions on the class conditional distributions. The theory suggests that the variance of the norm of the feature vectors plays a role, so the paper explores a few feature transformations to reduce this. It demonstrates on a few benchmark datasets that transforming the feature vectors can indeed allow us to create prototypical classifiers from pre trained networks. As a minor quibble, the paper twice refers to â€œdirection of the norm of class mean vectorsâ€   This should just be the direction of the class mean vectors, right? Norm is not a direction, itâ€™s a magnitude.

During the discussion phase, a number of questions arose, mainly around the clarity of the presentation and a request for a few additional baselines (e.g., L2 normalization combined with LDA/V N). These points were resolved by the authors. The main outstanding issue is whether there is enough novelty/significance in the paper to merit acceptance, and on that point, the reviewers felt this is borderline. On the one hand, the theory is more general and does directly point to aspects of the feature space that can yield better generalization results. On the other hand, tricks like L2 normalization are already known, and the utility of prototype classifiers over linear classifiers like SVMs is unclear.

After careful consideration, further discussion with the reviewers, as well as the program committee, it was generally agreed that this paper does not quite meet the bar in terms of the novelty or significance of its contribution. The authors mentioned time and space benefits relative to fine tuned classifiers in their response, and I think one way to improve the paper would be to demonstrate this advantage in a real world application or challenging scenario.
This paper presents work on action anticipation.  The reviewers appreciated the message passing based method.  However, concerns were raised regarding novelty, effectiveness, presentation, empirical results, and magnitude of impact for ICLR.  The reviewers considered the authors  response in their subsequent discussions but felt the concerns were not adequately addressed.  Based on this feedback the paper is not yet ready for publication in ICLR.
The paper proposes to substitute the gradient in the second moment estimation term with the "momentumized" version, arguing that it improves both optimization and generalization. Some theoretical results are shown as well as empirical results.

The paper has been widely discussed by the reviewers and several weak points have been raised. Let me list some of the most important ones.
  The theory appears to be incremental and overall very weak. The authors themselves acknowledged that this "is not a pure optimization theory paper". In details, the generalization analysis is a straightforward extension of Zhou et al. [NeurIPS 2020], while the optimization analysis inherits all the known weaknesses of previous similar analysis in deep learning optimization papers. In particular, *none* of the following is correct: the use of a regret analysis for a stochastic non convex optimization algorithm, the assumption of bounded iterates, Assumption 5, the assumption in Theorem 2 on $\alpha_t/\sqrt{v_t}$. The fact that similar mistakes were done in previous papers does not make them correct: The community should aspire at doing better not at reiterating known mistakes.
  On $\epsilon$: the reviewers correctly pointed out that moving $\epsilon$ under the square root and not changing its value is not fair. The answers of the authors on this point were unconvincing.
  Doubts on empirical results: it seems that not all the possible hyperparameters of the baselines were properly tuned. For example, despite being common practice, epsilon should also be tuned, see for example the experiments in Agarwal et al. 2020.

I didn t consider the discussion on AdaBelief because only marginally relevant to this paper.

Overall, the paper does not seem interesting from a theoretical point of view and its empirical comparison cannot be fully trusted for the presence of some weaknesses.
The submission received split reviews: two reviewers recommended weak accepts, and the other two weak rejects.  The AC went through the reviews, responses, and discussions carefully.  The AC agrees that this paper is well written and has demonstrated the possibility of using transformers for particle based physical simulation.  The AC also believes that the authors have addressed the concerns of reviewer dYcg, despite that the reviewer didn t engage in the discussions.  The contributions are however not most exciting, and none of the reviewers would like to champion the submission.

Further, the AC agrees with the knowledgeable and responsible reviewer ZsKn that the presentation and experiments can be better positioned to highlight the key contribution. As reviewer ZsKn has summarized, it s recommended that "the authors took the approach of integrating the different parts of the newly proposed layer into existing architectures (possibly including non simulation settings), and try to understand better that way how the new layer may help in a more apples to apples comparison."

The recommendation is reject, and the authors are encouraged to revise the paper for the next venue.
The paper revisits importance sampling as an approach for combating distribution shift when training over parameterized neural networks. Contrary to recent results that suggest that importance sampling is perhaps incompatible with over parameterization, the authors find that the exponential tail of losses such as the logistic loss is the root cause. For polynomial tailed losses, authors analyze gradient descent on importance weighted polynomially tailed losses and demonstrate the advantage of importance sampling in a label shift setting.

There paper is well written and the results are sound. Overall, a good paper.
This paper experimentally investigate the inductive bias of deep neural networks tending to produce low rank embeddings of data, which is important to explain why over parameterized DNN can generalize. In particular, the paper empirically finds that deeper networks are more likely to produce lower rank embedding through thorough numerical experiments with different network architectures, hyperparameters and so on. The authors also proposed a linear over parameterization technique to induce low rank bias and empirically justifies its effectiveness. 

Overall, this paper is well written, and the numerical experiments are carefully executed. However, the main drawback of the paper is that the low rank inductive bias itself is a well known phenomenon and this paper gives a kind of additional confirmation to it. I acknowledge that there are several differences from existing papers, but overall we see rather limited insight from the results. Indeed, some of existing studies gave theoretical analyses to understand "why it happens", but this paper does not give a sufficiently novel insight to reveal the reason. 

To summarize the decision, this paper lacks deeper insight compared with existing work although the authors did a good job to execute through experiments. Therefore, it is a bit below the acceptance threshold.
This paper presents a new perspective for understanding reinforcement learning policies based on meta states, as an effort to improve the explainability of RL control policies. After reviewing the revised paper and reading the comments from the reviewers, here are my comments:
 
  The paper is well written and very concise.
  The strategy is novel and deserves merit.
  The utility of the explanation is not well described.
  The main concerns of the proposal are the utility of the explanation (that is not well described) and its usage in large discrete state spaces or continuous state spaces domains. 
 
From the above, it is difficult to see the contribution and applicability of the paper in a clear manner.
Paper studies if DNNs are modular and proposes statistical methods to quantify modularity.
cluster the neurons of the network using spectral clustering applied to a graph that is weighted by similarity between the neurons.

While the reviewers find the question of modularity relevant, they raise the issue that the results are inconclusive regarding the main stated contribution of the paper (i.e., if modularity is appropriately measured). After discussion, some concerns are answered. However, the main problem of inconclusive results stands. Therefore, this borderline paper is rejected.
In the paper, it introduces a forget and relearn framework to the iterative learning algorithm. It provides serval new insight that forgetting could be favorable to learning and validates the insights via image classification and language tasks. The idea is novel and inspiring. Although there are some debates on the experiment and the generality of the proposed method, I think authors answered those questions decently and many researchers would be interested in this direction.
The paper studies the noisy labels problem in semi supervised node classification and proposes a method that leverages pairwise interactions to explicitly force the embeddings for certain node pairs to be close to each other leading to better robustness.

The reviewers agreed the proposed method is promising. However, the reviewers also had concerns about the novelty, and that certain aspects of the method could be justified better and the experiments should consider larger scale settings to make the paper more convincing. These were the key reasons for rejection.
The paper proposes an efficient attention variant inspired by quadtrees, for use in vision transformers. When applied to several vision tasks, the approach leads to better results and/or less compute. 

The reviews are all positive about the paper, after taking into account the authors  feedback (one reviewer forgot to update their official rating, apparently). They point out that the idea is reasonable and the empirical evaluation is thorough and convincing, with good gains on several tasks and datasets.

Overall, I recommend acceptance.
The authors introduce the Time Aware Multiperistence Spatio Supra Graph CN that uses 
multiparameter persistence to capture the latent time dependencies in spatio temporal data. 

This is a novel and experimentally well supported work. The novelty is achieved by combining research in topological analysis (multipersistence) and neural networks. Technically sound. Clear presentation and extensive experimental section.
Reviewers were uniformly positive, agreeing that the approach was interesting and well motivated, and the experiments convincing. Some concerns that were raised were successfully addressed by the authors and revised in the manuscript. 

Happy to recommend acceptance. A veru nice paper!
All of the reviewers appreciate the clarity of exposition and the importance of the problem studied. That said, I agree with Reviewer P9Ys that the results are somewhat underwhelming. The baselines appear weak and are likely not well tuned on the Stanford car dataset. Key question that remains unanswered in my opinion is whether this is the most effective approach to using synthetic data to improve classification accuracy (e.g., in contrast to [Ravuri & Vinyals, 2019](https://arxiv.org/abs/1905.10887) and follow up work). Nevertheless, I believe the community will benefit from this paper s contributions and this line of work.
The paper presents an unsupervised method for learning Full Waveform Inversion in geophysics, by combining a differentiable physics simulation with a CNN based inversion network.

The reviewers agreed that the paper was well written and described an important advance but were concerned about limited novelty and a potential sim2real gap. The authors responded to their critique with significant new experiments and clarified the novelty of their method relative to prior work.

Based on the author responses, I recommend acceptance.
The reviewers were split, with one of them leaning towards rejection, primarily due the (perceived) limited impact of the study. I tend to agree with the other reviewers that this paper provides an interesting and original framework for analysis of learning models, and while there are substantial shortcomings, they are outweighed by the positives (including the promise this approach may hold for analysis of learning in more realistic scenarios). I therefore recommend acceptance, if space in the proceedings allows.
Description of paper content:

A mixed theoretical and experimental paper that investigates the robustness of distributional RL to perturbations of state observations as compared to expectation based value function learning. They provide sufficient conditions for TDâ€™s convergence and prove the Lipschitz continuity of the loss of a histogram based KL version of distributional RL with respect to the state features, whereas this is not true for expected RL. This continuity indicates a certain robustness of the loss with respect to perturbations of the state. The theoryâ€™s tie to experiment is weak in the sense that it is not predictive of the actual performance of any algorithm. The theoretical methods are based on a previously published paper SA MDP.

Summary of paper discussion:
The reviewers raised concerns about the statistical significance of the experimental results, the clarity and organization of the writing, the novelty of the theoretical setting, and its usefulness for describing a real problem setting. The majority of reviewers rejected the paper and did not lift the scores after the rebuttal. 

(I personally wonder if the community would not benefit from conducting some of these kinds of theoretical analyses and experiments on LQR systems rather than Atari (etc.) environments.)
Summary of the paper: This work considers the problem of generating sets and graphs conditioned on a latent representation (a.k.a. one shot set generation) and makes two contributions.

First, it provides sufficient conditions for a learning algorithm to be able to handle permutation equivariance (the (F, l  ) equivariance). Second, it proposes Top n, an approach for set generation that builds on the set generation method proposed by [1]. Top n first generates an "angle vector" from a latent representation of the query vector, and then uses cosine similarity to select the closest n elements from a learnable reference set. 

The authors compare their method with competing methods for set generation, including MLP based generation, random iid generation, and First n creation. Their approach improves over these baselines on SetMNIST, synthetic molecule like 3D structures, and the QM9 dataset. 

Summary of discussions: The authors engaged extensively with the reviewers during the response period and were able to address significant reviewer concerns. While the reviewers are overall positive about the paper, it is expected that the authors will address some major concerns in the final camera ready version. These include the lack of experiments on tasks used by [1], comparison with recursive methods, and discrepancies in TSPN results. Finally, the utility of the (F, l) equivariance is unclear, as most existing generative models already satisfy these conditions (as mentioned in authors  discussions with reviewer tgJk). Thus, the authors should adjust their claims accordingly and add necessary clarifications.
Description of paper content:

The paper studies the problem of achieving coordination among a group of agents in a cooperative, multi agent task. Coordination graphs reduce the computational complexity of this problem by reducing the joint value function to a sum of local value functions depending on only subsets of agents. In particular, the Q function of the entire system is â€œexpandedâ€ up to second order in agent interactions: Q   \sum_{i \in [n]} q_i + \sum_{(i,j) \in G} q_{ij}, where the q_i is function of the i th agentâ€™s history and current action, and q_{ij} is a function of two agentsâ€™ histories and current actions. As G does not include higher order (third and above) terms, the algorithm does not have exponential dependence on the number of agents. If G includes only a subset of pairs of agents, then the computational complexity is reduced to less than quadratic. Since the coordination problem is cooperative, the authors propose a meta agent (â€œcoordinatorâ€) that selects the graph G in a dynamic (state by state) fashion in order to maximize return. The optimization problems of the meta agent and the sub agents are performed by deep Q learning.

Summary of paper discussion:

The critical comment made by one reviewer was: â€œGoing back on that trend now only to pursue the polynomial time nature of the running algorithm would in my opinion require far more diverse evaluation examples, backed by a stronger motivation highlighting real world threats of all the other MARL algorithms taking longer than polynomial time. As is, SOP CG does not contend amazingly against other MARL algorithms that chose the "NP hard? Curse of dimensionality? Fine. We ll approximate, approximate, approximate." path rather than the "Polynomial time is our topmost priority; function expressiveness can wait." path. That leads me back to the question of why pursue polynomial time at the cost of losing both the function expressiveness and the peak performanceâ€¦.â€

Comments from Area Chair:

Looking at the experiments, the number of agents in the empirical problems is not large. For example, there are 15 agents in "Sensor." Any focus on computational complexity at this scale is hard to justify, especially with algorithms that are approximate. It seems favorable at this small scale to use function approximators that can take in all the agents  histories and actions. This obvious baseline is not included in comparisons. It is hard to justify inclusion of this paper in the conference.
The paper investigates the use of equivariant neural network architectures for model free reinforcement learning in the context of visuomotor robot manipulation tasks, exploiting rotational symmetries in an effort to improve sample efficiency. The paper first provides a formal definition and theoretical evaluation of a class of MDPs for which the reward and transition are invariant to group elements ("group invariant MDPs"). It goes on to describe equivariant versions DQN, SAC, and learning from demonstration (LfD). Experiments on a set of different manipulation tasks reveal that the proposed architectures outperform contemporary baselines in terms of sample complexity and generalizability, while ablations demonstrate the contribution of the different model components.

The idea of structuring a neural architecture to exploit symmetry present in a domain as a means of improving sample complexity is compelling and principled. The contributions of the paper are two fold. First, while the idea of exploiting symmetry in the context of deep RL is not new, the paper describes a variation of equivariant DQN that is effective for visual control domains (visuomotor control) that are more challenging and realistic than those considered previously. Second, the paper proposes novel equivariant versions of SAC and LfD and validates their effectiveness through extensive experiments. Following a detailed author response to the initial reviews together with the inclusion of additional experiments and other updates to the paper, the reviewers largely agree on the significance of these contributions and value of the paper as a whole.
This paper points out connections between the self attention module in transformers and some prior art, including kernel regression, the non local mean algorithm, locally linear embeddings, and the self expression algorithm for subspace clustering. Based on these observations, the authors argue that the innovation of self attention is not modeling the long range relation, which is also proposed in prior work, but the learnable parameters and the multi head design. The authors also suggest several directions for future work, such as using self attention for manifold clustering.

Reviewers pointed out several weaknesses with this paper: that some connections (e.g. connection to kernel regression) had been pointed out before, that the relation between self attention and locally linear embedding and self expression in subspace clustering is a bit nuanced, as pointed out by one of the reviewers, and that while some speculative future directions might be interesting, the paper falls short in actually trying some of them out empirically, or building a proof of concept. 

In the discussion period, the authors pointed out that this is a position paper (which unfortunately was not expressed so assertively in their submission), which according to their view liberates them from digging deeper and test empirically some of these connections and speculative directions. According to the authors, a core contribution of their position paper is that "it expresses the opinion that the original attention paper failed to cite and acknowledge that attention mechanisms build upon a series of prior works in sparse coding, subspace clustering, and locally linear embedding." 

There are no specific guidelines to review position papers at ICLR that I know of, but I will base my assessment on the assumption that a good position paper should:
  provide a good historical perspective of a subject
  connect previously unrelated lines of work in non obvious ways
  inspire the research community to look at new directions.

While a good position paper can be extremely valuable and enlightening, I am not convinced that this particular paper achieves either of the goals above, and therefore it is my opinion that it does not deserve publication at ICLR. 

As pointed out both by the authors and the reviewers, the connection between self attention and kernel regression and non local mean denoising is not new, and so it is not an original contribution of this paper. The relation between self attention and locally linear embedding and self expression in subspace clustering appears to be new, but this relation is a bit nuanced, as pointed out by one of the reviewers. 

The tone of this position paper is that some of these connections were missing in the original attention paper   the authors say "attention did not properly acknowledge prior art" in one of their responses (it is not clear if they are referring to Bahdanau et al. s attention paper or to Vaswani et al. s transformer paper). However, the historical perspective of how attention mechanisms came to be seems to be missing from this position paper   attention has been proposed by Bahdanau et al. for machine translation, inspired by the idea of word alignment that has been prevalent in machine translation for decades. Later, in the transformer paper, self attention was suggested as an alternative to recurrent and convolutional models for machine translation (note that self attention has been used before the transformer paper, see e.g. [1]). While a theoretical connection with kernel regression etc. exists, this was not related to the original motivation of these works. There are many ways of arriving at the same construction! And given the simplicity of attention mechanisms it doesn t surprise me that connections with other lines of research exist. Had they been noticed, they would probably be a parenthesis in the original papers, because attention is derived there in a much more direct way (this doesn t mean that the connections aren t interesting, but that they are not _essencial_ to the construction). 

In their response, the authors dismissed a constructive suggestion from one of the reviewers which in my opinion would have strengthen this paper   the connection with graph neural networks. If the point of the paper is to point out past research that connects fundamentally to the idea of attention mechanisms, why leaving this out?

In sum, in my view this paper lacks the rigor, the insight, and the historical perspective that should characterize a strong position paper, and as such I cannot recommend acceptance. I strongly suggest that the authors take into account some of the insightful suggestions given by the reviewers in future iterations of their work. 

[1] Ankur Parikh, Oscar TÃ¤ckstrÃ¶m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.
The paper proposes Virtual MCTS, an early termination rule for MCTS to improve its efficiency. 
The basic idea is to introduce a termination rule that prunes the search process when the final policy at the root node is unlikely to change from the current one. The proposed approach is empirically evaluated on 9x9 Go and Atari games.

After reading the authors  feedback, all reviewers participated in the discussion without reaching a consensus.
Although all reviewers appreciated the authors  answers to their concerns, only one reviewer voted for acceptance. The other two reviewers,  while acknowledging some merits, still have concerns: the technical contribution is minor, the theoretical findings are quite trivial, it is unclear when the proposed termination strategy is could help.
In summary, this paper is borderline and I think it still needs some work to clearly break the bar of a top conference.
All reviewers recommended reject. No responses from the authors.
This work describes a two stage method for learning with noisy labels. The crux of the reviews, discussions with the authors and post rebuttal discussions between reviews (and myself) was related to the novelty of this work. The main concern is that while this body of work presents a relatively solid method (from an empirical point of view), the underlying components are not altogether that novel, and have been used in the context of learning with noisy labels before. Fundamentally, the proposed S3 method did not feel *convincingly* better, given its relative lack of novel technical insights. I appreciate that this is a frustrating reasoning to get   after all, much of what we do in empirical ML is combinations of existing things. Ultimately, there was consensus amongst the reviewers that the work did not have sufficient insights or such outstanding empirical results so as to overcome this relative lack of technical novelty. 

All the reviewers have engaged meaningfully in discussions, provided constructive feedback and I hope that this will make subsequent iterations of this work better in many dimensions.
This paper tackles the open set recognition problem, specifically the subset that looks at rejecting test data that with unknown classes that are related to the training data. The proposed approach uses an existing distance based classifier (based on LDA) combined with a new background class regularizer. Results, comparing to a few prior OSR methods, are shown across image/text datasets. 

  The reviewers gave a mixed set of scores, with concerns about visualization/ablation studies and the lambda parameter with affect on classification accuracy (1wRX, ujMG, rop6), computation complexity and efficiency (1wRX), limited novelty and discussion of relationship to prior works (Mkdh, rop6), and limited comparison to state of art as only a few algorithms are compared to the proposed approach (Mkdh), and initialization method. Notably, the authors make a strong claim for the latter point that the method should only be compared to previous BCR methods (as opposed to softmax based classifiers, for example); this seems to ignore whole classes of different methods that can approach the OSR problem. While it is true that comparing to previous BCR methods can directly show your approach is superior to them under similar class of algorithms (thereby showing that it is an improvement), putting the method within the context of the entire literature is absolutely necessary to discuss relative impact to the field. For example, the improvements in AUROC are not that great (and in some cases worse) than even the methods you compare to, while OSCR is improved significantly, so it is not clear how it stacks up with respect to the current state of art. Even if it doesn t beat it, you could argue your contribution, but not presenting it all prevents a holistic perspective that is necessary. 

  The authors provided thorough rebuttals, including additional ablations and experiments. However, after the review period the scores remain mixed (5,5,6,8) and the reviewers expressed remaining concerns about novelty and comparison to the current state of art (not just BCR based methods). As a result of these remaining concerns, I recommend rejection at this time.
The paper presents a prompt learning method for few shot learning in NLP.  In particular, they proposed DART, a new soft prompt tuning method, to optimize the label representations and template. 

Overall, the paper is well written and well motivated. The proposed approach is interesting. The experiments were well justified and sufficient experimental analyses are provided. All reviewers support the paper. 

There are a few remaining critics of the papers. 

  The major one is the positioning of the paper raised by the reviewers. I agree with the reviewers that it is a bit misleading to emphasize the approach requires no external architecture. Although the approach can reuse the same transformer architecture (rather than additional LSTM) so that it enjoys the beauty of simplicity, it is still required additional parameters. I would suggest better clarifying this point in the final version. 

  There is also a critic that the paper is related to ADAPET. However, the key ideas in this paper are sufficiently different from ADAPET. Also, ADAPET is published at EMNLP 21 after the paper is submitted, although it was in Arxiv earlier. It s fair to say this work is concurrent with ADAPET.
The authors theoretically analyze the approximation of Korobov
functions by neural networks, obtaining upper and nearly matching
lower bounds, for shallow and deep networks, with different activation
functions.  The bounds are stronger than what can be proved for the
more commonly studied Sobolev functions.  But Korobov functions are a
natural and fairly wide class of functions.  This work makes a
substantial step forward in clarifying what kind of functions are
especially amenable to representation by neural networks.  The
reviewers also appreciated the clarity of the writing.
This paper proposes a novel benchmark for neural architecture search methods, which consists of 25 different combinations of search spaces and datasets. The main motivation is that existing NAS benchmarks, such as NAS Bench 201, consider very small search space and few datasets, such that conclusions drawn with them do not generalize to unseen settings with different search spaces and datasets. The authors first describe the 25 different combinations of the search space and tasks for the given benchmark, and then conduct an extensive empirical study of existing NAS methods and performance predictors with the proposed benchmark, to show that architectures and hyperparameters found with the popular benchmarks do not generalize to other settings, which is consistent with their assumption.

â€”

All reviewers were initially positive about the paper, and remained positive throughout the discussion period. The reviewers found the paper well motivated, and the proposed benchmark useful, as they agree with the need of introducing a single, unified framework that can validate a NAS method under diverse settings, since existing benchmarks only consider specific datasets and search spaces. However, the reviewers were also concerned with the weak technical novelty (Reviewer 2xvD), and that the work lacks deeper insights that could guide the community towards better methods (Reviewer Gku7). 

I also agree with the authors and the reviewers on the necessity of having a unified benchmark that incorporates all different settings considered in the previous benchmarks, and find the extensive empirical study of existing NAS methods useful. 

However, I find the work as rather technically weak as mentioned by R2xvD, since the authors spent too much time describing and showing the limitations of existing benchmark methods, while what is more important for benchmarks, is to justify how the proposed benchmark can evaluate the performance of different methods in a fair manner, while being representative of the practical settings. In short, the authors need to justify their design choices. Yet, the 25 settings proposed in the paper seem to have been arbitrarily chosen, and it is not clear if having a good performance on this benchmark is indeed a fair evaluation, or well reflects how the NAS method will perform in practice. The proposed benchmark also does not really consider a novel search space or setting that have been overlooked in the past either, and does not provide much insights on the problem, as mentioned by Reviewer Gku7. 

Thus, although I recommend an acceptance for its practical value acknowledged by the reviewers, the authors need to put a considerable amount of effort in revising the paper, and If this were a journal submission, the paper may need to undergo a major revision. Most importantly, as described, the authors should justify their design choices as well as whether evaluating a model on the benchmark yields â€œfairâ€ and â€œrepresentativeâ€ results, focusing more on describing the proposed benchmark itself.
The paper addresses the learning of robot controllers for changing or unknown environments. It makes use of differentiable physics for online system identification and of reinforcement learning for offline policy training. A universal controller is trained on a distribution of simulation parameters in order to ensure its robustness. Differentiable physics is used to estimate the simulation parameters from the recent observation history. These parameters are fed to the controller so as to modulate the policy. This approach is evaluated on three benchmarks (2 + 1 added during the rebuttal).

The main originality of the paper is the use of differentiable physics for the identification of the parameters in the context of varying environments. The topic is of interest and in line with the recent developments for robotics. However, the novelty is limited, and all evaluators were concerned about the limited experimental contribution. The authors have added a new experiment during the rebuttal but this was not considered sufficient to change the evaluation. Overall this is considered as a promising contribution, but the experimental setting should be largely improved with additional problems and comparisons with SOTA methods from the recent RL literature.
This is an interesting contribution to the Boltzmann machine (BM) literature that makes a nice connection to DEQ models. On a positive note, reviewers found that it was well written, clear, and interesting. Unfortunately, there were significant concerns with the manuscript that were not fully addressed in the revision: inappropriate or incomplete baselines, insufficient credit given to previous works, and the fact that this model is limited as compared to its BM relatives.

I would recommend that the authors take into account the reviewers  feedback in a revision of the work.
This paper proposes a new approximate sampling approach called Quasi Rejection Sampling (QRS) to exploit global proposal distributions without requiring to know a bound on the associated importance ratio, and providing a trade off between the approximation quality of the
sampler and its efficiency. QRS is demonstrated on EBM based text generation tasks. The reviews acknowledge the simplicity of the approach which when combined with advances in learning proposal distributions opens up many potential applications. At the same time, the reviews indicate that more work could be done to make the empirical demonstrations more compelling, with a more thorough coverage of comparisons with MCMC and other alternatives. The authors are encouraged to revise their submission and clarify significance and novelty.
The paper presents a simple and intuitive method to prune the missing value in the learning and inference steps of the neural networks, leading to similar prediction performance as other methods to impute missing value. It has some really useful insights, but could benefit from one more round of revision for a strong publication: 
1. improving the writing so that its sets up the right expectations on the contributions of the paper; 
2. providing discussions on its connections (and differences) with zero imputation and missing indicator methods; 
3. thoroughly investigating the experiment results to illustrate the advantages of the proposed method.  

The recommendation of reject is made based on the technical aspect of the paper.
 
During the rebuttal phase, the authors misused the interactive and transparent (for the better or worse) openreview system by writing inappropriate comments with personal accusations to the reviewers who write negative reviews. We would like to extend the apologies to the reviewers for this unpleasant experience and thank the reviewers for their engagement and work, as well as their fair assessment of the paper.
This paper proposes a stepped sampler for LSTM based video detection. However, reviewers raised a series of issues of this paper, including the weakness in novelty, experiment evaluations, and generalizability of the method. Considering the limited contribution of this paper, and limited experiment evaluations, the AC agrees with the reviewers and recommends reject for this paper.
I thank the authors for their submission and active participation in the discussions. This papers is borderline. On the positive side, reviewers emphasized this is a well written [ovqB,1zPe] and sound paper [BUDa] with good theoretical [td5N,ovqB,1zPe] and empirical [BUDa,td5N,ovqB] results. On the negative side, reviewers remarked clarity [KyZj,AVki], incremental with respect to Tasse et al (2020) [KyZj], relatively restricted Boolean task algebra [td5N], toyish nature of the environments considered [ovqB], and some missing details [1zPe]. During discussion, the sentiment seems to be somewhat lukewarm with none of the reviewers strongly favoring acceptance or rejection. It seems the main remaining concern is around the toyish nature of the environments used in this paper. I acknowledge that and I believe the authors could include experiments on more complex environments. However, I also give the authors credit for addressing most of the reviewer s concerns during rebuttal and for presenting a solid empirical and theoretical result that the research community can build upon in the future. I am therefore recommending acceptance of this paper and highly encourage the authors to further improve their paper based on the reviewer feedback.
This paper proposes a new method for domain generalization. The main idea is to encourage higher inner product between gradients from different domains. Instead of adding an explicit regularizer to encourage this, authors propose an optimization algorithm called Fish which implicitly encourages higher inner product between gradients of different domains. Authors further show their proposed method is competitive on challenging benchmarks such as WILDS and DomainBed.

Reviewers all found the proposed algorithm novel and expressed that the contributions of the paper in terms of improving domain generalization is significant. A major issue that came up during the discussion period was that we realized that the presented results on WILDS benchmark are misleading. In particular, the following statements in the manuscript are false because on "CivilComments" and "Amazon", Fish utilizes a BERT model (Devlin et al., 2018). However, other methods at WILDS benchmark use DistilBERT (Sanh et al., 2019):

  Section 4.2: "For hyper parameters including learning rate, batch size, choice of optimizer and model architecture, we follow the exact configuration as reported in the WILDS benchmark. Importantly, we also use the same model selection strategy used in WILDS to ensure a fair comparison."

  Appendix C2: "Results: We compare results to the baselines used in the WILDS benchmark over 3 random seed runs in Table 10. All models are trained using BERT (Devlin et al., 2018)."

Authors explained that the mismatch is because at the time they evaluated their model, an earlier version of WILDS benchmark was available but they later updated other methods  results on a newer version of WILDS benchmark. Of course, I do not think that this explanation makes the misleading statements OK. Authors promised to do the following for the camera ready version to make sure it is not misleading:

  Using "Worst U/R Pearson r" as the comparison measure for "PovertyMap"
  Submitting their method to WILDS benchmark making sure everything matches the baselines and then reporting the results on "Amazon" and "CivilComments" datasets.

Therefore, I recommend acceptance and I hope that authors would stick to their promise and update the manuscript to include these changes.
This paper formulates and solves a capacitated vehicle routing problem (CVRP) in the presence of costs for deploying additional vehicles: a mixture of supervised learning, algorithms, and OR techniques is used. In particular, a mix of greedy decoding, repairing of the solution, and post processing with OR tools is used to extract a feasible solution from the probabilistic prediction. 

The paper makes a good case that existing methods do not solve the CVRP with a hard constraint on the fleet size.  On the other hand, there is a strong dependence on heuristic improvements: e.g., a strongly dependence on the post processing, and an additional repair procedure for the decoding process. The authors are encouraged to investigate how such improvements would work with existing approaches: i.e., how novel the new modelâ€™s contributions are.
The paper proposes contrastive learning for tabular data to improve anomaly detection. 
Strengths:
  Interesting and important problem.
  Usage of contrastive learning for anomaly detection in general multi variate datasets is novel (as prior work mostly focuses on images)
  Extensive experiments with comparisons to multiple baselines on multiple datasets
  Well written paper

The reviewers raised some concerns about novelty (in particular, the relationship to the closely related paper "Neural Transformation Learning for Deep Anomaly Detection Beyond Images"), hyperparameter tuning and additional baselines. The authors did a great job of addressing the concerns and multiple reviewers raised their scores. During the discussion phase, the consensus decision leaned towards accept. I recommend acceptance and encourage the authors to address any remaining concerns in the final version.

Additional AC comments:
  Please make sure that the camera ready version does not exceed page limits. https://iclr.cc/Conferences/2022/CallForPapers 
  " As far as we can ascertain, masking was not used for one class classification before.": There s some related work on pre training BERT for OOD detection (cf. https://arxiv.org/abs/2004.06100 or https://arxiv.org/abs/2106.03004) which might be worth discussing.
This paper studies a data driven similarity metric for physical simulation data, based on entropy rate of a physical system. The authors consider a one parameter family of spatial fields obtained by varying certain parameter, and use those in a self supervised setup. 
Reviewers were split in this submission. While some reviewers highlighted the novelty in the problem setup and the idea of considering one parameter families, they also expressed concern about the lack of proper justification of the entropy analogy, as well as doubts on the empirical evaluation. Ultimately, and taking all these considerations into account, the AC believes this work would greatly benefit from another review cycle, by addressing the concerns expressed here. Therefore, the AC recommends rejection at this time.
This submission presents an interesting contribution on differentiable sorting, providing an analysis of monotonicity for these operations.

The reviewers overall argue for acceptance.
The paper describes a new offline meta RL technique that addresses the distributional shift problem with a self supervised online exploration phase where reward labels are not available.  The framework is novel and interesting.  The authors addressed many concerns of the reviewers.  However, the additional experiments raised additional questions.  For instance, why does meta BC perform so well, even better than the proposed method without online data, and other baselines seem not to work at all?  In the discussion, the reviewers expressed concerns about the experimental results in the case of changing dynamics.  Those experiments are questionable since the proposed method only considers the reward information to deal with different dynamics.  Finally, an important question regarding SMAC remains unanswered: how much does the proposed method depend on the quality of the offline dataset and the quality of the reward decoder? Overall, the work is promising and the authors are encouraged to continue their work by addressing the reviewers concerns.
The paper introduces a graph neural network for molecules which takes into account motif level relationships. The paper received borderline reviews, with three reviewers voting for reject, and one for accept.  After the rebuttal, the reviewers did not change their scores. Overall, it seems that the paper has some merit, with good experimental results. Nevertheless, it suffers from two issues (i) the positioning with respect to other motif based approaches is not clear enough, making the novelty hard to assess; (ii) there is a lot of room for improvement in terms of clarity. Therefore, the area chair follows the majority of the reviewers  recommendations and recommends a reject.
While the reviewers appreciated the theoretical analysis performed in this work, some concerns were raised during discussion, such as how relevant the obtained results are wrt current contrastive learning practices, and whether the comparison against a simple auto encoder (basically PCA) is fair or insightful. The authors  response did not satisfactorily address these concerns. Overall, the current work appears to be preliminary, and important questions were left out: (a) how realistic the assumptions are (linear, spike covariance, Bernoulli random augmentation)? (b) performing better than PCA in a specifically designed setting may not be as impressive as it appears; what can we say about the optimality of CL against any algorithm? (c). validation on existing benchmark and CL algorithms would be welcome. The authors are encouraged to revise the current draft by incorporating the reviewers  comments and submit again. In its current form, we believe this work is not ready yet.
Three of four reviewers rated this paper as an 8. 
These positive reviewers felt that this paper provided a lot of value through extensive experimentation with MAML in the few shot setting. It was felt that the detailed analysis of the inner and outer loop of MAML provided a lot of understanding to the reader regarding the behaviour of MAML. The fourth reviewer giving a score of 3 remains concerned about high variance on some experiments. However the strength of ratings from the other reviewers make the AC more than comfortable giving an accept recommendation for this work.
A variational function space prior is proposed, resulting in a variational Dirichlet posterior. After rebuttal, reviewers still had many remaining questions or concerns about the paper. For instance, rF5E outlines several concerns, many relating to factorization assumptions. Reviewer 7nPR also provides several suggestions. I will not repeat them here, but do encourage the authors to look closely at these questions and suggestions. At this particular time the paper is not strongly resonating with reviewers, but could be updated so that the value of the contributions is more obvious.
This paper addresses the challenging application of denoising FIB SEM images. State of the art results are reported on a real and a noisy simulated dataset. Unfortunately, this paper failed to convince the reviewers and received 4 negative ratings. The paper misses critical comparisons against baselines and appears rather limited in scope. The authors failed to provide adequate answers to some of the reviewers  points.
This paper presents an approach for distilling a larger teacher model into a set of students that can run in parallel at lower cost. The main strengths are that the approach appears conceptually sound and reasonably well executed. The main weaknesses are that the differences relative to previous work is fairly slim, and the experimental results are overly idealized. While the main benefit of the approach is improvement in latency, the experiments evaluate in terms of FLOPS. There was some back and forth between the authors and reviewers about these points. Authors added some additional results and seem to acknowledge the limitations, e.g., saying â€œOur time and hardware constraints did not allow us to perform experiments in a realistic deployment.â€ However, for a paper primarily concerned with reducing latency, reviewers were unconvinced that this evaluation was sufficient.
Description of paper content:

The authors propose a dynamics model that can generalize to novel environments. The train and test MDPs have the same state and action spaces but different dynamics. Environment specific inference is achieved by estimating latent vectors Z that describe the non stationary or variable part of the dynamics. These Z s are inferred from trajectory segments in unlabeled environments. The Z s are learned contrastively: Z s from the same trajectory are pulled together, and Z s from separate trajectories are pushed apart. However, to mitigate the error of distancing Z s from different trajectories but the same environment, Z s on trajectories with similar transitions are also pushed together using a soft clustering penalty. These losses are justified based on ideas from Pearlâ€™s causal inference.

Summary of paper discussion:

The reviewers concluded that the contributions are conceptually interesting and â€œsomewhatâ€ novel. The reviewers felt that the empirical performance gains of the method over baselines were demonstrated but not extremely impressive.
This paper reviews a number of parameter decomposition methods for BERT style contextual embedding models. The authors argue for the application of Tucker decomposition to the attention and feedforward layers of such models. Evaluation is performed for a range of models on the GLUE benchmark. Further ablation studies indicate that the distillation procedure employed is crucial for obtaining competitive results and the raw decomposition approaches are ineffective at directly approximating the original pre trained model.

Strengths: The reviewers generally agree that the methods explored and results presented in this paper are interesting and could be of use to those deploying large embedding models. The authors review a range of possible decomposition methods and use this to motivate their approach. The resulting levels compression are high while maintaining good performance, while the ablation study clearly shows the contribution of the various steps of the training pipeline.

Weaknesses: The main weakness identified by the reviewers is the incremental nature of this work in comparison to previous works applying various decomposition and compression techniques to neural networks. They also highlight that many of the techniques discussed early in the paper are not compared in the evaluation. The authors have effectively responded to this issue by providing further comparisons and justification for their modelling choices (e.g. not compressing the embedding layers). 

Overall, despite the incremental nature of this work, I believe that there are enough though provoking ideas and results presented to warrant publication. Interestingly, as the authors emphasise in their response, the ablation study highlights that this work is not really about approximating the original models weights, as all of the work appears to be being done by the distillation procedure in concert with the choice weight decomposition. In general I wonder whether this paper would be better presented as exploring a structured distillation procedure rather than weight compression.
The paper proposes an approach to learn the task specific weights in pretraining or mutli task learning. It provides theoretical guarantees to the algorithm, as well as strong empirical results on several NLP problems. All the reviewers agreed that the work is interesting and the paper is well written. During the discussion period, the authors committed to address in the revised version (relatively minor) concerns raised by reviewers, including providing additional clarifications and additional comparisons to related methods. Overall, this is a strong paper that merits an acceptance.
The paper focuses on providing generalization bounds for SGD for functions that are invariant under scaling. The paper s analysis is based on the stability framework but instead focuses on a metric that is based on the anglular distance as compared to the euclidean distance.

Overall the reviewers found the paper to be interesting and the results to be useful. However the reviewers found the paper to be significantly lacking in terms of its presentation. In particular a clear exposition of the central object of the paper, i.e. normalized loss function was missing as well as clear comparisons between the presented results and existing results. I recommend the authors to motivate their results better and contrast their presented results with existing results to fully highlight the impact of their presented result. Hopefully the suggestions made by the reviewers in terms of presentation will be helpful to the authors towards improving the paper.
This paper received 4 quality reviews. The reviewers like the problem formulation and various ideas presented to enabling a working pipeline. However, almost all experiments are conducted on synthetic data. Concerns are also raised regarding its usage and application on real world data. In the updated version, the authors add some results on real world data, yet without quantitative evaluation. After the rebuttal and discussion, the final rating is 6 from 3 reviewers, and 5 from 1 reviewer (note that reviewer SKNa stated that the rating will be increased to 6 but did not end up changing it in the "recommendation" entry). The AC sees both pros and cons of this work. Given that a conference paper does not have to be comprehensive in all frontiers and the novel idea presented in this paper, the AC is leaning toward in favoring of this work and thus recommends acceptance.
The paper introduces a method to solve inverse problems: given y, find x such that P(x) y, for a given physical simulator P. A standard approach is to learn a neural net such that the inverse x NN(y;\theta). The authors state that this is problematic because it is difficult to take "higher order" gradient information into consideration when using this standard approach. The method assumes that there is an approximate inverse solver inv(P) and discusses an alternative "Physical Gradient" objective that can incorporate knowledge of an approximate inv(P) and a neural network. The experiments are good though comparing performance on an iteration basis is not always fair since an iteration of the PG method can be much more expensive than standard approaches.

The biggest issue that reviewers had was the clarity of the presentation. The authors have made a reasonable attempt to correct this, but I m inclined to agree with the general reviewer sentiment that the presentation is still not at the required level. I agree that there are many things that are not clear, including the confusing discussion in section 2.1 about how the method takes higher order information into consideration. It only becomes partially transparent later in the experiments what is meant by higher order information.

Overall, I feel this is the basis of a potentially valuable contribution but that the current presentation is quite confusing. As mentioned by others, I would also suggest to find a different name since Physical Gradient is also rather misleading. 

The following points were not part of the review process and I do not base the final decision on them, but the authors may want to consider the following: 

I believe there is also an error in the basic approach, or at least an approximation is made which is not explained. The error is that the approximate inv(P) depends also on the parameter \theta (since this is used to initialise inv(P)). This dependency is not taken into consideration in the paper. For example, in theorem 1 in appendix A, the calculation of the gradient dM/d\theta is incorrect since the authors assume that inv(P) is independent of \theta, which it is not (since the preconditioner value depends on \theta). If we do take this into account, we would need to know the derivative of inv(P|x) with respect to the preconditioner x. This dependency would alter the gradient, potentially considerably. The gradient in figure 2 for the PG is also incorrect. One may of course simply say that the paper discounts this correction term in order to retain tractability; however, this would need to be stated as an approximation.
The paper sparked a very substantial discussion, not just about its scientific content, but also, as the authors will have seen, from the narrative standpoint. I would like to thank the reviewers who devoted time and efforts to discuss the paperâ€™s content. I encourage the authors to polish further their paper prior to camera ready, using the numerous scientific comments made (e.g. R6VS, KWuM, the refs of Cn5V). 

AC.
The paper studies the length distortion in a random (deep) ReLU network â€” namely, it bounds the expectation and higher moments of the length of the curve in feature space produced by applying a random ReLU work to a smooth curve. Because the product of layer norms grows exponentially in the depth, it might be natural to conjecture that the length grows exponentially in depth. Indeed, this has been claimed in previous theoretical work. The submission argues through rigorous mathematical analysis and corroborating experiments that this claim is incorrect. In fact, the length exhibits a slow (1/depth) contraction as the network depth increases. The paper also works out higher order moments and extensions to higher dimensional volumes. These results are obtained using nice (and natural) independence arguments and calculations. 

Initial reviews were mostly positive, with the reservation that the initial submission may have slightly over claimed (the reviewer correctly notes that it is impossible to prove interesting results about the NTK of deep networks with the incorrect exponential growth hypothesis, and that related, and correct, arguments are embedded in the proofs of a number of NTK adjacent papers). After responses and revisions from the authors, the reviewers uniformly recommend acceptance. This is a solid paper, with an important conceptual point â€” length/volume contraction is critical to reasoning correctly about feature evolution in deep networks. In addition, it corrects existing errors in the literature, and provides relatively transparent justifications of its main claims. 

The AC concurs with the reviewersâ€™ evaluation of the paper, and recommends acceptance.
All four reviewers agree that the paper should be rejected in its current form, but make numerous suggestions for improving it. The main points of concern were the motivation of the proposed method, novelty and the quality of the presentation of the work. The authors did not provide a response. The AC agrees with the reviewers and recommends rejecting the paper.
The paper proposes a domain adaptation method that is specific to auto encoder and wireless domain. The proposed method shows solid gain over baseline and is simple. There were multiple complaints on reviewer s side regarding clarity of the abstract and application of the work and related work that was responded to by the authors during the rebuttal period. However, the modifications were not enough to address all concerns. Mainly, the assumptions for the method to work such as source and target having the same number of components and diagonal covariance. It would help the paper to discuss the cases where the model fails. In addition, the paper will benefit from a stronger baseline.
The paper describes a VAE model for individual protein families that uses phylogenetic trees through an Ornstein Uhlenbeck process on latent space. They also use a sequence likelihood which does not factorize over positions. The paper claims these two advances represent a more expressive and efficient model of protein evolution and apply it to ancestral sequence reconstruction.

Strengths:

  The technical novelty of relaxing independent sites is interesting and important
  The use of a tree structured OU process over latent space is novel and natural for this problem setting
  The exposition of the model itself is easy to follow and well written
  A statistically well grounded approach

Weaknesses:

  The evaluations do not enable strong conclusions to be reached yet. More careful ablations and comparisons are needed to understand implications of this work for scalability, representation learning, and evolutionary modeling
  Lack of computational cost details

A majority of reviewers voted with high confidence for acceptance. The only reviewer voting for rejection did not respond to the author s rebuttal and did not give strong arguments for rejection. The authors are encouraged to address the reviwers  comments and improve the paper for the camera ready version.
The reviews are of adequate quality. The responses by the authors are commendable, but ICLR is selective and reviewers continue to believe that more experiments and more rigorous analysis are needed.
All reviewers agree that this paper is a useful and valuable contributions to ML engineering.
   insightful analysis .. highly user friendly operator design
   "useful and I can see it having large adoption in the community of scientific computing" ... "
   "Personally I tend to buy these advantages of einops" ... "However, there is a lack of solid empirical study to validate the effectiveness and efficiency of the design"
   "a useful and appealing new coding tool."

The negative reviewers appear fixated on the (true) observation that the paper does not look like a conventional ICLR paper, thati it "reads like a technical blog", and "lacks rigour".
I belive it is fair and measured to state that these reviews may be considered to exhibit aspects of gatekeeping: requiring more "mathiness" that does not help the paper, or more "rigour" through user studies that are in fact less valuable than the reviewers  own observations "I could see myself...", "I tend to buy...".

This is a paper about design, not about models or algorithms (although the algorithmic work is good).  It is about the design of tools that we all use, and about the decisions and thought processes that led to that design.  A reviewer decries "many non rigorous claims".  These are claims about the usability of existing systems, and mostly appear in the discussion and footnotes, as the authors note in rebuttal.  Of course, one could have run user studies to back up each claim, but I am just as convinced by the examples shown in the paper.  It matters not to me what some users corralled into a user study thought.  It matters what I and my colleagues will think, and I am now sure to recommend einops to colleagues.  I would not have met it had the paper not been submitted to ICLR, and hence I am certain it should be accepted, so more can see that we care not just about mathiness, but actually enabling progress in our field.

The job of a conference like ICLR is to expose researchers and practitioners in machine learning to ideas and techniques that may advance their research and practice.  Programming, and the translation of mathematical ideas to efficient computer code, are fundamental to all of machine learning, and hence programming models are very much suitable for presentation to an ICLR audience.  I am certain that this paper, and the technology it describes, are more important to ICLR readers than knowing that if module A is co trained with module B, then combined with compression C, the SOTA on some arbitrary benchmark is increased by 0.31 +/  1.04. 

Reviewer gRMH says "there is no code", but the code has been in the open for three years; it is an accident of our misapplication of the principles of blind review that the reviewer felt they could not search for the code, and that the authors felt they could not bring to bear the evidence that three years of real world usage have brought.  

Reviewers say the work is just an extension of einsum, while noting that the extension is useful and nontrivial.  Yes, it is an extension, and the paper s examples show how it yields more compact code that is also more readable and maintainable.

I could add more examples, but in short, I tend to side with the authors  response at almost every point.  At the same time, the final version of the paper has been strengthened by this dialectic, and I expect further strenghtening through exposure to the ICLR community.

To the authors: Listing 1 is useful, but should be in an appendix.  Instead, add examples of ellipses on P5, and show more inline examples in general.  The paper would be strengthened by another pass over the English   after the decision is made I would be happy to volunteer to help.
All reviewers agreed that the paper contains interesting experiments. However, as this paper is a systems paper without much algorithmic contributions, all reviewers felt that the paper felt short in terms of describing the results, has too many unsupported claims and it is unclear how the presented results transfer to slightly different domains. I therefore agree with the reviewers and recommend rejection of the paper.
DictFormer is a method to reduce the redundancy in transformers so they can deployed on edge devices. In the method, a shared dictionary across layers and unshared coefficients are used in place of weight multiplications. The author proposed a l1 relaxation to train the non differentiable objective  to achieve both higher performance and lower parameter counts.

All reviewers ended up giving the paper a score of 6 after increasing their scores during discussions. While the results are strong (better performance at much lower parameter counts), the paper is not clearly written. Several reviewers noted that the paper is difficult to understand and has a few unresolved points. For example, the method also ended up performing better than the base transformer model that DictFormer is supposed to compress. There seems to be a lack of understanding about what part of the model delivered the improvements. One reviewer said that this is potentially a great paper that deserves to be better explained. The basic concept of sharing a dictionary across layers should be simple enough to explain well and deserve a better explanation than eq 5. 

The authors promise to release the code, which would be necessary for a full dissemination of this work. I recommend accept.
This paper introduces a novel task (i.e., modelling the iterative process of editing sequences) and proposes a Transformer based architecture to address it tractably. The paper also elects a number of metrics that are argued to shed enough light onto the merits of the proposed architecture. 

In our view, the current version is not ready for acceptance. Here are some of the reasons I d highlight: 

* It is not entirely clear to us that the task in consideration has enough substance to grant acceptance nor that it speaks to a large enough audience. Perhaps the challenges identified here are more general and the developments for this task can be extended to related generation problems? If so, this is something one could consider for a revised version of the paper. 
* The motivation does not seem to align well with the datasets used to demonstrate the task. Perhaps the difficulty to find a dataset that matches the motivation is an indication that the task and its challenges are tad too specific. 
* It s the impression of more or less everyone involved that the paper lacks comparisons, and that the evaluation is not thorough enough, and the rebuttal did not ease our concerns sufficiently.
The paper describes a system for learning rules in a quasi NL format: roughly Horn clauses where a predicate p(X1,...,Xk) is replaced by a natural language pattern interleaving ground tokens and variables.  The method is to propose ground sentences   using one of several task specific approaches   use anti unification of pairs to variabilize, and then find a minimal theory from these proposed pairs by reduction to maxsat.

Pros:
   QNL is a neat idea, and makes symbolic rule learning possible to some NLP tasks
   The use of maxsat is novel in rule learning AFAIK

Cons:
   Unification is a highly simplified model of the NL task of cross document co reference
   It s unclear if maxsat process will work in the presence of noise, or how well it scales
   The datasets use clean text generated from templates or synthetic grammars
   Experimentally, the generality of the system is not well demonstrated, because there are differences in how it is applied: eg a subset of short examples for scan, input engineering ($TRUE, $FALSE) for RuleTaker, plus the "heuristics for filtering invalid rules generated by anti unificationâ€
   It s not clear if this work really speaks to the main "point" of the SCAN and RuleTaker datasets.  These are both the kind tasks that symbolic systems would be expected to do well, and are used as ANN benchmarks because ANNs perform in unexpected ways: worse than one would expect for SCAN, and better for RuleTaker.  They are important for understanding ANNs but I m not certain what the research benefit is of using them for symbolic methods as a benchmark.
The authors propose three strategies for coreset selection in the context of continual learning. In particular, the authors consider class imbalance and noisy scenarios. The authors run extensive benchmarks and ablation showing that the approach can be effective in practice. All reviewers were positive about this work, but found that the methodological contributions were relatively modest. The clarifications provided by the authors were highly appreciated. I would encourage the authors to revise the paper to incorporate these additional details as there were a number of concepts that reviewers found were not sufficiently documented/explained and lacked clarity. I would also highly encourage the authors to explain their use of "online continual learning" as this reads like a tautology.

Finally, I would like to ask the authors to reflect on their insistance with the reviewers; while we would all want engaging and long discussions about our work, the reality is that reviewing papers and discussing them is time consuming and taxing, especially in the middle of continued pandemic. The authors should be grateful of the time reviewers have spent reading their work and providing feedback, and it is not in the authors  interest to ask for a revision of the scores.
The paper proposes an algorithm for unsupervised skill transfer between robots with different kinematics. Integral to the approach is the idea that while the robots differ, they may use similar strategies to perform similar tasks. Without access to paired data, the paper formulates the problem of learning correspondences between robots as one of matching skill distributions across robots. Drawing insights from work in machine translation, the paper proposes an unsupervised objective that encourages the model to learn to align the distribution over skill sequences. Experimental results demonstrate the ability to use learned skill correspondences to support transfer across different robots in different domains.

As several reviewers point out, the problem of learning to transfer skills across robots with different kinematic designs from video demonstrations raises a number of interesting challenges that are relevant to the robotics and learning communities. Among them, a fundamental contribution of the paper is the ability to learn skill correspondences in an unsupervised manner based on unlabeled demonstrations. The approach by which this is achieved (i.e., using distribution matching) is sensible and clearly described. While the reviewers agree on the significance of the research problem, they raise a few key concerns regarding the initial submission. Among them are questions about the nature and extent of the domain variations that the method can handle (e.g., between robots with different degrees of freedom); the significance of the contributions; and how this work is situated in the context of existing approaches to robot skill learning. Several reviewers question the definition of morphological variation and comment that these variations may violate the assumption that task strategies are similar across designs. The authors provided detailed feedback to each of the reviews, which helps to clarify several of these concerns. Unfortunately, several reviewers did not respond to multiple requests to update their reviews. The one who did decided to maintain their score.

The paper tackles an important problem in robot learning and the work has the potential to have significant impact on the way in which robots acquire new skills. The original submission together with the author responses suggest that there is are solid contributions here. The authors are strongly encouraged to revisit the discussion of the approach to more clearly convey the novelty of the approach and to consider experimental evaluations that better support these claims.
Although two reviewers have given score 6, the other reviews have clearly indicated that the paper is not good enough for ICLR. The paper does not give any new insight into the considered problem, many existing papers are ignored, and the only interesting part about evaluation of label importance is rather very shallow borrowing ideas from other fields without any deep discussion or analysis.
Wide agreement from the reviewers.  Interesting theorems.  Empirical work illustrates the theory.
Claim and insight: failure of VAEs is caused by the inherent limitations of ELBO learning with inflexible encoder distribution.
Good discussion pointed out related work and insights from the experiments.
This paper tackles the problem of feature interactions identification in black box models, which is an important problem towards achieving explainable AI/ML. The authors formulate the problem under the multi armed bandit setting and propose a solution based on the UCB algorithm. This simplification of the problem leads to a computationally feasible solution, for which the authors provide several theoretical analyses. The importance of the learned interactions is showcased in a new deep learning model leveraging these interactions, leading to a reduction in model size (thereby competing against pruning methods) as well as an improvement in accuracy (thereby competing against generalization methods). Although the proposed approach essentially builds on the specific UCB algorithm, it could likely be extended/modified to other (potentially more efficient) bandit strategies. A drawback of this work resides in the experiments being entirely synthetics. In order to close the gap with practice, experiments on real datasets of higher dimensionality should be conducted.
The current paper presents a new method for communication and cooperation in multi agent settings. Specifically, the authors propose to model other agents  intentions and internal states using ToM nets and using these predictions to then decide how to communicate/coordinate. The authors present experiments in two multi agent cooperation tasks (multi sensor multi target coverage and cooperative navigation), compare against 4 previous methods (TarMAC, I2C, MAPPO and HiT MAC) and perform the necessary ablations studies and find that their method achieve better rewards in both environments.
All reviewers have found the present study to be novel with convincing experimental findings. Reviewers have raised some concerns however a great deal of those have been addressed by the authors during the rebuttal and many of these points have now been incorporated in the paper. 

Having read the paper and considering the reviews I agree with the reviewers that this manuscript will make a good addition to the program of ICLR and as such I recommend its acceptance.
This paper studies a stochastic approximation framework for multi agent consensus algorithms driven by Markovian noise in the spirit of the classical paper of Kushner & Yin. The authors  main result is that   modulo a series of assumptions, some conceptual, some technical   the generated sequence of play reaches a consensus, and they also estimate the rate of this convergence. 

Even though the paper s premise is interesting, the reviewers identified several weaknesses in the paper, and the reviewers that raised them where not convinced by the authors  replies (especially regarding the relative lack of numerical evidence to demonstrate the claims that are not supported by the theory, such as the role of Assumption 6). After my own reading of the paper and the discussion with the reviewers during the rebuttal phase, I concur that this version of the paper does not clear the bar for acceptance   but, at the same time, I would encourage the authors to submit a suitably revised version at the next opportunity.
This paper aims to relate brain activity (of people reading computer
code) to properties of the computer code. They relate the found
representations to those obtained from ML computational language 
models applied to the same programs.  The paper is clearly written and
an interesting idea.

There was a lot of discussion and the author(s) updated their paper a
lot.  Program length as a potential confound was raised and 
successfully rebutted.  The extent of novelty from Ivanova et al 2020
was also discussed and successfully rebutted.  In the end, the main
issues the reviewers had were 1) that the paper had been updated
substantially since submission (and would therefore benefit from a
thorough re review) and 2) whether the results provide enough new  
insights about the brain or about ML language models.  

To summarize, the authors spent a lot of time addressing issues in the 
rebuttal phase and the paper got a lot better with the reviewers 
suggestions, but reviewers agreed it would benefit from more work and
further review before acceptance.  I agree with this assessment.
This paper introduces Back2Future, a deep learning approach for refining predictions when 
backfill dynamics are present.  

All reviewers agree on that the authors successfully motivate their work and 
introduce a topic of great interest, i.e. that of dealing with the effect of revising previously recorded data and its effect 
timeseries predictions. The reviewers also underline the strong and thorough experimental section.
Among the reviews is also underlined the potential impact of the work for the research domain. 

Many thanks to the authors for replying to the minor concerns raised.

I concur with the reviews and find this submission very interesting, convincing and thus 
recommend for accept. 

Thank you for submitting the paper to ICLR.
This paper comprehensively evaluated 18 different performance predictors on ten combinations of metrics, devices, network types, and training tasks for NAS. While evaluating and comparing different prediction models is not itself novel, the authors provided many insights that are potentially interesting to future NAS developments. 

Reviewer reactions to this paper are rather mediocre and lukewarm. It is in general consensus that this work gives a good empirical analysis on hardware metric predictors for NAS, but the novelty is low and it is perhaps a bit incremental (e.g., nothing "shockingly new" was revealed, and observations are mostly "as expected"). Despite the authors improving the paper during rebuttal with new plots/tables, there remain to be unaddressed comments, e.g., adding experiments that run BO / evolution / etc with different hardware predictors and comparing the quality of the Pareto front. Those missed points were also raised in the private discussion. 

After personally reading this paper, AC sides with most reviewers that this paper lacks true novelty nor technical excitement. While the empirical study is valuable, it perhaps suits venues other than ICLR, e.g., the NeurIPS benchmark track.
This paper explores addition of a version of divisive normalization to AlexNets and
compares performance and other measures of these networks to those
with more commmonly used normalization schemes (batch, group, and
layer norm).  Various tests are performed to explore the effect of
their divisive normalization.

Scores were initially mixed but after clarifications for design and
experiment decisions, and experiments run in response to comments by
the reviewers the paper improved significantly.  While reviewers still
had several suggestions for further improvements, after the authors  
revisions reviewers were in favor of acceptance which I support.
The paper proposes to incorporate an autoencoder to transformer based summarization models in order to compress the model while preserving the quality of summarization. The strengths of the paper, as identified by reviewers, are in extensive experiments presented in the paper and in a relatively clear write up. However, the reviewers identify several weaknesses, including missing state of the art summarization baselines and missing relevant compression/knowledge distillation baselines. Although the author response have addressed some of reviewers  concerns, all the reviewers agree that the draft is not yet ready for publication.
This submission receives four negative reviews. The raised issues include paper organizations, presentation clarity, more experimental evaluations, the trade off between technical contribution and application configuration, and the potential impact on more general visual recognition scenarios. In the rebuttal and discussion phases, the authors do not make any response to these reviews. Overall, the AC agrees with four reviewers that the current submission does not reach the publication bar. The authors are suggested to improve the current submission based on the reviews to make further improvements.
This paper introduced a probabilistic extension to a pipeline for 3D scene geometry reconstruction from large scale point clouds. 
All reviewers recognized the significance of the proposed approach and praised the simplicity of deriving a probabilistic version of Generative Cellular Automata that performs well in a number of reconstruction benchmarks. Authors were responsive during rebuttal and managed to clarify the concerns raised about the limited scope of the experiments and certain parameters involved, and also raise one reviewer s scores.
The arguments the paper makes require a stronger foundation and justification. The reviewers and AC didn t find the author response sufficient. For example, in response to ZbHJ, the authors argue that their benchmark doesn t use automatically generated trajectories and therefore the language is not synthetic in some sense. It s not clear how it s related to synthetic language, but generated trajectories does create artificial regularities in the task, so an issue, but one that the author must address accurately. This argument also seems to focus on ALFRED and R2R, and ignored many other benchmarks, like the data used in DRIF (mentioned later), RxR, Touchdown, etc. There is also mis used of technical term (e.g., Decision Transformer). Generally, the reviewers consider the work of potential, but it requires significant refinement, which the author response did not provide.
The paper concerns learned Q functions in continuous action spaces wherein the action value function is assumed to be quadratic in the action variable, and thus can be maximized in closed form. The paper identifies a class of optimal control problems for which the approximation is reasonable and produces a proof to this effect.

Reviewers found the manuscript clear (6AfP). Reviewer J1Yy notes that the main result of the paper is useful and good to have, as "problems with non linear dynamics but costs quadratic in the control effort are very common in practice and of high practical significance". On the negative side, reviewer J1Yy considered the contribution beyond the central proof rather light and the empirical study inconclusive and questioned the appropriateness of the venue; a comparison to DDPG was added and while a convincing argument was made as to ICLR s appropraiteness, J1Yy was not willing to move their score beyond a 4 (it does not seem the upward adjustment was actually made). 6AfP noted concerns with the presentation and number of seeds, though their concerns seem to have been addressed in an update. 8xxB was the paper s most ardent critic, who found fault with the presentation (starting with the title). The core of 8xxB s criticism seems to be that by narrowing the scope of problems considered, we are left with a problem domain that is already well solved by classical control, as well as contention about the use of "continuous". The authors offered a thorough rebuttal but the authors and 8xxB were unfortunately unable to see eye to eye on these issues. 8xxB requested more explanation, though a request by the authors to specify the precise scope of what further was required went unanswered.

The AC s own reading of the paper matches J1Yy s assessment most closely. There is a contribution here, in the form of connecting previously proposed RL algorithms for continuous action spaces and discretized time to the literature on optimal control, and exploring cases that match NAF s inductive assumptions, but agree that the contribution is of a rather limited nature. I also believe that the paper has improved through the feedback of reviewers J1Yy and 6AfP. I hesitate to recommend acceptance given a universally negative appraisal and in particular the fact that 8xxB was not satisfied in the end. I hope the authors will continue to improve the manuscript with a more thorough empirical interrogation and adjustments in presentation in light of 8xxB and 6AfP s feedback.
This paper regards video understanding as an image classification task, and reports promising performance against state of the arts on several standard benchmarks. Though the method is quite simple, it achieves good results. The visualization in this paper also provides good insight. All reviewers give positive recommendations for this paper.
This paper proposes a new softmax like operator, to be used instead of eps greedy or softmax in Q learning algorithms. There has been some previous work in this direction, most notably Mellowmax, but the proposed operator is more computationally efficient, and there is some experimental evidence that it improves DQN performance.
The reviews were mixed, with two mildly positive reviewers (6), who found the work interesting, and two negative reviewers (3,5), who raised issues about the impact of the work when taken as a part of a larger RL algorithm, and about the generality of the work w.r.t. to other RL algorithms like policy gradients. During the discussion, the reviewers did not reach an agreement.
My decision to reject the paper is based on the following: while the idea is novel, and the contraction analysis is appropriate, the main interest to the community in such an idea is either experimental   can it be used to push the state of the art RL algorithms? or theoretical   can we glean new theoretical insights using this method? In its current presentation, there is not enough evidence in the paper to support either of these. 
I encourage the authors to either dig deeper into the experimental evaluation and produce more convincing results, or dig deeper into the theory and show some theoretical benefit of Resmax.
The paper uses quasi potential theory to analyze the escape behavior of SGD. Although this is a topic of interest to the ML community, the reviewers found a critical issue with the paper, which the authors admit can not be fixed during this submission. I, therefore, do not think there is a need for a longer discussion.
This manuscript proposes and analyzes a distillation approach to address heterogeneity in distributed learning. The main paper focuses on a relatively simple two agent kernel regression setting, and the insights developed are extended (and partially analyzed) for a multiagent setting. 

There are four reviewers, all of whom agree that the method addresses an interesting and timely issue. However, reviewers are mixed on the paper score. While all reviewers agree that the setting is somewhat stylized, a subset of reviewers highlights that the results give some deep insight that might drive future analysis and implementation in the area. Other concerns raised include potential issues with the communication overhead and the simplicity of the kernel regression setting vs real world deep learning. There are initial concerns about whether the failure case is realistic, which the authors address. Extensions to the multi agent setting and a partial analysis are also addressed by the authors and partially satisfy the reviewers. Nevertheless, after reviews and discussion, the reviewers are mixed at the end of the discussion. 

The area chair finds, first, that the paper is much improved, and much more applicable in the updated form than in the original version, and indeed, the insights from the simple model may be informative for practice. However, the concerns raised about the distance between theory and practice are valid. The final opinion remains borderline. Authors are encouraged to address the highlighted technical concerns in any future submission of this work. In particular, the muti0agent setting should probably be central in the discussion of this work. More ambitious empirical evaluation showing that the theory translates to practice )even if there is a gap) would also help.
This work proposes a continuous disentanglement variational autoencoder. The approach is a direct extension of Sha & Lukasiewcz (2021). The proposed method appears effective in learning disentangled factors on synthetic data. However, the approach is a minor change to Sha & Lukasiewcz (2021) that samples a weighted sum over all style values. This limits the novelty of the paper. Additionally, evaluation is only on small synthetic datasets that was created for this paper. The lack of evaluation on standard datasets such as an emotion dataset as motivated in the paper, means the results may be due to data selection rather than a superior method. This raises doubts as to whether the approach would generalize to other datasets. In the rebuttal the authors state they wanted to focus on a synthetic dataset since various metrics are easily method but additional real world/standard dataset results can be added while keeping the synthetic results.
The paper proposes to overcome the challenge of annotating datasets to train convolutional networks by considering instead an architecture that is composed of stacked support vector machine layers. Each support vector machine is trained on a small patch from the input image. A voting mechanism is used to aggregate the predictions. Results show better performance by the model in the small data regime compared with larger convolutional neural networks trained from scratch.

The reviewers appreciated the relevance of the problem and the originality of the approach. The reviewers also appreciated several parts of the experimental evaluation that were carefully conducted in particular the sensitivity of the analysis with respect to the patch size and the multiple datasets considered for the experimental evaluation. The reviewers also expressed concerns about the adequacy of the evaluation (unfair comparisons), the completeness of the baselines (missing baselines), and the significance of the improvements. In particular, the experimental evaluation was considered too limited given the problem considered.

The authors submitted responses to the reviewers  comments. After reading the response, updating the reviews, and discussion, the reviewers considered that â€˜the experimental evaluation improved a bit , that several concerns were satisfactorily addressed, and yet that the updated results  do not show a significant improvement of the proposed method over existing works, simple baselines or pre trained ResNet . We encourage the paper to pursue their approach further taking into account the reviewers  comments, encouragements, and suggestions. The revision of the paper will generate a stronger submission to a future venue.

Reject.
This paper proposed a cluster based task aware meta learning (CTML) approach with task representation learned from its own learning path. Based on the prior work of feature based task characterization, it integrates the rehearsed task gradient descent trajectory into task representation, and further improves computational efficiency by learning a different network to estimate the rehearsed task trajectory characterization from the feature representation. Experiments were conducted on few shot image classification (meta dataset and miniimagenet) and cold start recommendation tasks. 

Reviewers had raised various concerns about the work including technical novelty, the shortcut tunnel assumption, empirical comparison, scalability issue, more ablations for in depth analysis, etc. The reviewers and AC appreciate authors for putting good efforts in the rebuttal by replying the review questions carefully and making changes to improve their experiments and paper. 

Overall, this paper is a borderline case, where reviewers agree some clear merits (well written, easy to follow, good execution of an interesting idea with code provided, etc). Despite the improvements during rebuttal, some major concerns on the weaknesses still remain (e.g., technical novelty, more convincing justification on the assumption, significance of empirical gains). Therefore, I cannot recommend it for acceptance at its current form, but I hope to see it accepted in the near future after these issues are fully addressed.
All reviewers recommended accept after author responses. AC doesn t find any reason to overturn this consensus.
This paper adapts a method called "real time recurrent learning" for training recurrent neural networks. The idea is to project the true gradient onto a subspace of desired dimensionality along a candidate direction. There are a variety of possible candidates: random directions, backpropagation through time, meta learning approaches, etc. 

The main strength of the paper is that it is a very simple idea that seems to have practical utility.

While often presented in different contexts, it should be clearly noted by the authors that the general idea of using low dimensional directional derivatives for computational efficiency is fairly common in optimization. Reviewers mention sketch and project methods. This has also been looked, for example, in the context of Bayesian optimization, with [random selection](https://bayesopt.github.io/papers/2016/Ahmed.pdf) and [value of information based](https://proceedings.neurips.cc/paper/2017/file/64a08e5f1e6c39faeb90108c430eb120 Paper.pdf) criteria.


Reviewers appreciated aspects of the paper, though had concerns about relations to sketch and project methods, computational costs, and experimental demonstrations and baselines. Through the rebuttal period, reviewers were mostly satisfied that the concerns about computational costs were well addressed. A better job could still be done about describing relation to other work. There was also still some desire for more thorough experimental demonstrations and consistent baselines, as described in the reviews. The paper also could use some additional proof reading as it contains several grammatical errors. On the whole, the paper makes a nice simple practical contribution. Please carefully account for reviewer comments in updated versions.
The authors describes a drug design method that generates molecules by simulating adding or deleting parts of the molecules, and using graphnets to capture atom and fragment level information and construct new molecules.  Simulated annealing is used to â€˜editâ€™ the 3D structures, and docking simulations, drug likeness and synthesizability are used to provide information back into training.  The authors compare with multiple baselines on a test set of 12 targets, including the current SOTA model, and report improved performance.

Strengths:

  The proposed model outperforms other baselines in the multi objective molecules optimization benchmark.
  The model doesn t rely on a data driven biological activity predictor.

Weaknesses:

  The reviewers point out that the model seems to be incremental with respect to previous work.
  The reviewers have concernts about the reproducibility of the work and find a lot of details lacking.

This is a borderline paper with a majority of reviewers voting for rejection. I recommend the authors to addrses the weaknesses above and resubmit to another venue.
This paper was a tough call. The key contribution of the paper is a genuinely useful technique for generating chemical compounds satisfying desired properties. However, there are some key issues with paper.

Reviewer *BjiD* found out that baselines are weak. Most importantly, he run thorough experiments with GraphGA, outperforming by a significant margin the baselines. With minor tweaks (e.g. enabling generating larger molecules). GraphGA achieves comparable though slightly weaker results to DST. Importantly, as pointed out by Reviewer BjiD, there is an important flaw in the experiments that some methods have a cap on the number of atoms they can add. For example, on the logP optimization task, it is possible to optimize the score by just adding carbon atoms. I would like to thank very much Reviewer for going beyond and running these experiments.

All reviewers emphasized the novelty as a key contribution. In internal discussion, I raised concern about novelty and framing of the work. One could argue that any autoregressive model (i.e. adding atoms and bonds at each step) forms a DST. One could also argue that training LSTM to produce the distribution of interest, like in [1], is also a DST because the fine tuned LSTM encodes the distribution of many molecules and is differentiable with respect to the distribution it encodes.

Despite these flaws, it is a solid contribution, which is likely to be useful for the community. Thank you for your submission, and it is my pleasure to recommend acceptance.

For the camera ready please: (a) include a well tuned GraphGA (implementing different tradeoffs of diversity and fitness), (b) include LSTM as implemented in Guacamole as baseline, (c) discuss much more clearly novelty of the work. Additionally please ensure that other baselines are not hampered by limit on number of atoms they can add.

References:

[1] Generating Focussed Molecule Libraries for Drug Discovery with Recurrent Neural Networks, Segler et al, [https://arxiv.org/abs/1701.01329](https://arxiv.org/abs/1701.01329)
The papers studies a novel problem and proposes an interesting algorithm. That said, the reviewers question the motivation of the paper. That is whether this method presents a viable attack on existing MT systems. The attack is not black box and MT systems often have an output length threshold beyond which the output is trimmed. Given the motivational concerns, I recommend that the paper is revised and resubmitted to other venues.
This paper proposes a risk sensitive actor critic reinforcement learning (RL) method that optimizes the policy with respect to a dynamic (iterated) expectile risk measure.  The expectile risk measure has the elicitability property and can be expressed as the minimizer of an expected scoring function, which is exploited in critic update.  The proposed approach is applied to option pricing and hedging.

A main point of discussion was the applicability and effectiveness of the proposed method beyond particular financial problems.  The original submission was indeed specialized to particular financial tasks.  The authors have rewritten the paper in a way that it claims to propose a risk sensitive RL method for the general MDP with finite horizon.  This however leaves the question regarding the advantages of the proposed approach over existing methods of risk sensitive RL, including those that work with non coherent (dynamic) risk measures (since coherence is often not needed in domains outside finance).
The paper proposes to use covariance of the approximate posterior to induce a metric on the latent space of the VAE and use it to sample from the Riemannian manifold learned by a VAE. Experiments on MNIST and CelebA show the method outperforms vanilla VAE in terms of sample quality (FID and PR scores). It is also shown to work better than baseline VAE models on a medical imaging classification task. While the reviewers have acknowledged the contributions of the paper, the novelty in the contributions and their importance/impact was seen to be rather limited. The main concern from the reviewers is   while the paper is mainly based on the use of inverse covariance as the metric for manifold, it doesn t give a reasonable theoretical justification on it is a sensible metric that captures the intrinsic geometry of data. Authors in their response justify it as   since the covariance matrices are learned from the data and favor through the posterior sampling some direction in the latent space, it is a natural choice as metric. This is not very convincing. A more technical justification for this will certainly make the paper more convincing. I suggest the authors to look at "Kumar, Abhishek, and Ben Poole. "On Implicit Regularization in $ Î² $ VAEs." International Conference on Machine Learning. PMLR, 2020" which theoretically connects inverse covariance and the Riemannian metric in Sec 5.2, and see if it can be adapted in their context.
This paper performs a comprehensive investigation on self supervised pre training with streaming data. Reviewers agreed that the task studied in this paper is highly practical and important, and the analysis is insightful. Meanwhile, reviewers raised some concerns such as empirical setups and insights. In the revised paper, the authors provided more justifications and added more analysis such as few shot evaluation and uniformity analysis. After the discussion period, most reviewers are positive about this paper.

Overall, I recommend to accept this paper. I encourage the authors to take the review feedback into account in the final version.
This paper focuses on generalization bounds for exponential family Langevin dynamics, which extends related recent work for stochastic iterative algorithms such as SGLD in several ways. They derive expected stability bounds for a more general class of noisy stochastic iterative algorithms, leading to an exponential family variation of Langevin dynamics and a noisy version of the sign SGD algorithm. The contributions are technical and quite positively received by one reviewer, while the others were not convinced to change their opinions during the author response as there were concerns on the limitation of the theoretical contributions and the extent to which these contributions have implications on achieving state of the art performance. While I find it valid that the scope of the paper focuses on generalization bounds and provide improvements over the existing literature, rather than on empirical benchmarks or on optimization related aspects, the overall borderline impression of the reviewers on the whole suggests that a refined version of the paper that further clarifies the contributions and makes clear its impacts as well as limitations may make for a stronger and more impactful paper.
The AC summarizes the major strengths and weaknesses of the paper pointed out by the reviewers (with possible omissions, and additions by the AC)

Strengths:
1. The paper makes an important observation that the linear MDP assumptions can be met when the true dynamics has additive noise 
2. Inspired by the theory, the paper proposes a new algorithm that empirical outperforms SAC. The success of the algorithm is very interesting (and surprising to some degree.) 

Weaknesses
3. Most of the reviewers and the AC thinks the representation learning perspective is questionable. If one strongly believes that the $\phi, \mu$ in the linear MDP assumption should be interpreted as representations, then yes, this paper is about representation learning in RL and the representation learning is a free lunch. However, suppose one ignores the linear MDP perspective for the moment, and only looks at the modeling assumption $s    f^*(s,a) +\epsilon$, then $f^*$ can only be interpreted as a "dynamics model" and has nothing to do with the term "representation" that is commonly used in practice. (representation means the penultimate layer of the neural nets typically in emprical RL.)  Moreover, in the theory part of the paper, the dynamics model is learned via a (standard) model based approach fitting $f(s,a)$ to $s $ which also suggests that $f$ should be interpreted as a dynamics model instead of a representation. How to reconcile these two perspectives? The AC s own opinion is that this suggests we shouldn t blindly call the $\phi$ in the linear MDP formulation a representation in all scenarios. But regardless of AC s own opinion, I suspect that the paper needs to very explicitly discuss and clarify these discrepancies (instead of somewhat sweeping it under the rug and claiming the paper is about representation learning without a stronger justification.) 

4. The sample efficiency depends on the Eluder dimension, which is only known to be polynomial for linear models. Recent works have shown that the Eluder dimension for even simple nonlinear models can be exponential. The analysis seems to be also quite related to previous analysis that uses the Eluder dimension. I think this fact limits the theoretical contribution of the paper. 

5. There could be a better exposition of the empirical implementation in the paper. It appears that the implemented algorithm still has some major differences from the theoretical algorithms. 

6. It s unclear if the paper should only compare with model free algorithms. At least the theoretical algorithm fits $f(s,a)$ to $s $ explicitly (in the definition of confidence region). Therefore, it does not seem to be quite fair to compare with model free algorithms. 

Given these considerations, and given that the majority of the reviewers express some concerns about various subsets of these concerns (3 6), the AC would recommend the authors revise the paper and resubmit to another top ML venue. The AC thinks that the paper contains really interesting and novel observations, but the interpretation of the observation might require more thoughts and clarification.
PAPER: This paper addresses the problem of learning methods for general speech restoration which generalizes across at least 4 tasks (additive noise, room reverberation, low resolution and clipping distortion).  The proposed approach is based on a two stage process, which includes both analysis and synthesis stages. 
DISCUSSION: The reviewers wrote very detailed reviews which ask some important questions and point to some potential issues. The authors responded to all reviews, but only addressed a subset of the issues and questions mentioned by the reviewers. Novelty and comparison with previous approaches was one of the issues mentioned by reviewers.
SUMMARY: While reviewers are supportive of this line of research, reviewers were also concerned with the novelty of the proposed approach and details of the experiments. In its current form, the paper may not be ready for publication.
This paper proposes a novel strategy for deep active learning based on the training dynamics of the underlying deep model, defined as the derivative of the loss of the ultra wide NTK. All reviewers enjoy the clean story and motivation of the proposed acquisition/objective function and appreciate the authorsâ€™ effort in providing theoretical justification and analysis. 

One note is that   as Reviewer 8dDv highlighted   part of the analysis pertaining to the incompatibility between the generalization bound of NTK and the non iid nature of active learning rely on numerical evidence: the MMD under the covariate shift setting (i.e. assuming that the conditional distributions P(Y |X) remains consistent) is shown empirically to be smaller than the dominant term of the generalization bound. This serves as a reasonable empirical motivation/ justification of the dynamicAL heuristic under the AL setting, but I would suggest the authors be more precise in the abstract / intro (e.g. abstract) that this is an empirical result. 

While the theoretical results are interesting, not all reviewers are convinced that the experimental results are sufficiently compelling. In particular, Reviewer YgGb points out that the significant performance boost reported in the main paper was mainly due to the non retraining (i.e. not retraining the model (from scratch)) constraints imposed by the problem setup. Reviewer p3z9 shares the same concern that such a setting would be far from realistic at least for the data sets/labels considered in the experiments. The authors refer to Ostapuk et al, 2018 as a justification of the non retraining setting; yet they assumed a high budget, e.g., up to 50% of all labels of datasets. 

In summary, this is a theoretically well motived work, but the empirical components need to be further clarified and supported with more realistic experiments to merit acceptance for the proposed solution.
This paper studies graphon mean field games, whereby a continuum of agents are connected by a graphon. They study a discrete time version and show existence of a Nash equilibrium (under Lipschitz conditions). Moreover they prove that it corresponds to an approximate Nash equilibrium for the game with a finite number of players, thereby validating graphon mean field games as a natural abstraction when the number of players is sufficiently large. Finally they give algorithms based on fixed point iterations (one based on discretizing the graphon index, the other based on reformulating it as a classical mean field game) for computing such an equilibrium. They give numerical experiments to validate their approach. The reviewers pointed out various writing issues or other results that would help complete the picture. Many of these were addressed and/or clarified by the authors in their revision. Overall the paper provides an appealing and relatively complete characterization of equilibria in graphon mean field games.
The paper studies network architecture search in the context of reinforcement learning. In particular it applies the DARTS method to the Procgen RL benchmark, and conducts extensive experimental evaluations. It identifies a number of issues that could potentially prevent DARTS from working well in the RL setting (such as nonstationarity and high variance), but in the end shows good performance without needing to modify DARTS substantially.

The reviewers agreed that a key strength of the paper is in its experiments. But they also identified a weakness in novelty: if a paper s main contribution is to combine two previously well explored ideas (in this case, RL and DARTS) then there is a high bar for the quality of exposition and positioning, and the reviewers did not feel that this bar was met. (Though the authors  updates during the rebuttal period did help substantially with clarity and relationship to other methods   thank you for these!)

Recommended decision: while the paper makes a worthwhile contribution, it does not in its current form rise to the level of novelty and general interest that is needed for publication in ICLR.
This submission has been withdrawn. 

The reviews are of good quality. The authors should consider writing two separate papers: one about the problem and solution from an ML perspective, and the other about the application to radiology. Papers that provide a new method in the context of a single application domain run the risk of making a contribution to neither, and of being evaluated by reviewers who are not experts in both.
This paper proposed a joint learning approach which combines item based representations and interest based representations to improve recommender systems. Overall the scores are negative among all the reviewers. The reviewers acknowledge that the proposed approach provides a simple yet effective way to improve the existing item based representations. However, all the reviewers pointed out concerns around the motivation and limited novelty (the proposed approach mostly combines a few existing approaches together without careful examination/exploration in the experiments). Furthermore, the baselines considered in the paper are on the relatively weak side. The authors didn t provide any response. Therefore, I vote for reject.
The paper demonstrates that one phase of de novo assembly, specifically the layout phase, can be replaced with graph neural network based methods. The paper clarifies in the rebuttal that it focuses on building a method for assembling high quality long reads. 

All four reviewers rated the paper as below the acceptance threshold. The reviewers largely agree that the idea of using GNNs to assemble a genome from reads is novel, interesting, and has the potential to be very useful. 
The reviewers raise the following concerns: The paper only considers synthetic data, and the synthetic reads used in the simulations are error free. In practice, reads are not error free, and thus simulations on real data or at the very least on reads with errors are needed. The authors acknowledge that, and state that they ll provide such experiments in future work. In summary, the reviewers found the experiments to be insufficient to support the claims, even though it is understood by the reviewers and me that the paper only presents a proof of concept idea. I agree with the reviewers that simulations on erroneous reads, ideally real data, would be needed for acceptance.

I recommend to reject the paper, since the paper provides insufficient experiments to understand the merits of the proposed approach.
The paper provides a deep learning technique aimed for tabular data via a unified view of factorization machines and other DNN approaches. The reviews are overall positive when discussing the provided technique, the motivation behind it and the writing. However, there are major concerns related to the experiments. The most dominant one is that of significance, meaning the advantage of the provided method when compared to existing literature. Other claims such as unclear details or different methods of reporting might be possible to resolve via minor edits, but this concern was not resolved in the rebuttal period. Before the paper can be published in a venue such as ICLR, it should provide a clearer comparison against previous works showing exactly where it improves upon them. At its current state, it doesnâ€™t seem to be ready.
This paper shows how to back propagate through a kernel between graphs that counts common random walks of infinite length between the graphs. Reviewers tend to agree that the paper is well written and the technical contributions are sound. However, there are concerns about the significance and novelty of the method relative to related work, alongside mixed experimental results. Overall that puts it as a very borderline paper. In the rebuttals, the authors argued for the significance of the contribution, but reviewers were generally unconvinced.
The authors study the problem of video classification and propose a new module which promises to increase accuracy while keeping the computational overhead low. The main idea is not to share the spatial convolution weights over different time steps, but allow some modulation based on pooled local and global frame descriptors. The resulting module can be used as a drop in replacement for spatial convolutions in existing models and yields competitive performance on multiple video action recognition and localisation benchmarks.

The reviewers appreciated this challenging setting and the simplicity of the main idea. They found that the paper was clearly written, well organised, and easy to follow. The reviewers raised some issues in connections with related work and the empirical evaluation which were successfully resolved during the discussion phase.

Given that computational efficiency remains as one of the most challenging topics in video understanding, I believe that this technique will be relevant for the larger video understanding community. I strongly suggest that the authors incorporate the feedback received during the discussion, especially the GFLOPS vs accuracy plots, and further clarify the relationship to existing work.
Most of the discussion centered around whether the underlying question in the literature is setup correctly in terms of its relationship to causality as the question being asked is one of an intervention. The underlying literature makes an attempt at not including things that can t be intervened on like age, but the setup of a "counterfactual" could benefit from a causal take.

Holding that aside, the paper makes progress on an established question though analysis that reveals that the Lipschitz continuity and confidence are important for causality and Stable Neighbor Search for generating counterfactuals.

The most negative reviewer in discussion writes that they re okay with the paper being accepted if the rest of the reviewers are positive. The rest of the reviewers are positive with one mentioning that the paper is well written and interesting in the discussion and that the author replies cleared up the issues about counterfactuals.
The paper proposes an application of the CNN to the microscopy problem of constructing 3D volumes from 2D captured images. The four reviewers thought the paper was a straightforward application of existing techniques to a new problem, while they were borderline towards accept the overall sentiment was that the technical novelty was very low from an ML perspective, and the ICLR community would only find the application potentially of interest. (Two reviewers changed from borderline reject, while the other two chose not to change their scores following the authorsâ€™ request.)
This paper studies the important statistical phenomenon of double descent, a very timely topic, using influence functions, and thereby derives lower bounds for the population loss. The reviewers generally appreciated the conceptual as well as the technical contributions in the work, but argued that the set of assumptions taken by authors can potentially diminish the significance of the analysis. This, as well as additional issues regarding the empirical and the analytical support for the modeling assumptions (and the implied scope of applicability, i.e. lazy\kernel vs. rich regime) have generated considerable discussion between the reviewers and the authors. Along the process, major and minor concerns were addressed to the satisfaction of the reviewers, resulting in a substantial improvement of the overall evaluation. Thus, this AC recommends acceptance.
The paper studies subpopulation shift in object recognition when classes obey a hierarchy. It proposes an architecture, a relevant metric and a dataset (subset of imagenet).  The problem of classification in hierarchical label spaces is important and of great interest, and the effect on domain shift is interesting. Naturally, this problem was studied quite intensively over the years. 

Reviewers were concerned that the current proposal was not placed well enough in context of previous literature, both in terms of the method and in terms of experimental results.  Also, the paper would be strengthen if it provides more theoretical analysis about how the hierarchy helps with the domain shift. The authors addressed some of these issues in the rebuttal, adding references and highlighting the differences from previous methods, but the paper would need more time to make the proper experimental comparisons with previous work and subsequent analysis. As a result, the paper is still not ready for acceptance to ICLR in its current form.
The authors in the paper perform empirical studies to investigate the trade off between accuracy and privacy (measured by membership inference attacks) in deep ensembles. They find out that the level of correct agreement among models is the most dominant factor that improves the performance of MI attacks in deep ensembles. They support their claim by visualizing the distribution shifts of correct agreement in train/test examples. They further implement a variety of existing defenses, such as differential privacy and regularizations, etc., to investigate the â€‹â€‹effects of existing defense mechanisms. Overall, the paper is well written and the experiments are well conducted.
While these findings are interesting, they do not reveal something useful or surprising about deep ensemble learning. It is not clear what the contribution is to the membership inference attack literature and private machine learning literature. they do not propose anything new to make the attacks stronger or defenses stronger.
The paper proposes extracting multiple scale features using denoising score matching. Reviewers pointed out the limited novelty in the work and that it does not cite various previous work and how it connects to them.  The paper needs some further  polishing on the writing, and in making the use of lambda divergences more rigorous and principled as explained in the comment of Reviewer VdM1 .
**Summary**

This paper proposes a novel offline model based meta RL approach called MerPO. MerPO combines conservative value iteration with proximal RL policy iteration.  The proposed method is novel despite having some similarities to approaches like COMBO. The paper compares against it in the experiments. The paper provides both empirical and theoretical justification for the proposed approach.

**Final Thoughts**

Overall, I think the authors did a pretty good job at addressing the reviewers  concerns. Overall, I think this is an interesting contribution to the ICLR community. The reviewers were all positive about this paper. For the camera ready version of the paper, I would recommend the authors to go over the reviewers  concerns again and make sure that those concerns are addressed in the paper too as they did in the rebuttal. Some captions are pretty short; for example, see the captions of figure 6 and figure 7. I would recommend the authors add more description to the captions in the camera ready version of this paper.
The paper received 3,3,1 as reviews. All reviewers have the consensus on the weaknesses, i.e. limited technical novelty and weak boost in performance in datasets that may not be the state of the art anymore. The authors have submitted a rebuttal however the rebuttal did not improve the score of the reviewers. Following the reviewers recommendation, the AC recommends rejection.
This paper presents a neural version of individual refinement (IR) architecture for improving the expressiveness of GNN in terms of isomorphism tests. As IR is the dominant approach of practical graph isomorphism tests, adapting IR to GNN is a novel and important idea. As IR suffers from the exponential number of branches, the paper adapts particle filtering algorithms to sample K paths to approximate the full version of the IR algorithm. Simulation and real world datasets are used to demonstrate the improvement over base GNN.

Strengths: 
+ The paper is well written and easy to follow. 

+ The originality of the paper is high since it is both technically rich. Adapting individual refinement (IR) to Neural and GNNs is a novel and important contribution. The designed algorithm does improve over base GNNs.

+ The particle filtering algorithm is an elegant and low complexity realization of the IR algorithm.


Weaknesses:

  PF GNN is mainly evaluated on synthetic datasets, while only three real world datasets are employed in total. It is not thus entirely clear how effective the proposed model is in real world scenarios. The authors added a new real world dataset, OGB molhiv, during the discussion period. 

  The major weakness is the scalability and practical complexity. The designed model is T times deeper and larger than the base model, and K path sampling needs K times larger memory for parallel computation. Although the reviewers still have some concerns, such as â€œsampling method may judge isomorphic graphs as non isomorphicâ€, the reviewers appreciate the authorâ€™s hard work on partially addressing the problem during the discussion period. We encourage the author to continue to improve the paper along this direction.
This submission proposes a simple, efficient, and effective position representation method for the Transformer architecture called ALiBi. ALiBi enables better extrapolation and performance (in terms of efficiency and task performance). The submission also includes careful analysis and extensive experiments, and notably suggests that the gains of ALiBi may be less pronounced in more scaled up settings. All reviewers agreed the paper should be accepted. I think it s reasonably likely that ALiBi will become a common choice in future Transformer models, or at the very least that this work will prompt further work on developing improved position representations for Transformer models. I therefore recommend acceptance.
This paper is a follow up paper of Zhang et al. (2021), that proposed a new network architecture for adversarial robustness, l_\infty distance net. Although the l_\infty network is provably 1 Lipschitz w.r.t. the l_\infty distance, its training procedure exploits the l_p relaxation to overcome the non smoothness of the model but suffers from an unexpected large Lipschitz constant at the early training stage, an issue to be solved. This paper resolves this issue by a new loss design of scaled cross entropy loss and clipped hinge loss. Without using MLP on top of the l_\infty distance net backbone, the proposed new training method empirically outperforms the original one in Zhang et al. (2021) and improves over the state of the art by more than 5% for 8/255 and other radiuses. Moreover, the paper shows the theoretical expressive power of l_\infty distance net for well separated data.

There are some concerns about the moderate novelty and reproducibility of the results. Since the empirical results are indeed impressive, the paper could be accepted conditional on that the authors release their reproducible codes to the public.
This paper revisits the information bottleneck principle, but in terms of the compression inherent in the weights of a neural network, rather than the representation. This gives the resulting IB principle a PAC Bayes flavor. The key contribution is a generalization bound based on optimizing the objective dictated by this principle, which is then tractably approximated and experimentally verified. Reviews raise concerns about assumptions made to achieve the tractable version and a public discussion debates whether this is truly a PAC Bayes bound. The authors address these adequately. Another concern is whether improvements in experiments can be ascribed to the new objective. Authors add new experiments in support of this. Additional concerns about the clarity of certain aspects of the paper were or were promised to be addressed by the authors. Overall, the perspective of this paper, its technical contributions, and experimental evaluations appear to be worthwhile to share with the community, as they advance the applicability of the information bottleneck principle.
The paper proposes a method for hybrid model based/ML learning, where a model is decomposed into an interpretable parametric prior and a neural net residual.  In this case, the prediction error minimization does no identify the parametric component, and an alternating optimization method is proposed to augments prediction error loss with component specific losses. Empirical and theoretical results are obtained.  Initial questions of several reviewers were addressed.
The authors propose modifications to the Transformer architecture in BERT by using grouped FFN and an additional convolution module.
The paper doesn t have all the results and comparison that should be done for a model that has seen similar architecture modification in the previous papers. While it is not necessary to show improvements on multiple hardware systems, it is important to see comparisons to more, stronger baselines and metrics on the full downstream GLUE eval rather than just Squad to establish improvements.
A reject.
The paper presents an approach to weak supervision to address the possibly low coverage of rule based labeling functions, by assigning similar labels to similar instances (where the similarity is computed in feature space).

The reviewers main concerns were the presentation, as well as the experimental protocol and results. Several directions for improvement have been identified by the reviewers and acknowledged by the authors, but in the current state of the submission the consensus is that the paper is not ready for publication.
This paper enhances Lagrangian neural networks by adding conservation of the angular and linear momenta. According to the reviewers, the technical contribution of the paper is marginal, it is a incremental change of an existing model, and it seems that there is some over claim on the generalization of the model to unseen systems. The theoretical contributions in the paper are not significant, and the experiments have not demonstrate the practical potential of the proposed model yet. After the reviewers provided their comments, the authors did not submit their rebuttals. Therefore, as a result, we do not think the paper is ready for publication at ICLR.
The paper addresses the problem of inconsistent gradients in multi task learning, proposing ways to handle both their magnitude nd direction. Gradient directions are aligned by introducing a rotation layer between the shared backbone and task specific branches. 
Reviewers appreciated the technical approach, higlighting the novelty of the rotation layers in this context. The empirical evaluations are systematic fair and insightful, and the presentation is polished. Reviewers unanimously supported accepting the paper.
All reviewers agree that the paper is below the acceptance threshold and the authors did not respond to the reviews.
In summary, this is a clear reject
Casting domain generalization as a rate distortion problem and developing an information theoretic approach to solving it looks like an interesting idea. While the proposed method is technically sound, the assumption made in the proposed method is too strong to hold in real world applications. Though in the rebuttal the authors provided additional experiments on two benchmark datasets, reviewers  concerns about the strong assumption made in the proposed algorithm still remain. To address this issue, I think besides conducting more extensive experiments, the authors also need to analyze when the assumption does not hold in practice, why the proposed algorithm could still perform well compared with other domain generalization methods.

In summary, this is a borderline paper below the acceptance bar of ICLR.
This submission received 4 diverging ratings: 6, 5, 5, 3. On the positive side, reviewers appreciated the central idea and a quality manuscript. At the same time, they have raised important concerns around unfair comparisons with baselines, experiments not fully supporting the claims and lack of comparisons with some prior methods. After discussions with the authors most reviewers stayed with their original ratings. 
The AC agrees that the weaknesses in this case outweigh the strengths. The final recommendation is to reject.
The paper addresses the problem of offline meta reinforcement learning. The authors build on the FOCAL algorithm, adding intra task attention and inter task contrastive representation learning objectives. The resulting FOCAL++ algorithm outperforms several strong baseline, including FOCAL and a theoretical analysis attempting to show that FOCAL++ provably improves on FOCAL is included.

Reviewers agreed that the novelty of the proposed approach is limited since attention and contrastive representation learning have been used in the closely related (online) meta RL setting. At the same time reviewers agreed that the results in the paper and the rebuttal show that FOCAL++ improves on a strong set of baselines.

The main shared concern was regarding the significance and validity of the theoretical analysis. After considering the rebuttal reviewers voting for both acceptance and rejection were in agreement that there are issues with the theoretical analysis/justification. While we agree with the authors that the algorithmic and experimental part of the paper is strong, we have to base our decision on the state of the whole paper. In the end we decided not to accept the paper because 1) the paper put a significant focus on a theoretical argument the reviewers found problematic and 2) the authors did not modify the paper to sufficiently address these concerns during the available window.
The paper presents tackles the problem of finding strategies that are   unlike Nash which is safe   both safe (non exploitable, to some extent) and able to exploit the opponent. The proposed solution is a convex combination of exploitation and safety that is efficient to compute. Overall, the paper is borderline. Given that the objective and its analysis are not especially surprising, a lot rides on the thoroughness of the empirical results, which could be improved.
The article proposes an approach to alter the approximate posterior distribution in order to remove some of the information (unlearning). The approach is applicable when the approximate posterior is obtained via (stochastic gradient) MCMC methods. Unlearning is done by shifting the approximate true posterior by some value delta, which is found via optimisation with an influence function.

The approach is novel, tackles an important problem and is mathematically sound. Reviewers have highlighted some of its limitations. In particular, the modified posterior is obtained by translating the original posterior, without changing its shape; many aspects of the data may therefore not be forgotten. The authors have partially addressed this concern in their response and provided additional experiments. Although there is still disagreement amongst reviewers, I recommend acceptance. 

A Minor comment:
I would not present MCMC merely as a "machine learning algorithm" (p.1), nor as a "sampling based Bayesian inference method" (p.1 and p.2). MCMC is a generic approach to approximate high dimensional integrals and obtain samples approximately sampled from some target distribution, dating back from the work of Metropolis (1953) and Hastings (1970). Its application to Bayesian inference/machine learning problems came much later, see e.g. the excellent review of C. Robert and R. Casella: A short History of MCMC: Subjective recollections from incomplete data. Statistical Science, 2011.
## A Brief Summary of the Paper

 In offline decentralized MARL, the discrepancy between the offline data and the interactions of agents in the environment can cause discrepancy and as a result, the policies will perform suboptimally. This issue in ORL is known as extrapolation error. This paper is trying to address the issue of extrapolation error with offline decentralized agents. It is possible to alleviate this problem by combining offline RL with online fine tuning. This paper introduces Online Transition Correction (OTC) approach to address this problem which aims to correct the biased transition dynamics with a form of importance sampling based on embedding and value based distance metrics.
 
 ## Summary of the Reviews
 
 Below I will outline some important concerns raised by the reviewers.
 
 ### Reviewer T8NR
 **Pros:**
   Interesting and practical setting.
   Extensive experiments to evaluate OTC.
 **Cons:**
   Lack of enough discussions about closely related works, for example, the MABCQ algorithm. 
   Computational cost of OTC.
   Experiments: comparisons against baselines (MA ICQ), Large variance in Figure 4. Small scale (only two agent) setting, can it scale to more agents? 

### Reviewer 2xuS
 
 **Pros:**
   The bias of transition dynamics in offline decentralized MARL problem is interesting.
 **Cons**
   Key baselines such as [[BREMEN]] and [[MUSBO]] that looks into the deployment constrained ORL is not compared against in this paper.
   The proposed OTC algorithm can easily be applied to Single agent settings. MARL vs SARL comparisons would be interesting.
   Large computational cost incurred from OTC. Because of the search procedure of finding most similar examples to the examples in the dataset.

### Reviewer a2PP
**Pros:**
  Well written.

**Cons:**
  Analysis on the behavior of the policy, in particular on novelty seeking during the online finetuning phase.
  The missing details of the transition function.
  Compute constraints and budget.
  Unclear experimental protocol: states vs observations...
  Missing baselines.

### Reviewer ZgL6
**Cons**
  Lack of novelty.
  Limited experimental results: transfer learning scenarios, lack of experiments on multiagent environments such as Starcraft II.

## Key Takeaways and Thoughts

Overall, the authors did a good job addressing the concerns raised by the reviewers. For example, the authors ran additional experiments and compared single agent BCQ with and without OTC on some D4RL tasks. The authors gave detailed responses to the questions related to the computational cost. However, the initial submission version of this paper feels rushed as it is submitted to the conference. I would recommend the authors, go through the reviews carefully and address the points raised by the reviewers carefully in a future version of this paper. As it stands now, it is difficult to evaluate the results reported by the authors during the rebuttal, due to the lack of clarity about their experimental details.

I think the writing can be improved further. There are several typos in the paper, and most reviewers were confused about the novelty of the paper. I would recommend the authors to provide more detailed discussions about the differences from other similar approaches in the literature. Also, justify the selected experimental protocol better.
In this paper, the authors propose a method for generating high quality synthetic datasets, and use their methods to evaluate a variety of causal effect estimators.

In general, the paper was not received very favorably by reviewers.  The primary concerns were: (a) issues with built in bias in the algorithm that generates synthetic data (due to collider stratification bias induced by conditioning on causally "downstream" variables, (b) issues with "replicating underlying counterfactuals," which indeed is a difficult problem, and (c) lack of "technical novelty."

First, I am personally very sympathetic to what the authors are trying to do.  Regardless of current reviewer reception, I think the causal inference community really needs more high quality benchmarks, and (semi)synthetic datasets, and validation approaches.  I urge the authors to continue this line of work.

That said, I think it is important (for causal benchmarks) to be clear about the distinction between the observed data distribution (e.g. p(C,A,Y) for the backdoor model), and the full data distribution (e.g. p(C, A, Y(0), Y(1)) for the backdoor model with a binary treatment).

Generally what makes a benchmark interesting is preserving some features of the _full_ data distribution, and allowing "knobs" that make the problem easier and harder.  Much of what the ACIC competition organizers did was provide such knobs.  Mimicking features of just the observed data distribution, even if they are complicated, isn t enough to make a causal benchmark interesting, since the problem is all about how full and observed data relate.

When revising the paper, please keep this difference in mind, and consider what features of p(C, A, Y(0), Y(1)) (or more complex versions of this) make for an interesting benchmark, while also generating p(C,A,Y) that "mimics observed data" in some way.
This work studies the relation between graph heterophily and the robustness of GNNs and theoretically shows that effective structural attacks on GNNs for homophilous graphs lead to increased heterophily level, while for heterophils graphs they alter the homophily level contingent on node degrees under some specific assumptions.

Overall, the findings in the paper are interesting and can be useful for other researchers trying to improve GNNs  robustness on homophilic and heterophilic datasets. After the discussion and rebuttal, the main concerns are: 
  while the paper has shown some interesting observations, no new methodology was proposed based on these findings. 
  The authors have attempted to relax assumptions and justified their setup on experiments, however the explanations are still limited. For example, Theorem 1 does not allow attention mechanism, different choices of aggregator, skip connection, and more GNN layers.
The paper presents a new approach to learning human behavior by observing a small number of interactions. To this end, it proposes a Bayesian learning framework where a Boltzmann type prior over human policies, based on an available reward function, governs default behavior. The prior is updated by observing actual trajectories taken by (human) agents, in principle. In practice, a full fledged implementation using Gaussian priors and features from a neural architecture is proposed and shown to be effective in practical benchmarks. 

The reviewers are all positive about the paper s contributions. One remaining concern is that the effect of the quality of the prior (Boltzmann vs. other type vs. features designed in a different way) on the learning process is not explored to a significant depth. Yet, the results and approach proposed in the paper are valuable to merit acceptance.
The paper provides an algorithm for the stochastic multi armed bandit (MAB) problem in the regime with fairness constraints. It continues a line of work that in high level define fairness as a requirement to ensure a minimum amount of exploration for every arm.
The main concern I found in the reviews regards the definition of fairness in this paper. Although it follows the same high level narrative of previous works its exact definition and difference from previous papers is not convincingly motivated, and seems to be tailored to the proposed algorithm rather to a real world fairness constraint. This issue could have been mitigated by a novel or generalizable technique, or insightful experiments, but this does not seem to be the case given the reviewers comment about the limited novelty and basic experiments.
The paper provides a new learning technique for problems that require learning embeddings. In particular, the authors analyze a technique that takes into account the frequency of items in an embedding layer to modify the learning rate for each embedding. The paper provides a theoretical analysis of this approached and contrasts it to that of SGD. It also provides experiments validating this approach empirically.

The reviewers agree that the paper provides a simple yet effective method, based on realistic assumptions (non uniform frequencies). In addition, the paper seems to be well written and easy to follow.
One issue raised in the reviews was about the focus of the paper, and the fact that the experiments are limited to recommendation systems even though the method is claimed to be generic for any model requiring embeddings. During the rebuttal the authors provided experiments for an NLP task that show favorable results to the new technique in another regime. Given the overall positive feedback and this new evidence validating the proposed method, I recommend accepting the paper.
This paper makes an interesting contribution to the literature on algorithmic recourse. More specifically, while existing literature assumes that there is a global cost function that is applicable to all the users, this work addresses this limitation and models user specific cost functions. While the premise of this paper is interesting and novel, there are several concerns raised by the reviewers in their reviews and during the discussion: 1) While the authors allow flexibility to model user specific cost functions, they still make assumptions about the kind of cost functions. E.g., they consider three hierarchical cost sampling distributions, each of which model percentile shift, linear shift, and a mixture of these two shifts. The authors do not clearly justify why these shifts and a mixture of these shifts is reasonable. Prior work already considers lot more flexible ways of modeling cost functions (in a global fashion). For example, Rawal et. al. 2020 actually learns costs by asking users for pairwise feature comparisons. Isn t this kind of modeling allowing more flexibility than sticking to percentile/linear shifts and their mixture? 2) Several reviewers pointed out that the main paper does not clearly explain all the key contributions. While the authors have updated their draft to address some part of this concern, reviewers opine that the methods section of the paper still does not discuss the approach and the motivation for the various design choices (e.g., why a mixture of percentile and linear shifts?) clearly. 3) Reviewers also opine that some of the evaluation metrics also need more justification. For instance, Why is fraction satisfied measured at k   1 i.e, FS@1 measured and why not FS@2 or FS@3? Will the results look different for other values of k here? 4) Given that Rawal et. al. 2020 is a close predecessor of this work, it would be important to compare with that baseline to demonstrate the efficacy of the proposed approach. This comparison is missing. 

Given all the above, we are unable to recommend acceptance at this time. We hope the authors find the reviewer feedback useful.
This paper establishes the guarantee for the generalization of fairness aware learning in binary classification under PAC learning and a more practical asymptotic framework. The paper is nicely written, and theorems and proofs are well organized. However, novelty of the contribution seems to be insufficient. A future version of the paper may benefit from additional theoretical results or more diverse experiments.
The reviewers appreciated the treatment of the topic of certifiable robustness done in this work and although they had a number of concerns, I feel they were adequately addressed by the authors.
This paper studies improving continuous control. The paper suggests a practical, beneficial combination approach that does well in the presented experiments. It also provides some overview and comparison over several recent insights in RL. While both are valuable, multiple reviewers had concerns that the paper has some limitations on both. In particular, the proposed ensemble approach is quite simple though valuable, and that reviewers generally felt that raised their expectations as to the strength of the empirical results which was not yet there. The reviewersâ€™ provided a lot of detailed feedback that may be useful in revising the contribution.
This paper proposes to interpret point cloud data in Euclidean space as samples from some underlying probability distribution. Thus a set of point cloud data can be given the structure of a statistical manifold with a Riemannian metric structure, namely the Fisher Information Metric.
Applications to point cloud autoencoders are then studied.

Reviewers generally agree that the idea of equipping point cloud data with the Fisher information metric is interesting and has potential. However, there are concerns that the theoretical properties of the proposed framework have not been explored in depth. Furthermore, the experimental work should be enhanced to demonstrate the practical utility of the proposed formulation.
This paper discusses a relatively new concept called "magnitude" for finite metric spaces and investigates its potential applications in machine learning, in particular for computer vision.

Reviewers generally agree that this is an interesting concept and appreciate the algorithm for reducing its computational cost. 
However, there are concerns (1) that the concept is not well motivated theoretically for machine learning problems 
(2) the experimental results, for edge detection and adversarial robustness, are not convincing. More rigorous empirical work should be carried out.
The paper investigates adversarial examples in deep neural networks from a frequency based perspective. Their main conclusion is that adversarial examples are neither in high  or low frequency components, but instead depend on data. The topic is clearly important and the paper is overall clearly written and makes some interesting observations, backed up by empirical evidence.

However, the reviewers raised a number of critical concerns, including:
  Discussion of prior work is not adequate. The paper should better explain their contribution in contrast to prior work. Specifically, the authors mention Bernhard et al. (2021) as concurrent work, although the reviewers note that the work was published 5 months before. I realize the authors most likely develop their own line of work without knowing about Bernhard et al. (2021), but I would still suggest focusing more on the differences between them. I did not take this factor into account in the final decision.
  Novelty. Prior work has already shown adversarial examples are data dependent
  Concerns about experimental setup (only investigate one particular attack, measure of average noise gradient not completely justified, ...)

After discussion, one reviewer downgraded their score and two others kept a more negative score. Only one reviewer was more positive with somewhat low confidence.

Overall, the paper is more on the reject side for now. Further work is needed and I strongly encourage the authors to clearly highlight the contributions of the paper in contrast to prior work. On the plus side, the work clearly has some potential and addresses an interesting topic.
This paper proposes a methodology to create cheap NAS surrogate benchmarks for arbitrary search spaces. Certainly, the work is interesting and useful, with comprehensive studies to validate such approach. It should be credited as belonging to the first efforts of introducing and comprehensively studying the concept of surrogate NAS benchmarks. In AC s opinion, it is a solid paper that will (or has already) inspire many follow up works. The paper is well written. 

This paper received highly mixed ratings. Although the authors might not see, all reviewers actually participated in the private discussions. Reviewer 1eb8 indicated hesitation in her/his support. Reviewer yTPb stated that if not considering the arXiv complicacy, she/he "would certainly raise score by one level".  AC also reached out to Reviewer yTPb about her/his mentioned possibility of updating scores, and got confirmed that her/his original opinions wasn t changing after rebuttals. Besides, AC agrees the arXiv/NeurIPS complicacy shouldn t brought into the current discussion, and ignored that factor during decision making. 

The main sticking (and considered as valid) critique is on the relatively outdated and incomplete selection of baselines. As a benchmark paper, it should capture and diversify the recent methods. For example, the authors might consider adding: https://botorch.org/docs/papers (latest methods in Bayesian Optimization) https://github.com/facebookresearch/LaMCTS (latest methods in Monte Carlo Tree Search) https://facebookresearch.github.io/nevergrad/ (latest methods in Evolutionary algorithms)

Given the above concerns, AC considers this paper to sit on the borderline, and perhaps with pros outweighing the cons. Hence, a weak accept decision is recommended at this moment.
The paper studies federated learning with various sketching techniques used for communication. 

The main concerns from the reviewers are: 

1) the presentation can be improved; 

2) the novelty and related work section is not satisfactory since there have been papers on sketched federated learning; 

3) there is no numerical study to validate the efficacy of the method. 

I suggest the authors to take into consideration the feedback from the reviewers in the revision of the paper.
The paper proposes a framework, named Disentaglement via Contrast (DisCo), to learn disentangled representations via contrastive learning on well pretrained generative models. The method aims at simultaneously discovering semantically meaningful directions in pretrained generative models and training and encoder to extract them. The method uses contrastive learning where random samples perturbed along the discovered directions are regularised to be similar. The method is versatile and can be applied to various pretrained non disentangled generative models including GAN, VAE, and Flow. Extensive experimental evaluation shows the benefits of the approach.

The authors provided a strong rebuttal addressing many of the concerns raised by the reviewers, including running new experiments (such as adding the JEMMIG metric to measure disentanglement as requested by Reviewer sBQs). This led to all reviewers recommending to accept the work.

The paper provides an exhaustive empirical evaluation testing several models and results are convincing. This was highlighted by all reviewers.

While the high level description of the method is clear, in practice the method is quite sophisticated requiring many heuristics (e.g. entropy based domination loss or flipping hard negatives). This requires tuning several hyperparameters and complicates the message. This is mitigated by an ablation study presented by the authors highlighting the importance of each component. This was highlighted by Reviewer j95X and the AC agrees. The paper does provide implementation details, and reproducibility is not a concern.

Related to this point, Reviewer Go6R points out that the paper falls short in providing clear explanations on why the method is able to find meaningful semantic directions, and on where do the gains of the proposed model come from. While the paper could improve in this direction, the proposed empirical validation is convincing.

Overall the paper presents an interesting method that performs well in practice. All reviewers recommend accepting the work. The AC agrees with this recommendation.
The paper proposes a design of interpretable neural networks where each neuron is hand designed to serve a task specific role, and the network weights can be optimized via a few interactions with the environment. The reviewers acknowledged that the interpretability of neural networks is an important research direction. However, the reviewers pointed out several weaknesses in the paper, and there was a clear consensus that the work is not ready for publication. The reviewers have provided detailed and constructive feedback to the authors. We hope that the authors can incorporate this feedback when preparing future revisions of the paper.
The reviews end up split. The most positive reviewer notes that the paper may be useful broadly to researchers and practitioners. That is the main promise of the work. However as another reviewer points out, the authors fall short of convincingly explaining how the said practitioners and researchers would benefit from the work. And, also as noted in a review, the paper does not quite go deep enough into the discussion of what saliency means for different methods analyzed. 
Contrary to the positive reviewers comment, I do not think that a paper must provide "novel work to built upon", but for a paper that doesn t, there is a significant threshold on how convincing it must be regarding the potential impact of the work. I think the paper in its current state, even after the rebuttal/revision, does not meet this threshold.
While this paper has divergent reviews, reviewer hSRE has by far the most detailed review, seems clearly the most informed on the subject, and is the least supportive.  The main issues with the paper seems to be the degree of novelty and reviewer hSRE s feeling that the results on MojuCo are unclear and not explained in the paper.  But this alone does not seem like an adequate reason for rejection and hSRE seems happy with the other aspects of the paper.  Some of hSRE s original complaints do not concern me, such as the fact that first order stationarity does not imply Pareto optimality.  I am recommending a poster.
This paper tackles the neural contextual bandit problem, for which existing approaches consists rely on bandit algorithms based on deep neural networks to learn reward functions. In these existing strategies, exploration takes place over the entire network parameter space, which can be inefficient for the large size networks typically used in NTK based approaches. In this work, the authors address this by building on an existing technique of shallow exploration, which consists in exploring over the final layer of the network only, allowing to decouple the deep neural network feature representation learning from most of the exploration of the network parameters. More specifically, they propose a simple and effective UCB based strategy using this shallow exploration scheme, for which they provide a theoretical analysis. The proposed approach builds on several ideas for previous works, including borrowing proof techniques and theoretical arguments. Although this limits the novelty of the work, connecting these ideas together is not obvious and constitutes a significant contribution. Moreover, the proposed approach fixes an important known issue due to the matrix inversion in LinUCB, which could have a strong impact on the bandit community.
This is a borderline paper which elicited much discussion.
The paper proposes to extract features from pre trained networks through Kernel functions. It develops the idea of Fisher kernels for Neural networks calling it NFK. The methodology applies to both supervised and un supervised setting. 
The paper shows that proposed kernel has low rank structure and serves as the basis for developing an algorithm for computing the kernel on large datasets. The idea of extending Fisher kernels, their efficient computation, and investigating their usage in both Supervised and Unsupervised are some of the key strengths of the paper.
The reviewers though appreciative suggested (1) several new experiments, (2) inclusion of more background work related to Power method 
and (3)  have more technical discussion clarifying the contributions related to background. 
The author(s) during rebuttal tried to incorporate most of the suggestions in the revised draft. 

Since there was consensus on the novelty, the detailed discussions, and the results of the additional experiments, one could potentially accept this paper if there is space. The results will be interesting will be those who investigate the interplay of kernel methods and Deep Networks.
The paper proposes a  communicate then adapt  framework for decentralized optimization, with both theoretical and empirical analysis. The reviewers  main concern is the comparison in theory with prior methods like the GT DAdam. The convergence to a stationary point of GT DAdam seems to be faster than the proposed method in the important non convex optimization. The reviewers are not convinced by the strong claim that  communicate then adapt  is better than  adapt then communicate  as such  adapt then communicate  method can also achieve same or better rates, possibly with less hyper parameter tuning. I would suggest the authors to make more proper comparison with related methods.
This paper is on the theme of active reinforcement learning with a human/assistant in the loop. Under partial observability, an agent acts as per an interaction policy that gathers state/goal information from the assistant, while an operational policy assumed pre learnt in this paper executes low level actions.  The reviews acknowledge the relevance of this topic and that the paper is well structured and coherently presented overall. However, there are unanimous concerns around experimental evaluation being unconvincing, lack of strong baselines and lack of thorough coverage of related work precluding an accurate assessment of claimed contributions. As such, the paper is not in a form that can be accepted at ICLR    the authors are encouraged to revise their submission as per review feedback.
Reviewers are in agreement that this work is a useful, clear, documentary piece of work that shows the utility of CLIP on a number of popular V+L tasks.  There is a somewhat persistent concern that simply demonstrating that a stronger visual encoder leads to improvements downstream is not an insightful result on which the community can build.
This work proposes a branch and bound framework for adversarial attacks, based on the MIP formulation of attacking against ReLU networks. It adopts several heuristic tricks to accelerate the attack efficiency, and shows better attack performance on hard examples, compared to off the shelf MIP solvers. 

The main concerns in the first round reviews include: 
1. The limited novelty, since the problem and formulation is not novel, and the proposed method is the combination of several heuristic tricks, without theoretical analysis. 
2. The experiments are insufficient, and only MIP solvers are compared, while other attack methods and the ablation study of different tricks are not presented. 
3. The efficiency is higher than off the shelf MIP solvers, but significantly lower than other adversarial attack methods.

The authors made great efforts to respond these concerns, such as adding some baseline attack methods and ablation studies. Most reviewers appreciated the authors  efforts and raised their initial score. However, after reading the revised manuscript, reviews and responses, I think many serious issues still exist. 
1. Since the proposed method is only applicable to the MIP formulation of attacking ReLU networks, it could not become a new baseline to attack mainstream deep neural networks, which significantly restrict its practical contribution to the community. And, it didn t provide novel theoretical tools or better theoretical results of analyzing the robustness of ReLU networks. It just reduce the gap to the verified robustness of existing verification methods. Thus, I cannot find significant contributions to the community of adversarial examples, from both practical and theoretical perspectives. 
2. The experiments are still insufficient. There have been massive advanced white box and black box attack methods. The added FAB, square (black box), DIFGSM, MIFGSM (white box), are popular methods, but not SOTA methods. Besides, only small images of MNIST and CIFAR 10 are tested, while large scale images like ImageNet are not tested. It has been observed in many works, and according to my own experience, the attack performance on between small images (MNIST and CIFAR 10) and large images (ImageNet) is not always consistent. And, the low efficiency weakness (compared to other attack methods) of the proposed method may be further highlighted. 

It is not easy to decide to reject a submission with average score 7. The authors s efforts during their submission and the responses are greatly appreciated. However, I would like to accept works that can bring in real contributions to the research community. Hope the reviews and meta review could be helpful to further improve this work.
This paper proposes SH LDM, which approximates the LDM model with a hierarchy of clusters. The authors should discuss the details about clustering and how this algorithm can benefit from sparsity in a more rigorous language.  

The authors should review the rich literature on scaling up distance based methods such as kNN and kernel methods, which this paper belongs to. The title is also misleading; the paper mainly discusses scalable link prediction rather than learning new embeddings.

The reviewers have raised several questions about the experiments. For example, the main results should be a table for comparing the speed rather than the accuracy of the algorithms. Also, the original LDM should be included in the accuracy tables. The settings in the experiments, such as embedding dimensions, are not appropriate for large graphs.
The paper provides a method to edit trained models, meaning fix mistakes in a local way so as to not ruin generalizability. The techniques provided in the paper allow for an efficient way that makes this task possible for very large models.
There is an overall concensus that the problem of model editing in general is an important one and that solutions such as naive finetuning are not applicable for various reasons. In addition, the reviewers are convinced that given the need for an ML based approach for large models, this technique is superior to previous work, and mostly appreciate the novelties of the paper.

A major concern raised regards possible simpler baselines. There is a potential baseline of implementing an â€œengineering trickâ€ that will simply memorize the data points where the original model was mistaken, either in their original form or as embeddings, and during inference will override its output. I tend to agree that a comparison with such a baseline would improve the paper. This being said, the discussion highlighted that this baseline has several flaws that make it clear that it cannot completely replace the method proposed here. A naive implementation of it will be â€œtoo localâ€ and would not handle simple rephrasing of sentences. An implementation operating on the embedding space will be possible only in a subset of tasks. 

To conclude, although the paper has room for some improvement (that might actually be possible towards the camera ready version), I believe that even without it the paper is in a good enough state to be published. It tackles an important problem and could lead to further advancements.
The paper is aimed at providing an explaining the perceived lack of generalization results for Adam as compared to SGD. To this end the paper decouples the effect of adaptive per parameter learning rate and the momentum aspect of Adam. The paper shows that the while adaptive rates help escape saddle points faster   they are worse when consider the flatness of minima being selected. Further momentum has no effect on the flatness of minima but again leads to better optimization by providing a drift leading to saddle point evasion. They also provide a new algorithm Adai (based on inertia) targeted at better generalization of adaptive methods. 

The paper definitely provides an interesting perspective and the approach to decouple the effect of momentum and adaptive LR and study their efficacy in escaping saddle points and flatness of minima seems a very useful perspective. The primary reason for my recommendation is the presentation of the paper in terms of the rigor its assumptions to establish the results. These aspects have been highlighted by the reviewers in detail. I suggest the authors to carefully revisit the paper and improve the presentation of the assumptions, adding rigor to the presentation as well as adding justifications where appropriate especially in light of non standardness of these assumptions in optimization literature.
This paper studies a generalization of the randomized SVD algorithm with non standard Gaussian vectors, which is then used to incorporate any covariance matrix and to Hilbert Schmidt operators.  It uses a new kernel related to products of weighted Jacobi polynomials; and extensive numerical experiments further strengthen the case for this generalization.  Reviewers had initial concerns that were addressed, and the method should be of broad interest.
At a high level, the novelty of this paper is limited: RL2 with transformers instead of RNNs. The emphasis is then placed on the experimental evaluation. Unfortunately, the reviewers felt that the experimental methodology and results were not strong enough at this stage to warrant publication. During the rebuttal, the reviewers did not engage nor discuss the author response, unfortunately, so I do not know what they think of the rebuttal. However, on evaluating the concerns of the reviewers against the updated manuscript, I think the updates do not go far enough to satisfy the concerns raised (experiments + baselines). Therefore, I recommend rejection.
This submission proposes a trainable quantum tensor network (QTN) for quantum embedding generation on a variational quantum circuit (VQC), which is followed by a significant empirical study on the QTN VQC performance on the MNIST test dataset. However, the discussion during the review period raises some serious concerns about both the theoretical and empirical contributions of the submission. 

On the theory side, we acknowledge the authors propose the use of tensor train network (TTN) as the dimension reduction layer of input features, design the tensor product encoding (TPE), and characterize the representation power of QTN VQC.  However, a general feeling among all reviewers is that these contributions are kind of observational, rather than deep theoretical insights, based on existing results. For example, TTN is a well established tool in dimension reduction, and the representation power of QTN VQC can be derived as simple corollaries of the existing universal approximation theorem for a feed forward neural network.  Moreover, the authors emphasize a lot that the use of TTN allows a genuine end to end quantum implementation of QTN VQC because TTN can be relatively easier to implement as quantum circuits. However, the benefit of this end to end quantum implementation is rather unclear, especially for NISQ applications. In particular, because NISQ machines have limited quantum resources, should not a classical quantum hybrid implementation be preferred in the interest of saving quantum resources? As genuine quantum implementation becomes the major motivation for using TTN, there is no other theoretical justification for selecting TTN. 

We appreciate the authorsâ€™ efforts in carrying out the empirical study and the active interaction during the discussion period. Unfortunately,  most of the reviewers are not convinced by the existing experimental results. This is partially because the authors seek to support a very strong claim that QTN VQC would outperform the state of the art classical solutions, whereas the limitation of current quantum devices prevents any empirical study of QTN VQC at the scale comparable to the classical solutions. Moreover, as pointed out by one of the reviewers, there is hardly any existing evidence that quantum neural networks would be useful at all for commonly used datasets in NLP and vision. Given that, it is very unlikely that a conclusion could be reached by an empirical study on small scale instances. Nevertheless, since the authors aim to compare QTN with other quantum embedding methods,  other kinds of experiments are possible without directly comparing with the state of the art classical solutions. 

We believe that the submission would benefit a lot from addressing the concerns for both the theory and empirical parts and hope the authors would pursue it.
I thank the authors for their submission and active participation in the discussions. This papers is borderline with the majority of three reviewers leaning towards rejection and one leaning towards acceptance. All four reviewers unanimously agree on the empirical validation being a major weakness of this paper with reviewer kQnm putting less emphasis on this shortcoming. Overall, I side with reviewers Bne6, 2GeS and HJjY, and believe the paper needs to a more thorough set of experiments. I therefore recommend rejection.
The main motivation of this work is to introduce robustness in federated learning, through a Wasserstein uncertainty set. The end result, however, leaves a mixed feeling: As the reviewers pointed out, the authors, perhaps for computational convenience, forgo strong duality and treat the important variable gamma as a hyperparameter, which renders large part of the work follow immediately from existing work: essentially, we simply use a different loss function in FedAvg. While there may be advantages to choose one loss over another in any specific application, this itself is not a significant contribution. The comparison against existing FL algorithms is also a bit weak: Despite of the reviewer s request, the authors did not compare to other robust FL algorithms (e.g., AFL), thus it is not clear what is the real advantage of the proposed algorithm. As a result, we believe the current draft is not ready for publication.
The paper proposes a boosting algorithm for RL based on online boosting. The main advantage of the result is that the sample complexity does not explicitly depend on number of states. Post rebuttal, some of the reviewers have changed their opinion on the paper. However, overall the reviewers still seem to be on the fence about this paper. Seems like the paper combines the techniques from Hazan Singhâ€™21 along with a frank wolfe algorithm to deal with non convex sets but the reviewers seem to view this as not as significant a new contribution. 

I see the paper as being interesting but do agree with some of the comments of the reviewers and am leaning to a reject.
This paper is proposed to address neural network pruning at initialization with the help of meta gradients considering the high order relations between loss and optimization of trainable sub network. The paper is well organized and written with the clear logic. The discussions of related works, as well as their limitations, are comprehensive. To verify the proposed method, the authors have tested it on various benchmarks with different settings. Overall, the meta learning idea for model pruning is relatively new, which may bring more inspirations to the community.
This paper presents a simple, reasonable, alternative to target networks.  Given the effectiveness of target networks, and the fact that they are still somewhat poorly understood, this is a good topic for consideration.

It is unfortunate that the paper did not have more depth, in terms of analysis and/or analytical experiments that expose the properties of the suggested approach, and the mechanism still seems heuristic (and inspired by the success of target networks, and similar) more than principled.  That said, the proposed mechanism does seem somewhat effective (even if performance differences are not very pronounced), and is clean and simple to implement.

This version of the paper is rejected because we believe the paper could be a lot better than it currently is.  If the proposed regularisation mechanism is really as good as the authors argue, then it should be possible (and hopefully even easy) to demonstrate this clearly in more settings (e.g., in more algorithms).  Alternatively or additionally, the authors could consider digging deeper into the understanding of the method.  For instance, the paper often argues that target networks slow down learning, but (naively?) one could argue the exact same point (in general) for regularisation: this will trade off stability for speed.  It could be that the proposed mechanism is indeed a better way to achieve this trade off, but this is currently argued heuristically and not really proven (either theoretically, or with sufficient empirical evidence)
(For what it is worth, I personally did not find Section 3.2 particularly enlightening, because it is known these TD algorithms are not actually gradient algorithms, and hence considering  losses  and  gradients  in this way does not convince me we are getting at an actual deeper understanding of the dynamics of these algorithms.)

I wholeheartedly encourage the authors to take the comments and suggestions to heart and use these to improve the paper (as they have already started to do during this reviewing cycle), because I believe that there could be quite a good paper on this topic.  I hope the authors can convince themselves and their readers more convincingly that this idea is an actual, lasting contribution to the literature.  Ultimately, if they can, this will make the paper more impactful.  So although I appreciate this decision will come as a disappointment, I hope the authors also see this as an opportunity to make a larger research impact.

In particular, I would encourage considering: 1) comparing to our current theoretical understanding of target networks (see, e.g., [1]); 2) considering the effect of multi step updates (shown in, e.g., [2] and [3] to be quite effective); and 3) considering whether the proposed approach (or a variation thereof) could be understood as a more fundamental idea: could this update be derived from first principles?

[1] Shangtong Zhang, Hengshuai Yao, Shimon Whiteson (2021). Breaking the Deadly Triad with a Target Network.

[2] Hessel et al. (2017). Rainbow: Combining Improvements in Deep Reinforcement Learning. 

[3] van Hasselt et al. (2018). Deep Reinforcement Learning and the Deadly Triad.
This paper received scores of 5,5,6,8. The reviewer giving a score of 8 stated that they would ve given a 7, but that that is not an option in the system. The other reviewer giving an acceptance scores mentioned that they would also be OK with a rejection. The details of the assessment are thus less enthusiastic than could be assumed with an overall average score of 6. I am therefore weakly recommending rejection.

The main criticisms of the reviewers are lack of novelty, lack of deeper analyses that really provide insights into why zero cost operation scoring works, and lack of the number of NAS benchmarks tested. Out of these, personally, I would not criticize a lack of novelty, since it is not trivial to put together zero cost and one shot methods and the results appear promising.
However, even the most positive reviewer criticized that the work focuses on NAS Bench 201 heavily (which is particularly problematic given that NAS Bench 201 uses a fixed wiring and only allows the choice of operations; this may make the proposed method particularly applicable). During the rebuttal, the authors added NAS Bench 1shot, which is a very good step, but the proposed technique does not actually work as well there. While this may be due to the special nature of operations in the nodes rather than in the edges for NAS Bench 1shot1, for a revision, it would be good to add additional experiments on further NAS benchmarks in order to allow for a better understanding under which circumstances the proposed method works well. In particular, it would be interesting how well the method works on a quite different search space, such as the one of MobileNet.
The paper is focussed on proposing a new evaluation metric for evaluating untrained, randomly initialized neural network architectures towards predicting their accuracy/performance after training. The metric they propose is based on evaluating the gradient sign. The method shown to outperform existing approaches on NAS benchmarks.

The reviewers found the paper s idea simple but effective. The experimental evaluation and efficacy of the proposed method were the main strong points of the paper. The paper was also significantly improved during the discussion period both in terms of presentation and the scope of experiments/comparisons was enlarged. 

While the metric is theoretically motivated, I personally found some of the theoretical statements weak in terms of assumptions/clarity. I would request the authors to consider taking this and other suggestions made by reviewers into account 

Overall I recommend acceptance based on the strong and thorough experimental results shown by the paper on a problem of clear interest to the community.
This work proposes a novel Transformer Control Flow model and achieves near perfect accuracy on length generalization, simple arithmetic tasks, and computational depth generalization.  All reviewers give positive scores. AE agrees that this work is very interesting and has many potentials. It would be exciting if the author could extend this framework to more challenging tasks (e.g. visual reasoning [1. 2]).  Given the novelty of the proposed model, AC recommends accepting this paper!

[1]  CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning. ICCV 2017.

[2] PTR: A Benchmark for Part based Conceptual, Relational, and Physical Reasoning, NeurIPS 21
The problem setting studied in this paper is an extension of the problem setting of multi domain learning, where domain information is missing in training. This is an interesting and practical problem setting. However, regarding technical novelty, the contributions are relatively limited. Specifically, first, the overall idea is an extension of an existing multi domain learning method [Rebuffi et al. 2017] by replacing the domain indicators with learnable gates. Second, the idea of introducing learnable gates is borrowed from some previous works. Third, the sparse activation is also based on an existing work of sparsemax. Though the authors claim sparsemax or sparse activation was only used in the NLP domain, this does not increase the technical novelty by applying sparsemax to the CV domain. 

To be fair, the combination of the aforementioned techniques to solve the so called latent domain learning problem looks reasonable, while the technical contribution is not significant. Overall, I feel slightly positive about this work and recommend a weak acceptance (it could be considered for publication only if there is space).
This paper presents the use of scalable evolution strategies (S ES) in hierarchical reinforcement learning. After reviewing the paper and reading the comments from the reviewers, here are my comments:
 
  The proposal is quite novel. It requires major improvements to clearly state how this proposal contributes in the field.
  The main concern is about the experimental results. There are some flaws in the comparative results. Also, they do not support the proposal.
The paper presents a comprehensive analysis of lottery tickets hypothesis (LTH) on automatic speech recognition. The authors verified the existence of highly sparse â€œwinning ticketsâ€ in ASR task, and analyzed its robustness to noise, transferable to other datasets, and supported with structured sparsity.

As agreed with the reviewers, the paper is well written, the justification is thorough, and the finding that LTH does perform well on ASR is interesting. Though the novelty is marginal as it s a direct application of the LTH, this is the first investigation of LTH and brings new insights to the community. 

The decision is mainly based on the thorough justification of the LTH to ASR and new insights it brings to the community.
This paper proposes to re organize the training data in such a way that padding can be avoided. The novelty is somewhat limited and the results are what one would expect   a nice speed up of 2x but nothing really game changing. While the reviewer scores straddle the decision boundary, nobody is very strongly supportive of acceptance and the positive reviews actually have lower confidence.
This paper proposes a learning based method for shape registration that conditions on regions of the shape rather than learning from the entire point cloud in one shot.  The reviewers point out several questions about the method, thanks to expository issues as well as missing comparisons/ablation studies.  As the authors have chosen not to submit a rebuttal, I will refer them to the original reviews for details here for additional points of improvement.
The paper considers the important problem of tensor network optimization. Unfortunately the authors did not respond to the reviewers comments. Hence, several concerns remain about the proposed greedy algorithm, including its relationship with prior work and the issue of the ALS method being stuck in local minima for important classes of problems. We strongly encourage the authors to carefully examine the reviewers points and revise their work accordingly.
This paper considers the problem of distributionally robust fair PCA for binary sensitive variables. The main modeling contribution of the paper is the consideration of fairness and robustness of the PCA simultaneously, and the main technical contribution of the paper is the provision of a Riemannian subgradient descent algorithm for this problem and proof that it reaches local optima of this non convex optimization problem. The results will be of interest to those working at the intersection of fair and robust learning.
The paper makes two contributions: (1) Multi task benchmarks where the pareto solution is known analytically; and (2) a verification method for testing if solutions are on the pareto front. The authors make the point that MTL methods are applied to large scale problems, but fail to find the pareto front in problems where it is known. 

Reviewers appreciated the discussion and insights by the approach, and the idea that correctness of scalable methods should be evaluated with problems that have analytic solutions, but they also had grave concerns. The primary concern is that without an efficient search, a verification method that builds on filters randomly generated solutions cannot scale to high dimensional problems. There were also disagreement about the role of LS and comparison with previous literature. 

As a result, the contribution of the submission is not sufficient for acceptance to ICLR
Meta Review of Robust Robotic Control from Pixels using Contrastive Recurrent State Space Models

This work investigates a recurrent latent space planning model for robotic control from pixels, but unlike some previous work such as Dreamer and RNN+VAE based World Models, they use a simpler contrastive loss for next observation prediction. They presented results on the DM control suite (from pixels) with distracting background settings. All reviewers (including myself) agree that this is a well written paper, with clear explanation of their approach. The main weaknesses of the approach are on the experimental side (see review responses to authorâ€™s rebuttal by skrV and cjX3). Another recommendation from me is to strengthen the related work section to clearly position the work to previous work   there is clear novelty in this work, but this should be done to avoid confusion. The positive sign is that in the discussion phase, even the very critical cjX3, had increased their score and acknowledged the novelty from previous related work. In the current state, I cannot recommend acceptance, but Iâ€™m confident that with more compelling experiments recommended by the reviewers, and better positioning of the paper to previous work, I believe that this paper will surely be accepted at a future ML conference or journal. Iâ€™m looking forward to seeing a revised version of this paper for publication in the future.
The authors propose zero shot recommendations, a scenario in which knowledge from a recommender system enables a second recommender system to provide recommendations in a new domain (i.e. new users & new items). The idea developed by the authors is to transfer knowledge through the item content information and the user behaviors.

The initial assessment of the reviewers indicated that this paper was likely not yet ready for publication. The reviewers all recognized the potential usefulness of zero shot recommendations but argued that the implications of the proposed setup were somewhat unclear. Most notably, the reviewers raised the issue of how widely applicable this was in terms of distance between source and target domains (presumably the quality of the zero shot recommendations depends on the distance). 

The reviewers also noted that this was an application paper. This is of course within the CFP, and recommender systems papers have been published at ICLR in the past (for example one of the initial Session based RecSys paper w. RNNs) but the potential audience for this work is somewhat lower at ICLR. I should also add that I agree with the authors that their model is novel, but it s very much tailored to this application and it was unclear to me how it might be impactful on its own. All in all, this did not play a significant role in my recommendation.

During the discussion, there were significant, yet respectful, disagreements between the authors and the reviewers. It also seems like perhaps the authors missed an important reply from reviewer hJB8 made available through their updated review (see "Reply to rebuttal"). So the discussion between reviewers and authors did not converge. Having said that, even the two most positive reviewers have scores that would make this paper a very borderline one (a 6 and a 5). 

Further, I do find that reviewer s hJB8 arguments have merit and require another round of review. In particular, I think the role and effect of your simulated online scenario should be further discussed (note that I did read the new paragraph on it from your latest manuscript). For example, comparing to a baseline that can train with the data from this new domain would be useful even if at some point it ends up being an upper bound on the performance of your approach. I also found the question raised by the reviewer around the MIND results to be pertinent. Further characterizing pairs of domains in which the approach/works fails (even if empirically) would add depth to this paper. 

All in all, this paper has interesting ideas and I strongly encourage the authors to provide a more thorough experimental setup that fully explores the benefits and limitations of their zero shot approach.
The paper builds upon parametric distributionally robust optimization (PDRO) and proposes ratio PDRO (R PDRO) where the ratio of the worst case distribution and training distribution is parameterized by a discriminative network. This has a benefit over PDRO which needs to do generative modeling of worst case distribution. The paper empirically demonstrates R PDRO improves over existing methods on group robustness problems. Reviewer are overall positive about the paper, and have appreciated the significance of the problem, writing clarity, and thorough empirical evaluation. There were some minor questions which have been adequately addressed by the authors.
Initially, we had some borderline scores for this paper. After the (indeed very convincing!) rebuttal and a the end of the discussion phase, however, all reviewers agreed that this is a very solid piece of work, with significant methodological and practical contributions. I fully share this positive impression of the paper!.
The paper proposes a novel curriculum learning method for RL based on the concept of boosting. The proposed method builds on the curriculum value based RL framework and uses boosting to reuse action values from previous tasks when solving the current task. The method is analyzed theoretically in terms of approximation accuracy and convergence. Moreover, extensive experiments demonstrate the effectiveness of the method. The reviewers acknowledged the importance of the studied problem setting and generally appreciated the results. I want to thank the authors for their detailed responses that helped in answering some of the reviewers  questions and increased their overall assessment of the paper. At the end of the discussion phase, there was a clear consensus that the paper should be accepted. The reviewers have provided detailed feedback in their reviews, and we strongly encourage the authors to incorporate this feedback when preparing a revised version of the paper.
We thank the authors for their response. The reviewers agree that this paper provides contributions in automating privacy analyses under the Gaussian differential privacy (GDP) framework. The reviewers also pointed out several drawbacks of the paper. Most importantly, the reviewers do not find the presented applications to be convincing. In particular, the presented result can be much strengthened if the proposed method can lead to improved privacy analysis for more sophisticated algorithms such as DP SGD across a wide regime of epsilon and delta. (In general, the privacy guarantee is very weak with delta bigger than 1/n.) Overall, the paper does not seem to provide enough evidence to showcase the usefulness of their proposed method.
This submission proposes a new manner to learn ordinary differential equations, aiming to improve their efficiency. While judging it interesting, the reviewers are quite split on this work. Overall there was no strong consensus to accept, nor anyone willing to champion this work.

The main stated weaknesses are

  The reliance on the existence of a diffeomorphism (and its choice in the method)
  The choice of the base and its expressiveness
  A somewhat limited experimental section, not indicating strongly how amenable this would be to more complex problems.
This paper makes the important, albeit somewhat unsurprising, finding, that cell based NAS search spaces, and in particular the DARTS search space, include some operations that are much better than others. Reducing the search space to these allows even random architectures to yield good performance, similarly to the findings of "Designing Network Design Spaces", https://openaccess.thecvf.com/content_CVPR_2020/html/Radosavovic_Designing_Network_Design_Spaces_CVPR_2020_paper.html

This paper received mostly positive scores (5,6,6,8). While I agree with the negative reviewer that it would be good to study this on other benchmarks as well, I follow the positive reviewers in recommending acceptance. I encourage the authors to fix the remaining typos (there are still many) and to open source their code. This would increase the paper s impact a lot.

Finally, I would like to ask the authors to avoid protraying the misconception that we don t need large and powerful search spaces. In fact, as already hinted on in Section 6, we *do* need larger and more exciting search spaces in order to discover entirely novel architectures. Also the multi objective nature of NAS is not to be undervalued, so the take away of the paper should *not* be that we should design NAS benchmarks with really small & strong search spaces, but that, given a specific problem and objective, it may be prudent to evaluate whether the whole power of a given NAS search space is needed or whether it can be reduced to its essential parts.
This paper expands the spectral bias, which has been studied in a constrained situation such as the fully connected network, to a more practical situation of a multi class classification situation, and proposes a novel technique that can measure the smoothness through linear interpolation of test examples.

Two reviewers highly evaluated the importance of the research question considered in this study and the value of diverse experiments applying the proposed method in various directions, and suggested acceptance. On the other hand, two other reviewers suggested rejection due to the lack of rigor in writing and experiments. I strongly agree with the reviewer s concern that  the method was only verified  on CIFAR10 and the rigor of the experiment was lacking. Unlike the spectral bias paper, which is the basis of this study, this submission is not a theoretical paper, but rather an experimental paper. I admit that it is impossible to verify in various domains as mentioned by the author. However, I believe that verification on more diverse, especially larger scale datasets is essential at least focusing on the image classification task.
The paper proposes a new method for the problem of learning under instance dependent noise (IDN). The idea is to construct a variational approximation to the ideal training objective, which involves learning a single scalar C(x) per instance. In turn, each such scalar is treated as an additional parameter to be learned by the network.

Reviewers generally found the basic idea of the proposal to be interesting and novel, with the response clarifying some initial questions on the design of the network to learn C(x). The paper is also well written, and presents experiments on image and text classification benchmarks. Some concerns were however raised:

(1) _Limited theoretical justification_. There is limited formal analysis of when the proposed method can work well.

(2) _Lack of comparison to IDN baselines_. The original submission did not include any IDN baselines as comparison. The revision included results of the method of (Zhang et al.,  21a), which is on par or better than the proposed method; it seems that this baseline really ought to have been included in the original submission, but it is appreciated that these have been added. A related concern was the marginal gains over the GCE method on the CIFAR datasets.

(3) _Sufficiency of learning a single parameter_. The paper learns a single scalar per sample. Several reviewers were unsure on the sufficiency of this parameter to capture the underlying noise distribution.

For (1), the authors acknowledge theoretical analysis as an important future direction. This is perfectly reasonable, but does then require weighting more any issues with the the conceptual and empirical contributions of the paper.

For (2), the response clarified that most of these operate either in the binary setting, or require auxiliary information. This is a valid motivation for the present work; it would however be more compelling to include results in a binary setting, to better understand the strengths and weaknesses compared to existing proposals. The response also clarified the present method does not claim to improve upon state of the art performance, but rather proposes a simple method which has additional applications (as shown in Appendix E). This is a reasonable claim; however, to my taste, there is insufficient discussion of the PLC method (Zhang et al.,  21a), and what new conceptual information the present work offers.

For (3), the response argued that the present results already demonstrate the efficacy of using a single parameter, and that using multiple parameters can be studied in future work. One reviewer was not convinced of the efficacy being shown in some of the results in Appendix E. It could strengthen the work if there is an empirical analysis of when the single parameter assumption starts to break down; e.g., perhaps under increasing levels of CCN noise?

Overall, the paper has interesting ideas and some nice analyses. At the same time, there was clear scope for improvement in the original submission. This was partially addressed in the revision, but given that several domain experts retain reservations (particularly in regards to comparisons against prior IDN works), it is encouraged that the authors incorporate the above comments for a future version of the paper.
This paper proposes a novel ensemble method that enforces specification of the base models to improve accuracy. Base models are specialized on sub regions of the latent space. To calculate the ensemble prediction for a given example base models are weighted based on how close the example embeddings are to a learned â€œanchorâ€ embedding. To derive the correlation between samples and anchors,  transformer like attention mechanism is used. 

Reviewers pointed out limitations in the experimental analysis. In turn authors added experiments on tiny image net with additional architectures and a comparison to several additional baselines on CIFAR 10/100 supporting their findings, which improved the paper. Nevertheless, the paper remained on the borderline after the discussion period and reviewers continued to have doubts about the the significance and novelty of the proposed method, therefore the paper can not be accepted in its current form.
This paper presents an algorithm for approximating the hypergradient for bilevel optimization using a trick based on evolution strategies. It seems like an interesting approach, somewhat reminiscent of STNs, so it s interesting to see experiments with it.

I have a big concern about the proposed justification of the method, namely that each iteration is more efficient than methods based on HVPs. The authors claim that because they only require gradient computations and not HVPs, their method is more efficient. However, as various reviewers point out, the proposed method requires numerous inner optimization runs. By comparison, a method based on unrolled backprop simply requires a single inner run, followed by backprop on the trajectory; hence it should be about as expensive as 2 3 inner optimizations (or less if it is truncated BPTT). Similarly, each HVP has a small multiple of the cost of an inner optimization step, so methods based on HVPs ought to be cheaper unless they re doing quite a lot of HVPs.

It s conceivable the proposed method could be more efficient than AID, etc. if each hypergradient estimate is more accurate. However, this isn t shown, and it would seem surprising for an ES based approximation to be more accurate than the exact gradient.

The authors claim in the rebuttal that the efficiency claims aren t based on the theoretical analysis, but rather on the experiments (which use Q 1); however, Section 3.2 still finishes with the conclusion that ESJ is more efficient, which seems problematic.

I encourage the authors to formulate their theoretical claims more carefully and to consider the reviewers  other feedback, and I think this could make an interesting submission for the next cycle.
The proposed method for set representation learning with an application to mete learning is well motivated and reasonable. Reviewers  original concerns about novelty and technical presentation have been well explained and addressed in the revision. If some theoretical analysis can be provided regarding the proposed method, it would make this work stronger.

In summary, a positive recommendation is given here.
Although the initial scores of the paper were not positive, the authors managed to properly address the questions/concerns of the reviewers and the changes they made to the paper convinced the reviewers  to update their scores. This clearly shows that there were flaws in the original presentation of the paper. So, I would recommend the authors to take the reviewers  comments into account when they prepare the camera ready version of their work.
This paper proposes to represent a deep neural network as a graph and analyze its learning dynamics as a time series of weighted graphs corresponding to the neural network. As the graph representations, the authors propose to use a rolled representation in addition to a unrolled representation. Then, they proposed to utilize the graph features of the representations for predicting its predictive accuracy. 

This paper presents an interesting idea which could be used for predicting the test accuracy from the first few epochs training. However, there are also several weaknesses as pointed out by the reviewers. First, the justification of using the graph structure to predict the accuracy is weak (indeed, the graph structure can be used for prediction, but its necessity is not well supported), and there is no theory to support the proposed method. Second, the problem setting is a bit wired. The training data is generated by using the same architecture and data set. Although the authors gave additional experiments on the architecture generalization, it is still difficult to see how convincing the method is for more general settings. Third, baseline methods are not shown in their experiments.  
In addition to that, the thresholds for the classification tasks seem to be too small (like 40% in CIFAR10) which would make the problem too easy. Therefore, the practicality of the method is rather unclear. 

This paper is quite on the borderline, but for the reasons listed above, it is a bit below the acceptance threshold.
This paper proposes to apply the Koopman operator theory framework for analyzing sequence neural models. The authors considered two particular applications, namely sentiment analysis and ECG (electrocardiogram) classification.

Reviewers generally agree that the results obtained on the two tasks are interesting. However, there are concerns that the paper lacks methodological novelty (concerning the Koopman operator framework, which the authors agreed) and that the paper would be more suited for an applied conference and/or journal.
This paper investigates the problem of uncertainty calibration under distribution shift. Based on a distributionally robust learning (DRL) framework, the paper estimates the density ratio between the source and target domains to achieve well calibrated predictions under domain shift. 
As a plug in module, the proposed method benefits downstream tasks of unsupervised domain adaptation and semi supervised learning in experiments on Office31, Office Home, and VisDA 2017, demonstrating the superiority over empirical risk minimization (ERM) and the temperature scaling method measured by expected calibration error (ECE), Brier Score, and reliability plots.

After extensive interactions and discussions on the paper, the final scores were 6/5/5/5. AC considered the paper itself, and all reviews, author responses, and discussions, and reject the paper from the following concerns:
+ *Overclaimed Novelty*: This paper is mainly based on the well established competitive distributionally robust learning (DRL) framework. The contribution of the newly proposed regularization form that can further promote smoothed prediction and improve the calibration performance is relatively trivial. The designs of the resulting predictive form and the learning using new gradients, mentioned by the authors in the rebuttal, need further exploration and elaboration to verify its contributions.
+ *Lack of Clarifications*: Some key points mentioned by several reviewers are still not clear. For example, how well the density ratio is estimated, especially in high dimensions? Further, a positive correlation between HSF and density ratio is not enough to prove the main argument of the paper.
+ *Some statements are not well supported*: For example, the statement that "the harder the examples are, the farther away the examples are from the source domain", claimed by the author in the rebuttal, is untenable.

In summary, this paper studies a promising research direction of uncertainty estimation, but the work cannot be accepted before addressing the reviewers  comments. I suggest the authors to substantially revise their work by incorporating all rebuttal material as well as addressing the remaining concerns.
The authors propose a data free quantization method that can be applied post training quantization without backpropagation.  The method takes advantage of approximate Hessian information in a certain scalable approximate way. Based on the assumptions and deductions in the paper, SQuant tries to optimize constrained absolute sum of error (CASE) instead of MSE.  There are good empirical results showing the effectiveness of the method, and the paper is well written, and the method should be of broad interest.
The submission focuses on a set norm normalization layer for neural network models, which stands in contrast to a batch norm.  The majority of the reviewers felt that this submission is not suitable for publication at ICLR in its current form.  These concerns remained after the post rebuttal discussion.  Quoting from the reviewer discussion, the following points remained as significant concerns:


1. lack of novelty. normalization layers have been used previously in other sets architectures. The systematic approach for normalization in the current paper is nice but I am not sure how valuable it is. There exists already extensive literature on normalization layers for graphs (a generalization of sets).
2. lack of motivation for deep networks. The main claim in the paper (see the introduction and figure 1) is that 50 layers should perform better and since it does not seem to be the case in figure 1, it requires studying normalization layers. I am not sure it is a well established claim. What are the assumptions leading to the conclusion that deep architectures should perform better on the task considered in figure 1? I am also concerned that normalization layers have a major effect on improving "not so deep" networks with 3 10 layers and not only the extreme 50 layers case, making the comparison in the paper between only 3 and 50 layers not enough for telling the full story. Thus evaluation on more depths is required.

On the balance, the paper does not meet the threshold for acceptance in this round of peer review.
This paper proposes cpl mixVAE, a method for fitting discrete continuous latent variable models based on mixture representations and a novel consensus clustering constraint. After extensive discussion, no one was willing to argue in favor of acceptance, and a majority of the reviewers felt another round of revision is needed. Ultimately, I concur that while the ideas are novel and potentially interesting, more effort is needed to convincingly demonstrate the efficacy of the method. Valid concerns were also raised regarding the claimed "unsupervised" nature of the proposed method, a claim which at the very least requires some additional context. At this point, these outstanding issues require an additional round of revision.
The paper shows interesting and discussion inspiring results on multi agent trajectory prediction, as needed, for instance, in autonomous driving. Among the key technical ideas is a â€œconditional scene transformerâ€ approach for flexible predictions for different agents.  Results on two  public benchmarks are impressive. Some reviewers are a bit torn about the significance of the technical contributions and the analyses of the results. Nevertheless, on average, the reviewers vote the paper to be above the acceptance threshold.
This paper considers initialization methods for the k means algorithm.  There is a lot of prior work in this area. The reviewers were mildly positive on the paper.  There were several concerns on how the results were presented as well as the comparison to prior work. Importantly, no reviewer felt that there was a lot of novelty in the paper over the line of work on k means initialization.
This paper proposes a multi scale fusion self attention model for phrase information, which incorporates convolutional models into self attention to explicitly handle word to phrase correlation. This is paired with a sparse masking strategy to balance between word to word attention and word to phrase attention. The model achieves good performance on downstream tasks.

While the proposed method is simple and looks effective, reviewers have expressed concerns with lack of novelty (see the suggested missing references), lack of clarity in the experimental details, and unclear writing. Unfortunately, there was no response from the authors, which makes me recommend rejection. We urge the authors to follow the reviewers  suggestions in a future iteration of their work.
This paper offers new ideas about the key question of how to extend modern Transformer architectures to solve problems that require more reasoning steps than the model can implement in a single step forward pass. Reviewers were unanimous that the problem is important, and that the paper is a step in a promising direction. However, reviewers were also unanimous that the proposed experiments are too narrow to be the basis for any confident new claims in this area and that, in addition, the experimental design has a confound that makes it difficult to interpret, even after the addition of a new condition during discussion.
This paper considers the generalized target shift setting for domain adaptation and proposes an optimal transport map based approach to it. The considered setting for domain adaptation is rather general and of practical use. The proposed method seems sensible, as supported by the theoretical identifiability and empirical results. 

It is worth noting that the way to cite previous work seems to be improved. For instance, in the first paragraph of Introduction, the authors reviewed various settings for domain adaptation. For model shift, the authors cited previous work. However, when discussing covariate shift, target shift, and generalized target shift, the authors did not cite the original work that provides the categorization. For completeness, the authors may want to consider including the setting of conditional shift as well, which has received a number of applications in domain adaptation in computer vision. I believe the categorization of target shift, conditional shift, and generalized target shift was provided by Zhang et al. (2013). This work should also be cited when the authors give the problem definition in Section 2.1. The quality of the paper will be even better if the authors cite previous work in all the right places this may also make the authors  contribution clearer.
This paper proposes an "embedding layer" in which points on a model are mapped into a feature space, trained using a reconstruction based pretext task.  Then, the resulting embedding layer can be applied to shape data before using different learning architectures for modalities like meshes and point clouds.  The work is particularly interesting in its attempt to derive a learned shape representation that is agnostic to modality.  Some questions remained about experiments (e.g. baselines), but these are relatively minor and partially addressed in the rebuttal phase; also, sometimes the improvement seems to be marginal in practice.  

Two reviewers championed this work during the discussion phase.  The AC tends to agree this work is an interesting direction for future work and contains insight that the vision/learning communities might be able to use in other settings.
This paper proposes an adaptive sparse Huber additive model for for forecasting non stationary time series. The prior work has considered similar models for Gaussian innovations which is overly restrictive for a variety of applications such as finance. The results are supported both by theory and experiments. The results are novel and are of interest to ICLR and machine learning communities in general.
This paper presents a transformer model for learning representations of assembly code blocks, trained using a variant of the masked language modeling objective that encodes the full code block token sequence into a single bottleneck vector and then uses that vector to decode all the masked out tokens.  Overall reviewer assessment for this paper is on the rejection side, mostly due to the not so novel model architecture and training objective.  Experiments show that this variant of the MLM performs significantly better than the standard MLM objective without the bottleneck, which surprisingly is even worse than the simple TF IDF in many tasks.  This raises questions and it is unclear from the paper why the variant with a sequence level bottleneck should perform better than the standard MLM.  Binary code similarity detection has many implications in security, so this is a good domain to explore more in, and I encourage the authors to continue to improve this work and send it to the next venue.

One related work also published in the ML community comes to mind that the authors might not be aware of: Graph matching networks for learning the similarity of graph structured objects by Li et al., ICML 2019, which also looked at binary code similarity detection, but works at the function level.  It would be good to also take a look for other potentially missing related work.
Positional encoding of the input coordinates using Fourier basis [as described in (1)] is a common tool in the context of multilayer perceptrons (MLP). The author propose to replace the Fourier basis with one on manifolds M (2), such as the classical spherical harmonics (M S^2), the Fourier basis on M SO(3) or on M S^2 x S^2.

MLPs form an important tool of our times. Unfortunately, as it is elaborated by the reviewers the Fourier basis of the investigated manifolds are widely studied and the presented results are well known; the submission lacks novelty.
While generative model can be used to input data, this work propose to a novel discriminative learning approach to optimize this data imputation phase by deriving a discriminative version of the traditional variational lower bound (ELBO). The resulting bound can be estimated without bias with Monte Carlo estimation leads to a practical approach, leading to encouraging experimental performances.

The reviewers recognised the novelty and suggest that this approach, given its novelty and wide applicability, could be considered for an oral presentation.