The paper addresess an important problem of neural net robustness verification, and presents a novel approach outperforming state of art; author provided details rebuttals which clarified their contributions over the state of art and highlighted scalability; this work appears to be a solid and useful contribution to the field.  
The reviewers point our concerns regarding paper s novelty, theoretical soundness, and empirical strength. The authors provided to clarifications to the reviewers.
The submission suggests reducing the parameters in a conv lSTM by replacing the 3 gates in the standard LSTM with one gate. The idea is to get a more efficient convolutional LSTM and use it for video prediction. Two of the reviewers found the manuscript and description of the work difficult to follow and the justification for the proposed method lacking. Additionally, the contribution of this submission feels rather thin, and the experimental results are not very convincing: the absolute training time is too coarse of a measurement (and convergence may depend on many factors), and the improvements over PredNet seem somewhat marginal.  Finally, I agree with the reviewer that mentioned that a proper comparison with baselines should be done in such a way that the number of parameters is comparable (if #params is a main claim of the paper!). It is entirely plausible that if you reduce the number of parameters in PredNet by 40% (in some other way), its performance would also benefit.  With all this in mind, I do not recommend this paper be accepted at this time.
This paper proposes an approach for probing an environment to quickly identify the dynamics. The problem is relevant to the ICLR community. The paper is well written, and provides a detailed empirical evaluation. The main weakness of the paper is the somewhat small originality over prior methods on online system identification. Despite this, the reviewer s agreed that the paper exceeds the bar for publication at ICLR. Hence, I recommend accept.  Beyond the related work mentioned by the reviewers, the approach is similar to work in meta learning. Meta RL and multi task learning has typically been considered in settings where the reward is changing (e.g. see [1],[2],[3],[4], where [4] also uses an embedding based approach). However, there is some more recent work on meta RL across varying dynamics, e.g. see [5],[6]. The authors are encouraged to make a conceptual connection between this approach and the line of work in model based meta RL (particularly [5] and [6]) in the final version of the paper.  [1] Duan et al. https://arxiv.org/abs/1611.02779 [2] Wang et al. CogSci  17 https://arxiv.org/abs/1611.05763 [3] Finn et al. ICML  17 https://arxiv.org/abs/1703.03400 [4] Hausman et al. ICLR  17: https://openreview.net/forum?id rk07ZXZRb [5] Sæmundsson et al. https://arxiv.org/abs/1803.07551 [6] Nagabandi et al. https://arxiv.org/abs/1803.11347 
The paper describes a clipping method to improve the performance of quantization. The reviewers have a consensus on rejection due to the contribution is not significant. 
The paper analyzes the interesting problem of image denoising with neural networks by imposing simplifying assumptions on the Gaussianity and independence of the prior.  A bound is established from the analysis of (Hand & Voroninksi, 2018) that can be algorithmically achieved through a small tweak to gradient descent.    Unfortunately, the contribution of this paper is incremental given the recent works of (Hand & Voroninksi, 2018) and (Bora et al., 2017); an opinion the reviewers unanimously shared.  Reviewer opinion differed on whether they found the overall contribution to be barely acceptable or simply insufficient.  No reviewer detected a major advance, and there seems to be a question of whether the achievement is significant given the strength of the assumptions required to achieve the modest additions.  After scrutiny, the main theoretical contributions of the paper appear to be a bit overstated.  For example, the bound in Theorem 1 is quite weak: it does not establish convergence to a global minimizer (even under the strong assumptions given), but only that Algorithm 1 eventually remains in a neighborhood of the global minimizer.  It is true that this neighborhood can be made arbitrarily small by increasing the strength of the assumptions made on epsilon and omega, but epsilon remains a constant with respect to iteration count.  The subsequent claim that the algorithm achieves a denoising rate of sigma^2 k/n is not an accurate interpretation of Theorem 1, given that this claim would require require (at the very least) that epsilon can be made arbitrarily small, which it cannot be.  More precision is required in stating supportable conclusions from the given results.  The algorithmic motivation itself is rather weak, in the sense that this paper only provides an anecdotal demonstration that there are no spurious critical points beyond the negation of the global minimizer the theoretical support for this claim already resides in (Hand & Voroninski, 2018).  The provenance of such a central observation was not made sufficiently clear in the paper nor in the discussion.  An additional quibble about the experimental evaluation is that it does not compare to plain gradient descent (or other baseline optimization techniques), which the authors observe almost always works in the scenario considered.  It seems that the "negation tweak" embedded in Algorithm 1 has no real impact on the experimental results, raising the question of whether the contributions do indeed have any practical import.  The descriptions offered in the current paper suggest that a serious algorithmic advantage has yet to be demonstrated in any real experiment.  The paper requires a far better evaluation of Algorithm 1 in comparison to standard baseline optimizers, to support the case that the proposed algorithmic tweak has practical significance.  This paper remained in a weak borderline position after the review and discussion period.  In the end, this was a very difficult decision to make, but I think the paper would benefit from further strengthening before it can constitute a solid publication.
 pros:   novel idea for multi step QA which rewrites the query in embedding space   good comparison with related work   reasonable evaluation and improved results  cons:  There were concerns about missing training details, insufficient evaluation, and presentation.  These have been largely addressed in revision and I am recommending acceptance.
The reviewers agree this paper is not good enough for ICLR.
After much discussion, all reviewers agree that this paper should be accepted. Congratulations!!
This paper proposes an “iterative” regularized dual averaging method to sparsify CNN weights during learning. The main contribution seems to be in an iterative procedure where the weights are pruned out greedily by observing the sparsity of the averaged gradients. The reviewers agree that the idea seems straightforward and novelty is limited. For this reason, I recommend to reject this paper. 
This work proposes to improve trust region policy search (TRPO) by using normalizing flow policies. This idea is a straightforward combination of two existing techniques and is not super surprising in terms of novelty. In this case, really strong experiments are needed to support the work; this is , unfortunately, is the not the case.  For example, it was notice by the reviewers that the Mujoco TRPO experiments does not use the best implementation of TRPO, which makes it difficult to judge the strength of the work compared with state of the art. 
Reviewers generally found the RKHS perspective interesting, but did not feel that the results in the work (many of which were already known or follow easily from known theory) are sufficient to form a complete paper. Authors are encouraged to read the detailed reviewer comments which contain a number of critiques and suggestions for improvement.
This paper is about representation learning for calcium imaging and thus a bit different in scope that most ICLR submissions. But the paper is well executed with good choices for the various parts of the model making it relevant for other similar domains.
The paper proposes a method that aims to combine the strenghts of VAEs and GANs.  The paper establishes an interesting bridge between GANs and VAEs. The experimental results are encouraging, even though only relatively small datasets were used. It is encouraging that the method results in better reconstructions then ALI, a related method.  Some reviewers think that the paper contains limited novelty compared to the wealth of recent work on this topic (e.g. ALI/BiGAN). The paper s contribution is seen as incremental; e.g. the training is very similar to InfoGAN. Also, the claims of better sample quality over ALI seem insufficiently supported by the data.
This paper proposes an approach for learning to transfer knowledge across multiple tasks. It develops a principled approach for an important problem in meta learning (short horizon bias). Nearly all of the reviewer s concerns were addressed throughout the discussion phase. The main weakness is that the experimental settings are somewhat non standard (i.e. the Omniglot protocol in the paper is not at all standard). I would encourage the authors to mention the discrepancies from more standard protocols in the paper, to inform the reader. The results are strong nonetheless, evaluating in settings where typical meta learning algorithms would struggle. The reviewers and I all agree that the paper should be accepted, and I think it should be considered for an oral presentation.
The paper proposes a method of training implicit generative models based on moment matching in the feature spaces of pre trained feature extractors, derived from autoencoders or classifiers. The authors also propose a trick for tracking the moving averages by appealing to the Adam optimizer and deriving updates based on the implied loss function of a moving average update.   It was generally agreed that the paper was well written and easy to follow, that empirical results were good, but that the novelty is relatively low. Generative models have been built out of pre trained classifiers before (e.g. generative plug & play networks), feature matching losses for generator networks have been proposed before (e.g. Salimans et al, 2016).  The contribution here is mainly the  extensive empirical analysis plus the AMA trick.  After receiving exclusively confidence score 3 reviews, I sought the opinion of a 4th reviewer, an expert on GANs and GAN like generative models. Their remaining sticking points, after a rapid rebuttal, are with possible degeneracies in the loss function and class level information leakage from pre trained classifiers, making these results are not properly "unconditional". The authors rebutted this by suggesting that unlike Salimans et al (2016), there is no signal backpropagated from the label layer, but I find this particularly unconvincing: the objective in that work maximizes a "none of the above" class (and thus minimizes *all* classes). The gradient backpropagated to the generator is uninformative about which particular class a sample should imitate, but the features learned by the discriminator needing to discriminate between classes shape those gradients in a particular way all the same, and the result is samples that look like distinct CIFAR classes. In the same way, the gradients used to train GFMN are "shaped" by particular class discriminative features when trained against a classifier feature extractor.  From my own perspective, while there is no theory presented to support why this method is a good idea (why matching arbitrary features unconnected with the generative objective should lead to good results), the idea of optimizing a moment matching objective in classifier feature space is rather obvious, and it is unsurprising that with enough "elbow grease" it can be made to work. The Adam moving average trick is interesting but a deeper analysis and ablation of why this works would have helped convince the reader that it is principled.   This paper was very much on the borderline. Aside from quibbles over the fairness of comparisons above, I was forced to ask myself whether I could imagine that this would be a widely read, influential, and frequently cited piece of work. I believe that the carefully done empirical investigation has its merits, but that the core ideas are rather obvious and the added novelty of a poorly understood stabilized moving average is not enough to warrant acceptance.
This method proposes a primal approach to minimizing Wasserstein distance for generative models. It estimates WD by computing the exact WD between empirical distributions.  As the reviewers point out, the primal approach has been studied by other papers (which this submission doesn t cite, even in the revision), and suffers from a well known problem of high variance. The authors have not responded to key criticisms of the reviewers. I don t think this work is ready for publication in ICLR. 
The reviewers in general like the idea of using the Cramer Wold kernel, noting that its heavy tails and closed form solution are appealing properties that lead to increased stability and improved training. The main concern was novelty, as this paper can be seen as simply changing the kernel in WAE MMD. One suggestion is to more heavily highlight the CW distance, and in particular to find another useful application for it outside of WAE MMD.  The paper emphasizes frequently that the closed form loss function is a critical feature of this approach, however I don’t see any experiments that optimize WAE MMD under the CW distance while sampling from the Gaussian. This is important to measure the degree to which any improvement is attributable to a closed form solution, or to the distance measure itself. 
The paper presents an unsupervised visual abstraction model, used for reinforcement learning tasks. It is trained through intrinsic rewards, generated from temporal differences of inputs. This is similar to "learning to control pixels". The method is tested in DM Lab (3D environment, 2D navigation tasks) and Atari (Montezuma s Revenge).  The paper is at times hard to follow, and it seems the improvements accompanying the rebuttals did not convince reviewers to change their notes significantly. The experiments do not contain enough comparisons to other models, baselines, nor ablations, to sustain the claims.  In its current form, this is not acceptable for publication at ICLR.
All reviewers are in agreement for a rejection decision. Details below.
This paper proposes a VAE model with arbitrary conditioning. It is a novel idea, and the model derivation and training approach are technically sound. Experiments are thoughtfully designed and include comparison with latest related works.  R1 and R3 suggested the original version of the paper was lack of comparison with relevant work and the authors provided new experiments in the revision. The rebuttal also addressed a few other concerns about the novelty and clarity raised by R3.  Based on the novel contribution in handling missing feature imputation with VAE, I would recommend to accept the paper. It is worth noticing that there is another submission to ICLR (https://openreview.net/forum?id ByxLl309Ym) that shares a similar idea of constructing the inference network with binary masking, although it is designed for a pre trained VAE model.  There are still two weaknesses pointed out by R3 that would help improve the paper by addressing them: 1. The paper does not handle different kinds of missingness beyond missing at random. 2. VAE model makes the trade off between computational complexity and accuracy. Point 1 would be a good direction for future research, and point 2 is a common problem for all VAE approaches. While the latter should not become a reason to reject the paper, I encourage the authors to take MCMC methods into account in the evaluation section. 
This paper tackles the task of end to end systems for dialogue generation and proposes a novel, improved GAN for dialogue modeling, which adopts conditional Wasserstein Auto Encoder to learn high level representations of responses. In experiments, the proposed approach is compared to several state of the art baselines on two dialog datasets, and improvements are shown both in terms of objective measures and human evaluation, making a strong support for the proposed approach. Two reviewers suggest similarities with a recent ICML paper on ARAE and request including reference to it and also request examples demonstrating differences, which are included in the latest version of the paper.
This paper proposes a method to encourage diversity of Bayesian dropout method. A discriminator is used to facilitate diversity, which the method deal with multi modality. Empirical results show good improvement over existing methods. This is a good paper and should be accepted. 
This paper shows convergence of stochastic gradient descent  for the problem of learning weight matrices for a linear dynamical system  with non linear activation.  Reviewers agree that the problem considered is both interesting and challenging. However the paper makes many simplifying assumptions   1) both input and hidden state are observed, a very non standard assumption, 2) analysis requires increasing activation functions, cannot handle ReLU functions. I agree with R2 and think these assumptions make the results significantly weaker. R1 and R3 are more optimistic, but authors response does not give an insight into how one might extend this analysis to the setting where hidden state is not observed. Relaxing these assumptions will make the paper more interesting. 
This paper introduces a very simple but effective method for the neural architecture search problem. The key idea of the method is a particular continuous relaxation of the architecture representation to enable gradient descent like differentiable optimization. Results are quite good. Source code is also available. A concern of the approach is the (possibly large) integrality gap between the continuous solution and the discretized architecture. The solution provided in the paper is a heuristic without guarantees.  Overall, this is a good paper. I recommend acceptance.
The reviewers found that paper is well written, clear and that the authors did a good job placing the work in the relevant literature.  The proposed method for using multiple discriminators in a multi objective setting to train GANs seems interesting and compelling.  However, all the reviewers found the paper to be on the borderline.  The main concern was the significance of the work in the context of existing literature.  Specifically, the reviewers did not find the experimental results significant enough to be convinced that this work presents a major advance in GAN training.  
The paper tackles an interesting problem, which is effectively modeling biological time series data. The advantages of deep neural networks over structured models like HMMs are their ability to learn features from the data, whereas probabilistic graphical models suffer from "model mismatch", where the available data must be carefully processed in order to fit the assumptions of the PGM. Any work advancing this topic would be extremely welcome in the world of machine learning in biology.  However, the reviewers each raised individual concerns about the paper regarding its clarity and quality, and the authors did not respond. Thus, the reviewers scores remain unchanged, and the rough consensus is a rejection.
This paper proposes an Optimal Binary Functional Search (OBFS) algorithm for searching with general score functions, which generalizes the standard similarity measures based on Euclidean distances. This yields an extension of the classical approximate nearest neighbor search (ANNS). As observed by the reviewers, this work targets an important research direction. Unfortunately, the reviewers raised several concerns regarding the clarity and significance of the work. The authors provided a good rebuttal and addressed some concerns, but not to the degree that reviewers think it passes the bar of ICLR. We encourage the authors to further improve the work to address the key concerns. 
This paper is entitled Lipschitz regularized deep networks generalize. In fact, the paper has nothing in particular to do with neural networks. It is really the study of minimizers of a Lipschitz regularized risk functional over certain nonparametric classes. The connection with neural networks is simply that one can usually achieve zero empirical risk for (overparametrized) neural networks and so, in deep learning practice, neural networks behave like a nonparametric class. Given the lack of connection with neural networks, one cannot logically learn anything specific about neural networks from this paper. It should be renamed... perhaps "Lipschitz regularization with an application to deep learning".  One could raise issues of technical novelty, as it seems many of the key results are known.  I also question the insight that the bounds provide: they end up depending exponentially on the dimension of the data manifold. In the noiseless case, this exponential dependence arises from a triangle inequality between an arbitrary data point x and the nearest training data point! In the noisy case, this exponential dependence appears in a nonasymptotic uniform law of large numbers over the class of L Lipschitz functions. There s no insight into deep learning here. It s also hard to judge whether these rates are what is explaining deep learning practice: it s unclear what the manifold  dimensionality is, but it seems unlikely that this bound explains empirical performance (even if it describes the asymptotic rate of convergence).   One of the main results shows that, in the face of corrupted label (corrupted in a particular way), Lipschitz regularization can ```"undo" the corruption. However, convergence is not measured with respect to the true labeling function, but rather to the solution to the population regularized risk functional. How this solution relates to the true labeling function is unclear.  The paper also purports to resolve a mystery of generalization raised by Zhang et al (ICLR 2017). In that paper, the authors point to the diametrically opposed generalization performance on "true" and "random" labels. In fact, this paper does not resolve this problem because Zhang et al. were interested in how SGD solves this problem without explicit regularization. That Lipschitz regularization could solve this problem is borderline obvious.  I wanted to make a few comments.  In the rebuttal with reviewers, the question of parametric rates comes up. I think there s some confusion on both the part of the reviewer and authors. The parametric rates are often apparent but not real. The complexity terms often have an uncharacterized dependence on the number of data (through the learning algorithm) and on the size of the network (which is implicitly chosen based on the data complexity). In practice, these bounds are vacuous.  At some point, the authors argue that "In practice, u_n(x) is rounded to the nearest label, so once |u_n u_0| < 1/2, all classification results will be correct after rounding." I m not entirely sure I understand the logic here. First, convergence to u_0 is not controlled, but rather convergence to u*. u* may spend most of its time near the decision boundary, rendering uniform convergence almost useless. One would need noise conditions (Tysbakov) to make some claim.  Some other issues:  1. in (1), u ranges over X\to Y, but is then applied also to a weight vector.  2.  Is"continuum variational problem" jargon? If so, cite. Otherwise, taking limits of rho_n and J makes sense only if J is suitably continuous, which depends on the loss function. You later address this convergence and so you should foreshadow.  3. Notation L[u;\rho] in (5) should be L[u,\rho], no?  4. (Goodfellow et al., 2016, Section 5.2) is an inappropriate citation for the term "Generalization".  5. in Thm 2.7.,  there is reference to a sequence mu_n and i assume the sequence elements is indexed by n, but then  n appears in the probability with which the bound holds, and so this bound is not about the sequence but about a solution for \rho_n for fixed n.  6. Id should not be italicized in the statement of Lemma 2.10.  Use mathrm not text/textrm.  it should also be defined.  7. "convex convex" typo.
A paper that studies two tasks: machine translation and image translation. The authors propose a new multi agent dual learning technique that takes advantage of the symmetry of the problem. The empirical gains over a competitive baseline are quite solid. The reviewers consistently liked the paper but have in some cases fairly low confidence in their assessment.
The paper introduces a new variant of the Dropout method. The reviewers agree that the procedure is clear. However, motivations behind the method are heuristic, and have to lean much on empirical evidence. A strong motivation behind the procedure is lacking, and the motivation behind the method is unclear. Furthermore, the empirical evidence is lacking in detail and could use better comparisons with existing literature.
The paper proposes a novel  lossless compression scheme that leverages latent variable models such as VAEs. Its main original contribution is to improve the bits back coding scheme [B. Frey 1997] through the use of asymmetric numeral systems (ANS) instead of arithmetic coding. The developed practical algorithm is also able to use continuous latents. The paper is well written but the reader will benefit from prior familiarity with compression schemes. Resulting message bit length is shown empirically to be close to ELBO on MNIST. The main weakness pointed out by reviewers is that the empirical evaluation is limited to MNIST and to a simple VAE, while applicability to other models (autoregressive) and data (PixelVAE on ImageNet) is only hinted to and expected bit length merely extrapolated from previously reported log likelihood. The work could be much more convincing if its compression was empirically demonstrated on larger and better models and larger scale data. Nevertheless reviewers agreed that it sufficiently advanced the field to warrant acceptance.
Important problem (explainable AI); sensible approach, one of the first to propose a method for the counter factual question (if this part of the input were different, what would the network have predicted). Initially there were some concerns by the reviewers but after the author response and reviewer discussion, all three recommend acceptance (not all of them updated their final scores in the system). 
This paper proposes using a tensor train low rank decomposition for compressing neural network parameters.  However the paper falls short on multiple fronts 1)lack of comparison with existing methods 2) no baseline experiments. Further there are concerns about correctness of the math in deriving the algorithms, convergence and computational complexity of the proposed method.  I strongly suggest taking the reviews into account before submitting the paper it again. 
The main idea of this paper is to use nearest neighbor search to to accelerate iterative thresholding based sparse recovery algorithms. All reviewers were  underwhelmed by somewhat straightforward combination of  existing results in sparse recovery and nearest neighbor search.  While the proposed method seems effective in practice, the paper has the feel of not being a fully publishable unit yet. Several technical questions were asked but no author feedback was provided to potentially lift this paper up.
Dear authors,  All reviewers pointed out that the proximity with Dropout warranted special treatment and that the justification provided in the paper was not enough to understand why exactly the changes were important. In its current state, this work is not suitable for publication to ICLR.  Should you decide to resubmit this work to another venue, please take the reviewers  comments into account.
see my comment to the authors below
The paper is on the borderline. From my reading, the paper presents a reasonable idea with quite good results on novel image generation and one shot learning. On the other hand, the comparison against the prior work (both generation task and one shot classification task) is not convincing. I also feel that there are many work with similar ideas (I listed some below, but these are not exhaustive/comprehensive list), but they are not cited or compared, I am not sure about if the proposed concept is novel in high level. Although some implementation details of this method may provide advantages over other related work, such comparison is not clear to me.  Disentangling factors of variation in deep representations using adversarial training https://arxiv.org/abs/1611.03383 NIPS 2016  Rethinking Style and Content Disentanglement in Variational Autoencoders https://openreview.net/forum?id B1rQtwJDG ICLR 2018 workshop  Disentangling Factors of Variation by Mixing Them http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Disentangling_Factors_of_CVPR_2018_paper.pdf CVPR 2018  Separating Style and Content for Generalized Style Transfer https://arxiv.org/pdf/1711.06454.pdf  Finally, I feel that the writing needs improvement. Although the method is intuitive and has simple idea, the paper seems to lack full details (e.g., principled derivation of the model as a variant of the VAE formulation) and precise definitions of terms (e.g., second term of LF loss).  
This paper introduces a method that aims to solve the problem of  posterior collapse  in variational autoencoders (VAEs). The problem of posterior collapse is well documented in the VAE literature, and various solutions have been proposed. Existing proposed solutions, however, aim to solve the problem by either changing the objective function (e.g. beta VAE) or by changing the prior and/or approximate posterior models. The proposed method, in contrast, aims to solve the problem by bringing the VAE optimization procedure closer to the EM optimization procedure. Every iteration in optimization consists of SGD updates to the inference model (E step), performed until the approximate posterior converges. This is followed by a single SGD update of the generative model. The multi update E step makes sure that the M step optimizes something closer to the marginal log likelihood, compared to what we would normaly do in VAEs (joint optimization of both inference model and generative model).  The experiments are relatively small scale, but convincing.  The reviewers agree that the method is clearly described, and that the proposed technique is well supported by the experiments. We think that this work will probably be of high interest to the ICLR community.
This paper combines probabilistic models, VAEs, and self organizing maps to learn interpretable representations on time series. The proposed contributions are a novel and interesting combination of existing ideas, in particular, the extension to time series data by modeling the cluster dynamics. The empirical results show improved unsupervised clustering performance, on both synthetic and real datasets, compared to a number of baselines. The resulting 2D embedding also provides an interpretable visualization.  The reviewers and the AC identified a number of potential weaknesses in the presentation in the original submission: (1) there was insufficient background on SOMs, leaving the readers unable to comprehend the contributions, (2) some of the details about the experiments were missing, such as how the baselines were constructed, (3) additional experiments were needed in regards to the hyper parameters, such as number of clusters and the weighting in the loss, and (4) Figure 4d required a description of the results.  The revision and the comments by the authors addressed most of these comments, and the reviewers felt that their concerns had been alleviated.  Thus, the reviewers felt the paper should be accepted.
Dear authors,  Your proposition of adding a noise scaling with the diagonal of the gradient covariance to the updates as a middle ground between the identity and the full covariance is interesting and tackles the timely question of the links between optimization and generalization.  However, the reviewers had concerns about the experiments that did not reveal to which extent each trick had an influence. I would like to add that, even though the term Fisher is used for both the true Fisher and tne empirical one, these two matrices encore very different kind of information. In particular, the latter is only defined when there is a dataset. Hence, your case study  (section 3.2) which uses the true Fisher does not apply to the empirical Fisher.  I encourage the authors to pursue in this direction but to update the experimental section in order to highlight the impact of each technique used.
This paper presents methods to scale learning of embedding models estimated using neural networks. The main idea is to work with Gram matrices whose sizes depend on the length of the embedding. Building upon existing works like SAG algorithm, the paper proposes two new stochastic methods for learning using stochastic estimates of Gram matrices.   Reviewers find the paper interesting and useful, although have given many suggestions to improve the presentation and experiments. For this reason, I recommend to accept this paper.  A small note: SAG algorithm was originally proposed in 2013. The paper only cites the 2017 version. Please include the 2013 version as well. 
The method presented here adapts an SGD preconditioner by minimizing particular cost functions which are minimized by the inverse Hessian or inverse Fisher matrix. These cost functions are minimized using natural (or relative) gradient on the Lie group, as previously introduced by Amari. This can be extended to learn a Kronecker factored preconditioner similar to K FAC, except that the preconditioner is constrained to be upper triangular, which allows the relative gradient to be computed using backsubstitution rather than inversion. Experiments show modest speedups compared to SGD on ImageNet and language modeling.  There s a wide divergence in reviewer scores. We can disregard the extremely short review by R2. R1 and R3 each did very careful reviews (R3 even tried out the algorithm), but gave scores of 5 and 8. They agree on most of the particulars, but just emphasized different factors. Because of this, I took a careful look, and indeed I think the paper has significant strengths and weaknesses.   The main strength is the novelty of the approach. Combining relative gradient with upper triangular preconditioners is clever, and allows for a K FAC like algorithm which avoids matrix inversion. I haven t seen anything similar, and this method seems potentially useful. R3 reports that (s)he tried out the algorithm and found it to work well. Contrary to R1, I think the paper does use Lie groups in a meaningful way.  Unfortunately, the writing is below the standards of an ICLR paper. The title is misleading, since the method isn t learning a preconditioner "on" the Lie group. The abstract and introduction don t give a clear idea of what the paper is about. While some motivation for the algorithms is given, it s expressed very tersely, and in a way that will only make sense to someone who knows the mathematical toolbox well enough to appreciate why the algorithm makes sense. As the reviewers point out, important details (such as hyperparameter tuning schemes) are left out of the experiments section.  The experiments are also somewhat problematic, as pointed out by R1. The paper compares only to SGD and Adam, even though many other second order optimizers have been proposed (and often with code available). It s unclear how well the baselines were tuned, and at the end of the day, the performance gain is rather limited. The experiments measure only iterations, not wall clock time.   On the plus side, the experiments include ImageNet, which is ambitious by the standards of an algorithmic paper, and as mentioned above, R3 got good results from the method.  On the whole, I would favor acceptance because of the novelty and potential usefulness of the approach. This would be a pretty solid submission of the writing were improved. (While the authors feel constrained by the 8 page limit, I d recommend going beyond this for clarity.) However, I emphasize that it is very important to clean up the writing. 
I agree with the reviewers that this is a strong contribution and provides new insights, even if it doesn t quite close the problem.   p.s.: It seems that centering the weight matrices at initialization is a key idea. The authors note that Dziugaite and Roy used  bounds that were based on the distance to initialization, but that their reported numerical generalization bounds also increase with the increasing network size. Looking back at that work, they look at networks where the size increases by a very large factor (going from e.g. 400,000 parameters roughly to over 1.2 million, so a factor of 2.5), at the same time the bound increases by a much smaller factor. The type of increase also seems much less severe than those pictured in Figures 3/5. Since Dzugate and Roy s bounds involved optimization, perhaps the increase there is merely apparent.
The authors present an interesting approach for combining finite state automata to compose new policies using temporal logic. The reviewers found this contribution interesting but had several questions that suggests that the current paper presentation could be significantly clarified and situated with respect to other literature. Given the strong pool of papers, this paper was borderline and the authors are encouraged to revise their paper to address the reviewers’ feedback.   
This paper proposes an approach for imitation learning from unsegmented demonstrations. The paper addresses an important problem and is well motivated. Many of the concerns about the experiments have been addressed with follow up comments. We strongly encourage the authors to integrate the new results and additional literature to the final version. With these changes, the reviewers agree that the paper exceeds the bar for acceptance. Thus, I recommend acceptance.
This paper explores the use of sequential information to improve imitation learning, essentially using recurrent networks (LSTM) instead of a simple NN in several existing imitation learning models (BC, GAIL, etc.). On the positive side, the empirical results are good, showing improvement in terms of attained rewards, convergence speed and stability. There are however some significant issues with the way the way the approach is motivated and positioned with respect to existsing work. In particular, the issue described in the paper is due to the fact they consider POMDPs (not MDPs): this should have been more clearly explained. There are also issues with the Related Work section. For these reasons, the paper is not quite ready for publication. 
The paper presents a novel strategy for statistically motivated feature selection i.e. aimed at controlling the false discovery rate. This is achieved by extending knockoffs to complex predictive models and complex distributions; specifically using a variational auto encoder to generate conditionally independent data samples with the same joint distribution.   The reviewers and ACs noted weakness in the original submission related to the clarity of the presentation, relationship to already published work, and concerns about the correctness of some main claims (this mostly seems to have been fixed after the rebuttal period). There are additional concerns about a thorough evaluation of the claimed results, as the ground truth is unknown. The authors (and reviewers) also note a similar paper submitted to ICLR with the same goal but implemented using GANs. Nevertheless, there remain significant concerns about the clarity of the presentation.
The manuscript proposes benchmarks for studying generalization in reinforcement learning, primarily through the alteration of the environment parameters of standard tasks such as Mountain Car and Half Cheetah. In contrast with methodological innovations where a numerical argument can often be made for the new method s performance on well understood tasks, a paper introducing a new benchmark must be held to a high standard in terms of the usefulness of the benchmark in studying the phenomenon under consideration.  Reviewers commended the quality of writing and considered the experiments given the set of tasks to be thorough, but there were serious concerns from several reviewers regarding how well motivated this benchmark is and restrictions viewed as artificial (no training at test time), concerns which the updated manuscript has failed to address. I therefore recommend rejection at this stage, and urge the authors to carefully consider the desiderata for a generalization benchmark and why their current proposed set of tasks satisfies (or doesn t satisfy) those desiderata.
The authors consider the use of MAML with model based RL and applied this to robotics tasks with very encouraging results. There was definite interest in the paper, but also some concerns over how the results were situated, particularly with respect to the related research in the robotics community. The authors are strongly encouraged to carefully consider this feedback, as they have been doing in their responses, and address this as well as possible in the final version. 
The paper addresses a variant of multi agent reinforcement learning that aligns well with real world applications   it considers the case where agents may have individual, diverging preferences. The proposed approach trains a "manager" agent which coordinates the self interested worker agents by assigning them appropriate tasks and rewarding successful task completion (through contract generation). The approach is empirically validated on two grid world domains: resource collection and crafting. The reviewers point out that this formulation is closely related to the principle agent problem known in the economics literature, and see a key contribution of the paper in bringing this type of problem into the deep RL space.  The reviewers noted several potential weaknesses: They asked to clarify the relation to prior work, especially on the principle agents work done in other areas, as well as connections to real world applications. In this context, they also noted that the significance of the contribution was unclear. Several modeling choices were questioned, including the choice of using rule based agents for the empirical results presented in the main paper, and the need for using deep learning for contract generation. They asked the authors to provide additional details regarding scalability and sample complexity of the approach.  The authors carefully addressed the reviewer concerns, and the reviewers have indicated that they are satisfied with the response and updates to the paper. The consensus is to accept the paper.  The AC is particularly pleased to see that the authors plan to open source their code so that experiments can be replicated, and encourages them to do so in a timely manner. The AC also notes that the figures in the paper are very small, and often not readable in print   please increase figure and font sizes in the camera ready version to ensure the paper is legible when printed.
This paper presents a new technique for modifying neural network structure, and suggest that this structure provides improved robustness to black box attacks, as compared to standard architectures. The paper is very thorough in its experimentation, and the method is simple and quite easy to understand. It also raises some important questions about adversarial examples.   However, there are serious concerns regarding the evaluation methodology. In particular, the authors claim "black box robustness" but do not test against any query based attacks, which are known to perform better against gradient masking based adversarial defenses. Furthermore, it is not clear why one would expect adversarial examples to transfer between models representing two completely different functions (i.e. from a standard model to a random mask model). So, the gray box evaluation is much more informative and, unfortunately, random mask seems to provide little to no robustness in this setting.  Given how fundamental sound and convincing evaluation is for proposed defense methods, the submission is not ready for publication yet. In particular, the authors are urged to (a) evaluate on stronger black box attacks, and (b) compare to a baseline that is known to be non robust, (e.g. JPEG encoding or SAP), to verify that these results are actually due to black box robustness and not simply obfuscation.
The paper proposes an unsupervised domain adaptation solution applied for semantic segmentation from simulated to real world driving scenes. The main contribution consists of introducing an auxiliary loss based on depth information from the simulator. All reviewers agree that the solution offers a new idea and contribution to the adaptation literature. The ablations provided effectively address the concern that the privileged information does in fact aid in transfer. The additional ablation on the perceptual loss done during rebuttal is also valuable and should be included in the final version.   The work would benefit from application of the method across other sim2real dataset tasks so as to be compared to the recent approaches mentioned by the reviewers, but the current evaluation is sufficient to demonstrate the effectiveness of the approach over baseline solutions. 
This paper introduces a novel idea, and demonstrates its utility in several simulated domains. The key parts of the algorithm are (a) to prefer keeping and using samples in the ER buffer where the corresponding rho_t, using the current policy pi_t, are not too big or small and (b) preventing the policy from changing too quickly, so that samples in the ER buffer are more on policy.    They key weakness is not better investigating the idea of making the ER buffer more on policy, and the effect of doing so. The experiments compare to other algorithms, but do not sufficiently investigate the use of both Point 1 and Point 3. Further, the appendix contains an investigation into parameter sensitivity and gives some confidence intervals. However, the presentation of this is difficult to follow, and so it is difficult to gauge the sensitivity of Ref ER. With a more thorough experimental section, better demonstrating the results (not necessarily running more things), the paper would be much stronger.   For more context, the authors rightly mention "It is commonly believed that off policy methods (e.g. Q learning) can handle the dissimilarity between off policy and on policy outcomes. We provide ample evidence that training from highly similar policy experiences is essential to the success of off policy continuous action deep RL." Q learning can significantly suffer from changing the state sampling distribution. However, adjusting sampling in the ER buffer using rho_t does not change the state sampling distribution, and so that mismatch remains a problem. Changing the policy more slowly (Point 3) could help with this more. In general, however, these play two different roles that need to be better understood. The introduction more strongly focuses on classifying samples as more on or off policy, to solve this problem, rather than the strategy used in Point 3. So, from the current pitch, its not clear which component is solving the issues claimed with off policy updates.   Overall, this paper has some interesting results and is well written. With more clarity on the roles of the two components of Ref ER and what they mean for making the ER buffer more on policy, in terms of both action selection and state distribution, this paper would be a very useful contribution to stable control. 
Reviewers are in a consensus and recommended to reject. Please take reviewers  comments into consideration to improve your submission should you decide to resubmit.
The authors have presented an empirical study of generalization and regularization in DQN. They evaluate generalization on different variants of Atari games and show that dropout and L2 regularization are beneficial. The paper does not contain any major revelations, nor does it propose new algorithms or approaches, but it is a well written and clear demonstration, and it would be interesting to the deep RL community. However, the reviewers did not feel that the paper met the bar for publication at ICLR because the experiments were not more comprehensive, which would be expected for an empirical study. The AC will side with the reviewers but hopes that the authors will expand their study and resubmit to another venue in the future.
All reviewers gave an accept rating: 9, 7 &6. A clear accept   just not strong enough reviewer support for an oral.
While the main idea of the paper is nice, the reviewers are not satisfied with the clarity of the material and the execution.
The reviewers unanimously agreed the paper did not meet the bar of acceptance for ICLR. They raised questions around the technical correctness of the paper, as well as the experimental setup. The authors did not address any reviewer concerns, or provide any response. Therefore, I recommend rejection.
The reviewers have agreed this paper is not ready for publication at ICLR. 
Strengths     Hallucinations are a problem for seq2seq models, esp trained on small datasets  Weankesses    Hallucinations are known to exists, the analyses / observations are not very novel     The considered space of hallucinations source (i.e. added noise) is fairly limited, it is not clear that these are the most natural sources of hallucination and not clear if the methods defined to combat these types would generalize to other types. E.g., I d rather see hallucinations appearing when running NMT on some natural (albeit noisy) corpus, rather than defining the noise model manually.     The proposed approach is not particularly interesting, and may not be general. Alternative techniques (e.g., modeling coverage) have been proposed in the past.      A wider variety of language pairs, amounts of data, etc needed to validate the methods. This is an empirical paper, I would expect higher quality of evaluation.  Two reviewers argued that the baseline system is somewhat weak and the method is not very exciting.    
This paper introduces a technique called EquiNorm, which normalizes the weights of convolutional layers in order to control covariate shift. The paper is well written and the reviewers agree that the solution idea is elegant. However, the reviewers also agree that the experiments presented in the work were insufficient to prove the method s superiority. Reviewer 2 also expressed concerns about the poor results on ImageNet, which calls into question the significance of the proposed method. 
The reviewers lean to accept, and the authors clearly put a significant amount of time into their response. I will also lean to accept. However, the comments of reviewer 2 should be taken seriously, and addressed if possible, including an attempt to cut the paper length down.
Unfortunately, this paper fell just below the bar for acceptance.  The reviewers all saw significant promise in this work, stating that it is intriguing, "novel and provides an interesting solution to a challenging problem" and that "many interesting use cases are clear".  AnonReviewer2 particularly argued for acceptance, arguing that the proposed approach provides a very flexible method for incorporating constraints in neural network training.  A concern of AnonReviewer2 was that there was no guarantee that this loss would be convex or converge to an optimum while statisfying the constraints.  The other two reviewers unfortunately felt that while the proposed approach was "interesting", "promising" and "intriguing", the quality of the paper, in terms of exposition, was too low to justify acceptance.  Arguably, it seems the writing doesn t do the idea justice in this case and the paper would ultimately be significantly more impactful if it was carefully rewritten.  
This paper suggests that adversarial vulnerability scales with the dimension of the input of neural networks, and support this hypothesis theoretically and experimentally.   The work is well written, and all of the reviewers appreciated the easy to read and clear nature of the theoretical results, including the assumptions and limitations. (The AC did not consider the criticisms raised by Reviewer 3 justified. The norm bound perturbations considered here are a sufficiently interesting unsolved problem in the community and a clear prerequisite to solving the broader network robustness problem.)   However, many of the reviewers also agreed that the theoretical assumptions   and, in particular, the random initialization of the weights   greatly oversimplify the problem. Reviewers point out that the lack of data dependence and only considering the norm of the gradient considerably limit the significance of the corresponding theoretical results, and also does not properly address the issue of gradient masking. 
this is a meta review with the recommendation, but i will ultimately leave the final call to the programme chairs, as this submission has a number of valid concerns.  the proposed approach is one of the early, principled one to using (fixed) dense vectors for computing the predictive probability without resorting to softmax, that scales better than and work almost as well as softmax in neural sequence modelling. the reviewers as well as public commentators have noticed some (potentially significant) short comings, such as instability of learning due to numerical precision and the inability of using beam search (perhaps due to the sub optimal calibration of probabilities under vMF.) however, i believe these two issues should be addressed as separate follow up work not necessarily by the authors themselves but by a broader community who would find this approach appealing for their own work, which would only be possible if the authors presented this work and had a chance to discuss it with the community at the conference. therefore, i recommend it be accepted. 
The paper proposes a novel  local combinatorial search algorithm for the discrete target propagation framework of Friesen & Domingos 2018, and shows a few promising empirical results.  Reviewers found the paper well written and clear, and two of them were enthusiastic about the direction of this research.  But all reviewers agreed that the paper is too preliminary, particularly in its empirical coverage. More extensive experiments are needed to compare with competitive approaches form the literature, for the task of training hard threshold networks. Experiments would need to evaluate the algorithms on larger models and data more representative of the field, to measure how the approach can scale, and to convince of the superiority or advantage of the proposed method.
Strengths: The work proposes a novel architecture for graph to sequence learning. The paper shows improved performance on synthetic transduction tasks and for graph to text generation.   Weaknesses: Multiple reviewers felt that the experiments were insufficient to evaluate the novel aspects of the submission relative to prior work.  Newer experiments with the proposed aggregation strategy and a different graph representation were not as promising with respect to simple baselines.   Points of contention: The discussion with the authors and one of the reviewers was particular contentious. The title of the paper & sentences within the paper such as "We propose a new attention based neural networks paradigm to elegantly address graph  to sequence learning problems" caused significant contention, as this was perceived to discount the importance of prior work on graph to sequence problems which led to a perception of the paper "overclaiming" novelty.  Consensus: Consensus was not reached, but both the reviewer with the lowest score and one of the reviewers giving a 6 came to the consensus that the experimental evaluation does not yet evaluate the novel aspects of the submission thoroughly enough.  Due to the aggregate score, factors discussed above (and others) the AC recommends rejection; however, this work shows promise and additional experimental work should allow a new set of reviewers to better understand the behaviour and utility of the proposed method.  
The paper received mixed reviews. The proposed ideas are reasonable and it shows that unpaired data can improve the performance of unseen video (action) classification tasks and other related tasks. The authors rightfully argue that the main contribution is the use of unpaired, multimodal data for learning a joint embedding (that generalizes to unseen actions) with positive results, but not the use of attentional pooling mechanism. Despite this, as the Reviewer3 points out, technical novelty seems minor as there are quite many papers on learning joint embedding for multimodal data. Many of these works were evaluated for fine grained image classification setting, but there is no reason that such methods cannot be used here. The revision only compares against methods published in 2017 or before. So more comprehensive evaluation would be needed to fully justify the proposed method. In addition, it seems that the proposed method has fairly marginal gain for the generalized zero shot learning setting. Overall, the paper can be viewed as an application paper on unseen action recognition tasks but the technical novelty and more rigorous comparisons against recent related work are somewhat lacking. I recommend rejection due to several concerns raised here and by the reviewers. 
The authors present an algorithm for label noise correction when the label error is a function of the input features.  Strengths   Well motivated problem and a well written paper.  Weaknesses   The reviewers raised concerns about theoretical guarantees on generalization; it is not clear why energy based auto encoder / contrastive divergence would be a good measure of label accuracy especially when the feature distribution has high variance, and when there are not enough clean examples to model this distribution correctly.   Evaluations are all on toy like tasks with small training sets, which makes it harder to gauge how well the techniques work for real world tasks.   It’s not clear how well the algorithm can be extended to multi class problems. The authors suggested 1 vs all, but have no experiments or results to support the claim.  The authors tried to address some of the concerns raised by the reviewers in the rebuttal, e.g., how to address unavailability of correctly labeled data to train an auto encoder. But other concerns remain. Therefore, the recommendation is to reject the paper. 
The authors propose a framework for compressing neural network models which involves applying a weight distortion function periodically as part of training. The proposed approach is relatively simple to implement, and is shown to work for weight pruning, low rank compression and quantization, without sacrificing accuracy.  However, the reviewers had a number of concerns about the work. Broadly, the reviewers felt that the work was incremental. Further, if the proposed techniques are important to get the approach to work well in practice, then the paper would be significantly strengthened by further analyses. Finally, the reviewers noted that the paper does not consider whether the specific weight pruning strategies result in a reduction of computational resources beyond potential storage savings, which would be important if this method is to be used in practice.  Overall, the AC tends to agree with the reviewers criticisms. The authors are encouraged to address some of these issues in future revisions of the work. 
This paper presents experiments showing that a linear mapping existing between the hidden states of RNNs trained to recognise (rather than model) formal languages, in the hope of at least partially elucidating the sort of representations this class of network architectures learns. This is important and timely work, fitting into a research programme begun by CL Giles in 92.  Despite its relatively low overall score, I am concurring with the assessment made by reviewer 1, whose expertise in the topic I am aware of and respect. But more importantly, I feel the review process has failed the authors here: reviewers 2 and 3 had as chief concern that there were issues with the clarity of some aspects of the paper. The authors made a substantial and bona fide attempt in their response to address the points of concern raised by these reviewers. This is precisely what the discussion period of ICLR is for, and one would expect that clarity issues can be successfully remedied during this period. I am disappointed to have seen little timely engagement from these reviewers, or willingness to explain why they are stick by their assessment if not revisiting it. As far as I am concerned, the authors have done an appropriate job of addressing these concerns, and given reviewer 1 s support for the paper, I am happy to add mine as well.
The paper proposes an information theoretic quantity to measure the performance of transferred representations with an operational appeal, easier computation, and empirical validation.   The relation of the proposed measure to test accuracy is not considered. The operational meaning holds exactly only in the special case of linear fine tuning layers. The paper seems to import heavily from previous works.   Reviewers found it difficult to understand whether the proposed method makes sense, that the computation of relevant quantities might be difficult in general, and that the comparison with mutual information was not clear. The revision addresses these points, adding experiments and explanations. Yet, none of the reviewers gives the paper a rating beyond marginally above acceptance threshold.   All reviewers found the paper interesting and relevant, but none of them found the paper particularly strong. This is a borderline case of a sound and promising paper, which nonetheless seems to be missing a clear selling point.   I would suggest that developing the program laid out in the conclusions could make the contributions more convincing, in particular the development of more scalable algorithms and the application of the proposed measure to the design of hierarchies for transfer learning. 
Strengths: Execution of paper well received. Results on new dataset. Convincing demonstration that the proposed approach learns good semantic representations.  Weaknesses: Reviewers felt the positioning with prior work was not as strong as it could be. Reviewers wanted to have seen an ablation study.  Contention: Some general agreement among both the one positive reviewer and negative reviewer that the representation of prior work is skewed.  Consensus: With two 5s and one 6 the numerical average of 5.33 is representative of the aggregated consensus opinion which is that the work is just below threshold in its current form.
Graph neural networks are an increasingly popular topic of research in machine learning, and this paper does a good job of studying the representational power of some newly proposed variants. The framing of the problem in terms of the WL test, and the proposal of the GIN architecture is a valuable contribution. Through the reviews and subsequent discussion, it looks like the issues surrounding Theorem 3 have been resolved, and therefore all of the reviewers now agree that this paper should be accepted. There may be some interesting followup work based on studying depth, as pointed out by reviewer 1, but this may not be an issue in GIN and is regardless a topic for future research.
The authors propose a GAN based anomaly detection method based on simulating anomalies (low density regions of the data space) in order to train an anomaly classifier.  While the paper addresses an interesting take on an important problem, there are many concerns raised by reviewers including novelty, clarity, attribution, reproducibility, the use of exclusively proprietary data, and a multitude of textual mistakes. Overall, the paper shows promise but does not seem to be a mature and polished piece of work. As there has been no rebuttal or update to the paper I have no choice but to concur with the reviewers  initial assessments and reject.
The authors have extended previous publications on curiosity driven, intrinsically motivated RL with this broad empirical study on the effectiveness of the curiosity algorithm on many game environments, the merits of different feature sets, and limitations of the approach. The paper is well written and should be of interest to the community. The experiments are well conceived and seem to validate the general effectiveness of curiosity. However, the paper does not actually have any novel contribution compared against prior work, and there are no great insights or takeaways from the empirical study. Therefore, the reviewers were somewhat divided on how confident they were that the paper should be accepted. Overall, the AC agrees that it is a valuable paper that should be accepted even though it does not deliver any algorithmic novelty.
This paper presents a recurrent tree structured linear dynamical system to model the dynamics of a complex nonlinear dynamical system. All reviewers agree that the paper is interesting and useful, and is likely to have an impact in the community. Some of the doubts that reviewers had were resolved after the rebuttal period.   Overall, this is a good paper, and I recommend an acceptance.
The reviewers appreciated the clarity of writing, and the importance of the problem being addressed. There was a moderate amount of discussion around the paper, but the two reviewers who responded to the author discussion were split in their opinion, with one slightly increasing their score to a 6, and the other remaining unconvinced. The scores overall are borderline for ICLR acceptance, and given that, no reviewer stepped forward to champion the paper.
The reviewers and ACs acknowledge that the paper has a solid theoretical contribution  because it give a convergence (to critical points) of the ADAM and RMSprop algorithms, and also shows that NAG can be tuned to match or outperform SGD in test errors. However, reviewers and the AC also note that potential improvements for the paper a) the exposition/notations can be improved; b) better comparison to the prior work could be made; c) the theoretical and empirical parts of the paper are somewhat disconnected; d) the proof has an error (that is fixed by the authors with additional assumptions.) Therefore, the paper is not quite ready for publications right now but the AC encourages the authors to submit revisions to other top ML venues.  
This paper proposes a method to compute embeddings of states and actions that facilitate computing measures of surprise for intrinsic reward. Though some of the ideas are quite interesting, there are currently issues with the experiments and the motivation.  The experiments have high variance across the 5 runs, with significant overlap of shaded regions representing just one standard deviation from the mean. It is hard to draw any conclusions about improved performance, and statements like the following are much too strong: "For vision based exploration tasks, our results in Figure 5 show that EMI achieves the state of the art performance on Freeway, Frostbite, Venture, and Montezuma’s Revenge in comparison to the baseline exploration methods." Further, the proposed approach has three new hyperparameters (lambdas), without much understanding into how to set them or their effect on the results. Specific values are reported for the different game types, without explanation for how or why these values were chosen.   Similarly strong claims, that are not well substantiated, are given for the proposed approach. This paper seems to suggest that this is a principled approach to using surprise for exploration, contrasted to other ad hoc approaches ("Other approaches utilize more ad hoc measures (Pathak et al., 2017; Tang et al., 2017) that aim to approximate surprise."). Yet, the paper does not define surprise (say by citing work by Itti and Baldi on Bayesian surprise), and then proposes what is largely a intuitive approach to providing a good intrinsic reward related to surprise. For example, "we show that imposing linear topology on the learned embedding representation space (such that the transitions are linear), thereby offloading most of the modeling burden onto the embedding function itself, provides an essential informative measure of surprise when visiting novel states." This might be intuitively true, but I do not see a clear demonstration in Section 4.2 actually showing that this restriction provides a measure of surprise. Additionally, some of the choices in Section 4.2 are about estimating "irreducible error under the linear dynamics model", but irreducible error is about inherent uncertainty (due to stochasticity and partial observability), not due to the choice of modeling class. In general, many intuitive choices in the algorithm need to be better justified, and some claims disparaging other work for being ad hoc should be toned down.   Overall, this paper is as yet a bit preliminary, in terms of clarity and experiments. In a further iteration, with some improvements, it could be a useful contribution for exploration in image based environments. 
This paper is on the problem of adversarial example generation in the setting where the predictor is only accessible via function evaluations with no gradients available. The associated problem can be cast as a blackbox optimization problem wherein finite difference and related gradient estimation techniques can be used. This setting appears to be pervasive. The reviewers agree that the paper is well written and the proposed bandit optimization based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements. 
Both R3 and R1 argue for rejection, while R2 argues for a weak accept. Given that we have to reject borderline paper, the AC concludes with "revise and resubmit".
AR1 is concerned with the presentation of the paper and the complexity as well as missing discussion on recent  embedding methods. AR2 is concerned about comparison to recent methods and the small size of datasets.  AR3 is also concerned about limited comparisons and evaluations. Lastly, AR4 again points out the poor complexity due to the spectral decomposition. While authors argue that the sparsity can be exploited to speed up computations, AR4 still asks for results of the exact model with/without any approximation, effect of clipping spectrum, time complexity versus GCN, and more empirical results covering all these aspects. On balance, all reviewers seem to voice similar concerns which need to be resolved. However, this requires more than just a minor revision of the manuscript. Thus, at this time, the proposed paper cannot be accepted.  
This work analyses the use of parameter averaging in GANs. It can mainly be seen as an empirical study (while also a convergence analysis of EMA for a concrete example provides some minor theoretical result) but experimental results are very convincing and could promote using parameter averaging in the GAN community. Therefore, even if the technical novelty is limited, the insights brought by the paper are intesting. 
All reviewers agree that the paper should be rejected and there is no rebuttal.
The paper presents both theoretical analysis (based upon lambda stability) and experimental evidence on stability of recurrent neural networks. The results are convincing but is concerns with a restricted definition of stability. Even with this restriction acceptance is recommended. 
After a healthy discussion between reviewers and authors, the reviewers  consensus is to recommend acceptance to ICLR. The authors thoroughly addressed reviewer concerns, and all reviewers noted the quality of the paper, methodological innovations and SotA results.
The paper provides a distributed optimization method, applicable to decentralized computation while retaining provable guarantees.  This was a borderline paper and a difficult decision.  The proposed algorithm is straightforward (a compliment), showing how adaptive optimization algorithms can still be coordinated in a distributed fashion.  The theoretical analysis is interesting, but additional assumptions about the mixing are needed to reach clear conclusions: for example, additional assumptions are required to demonstrate potential advantages over non distributed adaptive optimization algorithms.  The initial version of the paper was unfortunately sloppy, with numerous typographical errors.  More importantly, some key relevant literature was not cited:   Duchi, John C., Alekh Agarwal, and Martin J. Wainwright. "Dual averaging for distributed optimization: Convergence analysis and network scaling." IEEE Transactions on Automatic control 57.3 (2012): 592 606. In addition to citing this work, this and the other related works need to be discussed in relation to the proposed approach earlier in the paper, as suggested by Reviewer 3.  There was disagreement between the reviewers in the assessment of this paper.  Generally the dissenting reviewer produced the highest quality assessment.  This paper is on the borderline, however given the criticisms raised it would benefit from additional theoretical strengthening, improved experimental reporting, and better framing with respect to the existing literature.
Pros:   interesting algorithmic idea for using successor features to propagate uncertainty for use in epxloration   clarity  Cons:   moderate novelty   initially only simplistic experiments (later complemented with Atari results)   initially missing baseline comparisons   no regret based analysis   questionable soundness because uncertainty is not guaranteed to go down  All the reviewers found the initial submission to be insufficient for acceptance, and the one reviewer who read the rebuttal/revision did not change their mind, despite the addition of some large scale results (Atari).
The main strength of the paper is to provide a clear mathematical characterization of invertible neural networks. The reviewers and the AC also note potential weakness including 1) the exposition of the paper can be much improved; 2) it s unclear how these analyses can help improve the training algorithm or architecture design since these characterizations are likely not computable; 3) the novelty compared to previous work Carlsson et al. 2017 may not be enough for ICLR acceptance. These weakness are considered critical issues by the AC in the decision. 
The paper analyzes the performance of CNN models when data is mislabelled in different manners.  The reviewers and AC note the critical limitation of novelty of this paper to meet the high standard of ICLR.  AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.
+ an interesting task   learning to decompose questions without supervision    reviewers are not convinced by evaluation. Initially evaluated on MetaQA only, later relation classification on WebQuestions has been added.  It is not really clear that the approach is indeed beneficial on WebQuestion relation classification (no analysis / ablations) and MetaQA is not a very standard dataset.     Reviewers have concerns about comparison to previous work / the lack of state of the art baselines. Some of these issues have been addressed though (e.g., discussion of Iyyer et al. 2016)    
Pros:   an original idea: learn an additional inverse policy (that minimizes reward) to help find actions that should be avoided.  Cons:   not clearly presented   conclusions are not not validated    empirical evidence is weak   no rebuttal  The three reviewers reached consensus that the paper should be rejected in its current form, but make numerous suggestions for improving it for a future submission. 
The present work proposes to improve backdoor poisoning attacks by only using "clean label" images (images whose label would be judged correct by a human), with the motivation that this would make them harder to detect. It considers two approaches to this, one based on GANs and one based on adversarial examples, and shows that the latter works better (and is in general quite effective). It also identifies an interesting phenomenon that simply using existing back door attacks with clean labels is substantially less effective than with incorrect labels, because the network does not need to modify itself to accommodate these additional correctly labeled examples.  The strengths of this paper are that it has a detailed empirical evaluation with multiple interesting insights (described above). It also considers efficacy against some basic defense measures based on random pre processing.  A weakness of the paper is that the justification for clean label attacks is somewhat heuristic, based on the claim that dirty label attacks can be recognized by hand. There is additional justification that dirty labels tend to be correlated with low confidence, but this correlation (as shown in Figure 2) is actually quite weak. On the other hand, natural defense strategies against the adversarial examples based attack (such as detecting and removing points with large loss at intermediate stages of training) are not considered. This might be fine, as we often assume that the attacker can react to the defender, but it is unclear why we should reject dirty label attacks on the basis that they can be recognized by one detection mechanism but not give the defender the benefit of other simple detection mechanisms for clean label attacks.  A separate concern was brought up that the attack is too similar to that of Guo et al., and that the method was not run on large scale datasets. The Guo et al. paper does somewhat diminish the novelty of the present work, but not in a way that I consider problematic; there are definitely new results in this paper, especially the interesting empirical finding that the Guo et al. attack crucially relies on dirty labels. I do not agree with the criticism about large scale datasets; in general, not all authors have the resources to test on ImageNet, and it is not clear why this should be required unless there is a specific hypothesis that running on ImageNet would test. It is true that the GAN based method might work more poorly on ImageNet than on CIFAR, but the adversarial attack method (which is in any case the stronger method) seems unlikely to run into scaling issues.  Overall, this paper is right on the borderline of acceptance. There are interesting results, and none of the weaknesses are critical. It was unfortunately the case that there wasn t room in the program this year, so the paper was ultimately rejected. However, I think this could be a strong piece of work (and a clear accept) with some additional development. Here are some ideas that might help:  (1) Further investigate the phenomenon that adding data points that are too easy to fit do not succeed in data poisoning. This is a fairly interesting point but is not emphasized in the paper. (2) Investigate natural defense mechanisms in the clean label setting (such as filtering by loss or other such strategies). I do not think it is crucial that the clean label attack bypasses every simple defense, but considering such defenses can provide more insight into how the attack works e.g., does it in fact lead to substantially higher loss during training? And if so, at what stage does this occur? If not, how does it succeed in altering the model without inducing high loss?
The reviewers expressed some interest in this paper, but overall were lukewarm about its contributions. R4 raises a fundamental issue with the presentation of the analysis (see the D_infty assumption). The AC thus goes for a "revise and resubmit".
The paper addresses an important problem of supervised learning for predicting graph connectivity using both node features and the overall graph structure. The paper is clearly written, and the presented approach produces promising results on synthetic data. However, all reviewers agree that the paper could be improved by including more comparison with prior art and related work discussion, and strengthening empirical results by including real life  data and more through evaluation; they also find the novelty and significance of the proposed approach somewhat limited. We hope the authors will use the suggestions of the reviewers to further improve the paper. 
This paper presents a novel method for synthesizing fluid simulations, constrained to a set of parameterized variations, such as the size and position of a water ball that is dropped. The results are solid; there is little related work to compare to, in terms of methods that can "compute"/recall simulations at that speed. The method is 2000x faster than the orginal simulations. This comes with the caveats that:  (a) the results are specific to the given set of parameterized environments; the method is learning a  compressed version of the original animations; (b) there is a loss of accuracy, and therefore also a loss of visual plausibility.  The AC notes that the paper should use the ICLR format for citations, i.e., "(foo et al.)" rather than "(19)". The AC also suggests that limitations should also be clearly documented, i.e., as seen from the  perspective of those working in the fluid simulation domain.  The principle (and only?) contentious issue relates to the suitability of the paper for the ICLR audience, given its focus on the specific domain of fluid simulations.  The AC is of two minds on this: (i) the fluid simulation domain has different characteristics to other domains, and thu understanding the ICLR audience can benefit from the specific nature of the predictive problems that come the fluid simulation domain;  new problems can drive new methods.  There is a loose connection between the given work and residual nets, and of course res nets have also been recently reconceptualized as PDEs. (ii) it s not clear how much the ICLR audience will get out of the specific solutions being described; it requires understanding spatial transformer networks and a number of other domain specific issues. A problem with this type of paper in terms of graphics/SIGGRAPH is that it can also be seen as "falling short" there, simply because it is not yet competitive in terms of visual quality or the generality of fluid simulators;  it really fulfills a different niche than classical fluid simulators.  The AC leans slightly in favor of acceptance, but is otherwise on the fence.  
Several visualizations are shown in this paper but it is unclear if they are novel.
The paper looks at a novel form of physics constrained system identification for a multi link robot, although it could also be applied more generally.  The contributions is in many simple; this is seen in a good light (R1, R3) or more modestly (R2). R3 notes surprise that this hasn t been done before. Results are demonstrated on a simualted 2 dof robot and real Barrett WAM arm, better than a pure neural network modeling approach, PID control, or an analytic model.    Some aspects of the writing needed to be addressed, i.e., PDE vs ODE notations.  The point of biggest concern is related to positioning the work relative to other system identification literature, where there has been an abundance of work in the robotics and control literature. There is no final consensus on this point for R3;  R3 did not receive the email notification of the author s detailed reply, and notes that the author has clarified some respects, but still has concerns, and did not have time to further provide feedback on short notice.    In balance, the AC believes that this kind of constrained learning of models is underexplored, and notes that the reviewers (who have considerable shared expertise in robotics related work) believe that this is a step in the right direction and that it is surprising this type of approach has not been investigated yet.  The authors have further reconciled their work with earlier sys ID work, and can further describe how their work is situated with respect to prior art in sys ID (as they do in their discussion comments).  The AC recommends that: (a) the abstract explicitly mention "system identification" as a relevant context for the work in this paper, given that the ML audience should be (or can be) made aware of this terminology; and (b) push more of the math related to the development of the necessary derivatives to an appendix, given that the particular use of the derivations seems to be more in support of obtaining the performance necessary for online use, rather than something that cannot be accomplished with autodiff. 
Following the unanimous vote of the reviewers, this paper is not ready for publication at ICLR. The most significant concern raised is that there does not seem to be an adequate research contribution. Moreover, unsubstantiated claims of novelty do not adequately discuss or compare to past work.
This paper proposes an anomaly detection approach by augmenting VAE encoder with a network multiple hypothesis network and then using a discriminator in the decoder to select one of the hypothesis. The idea is interesting although the reviewers found the paper to be poorly written and the approach to be a bit confusing and complicated.  Revisions and rebuttal have certainly helped to improve the quality of the work. However, the reviewers believe that the paper require more work before it can be accepted at ICLR. For this reason, I recommend to reject this paper in its current state. 
The paper addresses normalisation and conditioning of GANs. The authors propose to replace class conditional batch norm with whitening and class conditional coloring. Evaluation demonstrates that the method performs very well, and the ablation studies confirm the design choices. After extensive discussion, all reviewers agreed that this is a solid contribution, and the paper should be accepted. 
The paper proposes an architecture search method based on graph hypernetworks (GHN). The core idea is that given a candidate architecture, GHN predicts its weights (similar to SMASH), which allows for fast evaluation w/o training the architecture from scratch. Unlike SMASH, GHN can operate on an arbitrary directed acyclic graph. Architecture search using GHN is fast and achieves competitive performance. Overall, this is a relevant contribution backed up by solid experiments, and should be accepted.
AR1 is concerned about lack of downstream applications which show that higher order interactions are useful and asks why not to model higher order interactions for all (a,b) pairs. AR2 notes that this submission is a further development of Arora et al. and is satisfied with the paper. AR3 is the most critical regarding lack of explanations, e.g. why linear addition of two word embeddings is bad and why the corrective term proposed here is a good idea. The authors suggest that linear addition is insufficient when final meaning differs from the individual meanings and show tome quantitative results to back up their corrective term.  On balance, all reviewers find the theoretical contributions sufficient which warrants an accept. The authors are asked to honestly reflect all uncertain aspects of their work in the final draft to reflect legitimate concerns of reviewers.
The paper proposes an interesting idea (using "reliable" samples to guide the learning of "less reliable" samples). The experimental results and detailed analysis show clear improvement in object detection, especially small objects.  On the weak side, the paper seems to focus quite heavily on the object detection problem, and how to divide the data into reliable/less reliable samples is domain specific (it makes sense for object detection tasks, but it s unclear how to do this for general scenarios). As the authors promise, it will make more sense to change the title to "Feature Intertwiner for Object Detection" to alleviate such criticisms.   Given this said, I think this paper is over the acceptance threshold and would be of interest to many researchers. 
The paper proposes a deep learning framework to solve large scale spectral decomposition.  The reviewers and AC note that the paper is quite weak from presentation. However, technically, the proposed ideas make sense, as Reviewer 1 and Reviewer 2 mentioned. In particular, as Reviewer 1 pointed out, the paper has high practical value as it aims for solving the problem at a scale larger than any existing method. Reviewer 3 pointed out no comparison with existing algorithms, but this is understandable due to the new goal.  In overall, AC thinks this is quite a boarderline paper. But, AC tends to suggest acceptance since the paper can be interested for a broad range of readers if presentation is improved.
The paper uses a multimodal prior in GANs and reconstructs the latents back from images in two stages to match the generated data modes to the latent space modes. It is empirically shown that this can prevent mode collapse to some extent (including intra class collapse). However the paper lacks a comparison with state of the art GANs that have been shown to get better FID scores (~21 for SN GAN [1] vs ~28 in the paper) so the benefit here is unclear, particularly in cases when the mode prior is unknown. Similarly for other applications used in the paper such as inference and attribute discovery, it falls short of demonstrating quantitative improvements with the approach. For example, there is a growing body of work on unsupervised disentanglement in generative models with several metrics to measure it, which could be used to evaluate the attribute discovery performance. R1 has brought up the point of lack of comparisons which the AC agrees with. Authors have made revisions in the paper including some comparisons but these feel insufficient to establish the benefits of the method over state of the art in preventing mode collapse.   A borderline paper as reflected in the reviewer scores but can be made stronger with experiments showing convincing improvements over state of the art in at least one of the applications considered in the paper.    [1] Miyato, T., Kataoka, T., Koyama, M., & Yoshida, Y. (2018). Spectral normalization for generative adversarial networks. ArXiv Preprint ArXiv:1802.05957.
While the paper contains interesting ideas, the reviewers suggest improving the clarity and experimental study of the paper. The work holds promises but is not ready for publication at ICLR.
This paper addresses a promising method for unpaired cross domain image to image translation that can accommodate multi instance images. It extends the previously proposed CycleGAN model by taking into account per instance segmentation masks. All three reviewers and AC agree that performing such transformation in general is a hard problem when significant changes in shape or appearance of the object have to be made, and that the proposed approach is sound and shows promising results. As rightly acknowledged by R1 ‘The formulation is intuitive and well done!’  There are several potential weaknesses and suggestions to further strengthen this work:  (1) R1 and R2 raised important concerns about the absence of baselines such as crop & attach simple baseline and CycleGAN+Seg. Pleased to report that the authors showed and discussed in their response some preliminary qualitative results regarding these baselines. In considering the author response and reviewer comments, the AC decided that the paper could be accepted given the comparison in the revised version, but the authors are strongly urged to include more results and evaluations on crop & attach baseline in the final revision if possible. (2) more quantitative results are needed for assessing the benefits of this approach (R3). The authors discussed in their response to R3 that more quantitative results such as the segmentation accuracy of the synthesized images are not possible since no ground truth segmentation labels are available. This is true in general for unpaired image to image translation, however collecting annotations and performing such quantitative evaluation could have a substantial impact for assessing the significance of this work and can be seen as a recommendation for further improvement.  (3) the proposed model performs translation for a pair of domains; extending the work to multi domain translation like StarGAN by Choi et al 2018 or GANimation by Pumarola 2018 would strengthen the significance of the work. The authors discussed in their response to R3 that this is indeed possible.  
The paper challenges claims about cross entropy loss attaining max margin when applied to linear classifier and linearly separable data. This is important in moving forward with the development of better loss functions.   The main criticism of the paper is that the results are incremental and can be easily obtained from previous work.   The authors expressed certain concerns about the reviewing process. In the interest of dissipating any doubts, we collected two additional referee reports.   Although one referee is positive about the paper, four other referees agree that the paper is not strong enough.     
The paper proposes a simple and general technique based on the information bottleneck to constrain the information flow in the discriminator of adversarial models. It helps to train by maintaining informative gradients. While the information bottleneck is not novel, its application in adversarial learning to my knowledge is, and the empirical evaluation demonstrates impressive performance on a broad range of applications. Therefore, the paper should clearly be accepted. 
This paper introduces a new way to estimate gradients of expectations of discrete random variables by introducing antithetic noise samples for use in a control variate.  Quality:  The experiments are mostly appropriate, although I disagree with the choice to present validation and test set results instead of training time results.  If the goal of the method is to reduce variance, then checking whether optimization is improved (training loss) is the most direct measure.  However reasonable people can disagree about this.  I also think the toy experiment (copied from the REBAR and RELAX paper) is a bit too easy for this method, since it relies on taking two antithetic samples.  I would have liked to see a categorical extension of the same experiment.  Clarity:  I think that this method will not have the impact it otherwise could because of the authors  fearless use of long equations and heavy notation throughout.  This is unavoidable to some degree, but 1) The title of the paper isn t very descriptive 2) Why not follow previous work and use \theta instead of \phi for the parameters being optimized? The presentation has come a long way, but I fear that few besides our intrepid reviewers will have the stomach.  I recommend providing more intuition throughout.  Originality:  The use of antithetic samples to reduce variance is old, but this seems like a well thought through and non trivial application of the idea to this setting.  Significance:  Ultimately I think this is a new direction in gradient estimators for discrete RVs.  I don t think this is the last word in this direction but it s both an empirical improvement, and will inspire further work.
The paper presents a novel idea with a compelling experimental study. Good paper, accept.
The paper proposes two simple generator architecture variants enabling the use of GAN training for the tasks of denoising (from known noise types) and demixing (of two added sources). While the denoising approach is very similar to AmbientGAN and could thus be considered somewhat incremental, all reviewers and the AC agree that the developed use of GANs for demixing is an interesting novel direction. The paper is well written, and the approach is supported by encouraging experimental results on MNIST and Fashion MNIST. Reviewers and AC noted the following weaknesses of the paper: a) no theoretical support or analysis is provided for the approach, this makes it primarily an empirical study of a nice idea.  b) For an empirical study, the experimental evaluation is very limited, both in terms of dataset/problems it is tested on; and in terms of algorithms for demixing/source separation that it is compared against.  Following these reviews, the authors added the experiments on Fashion MNIST and comparison with ICA which are steps in the right direction. This improvement moved one reviewer to positively update his score, but not the others. Taking everything into account, the AC judges that it is a very promising direction, but that more extensive experiments on additional benchmark tasks for demixing and comparison with other demixing algorithms are needed to make this work a more complete contribution. 
This paper provides a generalization analysis for graph embedding methods concluding with the observation that the norm of the embedding vectors provides an effective regularization, more so than dimensionality alone. The main theoretical result is backed up by several experiments.  While the result appears to be correct, norm control, dimensionality reduction and early stopping during optimization are all very well studied in machine learning as effective regularizers, either operating alone or in conjunction. The regularization parameters, iteration count, embedding dimensionality is typically tuned for an application. The AC agrees with Reviewer 2 that the paper does not provide sufficiently interesting insights beyond this observation and is unlikely to influence practical applications of these methods.   Both reviewer 2 and 3 have also raised points on the need for stronger empirical analysis.
perhaps the biggest issue with the proposed approach is that the proposed approach, which supposedly addresses the issue of capturing long term dependency with a faster convergence, was only tested on problems with largely fixed length. with the proposed k_n gate being defined as a gaussian with a single mean (per unit?) and variance, it is important and interesting to know how this network would cope with examples of vastly varying lengths. in addition, r3 made good points about comparison against conventional LSTM and how it should be done with careful hyperparameter tuning and based on conventional known setups.   this submission will be greatly strengthened with more experiments using a better set of benchmarks and by more carefully placing its contribution w.r.t. other recent advances.
This paper investigates the usage of the extragradient step for solving saddle point problems with non monotone stochastic variational inequalities, motivated by GANs. The authors propose an assumption weaker/diffrerent than the pseudo monotonicity of the variational inequality for their convergence analysis (that they call "coherence"). Interestingly, they are able to show the (asympotic) last iterate convergence for the extragradient algorithm in this case (in contrast to standard results which normally requires averaging of the iterates for the stochastic *and* mototone variational inequality such as the cited work by Gidel et al.). The authors also describe an interesting difference between the gradient method without the extragradient step (mirror descent) vs. with (that they called optimistic mirror descent).  R2 thought the coherence condition was too related to the notion of pseudo monoticity for which one could easily extend previous known convergence results for stochastic variational inequality. The AC thinks that this point was well answered by the authors rebuttal and in their revision: the conditions are sufficiently different, and while there is still much to do to analyze non variational inequalities or having realistic assumptions, this paper makes some non trivial and interesting steps in this direction. The AC thus sides with expert reviewer R1 and recommends acceptance.
This paper presents methods for telegraphic summarization, a task that generates extremely short summaries.  There are concerns about the utility of the task in general, and also the novelty of the modeling framework.  There is overall consensus between reviewers regarding the paper s assessment the feedback is lukewarm.
The submission introduces a model that does learning of multisensory representations (by predicting one from the other), with an autoencoder structure. Generally, the reviewers liked the overall idea of the work, but found the clarity lacking, the evaluation insufficient (and not particularly state of the art), the requirement for paired training data quite limiting and the choices (VAE sometimes, autoencoder other times) somewhat ad hoc.
The reviewers raised a number of concerns including the lack of clarity of various parts of the paper, lack of explanation, incremental novelty, and insufficiently demonstrated significance of the proposed. The authors’ rebuttal addressed some of the reviewers’ concerns but not fully. Overall, I believe that the paper presents some interesting extensions for multi agent communication but in its current form the paper lacks explanations, comparisons and discussions. Hence, I cannot recommend this paper for presentation at ICLR.
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.    The paper is clear and well motivated.   The experimental results indicate that the proposed method outperforms the SOTA   2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.     The novelty is somewhat minor.   An interesting (but not essential) ablation study is missing (but the authors promised to include it in the final version).  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.   There were no major points of contention.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be accepted. 
The paper presents new loss functions (which replace the reconstruction part) for the training of conditional GANs. Theoretical considerations and an empirical analysis show that the proposed loss can better handle multimodality of the target distribution than reconstruction based losses while being competitive in terms of image quality.
The reviewers and authors participated in modest discussion, with the authors providing direct responses to reviewer comments. However, this did not appreciably change the overall ratings of the paper (one reviewer raised their rating, while another grew more concerned), and in aggregate the reviewers do not recommend that the paper meets the bar for acceptance.
This paper introduces a "scratchpad" extension to seq2seq models whereby the encoder outputs, typically "read only" during decoding, are editable by the decoder. In practice, this bears quite a lot of similarity—if not in the general concept, then in the the implementation—to a variety of models proposed in the NLP community (see reviews for details). As the technical novelty of the paper is quite limited, and there are issues with the clarity both in the technical contribution and in presenting what exactly is the main contribution of the paper, I must concur with the reviewers and recommend rejection.
The current version of the paper receives a unanimous rejection from reviewers, as the final proposal. 
This paper proposes new GAN training method with multi generator architecture inspired by Stackelberg competition in game theory. The paper has theoretical results showing that minmax gap scales to \eps for number of generators O(1/\eps), improving over previous bounds. Paper also has some experimental results on Fashion Mnist and CIFAR10 datasets.   Reviewers find the theoretical results of the paper interesting.  However, reviewers have multiple concerns about comparison with other multi generator architectures, optimization dynamics of the new objective and clarity of writing of the original submission. While authors have addressed some of these concerns in their response reviewers still remain skeptical of the contributions. Perhaps more experiments on imagenet quality datasets with detailed comparison can help make the contributions of the paper clearer. 
This paper proposes adding a second objective to the training of neural network classifiers that aims to make the distribution over incorrect labels as flat as possible for each training sample. The authors describe this as "maximizing the complement entropy." Rather than adding the cross entropy objective and the (negative) complement entropy term (since the complement entropy should be maximized while the cross entropy is minimized), this paper proposes an alternating optimization framework in which first a step is taken to reduce the cross entropy, then a step is taken to maximize the complement entropy. Extensive experiments on image classification (CIFAR 10, CIFAR 100, SVHN, Tiny Imagenet, and Imagenet), neural machine translation (IWSLT 2015 English Vietnamese task), and small vocabulary isolated word recognition (Google Commands), show that the proposed two objective approach outperforms training only to minimize cross entropy. Experiments on CIFAR 10 also show that models trained in this framework have somewhat better resistance to single step adversarial attacks. Concerns about the presentation of the adversarial attack experiments were raised by anonymous commenters and one of the reviewers, but these concerns were addressed in the revision and discussion. The primary remaining concern is a lack of any theoretical guarantees that the alternating optimization converges, but the strong empirical results compensate for this problem.
All reviewers (including those with substantial expertise in RL) were solid in their praise for this paper that is also tackling an interesting application that is much less well studied but deserves attention.  
While the reviewers all agree that this paper proposes an interesting application of GANs, they would like to see clearer explanations of the technical details, more convincing evaluations, and better justifications of the assumptions and practical values of the proposed algorithms. 
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.  The paper tackles an interesting and relevant problem for ICLR: optical character recognition in document images.   2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.     The authors propose to use small networks to localize text in document images, claiming that for document images smaller networks work better than standard SOTA networks for scene text. As pointed out in the reviews, the authors didn t make any comparisons to SOTA object detection networks (trained either on scene text or on document images) so their central claim has not been experimentally verified.   The reviewers were unanimous that the work lacks novelty as object detection pipelines have already been used for OCR so a contribution of considering smaller detection networks is minor.   There were serious issues with formatting and clarity. These three issues all informed the final decision.  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.   There were no major points of contention and no author feedback.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be rejected. 
This paper characterizes a particular kind of fragility in the image classification ability of deep networks: minimal image regions which are classified correctly, but for which neighboring regions shifted by one row or column of pixels are classified incorrectly. Comparisons are made to human vision. All three reviewers recommend acceptance. AnonReviewer1 places the paper marginally above threshold, due to limited originality over Ullman et al. 2016, and concerns about overall significance. 
The authors provide a convolutional neural network for predicting the satisfiability of SAT instances. The idea is interesting, and the main novelty in the paper is the use of convolutions in the architecture and a procedure to predict a witness when the formula is satisfiable. However, there are concerns about the suitability of convolutions for this problem because of the permutation invariance of SAT. Empirically, the resulting models are accurate (correctly predicting sat/unsat 90 99% of the time) while taking less time than some existing solvers. However, as pointed out by the reviewers, the empirical results are not sufficient to demonstrate the effectiveness of the approach. I want to thank the authors for the great work they did to address the concerns of the reviewers. The paper significantly improved over the reviewing period, and while it is not yet ready for publication, I want to encourage the authors to keep pushing the idea to further and improve the experimental results. 
This paper takes inspiration from the brain to add a behavioral module to a deep reinforcement learning architecture. Unfortunately, the paper s structure and execution lacks clarity and requires a lot more work: as noted by reviewers, the link link between motivation and experiments is too fuzzy and their execution is not convincing.
The paper studies inductive biases in DRL, by comparing with different reward shaping, and curriculums. The authors performed comparative experiments where they replace domain specific heuristics by such adaptive components.  The paper includes very little (new) scientific contributions, and, as such, is not suitable for publication at ICLR.
The paper attempts to extend the recent analysis of random deep networks to alternative activation functions.  Unfortunately, none of the reviewers recommended the paper be accepted.  The current presentation suffers from a lack of clarity and a sufficiently convincing supporting argument/evidence to satisfy the reviewers.  The contribution is perceived as too incremental in light of previous work.
As the reviewers point out, the paper seems to be below the ICLR publication bar due to low novelty and limited significance. 
The revisions made by the authors convinced the reviewers to all recommend accepting this paper. Therefore, I am recommending acceptance as well. I believe the revisions were important to make since I concur with several points in the initial reviews about additional baselines. It is all too easy to add confusion to the literature by not including enough experiments. 
 pros:   Identification of several interesting problems with the original DNC model: masked attention, erasion of de allocated elements, and sharpened temporal links   An improved architecture which addresses the issues and shows improved performance on synthetic memory tasks and bAbI over the original model   Clear writing  cons:   Does not really show this modified DNC can solve a task that the original DNC could not and the bAbI tasks are effectively solved anyway.  It is still not clear whether the DNC even with these improvements will have much impact beyond these toy tasks.  Overall the reviewers found this to be a solid paper with a useful analysis and I agree.  I recommend acceptance.  
This paper combines two recently proposed ideas for GAN training: Fisher integral probability metrics, and the Deli GAN. As the reviewers have pointed out, the writing is somewhat haphazard, and it s hard to identify the key contributions, why the proposed method is expected to help, and so on. The experiments are rather minimal: a single experiment comparing Inception scores to previous models on CIFAR; Inception scores are not a great measure, and the experiments don t yield much insight into where the improvement comes from. No author response was given. I don t think this paper is ready for publication in ICLR. 
The paper is addressing an important problem, but misses many related references (see Reviewer 2 s comments for a long list of highly relevant papers).   More importantly, as Reviewer 3 pointed out (which the AC fully agrees):   "The gradient estimator the paper proposes is the REINFORCE estimator [Williams, ML 1992] re derived through importance sampling."  "The equivalence would not be exact if the authors chose the importance distribution to be different than the variational approximation q(z|x), so there still may be room for novelty in their proposal, but in the current draft only q(z|x) is considered."  
This paper proposes to improve MT with a specialized encoder component that models roles. It shows some improvements in low resource scenarios.  Overall, reviewers felt there were two issues with the paper: clarity of description of the contribution, and also the fact that the method itself was not seeing large empirical gains. On top of this, the method adds some additional complexity on top of the original model.  Given that no reviewer was strongly in favor of the paper, I am not going to recommend acceptance at this time.
This work proposes a new approximation method for softmax layers with large number of classes. The idea is to use a sparse two layer mixture of experts. This approach successfully reduces the computation requires on the PTB and Wiki 2 datasets which have up to 32k classes. However, the reviewers argue that the work lacks relevant baselines such as D softmax and adaptive softmax. The authors argue that they focus on training and not inference and should do worse, but this should be substantiated in the paper by actual experimental results.
The paper addresses generalized zero shot learning (test data contains examples from both seen as well as unseen classes) and proposes to learn a shared representation of images and attributes via multimodal variational autoencoders.  The reviewers and AC note the following potential weaknesses: (1) low technical contribution, i.e. the proposed multimodal VAE model is very similar to Vedantam et al (2017) as noted by R2, and to JMVAE model by Suzuki et al, 2016, as noted by R1. The authors clarified in their response that indeed VAE in Vedantam et al (2017) is similar, but it has been used for image synthesis and not classification/GZSL. (2) Empirical evaluations and setup are not convincing (R2) and not clear   R3 has provided a very detailed review and a follow up discussion raising several important concerns such as (i) absence of a validation set to test generalization, (ii) the hyperparameters set up; (iii) not clear advantages of learning a joint model as opposed to unidirectional mappings (R1 also supports this claim). The authors partially addressed some of these concerns in their response, however more in depth analysis and major revision is required to assess the benefits and feasibility of the proposed approach. 
This work proposes a method for extending hindsight experience replay to the setting where the goal is not fixed, but dynamic or moving. It proceeds by amending failed episodes by searching replay memory for a compatible trajectories from which to construct a trajectory that can be productively learned from.  Reviewers were generally positive on the novelty and importance of the contribution. While noting its limitations, it was still felt that the key ideas could be useful and influential. The tasks considered are modifications of OpenAI robotics environments, adapted to the dynamic goal setting, as well as a 2D planar "snake" game. There were concerns about the strength of the baselines employed but reviewers seemed happy with the state of these post revision. There were also concerns regarding clarity of presentation, particularly from AnonReviewer2, but significant progress was made on this front following discussions and revision.  Despite remaining concerns over clarity I am convinced that this is an interesting problem setting worth studying and that the proposed method makes significant progress. The method has limitations with respect to the sorts of environments where we can reasonably expect it to work (where other aspects of the environment are relatively stable both within and across episodes), but there is lots of work in the literature, particularly where robotics is concerned, that focuses on exactly these kinds of environments. This submission is therefore highly relevant to current practice and by reviewers  accounts, generally well executed in its post revision form. I therefore recommend acceptance.
While the proposed method is novel, the evaluation is not convincing. In particular, the datasets and models used are small. Susceptibility to adversarial examples is tightly related to dimensionality. The study could benefit from more massive datasets (e.g., Imagenet).
It seems that the reviewers reached a consensus that the paper is not ready for publication in ICLR. (see more details in the reviews below. )
 pros:   Good quantitative results showing clear improvement over other model based methods in sample efficiency and computational cost (though see Reviewer 2 s concerns about the need for more experiments on computational cost).   Cool qualitative results showing discovery of BFS and DFS   Potentially novel approach (see cons)  cons:   Lack of clarity especially concerning equation (1).  Both Reviewers 1 and 3 were unsure of the rationale for this equation which lies at the heart of the method.  It looks to me like a combination of surprise and value but the motivation is not clear.  There are a number of other such places pointed out by the reviewers where model choices were made that seem ad hoc or not well motivated.   In general it s hard to understand which factors are important in driving the results you report.  As Reviewer 3 points out, more ablation studies and analysis would help here.  Providing more motivation, explanation and analysis would help the reader understand better the reasons for the performance of the model.  The results are nice and the method is intriguing.  I think this potentially a very nice paper and if you can address the above concerns but isn t quite up to the acceptance bar for ICLR this year. 
The reviewers and the AC acknowledge the paper contains interesting ideas on using an incremental sequence of multiple generators to capture the diversity of the examples. However, the reviewers and the AC also note that the potential drawback of the paper is the lack of evaluation with other metrics such as inception score, FID score, etc. Therefore the paper is not quite ready for acceptance right now, but the AC encourages the authors to submit to other top venues with more thorough experiments. 
The paper proposes a nice approach to massively multi label problems with rare labels which may only have a limited number of positive examples; the approach uses Bayes nets to exploit the relationships among the labels in the output layer of a neural nets. The paper is clearly written and the approach seems promising, however, the reviewers would like to see even more convincing empirical results. 
The reviews agree the paper is not ready for publication at ICLR. 
The reviewers highlighted that the application in the paper is interesting, but note a lack of new methodology, and also highlight serious flaws in the testing methodology. Specifically, the reviewers are discouraged by the straightforward reuse of Siamese networks without clear modifications. Further, the testing setup might be unfairly easy, since chemical families are represented in both training and test sets, while in true application of the method would be exposed to previously unseen chemical families.  The authors did not participate in the discussion, and address concerns. The reviewer consensus is a rejection.
The paper presents "deep deducing", which means learning the state action value function of 2 player games from a payoff table, and using the value function by maximizing over the (actionable) inputs at test time.  The paper lacks clarity overall. The method does not contain any new model nor algorithm. The experiments are too weak (easy environments, few/no comparisons) to support the claims.  The paper is not ready for publication at this time.
The manuscript proposes to analyze the learning dynamics of deep networks with separable data. A variety of results are provided under various assumptions.  The reviewers and AC note the assumptions required for the analysis are quite strong, and perhaps too strong to provide useful insight into real problems. Reviewers also cite issues with writing and the breadth of the title (this was much improved after rebuttal).
This paper introduces an approach for reducing the dimensionality of training data examples in a way that preserves information about soft target probabilistic representations provided by a teacher model, with applications such as zero shot learning and distillation. The authors provide an extensive theoretical and empirical analysis, showing performance improvements in zero shot learning and finite sample error upper bounds. The reviewers generally agree this is a good paper that should be published.
This paper proposes an 8 bit quantization strategy for rapid DNN deployment. 3 reviewers all rated this paper as marginally below acceptance threshold due to lack of novelty. 8 bit quantization (including channel wise) is a well studied task. The paper lacks comparison with peer work. 
This paper proposes to improve the exploration in the PPO algorithm by applying CMA ES. Major concerns of the paper include: paper editing can be improved; the choices of baselines used in the paper may be not reasonable; flaws in comparisons with SOTA. It is also not quite clear why CMA can improve exploration, further justification required. Overall, this paper cannot be published in its current form.
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.    novel approach to audio synthesis   strong qualitative and quantitative results   extensive evaluation   2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    small grammatical issues (mostly resolved in the revision).   3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  No major points of contention.   4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be accepted. 
All reviewers agree in their assessment that this paper is not ready for acceptance into ICLR and the authors did not respond during the rebuttal phase.
All reviewers agree that the proposed method interesting and well presented. The authors  rebuttal addressed all outstanding raised issues. Two reviewers recommend clear accept and the third recommends borderline accept. I agree with this recommendation and believe that the paper will be of interest to the audience attending ICLR. I recommend accepting this work for a poster presentation at ICLR.
The paper aims to encourage deep networks to have stable derivatives over larger regions under networks with piecewise linear activation functions.  All reviewers and AC note the significance of the paper. AC also thinks this is also a very timely work and potentially of broader interest of ICLR audience.
This paper introduced a Neural Rendering Model, whose inference calculation corresponded to those in a CNN. It derived losses for both supervised and unsupervised learning settings. Furthermore, the paper introduced Max Min network derived from the proposed loss, and showed strong performance on semi supervised learning tasks.  All reviewers agreed this paper introduces a highly interesting research direction and could be very useful for probabilistic inference. However, all reviewers found this paper hard to follow. It was written in an overly condensed way and tried to explain several concepts within the page limit such as NRM, rendering path, max min network. In the end, it was not able to explain key concepts sufficiently.  I suggest the authors take a major revision on the paper writing and give a better explanation about main components of the proposed method. The reviewer also suggested splitting the paper into two conference submissions in order to explain the main ideas sufficiently under a conference page limit.
The paper develops and investigates the use of a spike and slab prior and approximate posterior for a VAE. It uses a continuous relaxation for the discrete binary component in the reconstruction term of the ELBO, and an analytic expression for the KL term between the spike and slab prior and approximate posterior. Experiments on MNIS, Fashion MNIST and CelebA convincingly show that the approach works to learn sparse representations with improved interpretability that also yield more robust classification   All reviewers agreed that this approach to sparsity in VAEs is well motivated and sound, that the paper is well written and clear, and the experiments interesting. One reviewer noted that the accuracy on MNIST remains really poor, so the approach does not cure VAEs yielding subpar representations for classification (although not the goal of this research).  The reviewers and the AC however all judged that it currently constitutes a too limited contribution because a) the approach is a straightforward application of vanilla VAEs with a different prior/posterior, and is thus rather incremental. b) the scope of the paper is rather limited, in particular as it does not sufficiently discuss and does not empirically compare with other (VAE related) approaches from the literature that were developed for sparse latent representations. 
After discussion, all reviewers agree to accept this paper. Congratulations!!
The reviewers all agreed that this paper makes a strong contribution to ICLR by providing the first asynchronous analysis of a Nesterov accelerated coordinate descent method.
This paper proposes a new approach to domain adaptation based on sub spacing, such that outliers are filtered out. While similar ideas have been used e.g. in multi view learning, their application to domain adaptation makes it a novel and interesting approach.   While the above is considered by the AC an adequate contribution to ICLR, the authors are encouraged to investigate further the implications of the assumptions made, in a way that the derived criteria seem less heuristic, as R1 pointed out.  There had been some concerns regarding the experiments, but the authors have been very active in the rebuttal period and addressed these concerns satisfactorily. 
This paper proposes a sparse binary compression method for distributed training of neural networks with minimal communication cost. Unfortunately,  the proposed approach is not novel, nor supported by strong experiments. The authors did not provide a rebuttal for reviewers  concerns.  
The reviewers raised a number of major concerns including the incremental novelty of the proposed (if any), insufficient explanation, and, most importantly, insufficient and inadequate experimental evaluation presented. The authors did not provide any rebuttal. Hence, I cannot suggest this paper for presentation at ICLR.
The paper seeks to obtain faster means to count or approximately count of the number of linear regions of a neural network. The paper improves bounds and makes an interesting contribution to a long line of work.   A consistent concern of the reviewers is the limited applicability of the method. The empirical evaluation can serve to better assess the accuracy of theoretical bounds that have been obtained in previous works, but the practical utility is not as clear yet.   This is a borderline case. The reviewers lean towards a positive rating of the paper, but are not particularly enthusiastic about the paper. The paper makes good contributions, but is just not convincing enough.   I think that the work program that the authors suggest in their responses could lead to a stronger paper in the future. In particular, the exploration of necessary and sufficient conditions for different neural networks to be equivalent and the use of number of linear regions when analyzing neural networks, seem to be very promising directions. 
The paper proposes a new optimization approach for neural nets where, instead of a fixed learning rate (often hard to tune), there is one learning rate per unit, randomly sampled from a distribution. Reviewers think the idea is novel, original and simple. Overall, reviewers found the experiments unconvincing enough in practice. I found the paper really borderline, and decided to side with the reviewers in rejecting the paper.
The paper proposes to apply Neural Architecture Search for pruning DenseNet.   The reviewers and AC note the potential weaknesses of the paper in various aspects, and decided that the authors need more works to publish. 
The paper proposes a new dataset for studying knowledge grounded conversations, that would be very useful in advancing this field. In addition to the details of the dataset and its collection, the paper also includes a framework for advancing the research in this area, that includes evaluation methods and baselines with a relatively new approach. The proposed approach for dialogue generation however is a simple extension of previous work by (Zhang et al) to user transformers, hence is not very interesting. The proposed approach is also not compared to many previous studies in the experimental results. One of the reviewers highlighted the weakness of the human evaluation performed in the paper. Moving on, it would be useful if further approaches are considered and included in the task evaluation.   A poster presentation of the work would enable participants to ask detailed questions about the proposed dataset and evaluation, and hence may be more appropriate. 
The paper proposes a regularization term on the generator s gradient that increases sensitivity of the generator to the input noise variable in conditional and unconditional Generative Adversarial networks, and results in multimodal predictions. All reviewers agree that this is a simple and useful addition to current GANs. Experiments that demonstrate the trade off between diversity and generation quality would be important to include, as well as the experiment on using the proposed method on unconditional GANs, which was conducted during the discussion period. 
Well written paper that motivates through theoretical analysis new memory writing methods in memory augmented neural networks. Extensive experimental analysis support and demonstrate the advantages of the new solutions over other recurrent architectures. Reviewers suggested extension and clarification of the analysis presented in the paper, for example, for different memory sizes. The paper was revised accordingly. Another important suggestion was considering ACT as a baseline. Authors explained clearly why it wasn t considered as a baseline, and updated the paper to include references and explanations in the paper as well.
The proposed ENorm procedure is a normalization scheme for neural nets whereby the weights are rescaled in a way that minimizes the sum of L_p norms while maintaining functional equivalence. An algorithm is given which provably converges to the globally optimal solution. Experiments show it is complementary to, and perhaps slightly better than, other normalization schemes.  Normalization issues are important for DNN training, and normalization schemes like batch norm, weight norm, etc. have the unsatisfying property that they entangle multiple issues such as normalization, stochastic regularization, and effective learning rates. ENorm is a conceptually cleaner (if more algorithmically complicated) approach. It s a nice addition to the set of normalization schemes, and possibly complementary to the existing ones.  After a revision which included various new experiments, the reviewers are generally happy with the paper. While there s still some controversy over whether it s really better than things like batch norm, I think the paper would be worth publishing even if the results came out negative, since it is a very natural idea which took some algorithmic insight in order to actually execute.  
I tend to agree with reviewers. This is a bit more of an applied type of work and does not lead to new insights in learning representations.  Lack of technical novelty Dataset too small
The paper proposed a GAN approach to robust learning against adversarial examples, where a generator produces adversarial examples as perturbations and a discriminator is used to distinguish between adversarial and raw images. The performance on MNIST, SVHN, and CIFAR10 demonstrate the effectiveness of the approach, and in general, the performance is on par with carefully crafted algorithms for such task.   The architecture of GANs used in the paper is standard, yet the defensive performance seems good. The reviewers wonder the reason behind this good mechanism and the novelty compared with other works in similar spirits. In response, the authors add some insights on discussing the mechanism as well as comparisons with other works mentioned by the reviewers.   The reviewers all think that the paper presents a simple scheme for robust deep learning based on GANs, which shows its effectiveness in experiments. The understanding on why it works may need further explorations.  Thus the paper is proposed to be borderline lean accept.   
 The paper addresses an important problem of detecting biases in classifiers (e.g. in face detection), using simulation tools with Bayesian parameter search. While the direction of research and the presented approach seem to be practically useful, there were several concerns raised by the reviewers regarding strengthening the results (e.g., beyond single avatar, etc), and suggestions on possibly a more applied conference as a better venue.  While thourough rebuttals by the authors addressed some of these concerns, which increased some ratings, overall, the paper was still in the borderline range. We hope the suggestions and comments of the reviewers can help to improve the paper.     
This paper proposes a new loss function that can be used in place of the standard maximum likelihood objective in training NMT models. This leads to a small improvement in training MT systems.  There were some concerns about the paper though: one was that the method itself seemed somewhat heuristic without a clear mathematical explanation. The second was that the baselines seemed relatively dated, although one reviewer noted that this seemed like a bit of a lesser concern. Finally, the improvements afforded were relatively small.  Given the high number of good papers submitted to ICLR this year, it seems that this one falls short of the acceptance threshold.
R4 recommends acceptance while R2 is lukewarm and R1 argues for rejection to revise the presentation of the paper. As we unfortunately need to reject borderline papers given the space constraints, the AC recommends "revise and resubmit".
While the reviews of this paper were somewhat mixed (7,6,4), I ended up favoring acceptance because of the thorough author responses, and the novelty of what is being examined.  The reviewer with a score of 4, argues that this work is not a good fit for iclr, but, although tailoring new metrics may not be a common area that is explored, I don t believe that it s outside the range of iclr s interest, and therefore also more unique.
I enjoyed reading the paper myself and I appreciate the unifying framework connecting RAML and SPG. While I do not put a lot of weight on the experiments, I agree with the reviewers that the experimental results are not very strong, and I am not convinced that the theoretical contribution meets the bar at ICLR.  In the interpolation algorithm, there seems to be an additional annealing parameter and two tuning parameters. It is important to describe how the parameters are tuned. Given the additional hyper parameters, one may consider giving all of the algorithms the same budget of hyper parameter tuning. I also agree with reviewers that the policy gradient baseline seems to underperform typical results. One possible way to strengthen the experiments is to try to replicate the results of SPG or RAML and discuss the behavior of each algorithm as a function of hyper parameters. 
The paper makes novel explorations into how MPC and approximate DP / value function approaches, with value fn ensembles to model value fn uncertainty, can be effectively combined.  The novelty lies in exploring their combination. The experiments are solid. The paper is clearly written.  Open issues include overall novelty, and delineating the setting in which this method is appropriate.  The reviewers and AC are in agreement on what is in the paper. The open question is whether the combination of the ideas is interesting.   After further reviewing the paper and results. the AC believes that the overall combination of ideas and related evaluations that make a useful and promising contribution. As evidenced in some of the reviewer discussion, there is often a considerable schism in the community regarding what is considered fair to introduce in terms of prior knowledge, and blurred definitions regarding planning and control. The AC discounted some of the concerns of R2 that related more to discrete action settings and theoretical considerations; these  often fail to translate to difficult problems in continuous action settings.  The AC believes that R3 nicely articulates the issues of the paper that can be (and should be) addressed in the writing, i.e., to describe and motivate the settings that the proposed framework targets, as articulated in the reviews and ensuing discussion.  
This paper combines two ideas: MAML, and the hierarchical Bayesian inference approach of Amit and Meir (2018). The idea is fairly straightforward but well motivated, and it seems to work well in practice.  The paper is well written and includes good discussion of the relevant literature. The experiments show improvements on various tests of Bayesian inference, and include some good analysis beyond simply reporting better numbers.  On the whole, the reviewers are fairly positive about the paper. (While the numerical scores are slightly below the cutoff, the reviewers are more positive in the discussion.) The reviewers  main complaint is the lack of comparisons against recently published methods, especially Gordon et al. (2018). The lack of comparison to this paper doesn t strike me as a big problem; the preprint was released only a few months before the deadline, their approach was very different from the proposed one, and the proposed approach has some plausible advantages (simplicity, computational efficiency), so I don t think a direct comparison is required for acceptance.  Overall, I recommend acceptance. 
A lot of work has appeared recently on recurrent state space models. So although this paper is in general considered favorable by the reviewers it is unclear exactly how the paper places itself in that (crowded) space. So rejection with a strong encouragement to update and resubmission is encouraged. 
The paper shows limitations on the types of functions that can be represented by deep skinny networks for certain classes of activation functions, independently of the number of layers. With many other works discussing capabilities but not limitations, the paper contributes to a relatively underexplored topic.   The settings capture a large family of activation functions, but exclude others, such as polynomial activations, for which the considered type of obstructions would not apply. Also a concern is raised about it not being clear how this theoretical result can shed insight on the empirical study of neural networks.   The authors have responded to some of the comments of the reviewers, but not to all comments, in particular comments of reviewer 1, who s positive review is conditional on the authors addressing some points.   The reviewers are all confident and are moderately positive, positive, or very positive about this paper. 
This manuscript proposes an extension of convolution operations for manifold valued data. The primary contributions include the development and description of the approach and implementation and evaluation on real data.  The reviewers and AC expressed concern about the clarity of the presentation, particularly for a general ICLR audience. Though the contributions are primarily conceptual/theoretical, reviewers expressed concern about the breadth and quality of the presented experimental results. Some additional concerns related to missing proofs and details were addressed in the rebuttal.
The authors present a new method for leveraging multiple parallel agents to speed RL in continuous action spaces. By monitoring the best performers, that information can be shared in a soft way to speed policy search. The problem space is interesting and faster learning is important. However, multiple reviewers [R2, R1] had significant concerns with how the work is framed with respect to the wider literature (even after the revisions), and some concerns over the significance of the performance improvements which seem primarily to come from early boosts. There is also additional related work on concurrent RL (Guo and Brunskill 2015; Dimakopoulou, Van Roy 2018 ; Dimakopoulou, Osband, Van Roy 2018) which provides some more formal considerations of the setting the authors consider, which would be good to reference.  
The paper proves that the locus of the global minima of an over parameterized neural nets objective forms a low dimensional manifold. The reviewers and AC note the following potential weaknesses:     it s not clear why the proved result is significant: it neither implies the SGD can find a global minimum, nor that the found solution can generalize. (Very likely, most of the global minima on the manifold cannot generalize.)    the results seem very intuitive and are a straightforward application of certain topological theorem. 
This paper proposes a novel dataset of bouncing balls and a way to learn the dynamics of the balls when colliding. The reviewers found the paper well written, tackling an interesting and hard problem in a novel way. The main concern (that I share with one of the reviewers) is about the fact that the paper proposes both a new dataset/environment *and* a solution for the problem. This made it difficult the for the authors to provide baselines to compare to.  The ensuing back and forth had the authors relax some of the assumptions from the environment and made it possible to evaluate with interaction nets.  The main weakness of the paper is the relatively contrived setup that the authors have come up with. I will summarize some of the discussion that happened as a result of this point: it is relatively difficult to see how this setup that the authors have and have studied (esp. knowing the groundtruth impact locations and the timing of the impact) can generalize outside of the proposed approach. There is some concern that the comparison with interaction nets was not entirely fair.  I would recommend the authors redo the comparisons with interaction nets in a careful way, with the right ablations, and understand if the methods have access to the same input data (e.g. are interaction nets provided with the bounce location?).   Despite the relatively high average score, I think of this paper as quite borderline, specifically because of the issues related to the setup being too niche. Nonetheless, the work does have a lot of scientific value to it, in addition to a new simulation environment/dataset that other researchers can then use. Assuming the baselines are done in a way that is trustworthy, the ablation experiments and discussion will be something interesting to the ICLR community.
This paper presents a novel technique for separating signals in a given mixture, a common problem encountered in audio and vision tasks. The algorithm assumes that training samples from only one of the sources and the mixture distributions are available, which is a realistic assumption in a lot of cases. It then iteratively learns a model that can separate the mixture by using the available samples in a clever fashion.  Strengths:   The novelty lies in how the authors formulate the problem, and the iterative approach used to learn the unknown distribution and thereby improve source separation.   The use of existing GLO masking techniques for initialization to improve performance is also novel and interesting.  Weaknesses   There are some concerns around guarantees of convergence. Empirically, the algorithm works well, but it is unclear when the algorithm will fail. Some analysis here would have greatly improved the quality of the paper.   The reviewers also raised concerns around clarity of presentation and consistency of notation. While the presentation improved after revision, there are parts which remain unclear (e.g., those raised by R3) that may hinder readability and reproducibility.    The mixing model assumed by the authors is additive, which may not always be the case, e.g. when noise is convolutive (room reverberation, for instance).   (Minor) Experiments can also be improved. The vision tasks are not very realistic. For the speech separation task, relatively clean speech is easy to obtain. Therefore, it would be worth considering speech as observed, and noise as unobserved. The authors cite separating animal sounds from background, but the task chosen does not quite match that setup.   Overall, the reviewers agree that the paper presents an interesting approach to separation. But given the issues with presentation and evaluations, the recommendation is to reject the paper. We strongly encourage the authors to address these concerns and resubmit in the future.
Strengths: The paper tackles a novel, well motivated problem related to options & HRL. The problem is that of learning transition policies, and the paper proposes a novel and simple solution to that problem, using learned proximity predictors and transition policies that can leverage those. Solid evaluations are done on simulated locomotion and manipulation tasks. The paper is well written.  Weaknesses: Limitations were not originally discussed in any depth.  There is related work related to sub goal generation in HRL. AC: The physics of the 2D walker simulations looks to be unrealistic; the character seems to move in a low gravity environment, and can lean forwards at extreme angles without falling. It would be good to see this explained.  There is a consensus among reviewers and AC that the paper would make an excellent ICLR contribution. AC: I suggest a poster presentation; it could also be considered for oral presentation based on the very positive reception by reviewers.
As per R3: This paper presents a novel approach for doing hierarchical deep RL (HRL) on UMDPs by: (a) use of hindsight experience replay at multiple levels;  combined with (b) max T timesteps at each level. By effectively learning from missed goals at multiple levels, it allows for fairly  data efficient learning and can (in principle) work for an arbitrary number of levels. HRL is an important open problem.  The weaknesses described reviewers include limited comparisons to other HRL methods; its applicaiton to fairly simple domain; its still unclear what the benefit of > 4 levels is, and what the diminishing returns are wrt to the claim of working for an arbitrary number of levels.  R1(5) and R3(7)  stand by their scores. R1(5) still has some remaining concerns regarding some experiments not being done across all tasks, an older version of the HAL algo baseline being used, and  lack of insight regarding >  4 levels.  Based on the balance of the reviewers comments and the AC s own reading of the paper and results,  and the importance of the problem, the AC leans towards accept.  Using Hindsight Exp Replay across multiple levels is a simple but interesting idea, and the terminate after T steps is an interesting heuristic to make this effective. While the paper does not give insight for large (> 4) levels, it does make for an interesting framework that will inspire further work.  The AC recommends that the claims regarding an "arbitrary number of levels" be significantly toned down. 
All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.
The authors propose a technique for pruning networks by using second order information through the Hessian. The Hessian is approximated using the Fisher Information Matrix, which is itself approximated using KFAC. The paper is clearly written and easy to follow, and is evaluated on a number of systems where the authors find that the proposed method achieves good compression ratios without requiring extensive hyperparameter tuning.   The reviewers raised concerns about 1) the novelty of the work (which builds on the KFAC work of Martens and Grosse), 2) whether zeroing out individual connections as opposed to neurons will have practical runtime benefits, 3) the lack of comparisons against baselines on overall training time/complexity, 4) comparisons to work which directly prune as part of training (instead of the train prune finetune scheme adopted by the authors). In the view of the AC,  4) would be an interesting comparison but was not critical to the decision. Ultimately, the decision came down to the concern of lack of novelty and whether the proposed techniques would have an impact on runtime in practice.   
There is consensus among the reviewer that this is a good paper. It is a bit incremental compared to Gregor et al 2016. This paper show quite better empirical results.
The paper conveys interesting study but the reviewers expressed concerns regarding the difference of this work compared to existing approaches and pointed a room for more thorough empirical evaluation.
This manuscript proposes a gradient based learning scheme for non differentiable and non decomposable metrics. The key idea is to optimize a soft predictor directly (instead of aiming for a deterministic predictor), which results in a differentiable loss for many of these metrics. Theoretical results are provided which describe the performance of this approach.  The reviewers and ACs noted weakness in the original submission related to the clarity of the presentation and novelty as related to already published work. There was also a concern about the usefulness the main theoretical results due to asymptotic assumptions. The manuscript would be significantly strengthened if the reliance on infinite sample sizes is resolved, or sufficient empirical evidence is provided which suggests that the asymptotic issues are not practically significant.
Though the overall direction is interesting,  the reviewers are in consensus that the work is not ready for publication (better / larger scale evaluation is needed, comparison with other non autoregressive architectures should be provided, esp Transformer as there is a close relation between the methods). 
The authors have taken inspiration from recent publications that demonstrate transfer learning over sequential RL tasks and have proposed a method that trains individual learners from experts using layerwise connections, gradually forcing the features to distill into the student with a hard coded annealing of coeffiecients. The authors have done thorough experiments and the value of the approach seems clear, especially compared against progressive nets and pathnets. The paper is well written and interesting, and the approach is novel. The reviewers have discussed the paper in detail and agree, with the AC, that it should be accepted.
The paper proposes using GANs for disentangling style information from speech content, and thereby improve style transfer in TTS. The review and responses for this paper have been especially thorough! The authors significantly improved the paper during the review process, as pointed out by the reviewers. Inclusion of additional baselines, evaluations and ablation analysis helped improve the overall quality of the paper and helped alleviate concerns raised by the reviewers. Therefore, it is recommended that the paper be accepted for publication.
The reviewers that provided extensive reviews agree that the paper is well written and contains solid technical material. The paper however should be edited to address specific concerns regarding theoretical and empirical aspects of this work. 
The reviewers think that incorporating class conditional dependencies into the metric space of a few shot learner is a sufficiently good idea to merit acceptance. The performance isn’t necessarily better than the state of the art approaches like LEO, but it is nonetheless competitive. One reviewer suggests incorporating a pre training strategy to strengthen your results. In terms of experimental details, one reviewer pointed out that the embedding network architecture is quite a bit more powerful than the base learner and would like some additional justification for this. They would also like more detail on the computing the MAML gradients in the context of this method. Beyond this, please ensure that you have incorporated all of the clarifications that were required during the discussion phase.
This paper studies the really hard problem of zero shot learning in acoustic modeling for languages with limited resources, using data from English. Using a novel universal phonetic model, the authors show improvements compared to using an English model for 20 other languages in phone recognition quality.  Strengths   Reviewers agree that the problem is an important one, and the presented ideas are novel.   Universal phonetic model to represent phones in any language is interesting.  Weaknesses   The results are really weak, to the point that it is unclear how effective or general the techniques are. The work is an interesting first step, but is not developed enough to be accepted at this point.   The universal phonetic model being trained only in English might affect generalizability to languages that do not share phonetic characteristics. The authors agree partly, and argue that the method already addresses some issues since the model can already represent unseen phones. But, coupled with the high phone error rates, it is still unclear how appropriate the technique will be in addressing this issue.   Novelty: Although the idea of mapping phones to attributes, and using those for ASR is not novel (e.g., using articulatory features), application for zero shot learning is. The work assumes availability of a small text corpus to learn phone sequence distribution, so is similar to other zero resource approaches that assume some data (audio, as opposed to text) is available in the new language.  This paper presents interesting first steps, but lacks sufficient experimental validation at this point. Therefore, AE recommendation is to reject the paper. I encourage the authors to improve and resubmit in the future.
This paper considers the task of web navigation, i.e. given a goal expressed in natural language, the task is to navigate webs by filling up fields and clicking links. The proposed model uses reinforcement learning, introducing a novel extension where the graph embedding of the pages is incorporated into the Q function. The results are sound, and the paper is overall well written.  The reviewers and AC note the following potential weaknesses. The primary concern that was raised was the novelty. Since the task could potentially be framed as semantic parsing, reviewer 4 mentioned there may be readily available approaches for baselines that the authors did not consider. The comparison to semantic parsing required a more detailed discussion, pointing not only the differences but also the similarities, that would encourage the two communities to explore novel approaches to their tasks. Further, reviewer 2 was concerned about the limited novelty, given the extensive work that combines GNN and RL, such as NerveNet.  The authors provided comments and a revision to address these issues. They described why it is not trivial to formulate their setup as a semantic parsing problem, partly due to the fact that the environment is partially observable. Similarly, the authors described the differences between the proposed approach and methods like NerveNet, such as the use of a dynamic graph and off policy RL, making the latter not a viable baseline for the task. These changes addressed most of the concerns raised by the reviewers.  The reviewers agreed that this paper should be accepted.
This paper offers a new method for sentence representation learning, fitting loosely into the multi view learning framework, with fairly strong results. The paper is clearly borderline, with one reviewer arguing for acceptance and another arguing for rejection. While it is a tough decision, I have to argue for rejection in this case.  There was a robust discussion and the authors revised the paper, so none of the remaining technical issues strike me as fatal. My primary concern is simply that the reviewers could not reach a consensus in favor of the paper. In particular, two reviewers expressed concerns that this paper makes too small an advance in NLP to be of interest to non NLP researchers. I think it should be possible to broaden the scope of the paper and resubmit it to another general ML venue, and (as one reviewer suggested explicitly), this paper may have a better chance at an NLP specific venue.  While neither of these factors was crucial in the decision, I d encourage the authors (i) to put more effort into comparing properly with the Subramanian and Radford baselines, and (ii) to clarify the points about the human brain. For the second point: While none of the claims about the brain are false *or misleading*, as far as I know, the authors do not make a convincing case that the claims about the brain are actually relevant to the work being done here.
The paper proposes a generative model that generates one object at a time, and uses a relational network to encode cross object relationships. Similar  object centric generation and object object relational network  is proposed in "sequential attend, infer, repeat" of Kosiorek et al. for video generation, which first appeared on arxiv on June 5th 2018 and was officially accepted in NIPS 2018 before the submission deadline for ICLR 2019. Moreover, several recent generative models have been proposed that consider object centric biases,  which the current paper references  but does not compare against, e.g.,  attend, infer, repeat  of Eslami et al., or "DRAW: A Recurrent Neural Network For Image Generation" of Gregor et al. . The CLEVR dataset considered, though it contains real images, the intrinsic image complexity is low because it features a small number of objects against table background. As a result, the novelty of the proposed work may not be sufficient in light of recent literature, despite the fact that the paper presents a reasonable and interesting approach for image generation.  
This paper studies the properties of L1 regularization for deep neural network. It contains some interesting results, e.g. the stationary point of an l1 regularized layer has bounded number of non zero elements. On the other hand, the majority of reviewers has concerns on that experimental supports are weak and suggests rejection. Therefore, a final rejection is proposed.
While this was a borderline paper, concerns about the novelty and significance of the presented work exist on the part of all reviewers, and no reviewer was willing to argue for acceptance. Many good points to the work exist, and a stronger case on these issues would greatly strengthen the paper overall. I look forward to a future submission.
The paper presents a multi scale extension of the hourglass network. As the reviewers point out, the paper is below ICLR publication standard due to low novelty (i.e., multi scale extension is not a new idea) and significance (i.e., not a significant performance gain against the state of the art method or other baselines).
This paper proposes a generalization metric depending on the Lipschitz of the Hessian.  Pros: Paper has some nice experiments correlating their Hessian based generalization metric with the generalization gap,   Cons: The paper does not compare its results with existing generalization bounds, as there is substantial work in the area now.  It is not clear whether existing generalization bounds do not capture this phenomenon with different batch sizes/learning rates, and the necessity of having and explicit dependence on the Lipschitz of the Hessian.  The bound by itself is also weak because of its dependence on number of parameters  m .   The paper is poorly written and all reviewers complain about its readability.  I suggest authors to address concerns of the reviewers before submitting again. 
The authors admit the paper "was not written carefully enough and requires major rewriting."  This seems to be a frustratingly common phenomenon with work on the information bottleneck.  
The reviewers seem to reach a consensus that the contribution of the paper is somewhat incremental give the prior work of Goel et al and that a main drawback of the paper is that it s not clear the similar technique can be applied to multiple **convolutional filters**. The authors mentioned in the response that some of the techniques can be heuristically applied to multiple layers, but the AC is skeptical about it because, with multiple layers and multiple convolutional filters, one has to deal with the permutation invariance caused by the multiple convolutional filters. (It s unclear to the AC how one could have a meaningful setting with multiple layers but a single convolution filters.) 
All three reviewers agree that the research question—should pretrained embeddings be used in code understanding tasks—is a reasonable one. However, there were some early issues with the way in which the paper reported results (involving both metrics and baselines). After some discussion with the reviewers, it seems that the paper now presents a clear picture of the results, but that these results are not sufficiently strong to warrant acceptance.   I m wary to turn down a paper over what are basically negative results, but for results like this to be useful to the community, they d have to come from a very thorough experiment, and they d have to be accompanied by a frank and detailed discussion. Neither of the two more confident authors are convinced that this paper meets that bar.
This paper proposes Direct Sparse Optimization (DSO) NAS to obtain neural architectures on specific problems at a reasonable computational cost. Regularization by sparsity is a neat idea, but similar idea has been discussed by many pruning papers. "model pruning formulation for neural architecture search based on sparse optimization" is claimed to be the main contribution, but it s debatable if such contribution is strong: worse accuracy, more computation, more #parameters than Mnas (less search time, but also worse search quality). The effect of each proposed technique is appropriately evaluated. However, the reviewers are concerned that the proposed method does not outperform the existing state of the art methods in terms of classification accuracy. There s also some concerns about the search space of the proposed method. It is debatable about claim that "the first NAS algorithm to perform direct search on ImageNet" and "the first method to perform direct search without block structure sharing". Given the acceptance rate of ICLR should be <30%, I would say this paper is good but not outstanding. 
The work brings little novelty compared to existing literature. 
This paper conducted theoretical analysis of the effect of batch normalisation to auto rate tuning. It provides an explanation for the empirical success of BN. The assumptions for the analysis is also closer to the common practice of batch normalization compared to a related work of Wu et al. 2018.  One of the concerns raised by the reviewer is that the analysis does not immediately apply to practical uses of BN, but the authors already discussed how to fill the gap with a slight change of the activation function. Another concern is about the lack of empirical evaluation of the theory, and the authors provide additional experiments in the revision. R1 also points out a few weaknesses in the theoretical analysis, which I think would help improve the paper further if the authors could clarify and provide discussion in their revision.  Overall, it is a good paper that will help improve our theoretical understanding about the power tool of batch normalization.
The paper presents a method to stochastically optimize second order penalties and show how this could be applied to training fairness aware classifiers, where the linear penalties associated with common fairness criteria are expressed as the second order penalties.   While the reviewers acknowledged the potential usefulness of the proposed approach, all of them agreed that the paper requires: (1) major improvement in clarifying important points related to the approach (see R3’s detailed comments; R2’s concern on using the double sampling method to train non convex models; see R1’s and R3’s concerns regarding the double summation/integral terms and how this effects runtime), and (2) major improvement in justifying its application to fairness; as noted by R2, “there is no sufficient evidence why non convex models are actually useful in the experiments”. Given that fairness problems are currently studied on the small scale datasets (which is not this paper’s fault), a comparison to simpler methods for fairness or other applications could substantially strengthen the contribution and evaluation of this work. We hope the reviews are useful for improving and revising the paper.  
Some expert reviewers have raised novelty issues, that the authors have addressed in detail. Still, these expert reviewers are not entirely convinced. If this were a journal, I would recommend a major revision or reject and resubmit in order to allow the authors to anticipate the reviewers  concerns in the body of the paper and get some fresh reviews. I compliment the authors on the diligence they have put into the rebuttal stage, and look forward to reading the next version of the work. I will note that the bounds by Bartlett, Foster, and Telgarsky (and then the PAC Bayes versions by Neyshabur et al.) are numerically vacuous empirically, and so whether those bounds or these bounds for RNNs explain generalization is up for debate.
All reviewers agree in their assessment that this paper does not meet the bar for ICLR. The area chair commends the authors for their detailed responses.
The submission proposes a model to generate images where one can control the fine grained locations of objects. This is achieved by adding an "object pathway" to the GAN architecture. Experiments on a number of baselines are performed, including a number of reviewer suggested metrics that were added post rebuttal.  The method needs bounding boxes of the objects to be placed (and labels). The proposed method is simple and likely novel and I like the evaluating done with Yolov3 to get a sense of the object detection performance on the generated images. I find the results (qual & quant) and write up compelling and I think that the method will be of practical relevance, especially in creative applications.  Because of this, I recommend acceptance.
The paper proposes a decision theoretic framework for meta learning. The ideas and analysis are interesting and well motivated, and the experiments are thorough. The primary concerns of the reviewers have been addressed in new revisions of the paper. The reviewers all agree that the paper should be accepted. Hence, I recommend acceptance.
While there was some support for the ideas presented, the majority of the reviewers did not think the submission was ready for presentation at ICLR. Concerns raised included that the experiments needed more work, and the paper needs to do a better job of distinguishing the contributions beyond those of past work.
The paper presents a differentiable approximation of BLEU score, which can be directly optimized using SGD. The reviewers raised concerns about (1) direct evaluation of the quality of the approximation and (2) the significance of the experimental results. There is also a concern (3) regarding the significance of BLEU score in the first place, and whether BLEU is the right metric that one needs to directly optimize. The authors did not provide a response, and based on the concerns above (especially 1 2) I believe that the paper does not pass the bar for acceptance at ICLR.
The paper proposes a feature smoothing technique as a new and "cheaper" technique for training adversarially robust models.   Pros:  * the paper is generally well written and the claimed results seem quite promising  * the theory contribution are interesting  Cons:  * the main technique is fairly incremental  * there were concerns regarding the comprehensiveness of evaluations and baselines used
Strengths  The paper proposes to include exploration for the PETS (probabilistic ensembles with trajectory sampling) approach to learning the state transition function. The paper is clearly written.  Weaknesses  All reviewers are in agreement regarding a number of key weaknesses: limited novelty, limited evaluation, and aspects of the paper are difficult to follow or are sparse on details. No revisions have been posted.  Summary  All reviewers are in agreement that the paper requires significant work and that it is not ready for ICLR publication. 
This paper proposes Switchable Normalization (SN) that leans how to combine three existing normalization techniques for improved performance. There is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed SN.
 The paper presents a new annotation of the CIFAR 10 dataset (the test set) as a distribution over labels as opposed to one hot annotations. This datasets forms a testbed analysis for assessing the generalization abilities of the state of the art models and their robustness to adversarial attacks.   All the reviewers and AC acknowledge the contribution of dataset annotation and that the idea of using label distribution for training the models is sound and should improve the generalization performance of the models. However the reviewers and AC note the following potential weaknesses: (1) the paper requires major improvement in presentation clarity and in depth investigation and evidence of the benefits of the proposed framework – see detailed comments of R3 on what to address in a subsequent revision; see the suggestions of R2 for improving the scope of the empirical evaluations (e.g. distortions of the images, incorporating time limits for doing the classifications) and the requests of R1 for clarifications; (2) the related work is inadequate and should be substantially extended – see the related references suggested by the R2; also R1 rightly pointed out that two out of four future extensions of this framework have been addressed already, which questions the significance of findings in this submission. The R2 raised concerns that the current evaluation is missing comparisons to a) the calibration approaches and b) cheaper/easier ways of getting soft labels   see R2’s suggestion to use the Brier score for model calibration and to use a cost matrix about how critical a misclassification is (cat < > dog, versus cat < > car) as soft labels. Among these, (2) did not have a substantial impact on the decision, but would be helpful to address in a subsequent revision. However, (1) and (3) makes it very difficult to assess the benefits of the proposed approach, and was viewed by the AC as a critical issue.   There is no author response for this paper. The reviewer with a positive view on the manuscript (R3) was reluctant to champion the paper as the authors have not addressed the concerns of the other reviewers (no rebuttal).  
 This is an interesting topic but the reviewers had substantial concerns on the clarity and significance of the contribution. 
The paper proposes a method that learns mapping implicitly, by using a generative query network of Eslami et al. with an attention mechanism to learn to predict egomotion. The empirical findings is that training for egomotion estimation alongside the generative task of view prediction helps over a discriminative baseline, that does not consoder view prediction. The model is tested in Minecraft environments.  A comparison to some baseline SLAM like method, e.g., a method based on bundle adjustment, would be important to include despite beliefs of the authors that eventually learning based methods would win over geometric methods.  For example, potentially environments with changes can be considered, which will cause the geometric method to fail, but the proposed learning based method to succeed.  Moreover, there are currently learning based methods for the re localization problem that the paper would be important to compare against (instead of just cite), such as "MapNet: An Allocentric Spatial Memory for Mapping Environments" of Henriques et al.  and "Active Neural Localization" of Chaplot et al. . In particular, Mapnet has a generative interpretation by using cross convolutions as part of its architecture, which generalize very well, and which consider the geometric formation process. The paper makes a big distinction between generative and discriminative, however the architectural details behind the egomotion estimation network are potentially more or equally important to the loss used. This means, different discriminative networks depending on their architecture may perform very differently. Thus, it would be important to present quantitative results against such methods that use cross convolutions for egomotion estimation/re localization. 
This paper proposes a Bayesian alternative to dropout for deep networks by extending the EM based variable selection method with SG MCMC for sampling weights and stochastic approximation for tuning hyper parameters. The method is well presented with a clear motivation. The combination of SMVS, SG MCMC, and SA as a mixed optimization sampling approach is technically sound.  The main concern raised by the readers is the limited originality. SG MCMC has been studied extensively for Bayesian deep networks and applying the spike and slab prior as an alternative to dropout is a straightforward idea. The main contribution of the paper appears to be extending EMVS to deep net with commonly used sampling techniques for Bayesian networks.  Another concern is the lack of experimental justification for the advantage of the proposed method. While the authors promise to include more experiment results in the camera ready version, it requires a considerable amount of effort and the decision unfortunately has to be made based on the current revision.
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.   The paper tackles an interesting and relevant problem for ICLR: incremental classifier learning applied to image data streams.  2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    The proposed method is not clearly explained and not reproducible. In particular the contribution on top of the baseline iCaRL method is unclear. It seems to be mainly the use of CAE which is a minor change.   The experimental comparisons are incomplete. For example, in Table 4 the authors don t discuss the storage requirements of GAN and FearNet baselines.   The authors state that one of their main contributions is fullfilling privacy and legal requirements. They claim this is done by using CAEs which generate image embeddings that they store rather than the original images. However it s quite well known that a lot of data about the original images can be recovered from such embeddings (e.g. Dosovitskiy & Brox. "Inverting visual representations with convolutional networks." CVPR 2016.). These concerns all impacted the final decision.  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  There were no major points of contention and no author feedback.   4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be rejected. 
Important problem (making NN more transparent); reasonable approach for identifying which linguistic concepts different neurons are sensitive to; rigorous experiments. Paper was reviewed by three experts. Initially there were some concerns but after the author response and reviewer discussion, all three unanimously recommend acceptance.
there have been many variants of memory augmented neural nets since around 2014 when NTM, attention based NMT and MemNet were proposed. it is indeed still an interesting and important direction of research, but the bar for introducing yet another variant of memory augmented neural nets has been significantly raised, which is a sentiment shared by the reviewers. the author s response had not swayed the reviewers  opinion, and i am sticking to the reviewers  decisions.   i believe more streamlined and systematic comparison among different memory augmented networks across many different benchmarks (e.g., use the same set of latest variants of memory nets across all the benchmarks) in this submission would make it a better paper and increase the chance of acceptance. 
This is an interesting direction but multiple reviewers had concerns about the amount of novelty in the current work, and given the strong pool of other papers, this didn t quite reach the threshold.  
While there was some support for the ideas presented, unfortunately this paper was on the borderline. Significant concerns were raised as to whether the setting studied was realistic, among others.
This paper presents a model to identify entity mentions that are synonymous.  This could have utility in practical scenarios that handle entities.  The main criticism of the paper is regarding the baselines used.  Most of the baselines that are compared against are extremely simple.  There is a significant body of literature that models paraphrase and entailment and many of those baselines are missing (decomposable attention, DIIN, other cross attention mechanisms).  Adding those experiments would make the experimental setup stronger.  There is a bit of a disagreement between reviewers, but I agree with the two reviewers who point out the weakness of the experimental setup, and fixing those issues could improve the paper significantly.
The paper presents an empirical comparison of translation invariance property in CNN and capsule networks. As the reviewers point out, the paper is not acceptable quality at ICLR due to low novelty and significance. 
The reviewers found the paper insightful and the authors explanations well provided. However the paper would benefit from more systematic empirical evaluation and corresponding theoretical intuition.
This paper offers a new angle through which to study the development of comparison functions for sentence pair classification tasks by drawing on the literature on statistical relational learning. All three reviewers seemed happy to see an attempt to unify these two closely related relation learning problems. However, none of the reviewers were fully convinced that this attempt has yielded any substantial new knowledge: Many of the ideas that come out of this synthesis have already appeared in the sentence pair modeling literature (in work cited in the paper under review), and the proposed new methods do not yield substantial improvements for the tasks they re tested on.  I m happy to accept the authors  arguments that sentence to vector models have practical value, and I m not placing too much weight on the reviewer s comments about the choice to use that modeling framework. I am slightly concerned that the reviewers (especially R2) observed some overly broad statements in the paper, and I urge the authors to take those comments very seriously.  I m mostly concerned, though, about the lack of an impactful positive contribution: I d have hoped for a paper of this kind to offer a  a method with clear empirical advantages over prior work, or else a formal result which is more clearly new, and the reviewers are not convinced that this paper makes a contribution of either kind. 
This paper proposes to automatically learn the form of the non linearities of neural networks in deep neural networks, which the reviewers noted to be an interesting albeit significantly studied direction.   Overall, this paper falls just below the bar, with no reviewer really willing to champion for acceptance.  Reviewer 3 found the paper to be marginally above the acceptance threshold and found the insights provided in the paper (in Section 2) to be a neat and strong contribution.  Reviewers 1 2, however, found the paper marginally below the bar and seemed confused by the presentation of the paper.  They seemed to believe in the motivation and idea, but they found the paper hard to follow and not particularly clearly written.  It would seem that the paper could significantly benefit from careful editing and restructuring to disambiguate contributions from motivation and existing literature.  Also, the authors should provide clear justification for their design choices and modeling assumptions. 
The paper presents a method to learn inference mapping for GANs by reusing the learned discriminator s features and fitting a model over these features to reconstruct the original latent code z. R1 pointed out the connection to InfoGAN which the authors have addressed. R2 is concerned about limited novelty of the proposed method, which the AC agrees with, and lack of comparison to a related iGAN work by Zhu et al. (2016). The authors have provided the comparison in the revised version but the proposed method seems to be worse than iGAN in terms of the metrics used (PSNR and SSIM), though more efficient. The benefits of using the proposed metrics for evaluating GAN quality are also not established well, particularly in the context of other recent metrics such as FID and GILBO.  
This paper proposes a novel and interesting active learning approach, that  trains a classifier to discriminate between the examples in the labeled and unlabeled data at each iteration. The top few samples that are most likely to be from the unlabeled set as per this classifier are selected to be labeled by an oracle, and are moved to the labeled training examples bin in the next iteration. The idea is simple and clear and is shown to have a principled basis and theoretical background, related to GANs and to previous results from the literature. Experiments performed on CIFAR 10 and MNIST benchmarks demonstrate good results in comparison to baselines.  During the review period, authors considered most of the suggestions by the reviewers and updated the paper. Although the proposed method is similar to density based active learning methods, as also suggested by the reviewers, baselines do not include such approaches in the comparison experiments.
The paper proposes a new graph based regularizer to improve the robustness of deep nets. The idea is to encourage smoothness on a graph built on the features at different layers. Experiments on CIFAR 10 show that the method provides robustness over very different types of perturbations such as adversarial examples or quantization. The reviewers raised concerns around the significance of the results, the reliance on a single dataset and the unexplained link between adversarial examples and the regularization. Despite the revision, the reviewers maintain their concerns. For this reason this work is not ready for publication.
The authors have delivered an extensive examination of deep RL attacks, placing them within a taxonomy, proposing new attacks, and giving empirical evidence to compare the effectiveness of the attacks. The reviewers and AC appreciate the broad effort, comprising 14 different attacks, and the well written taxonomic discussion. However, the reviewers were concerned that the paper had significant problems with clarity of technical presentation and that the attacks were not well grounded in any sort of real world scenario. Although the authors addressed many concerns with their revision and rebuttal, the reviewers were not convinced. The AC believes that R1 ought to have increased their score given their comments and the resulting rebuttal, but the paper remains a borderline reject even with a corrected R1 score.
The paper is poorly written and below the bar of ICLR. The paper could be improved with better exposition and stronger experiment results (or clearer exposition of the experimental results.) 
The paper proposes a new way to tackle the trade off between disentanglement and reconstruction, by training a teacher autoencoder that learns to disentangle, then distilling into a student model. The distillation is encouraged with a loss term that constrains the Jacobian in an interesting way. The qualitative results with image manipulation are interesting and the general idea seems to be well liked by the reviewers (and myself).  The main weaknesses of the paper seem to be in the evaluation. Disentanglement is not exactly easy to measure as such. But overall the various ablation studies do show that the Jacobian regularization term improves meaningfully over Fader nets. Given the quality of the results and the fact that this work moves the needle in an important (albeit hard to define) area of learning disentangled representations, I think would be a good piece of work to present at ICLR so I recommend acceptance.
This paper proposes a new training approach for deep neural interfaces. The idea is to bootstrap from critics of other layers instead of using the final loss as target. The method is evaluated of CIFAR 10 and CIFAR 100 and found to improve performance slightly upon Sobolev training while being simpler. The reviewers found the idea interesting but were concerned about the strength of the experimental results. The datasets are similar and the significance of the results is not clear. The revision submitted by the authors was only able to address some of these issues such as the evaluation protocol.
+ experiments on an interesting task: inferring relations which are not necessarily explicitly mentioned in a sentence but need to be induced relying on other relations + the idea to frame the relation prediction task as an inference task on a graph is interesting     the paper is not very well written, and it is hard to understand what exactly the contribution is. E.g., the authors contrast with previous work saying that previous work was relying on pre defined graphs rather than inducing them. However, here they actually rely on predefined full graphs as well (i.e. full graphs connecting all entities).   (See questions from R1)    the idea of predicting edge embeddings from the sentence is an interesting one. However, I do not see results studying alternative architectures (e.g., fixed transition matrices + gates / attention), or careful ablation studies. It is hard to say if this modification is indeed necessary / beneficial.  (See also R3, agreeing that experiments look preliminary)    Extra baselines? E.g., what about layers of multi head self attention across entities? (as in Transformer). What about the number of parameters for the proposed model? Is there chance that it works better simply because it is a larger model? (See also R3)    evaluation only one dataset (not clear if any other datasets of this kind exist though)  Overall, though I find the direction and certain aspects of the model quite interesting, the paper is not ready for publication.
The paper proves that the Donsker Varadhan lower bound on KL divergence cannot be used to estimate KL divergences of more than tens of bits, and that more generally any distribution free high confidence lower bound on mutual information cannot be larger than O(ln N) where N is the size of the data sample. As an alternative for applications such as maximum mutual information predictive coding, a form of representation learning, the paper proposes using the cross entropy upper bound on entropy and estimating mutual information as a difference of two cross entropies. These cross entropy bounds converge to the true entropy as 1/\sqrt(N), but at the cost of providing neither an upper nor a lower bound on mutual information. There was a divergence of opinion between the reviewers on this paper. The most negative reviewer (R3) thought there should be experiments confirming that the DV bound fails when mutual information is high, was concerned that the theory applied only in the case of discrete distributions, and was concerned that the proposed optimization problem in Section 6 would be challenging due to its adversarial (max inf) structure. The authors responded that they felt the theory could stand on its own without empirical tests (a point with which R1 agreed); that although their exposition was for discrete variables, the analysis applies to the continuous case as well; and that they agreed with the point about the difficulty of the optimization, but that GANs face similar difficulties. Because R3 did not participate in the discussion and the AC believes that the authors adequately addressed most of R3 s issues in their response and revision, this review has been discounted. The next most negative reviewer (R2) wanted a discussion relating the ideas in this paper to kNN and kernel based estimators of mutual information, wanted an empirical evaluation (like R3), and was concerned about whether the difference of cross entropies provides an upper or lower bound on mutual information. In their response and revision the authors added some discussion of kNN methods (but not enough to make R2 happy) and clarified that the difference of cross entropies provides neither an upper nor a lower bound. The most positive reviewer (R1) thinks the theoretical contribution of the paper is significant enough to justify publication in ICLR. The AC likes the theoretical work and feels that it raises important concerns about MINE, but concurs with R2 and R3 that some empirical validation of the theory is needed for the paper to appear in ICLR. The authors are strongly encouraged to perform an empirical validation of the theory and to submit this work to another machine learning venue.
The authors obtain nice speed improvements by learning to skip and jump over input words when processing text with an LSTM. At some points the reviewers considered the work incremental since similar ideas have already been explored, but at the end two of the reviewers ended up endorsing the paper with strong support.
The manuscript studies a random matrix approach to recover sparse principal components. This work extends prior work using soft thresholding of the sample covariance matrix to enable sparse PCA. In this light, the main contribution of the paper is a study of generalizing soft thresholding to a broader class of functions and showing that this improves performance. The contributions of this paper are primarily theoretical.  The reviewers and AC note issues with the discussion that can be further improved to better illustrate contributions, and place this work in context. In particular, multiple reviewers assumed that "kernel" referred to the covariance matrix. The authors provide a satisfactory rebuttal addressing these issues.  While not unanimous, overall the reviewers and AC have a positive opinion of this paper and recommend acceptance.
The paper presents a technique of training robust classification models that uses the input distribution within each class to achieve high accuracy and robustness against adversarial perturbations.   Strengths:    The resulting model offers good robustness guarantees for a wide range of norm bounded perturbations    The authors put a lot of care into the robustness evaluation  Weaknesses:     Some of the "shortcomings" attributed to the previous work seem confusing, as the reported vulnerability corresponds to threat models that the previous work did not made claims about  Overall, this looks like a valuable and interesting contribution. 
There was discussion of this paper, and the accept reviewer was not willing to argue for acceptance of this paper, while the reject reviewers, specifically pointing to the clarity of the work, argued for rejection. There appear to be many good ideas related to wavelets, and hopefully the authors can work on polishing the paper and resubmitting.
This paper proposes a probabilistic model for data indexed by an observed parameter (such as time in video frames, or camera locations in 3d scenes), which enables a global encoding of all available frames and is able to sample consistently at arbitrary indexes. Experiments are reported on several synthetic datasets.   Reviewers acknowledged the significance of the proposed model, noted that the paper is well written, and the design choices are sounds. However, they also expressed concerns about the experimental setup, which only includes synthetic examples. Although the authors acknowledged during the response phase that this is indeed a current limitation, they argued it is not specific to their particular architecture, but to the task itself. Another concern raised by R1 is the lack of clarity in some experimental setups (for instance where only a subset of the best runs are used to compute error bars, and this subset appears to be of different size depending on the experiment, cf fig 5), and the fact that the datasets used in this paper to compare against GQNs are specifically designed.   Overall, this is a really borderline submission, with several strengths and weaknesses. After taking the reviewer discussion into account and making his/her own assessment, the AC recommends rejection at this time, but strongly encourages the authors to resubmit their work after improving their experimental setup, which will make the paper much stronger.
This paper presents a system which exploits semantic information of partial programs during program synthesis, and ensembling of synthesisers. The idea is general, and admirably simple. The explanation is clear, and the results are impressive. The reviewers, some after significant discussion, agree that this paper makes an import contribution and is one of the stronger papers in the conference. While some possible improvements to the method and experiment were discussed with the reviewers, it seems these are more suitable for future research, and that the paper is clearly publishable in its current form.
Dear authors,  Reviewers liked the idea of your new optimizer and found the experiments convincing. However, they also would have liked to get better insights on the place of AggMo in the existing optimization literature. Given that the related work section is quite small, I encourage you to expand it based on the works mentioned in the reviews.
 The authors present a novel method for tackling exploration and exploitation that yields promising results on some hard navigation like domains. The reviewers were impressed by the contribution and had some suggestions for improvement that should be addressed in the camera ready version. 
The paper focuses on hybrid pipelines that contain black boxes and neural networks, making it difficult to train the neural components due to non differentiability. As a solution, this paper proposes to replace black box functions with neural modules that approximate them during training, so that end to end training can be used, but at test time use the original black box modules. The authors propose a number of variations: offline, online, and hybrid of the two, to train the intermediate auxiliary networks. The proposed model is shown to be effective on a number of synthetic datasets.  The reviewers and AC note the following potential weaknesses: (1) the reviewers found some of the experiment details to be scattered, (2) It was unclear what happens if there is a mismatch between the auxiliary network and the black box function it is approximating, especially if the function is one, like sorting, that is difficult for neural models to approximate, and (3) the text lacked description of real world tasks for which such a hybrid pipeline would be useful.  The authors provide comments and a revision to address these concerns. They added a section that described the experiment setup to aid reproducibility, and incorporated more details in the results and related work, as suggested by the reviewers. Although these changes go a long way, some of the concerns, especially regarding the mismatch between neural and black box function, still remain.  Overall, the reviewers agreed that the issues had been addressed to a sufficient degree, and the paper should be accepted.
This paper presents a method for transferring source information via the hidden states of recurrent networks.  The transfer happens via an attention mechanism that operates between the target and the source.  Results on two tasks are strong.  I found this paper similar in spirit to Hypernetworks (David Ha, Andrew Dai, Quoc V Le, ICLR 2016) since there too there is a dynamic weight generation for network given another network, although this method did not use an attention mechanism.  However, reviewers thought that there is merit in this paper (albeit pointed the authors to other related work) and the empirical results are solid. 
This paper proposes a new measure to detect memorization based on how well the activations of the network are approximated by a low rank decomposition. They compare decompositions and find that non negative matrix factorization provides the best results. They evaluate of several datasets and show that the measure is well correlated with generalization and can be used for early stopping. All reviewers found the work novel, but there were concerns about the usefulness of the method, the experimental setup and the assumptions made. Some of these concerns were addressed by the revisions but concerns about usefulness and insights remained. These issues need to be properly addressed before acceptance.
This paper studies the question of memorization within overparametrised neural networks. Specifically, the authors conjecture that memorization is linked to the downsampling operators present in many convolutional autoencoders.   All reviewers agreed that this is an interesting question that deserves further analysis. However, they also agreed that in its current form, the paper lacks mathematical and experimental rigor. In particular, the paper does not follow the basic mathematical standards of proving any stated proposition/theorem, instead mixing empirical with mathematical proofs. The AC fully agrees with the points raised by reviewers, and therefore recommends rejection at this point, encouraging the authors to address these important points before resubmitting their work. 
Irrespective of their taste for comparisons of neural networks to biological organisms, all reviewers agree that the empirical observations in this paper are quite interesting and well presented. While some reviewers note that the paper is not making theoretical contributions, the empirical results in themselves are intriguing enough to be of interest to ICLR audiences.
Strengths:   This paper was clearly written, contained novel technical insights, and had SOTA results.  In particular, the explanation of the generalized dequantization trick was enlightening and I expect will be useful in this entire family of methods.  The paper also contained ablation experiments.  Weaknesses:   The paper went for a grab bag approach, when it might have been better to focus on one contribution and explore it in more detail (e.g. show that the learned pdf is smoother when using variational quantization, or showing the different in ELBO when using uniform q as suggested by R2).  Also, the main text contains many references to experiments that hadn t converged at submission time, but the submission wasn t updated during the initial discussion period.  Why not?  Points of contention:   Everyone agrees that the contributions are novel and useful.  The only question is whether the exposition is detailed enough to reproduce the new methods (the authors say they will provide code), and whether the experiments, which meet basic standards, of a high enough standard for publication, because there was little investigation into the causes of the difference in performance between models.  Consensus:   The consensus was that this paper was slightly below the bar.
This paper proposes distributional concavity regularization for GANs which encourages producing generator distributions with higher entropy.  The reviewers found the contribution interesting for the ICLR community. R3 initially found the paper lacked clarity, but the authors took the feedback in consideration and made significant improvements in their revision. The reviewers all agreed that the updated paper should be accepted.
This work highlights the problem of biased rewards present in common adversarial imitation learning implementations, and proposes adding absorbing states to to fix the issue. This is combined with an off policy training algorithm, yielding significantly improved sample efficiency, whose benefits are convincingly shown empirically. The paper is well written and clearly presents the contributions. Questions were satisfactorily answered during discussion, and resulted in an improved submission, a paper that all reviewers now agree is worth presenting at ICLR. 
The authors propose a technique for compressing neural networks by examining the correlations between filter responses, by removing filters which are highly correlated. This differentiates the authors’ work from many other works which compress the weights independent of the task/domain.  Strengths: Clearly written paper PFA KL does not require additional hyperparameter tuning (apart from those implicit in choosing \psi) Experiments demonstrate that the number of filters determined by the algorithm scale with complexity of the task  Weaknesses: Results on large scale tasks such as Imagenet (subsequently added by the authors during the rebuttal period) Compression after the fact may not be as good as training with a modified loss function that does compression jointly Insufficient comparisons on ResNet architectures which make comparisons against previous works harder  Overall, the reviewers were in agreement that this work (particularly, the revised version) was close to the acceptance threshold. In the ACs view, the authors addressed many of the concerns raised by the reviewers in the revisions. However, after much deliberation, the AC decided that the weaknesses 2, and 3 above were significant, and that these should be addressed in a subsequent submission.
The paper proposes to take advantage of implicit preferential information in a single state, to design auxiliary reward functions that can be combined with the standard RL reward function.  The motivation is to use the implicit information to infer signals that might not have been included in the reward function.  The paper has some nice ideas and is quite novel.  A new algorithm is developed, and is supported by proof of concept experiments.  Overall, the paper is a nice and novel contribution.  But reviewers point out several limitations.  The biggest one seems to be related to the problem setup: how to combine inferred reward and the given reward, especially when they are in conflict with each other.  A discussion of multi objective RL might be in place.
The paper proposes adversarial sampling for pool based active learning.  The reviewers and AC note the critical potential weaknesses on experimental results: it is far from being surprising the proposed method is better than random sampling. Ideally, one has to reduce the complexity under keeping the state of art performance. Otherwise, it is hard to claim the proposed method is fundamentally better than prior ones, although their targets might be different.  AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.
The authors propose an architecture for learning and predicting graphs with relations between nodes. The approach is a combination of recent research efforts into Graph Attention Networks and Relational Graph Convolutional Networks. The authors are commended for their clear and direct writing and presentation and their honest claims and their empirical setup. However, the paper simply doesn t have much to offer to the community, since the algorithmic contributions are marginal and the results unimpressive. While the authors justify the submission in terms of the difficult implementation and the extensive experiments, this is not enough to support its publication at a top conference. Rather, this could be a technical report.
The reviewers reached a consensus that the paper is not fit for publication for the moment because a) the paper lacks thorough experiments and b) the criteria provided by the paper are relatively evague (see more details in reviewer 3 s comments.）
The paper revisits the traditional bias variance trade off for the case of large capacity neural networks. Reviewers requested several clarifications on the experimental setting and underlying results. Authors provided some, but it was deemed not enough for the paper to be strong enough to be accepted. Reviewers discussed among themselved but think that given the paper is mostly experimental, it needs more experimental evidence to be acceptable. Overall, I found the paper borderline but concur with the reviewers to reject it in its current form.
The paper proposes batch constrained approach to batch RL, where the policy is optimized under the constrain that at a state only actions appearing in the training data are allowed.  An extension to continuous cases is given.  While the paper has some interesting idea and the problem of dealing with extrapolation in RL is important, the approach appears somewhat ad hoc and the contributions limited.  For example, the constraint is based on whether (s,a) is in B, but this condition can be quite delicate in a stochastic problem (seeing a in s *once* may still allow large extrapolation error if that only observed transition is not representative).  Section 4.1 gives some nice insights for the special finite MDP case, but those results are a little weak (requiring strong assumption that may not hold in practice)   an example being the requirement that s  be included in data if (s,a) is in data and P(s |s,a)>0 [beginning of section 4.1].  In contrast, there are other more robust and principled ways, such as counterfactual risk minimization (CRM) for contextual bandits (http://www.jmlr.org/papers/v16/swaminathan15a.html).  For MDPs, the Bayesian version of DQN (the cited Azizzadenesheli et al., as well as Lipton et al. at AAAI 18) can be used to constrain the learned policy as well, with a simple modification of using the CRM idea for bandits.  Would these algorithms be reasonable baselines?
This paper proposes several improvements for the MAML algorithm that improve its stability and performance. Strengths: The improvements are useful for future researchers building upon the MAML algorithm. The results demonstrate a significant improvement over MAML. The authors revised the paper to address concerns about overstatements  Weaknesses: The paper does not present a major conceptual advance. It would also be very helpful to present a more careful ablation study of the six individual techniques. Overall, the significance of the results outweights the weaknesses. However, the authors are strongly encouraged to perform and include a more detailed ablation study in the final paper. I recommend accept.
The issue of when model based methods can be used successfully in RL is an interesting one. However, the reviewers had a number of concerns about the significance and framing of this work with respect to the related literature. In addition, the abstract and title suggest a very generic contribution will be made, whereas the actual contribution is to a much more specific subclass. Some relevant papers (and their related efforts) include the following.  The Dependence of Effective Planning Horizon on Model Accuracy.  (AAMAS 15, best paper award) Nan Jiang, Alex Kulesza, Satinder Singh, Richard Lewis.   Self Correcting Models for Model Based Reinforcement Learning. Erik Talvitie. In  Proceedings of the Thirty First AAAI Conference on Artificial Intelligence (AAAI).  2017.  
 The paper proposes an augmented adversarial reconstruction loss for training a stochastic encoder decoder architecture. It corresponds to a discriminator loss distinguishing between a pair of a sample from the data distribution and its augmentation and pair containing the sample and its reconstruction. The introduction of the augmentation function is an interesting idea, intensively tested in a set of experiments, but, as two of the reviewers pointed out, the paper could be improved by deeper investigation of the augmentation function and the way of choosing it, which would increase significance of the contribution. 
Paper proposes a meta learning approach to interactive segmentation. After the author response, R2 and R3 recommend rejecting this paper citing concerns of limited novelty and insufficient experimental evaluation (given the popularity of this topic in computer vision). R1 does not seem be familiar with the extensive literature on interactive segmentation and their positive recommendation has been discounted. The AC finds no basis for accepting this paper.  
Strengths:  The paper presents an alternative regularized training objective for supervised learning that has a reasonable theoretical justification.  It also has a simple computational formula.  Weaknesses: The experiments are minimal proofs of concept on MNIST and fashion MNIST, and the authors didn t find an example where this formulation makes a large difference.  The resulting formula is very close to existing methods.  Finally the paper is a bit dense and the intuitions we should gain from this theory aren t made clear.  Points of contention: One reviewer pointed out the close connection of the new objective to IWAE, and the authors added a discussion of the relation and showed that they re not mathematically equivalent.  However, as far as I can tell they re almost identical in purpose:  As k  > \infty in IWAE, the encoder ceases to matter.  And as M  > \infty in VDB, we take the max over all encoders.  Could the method proposed in this paper lead to an alternative to IWAE in the VAE setting?  Consensus: Consensus wasn t reached, but the "7" reviewer did not appear to have put much though into their review.
All reviewers agree in their assessment that this paper has merits but is not yet ready for acceptance into ICLR. The area chair commends the authors for their responses to the reviews.
This work presents an interesting take on how to combine basic functions to lead to better activation functions. While the experiments in the paper show that the approach works well compared to the baselines that are used as reference, reviewers note that a more adequate assessment of the contribution would require comparing to stronger baselines or switching to tasks where the chosen baselines are indeed performing well. Authors are encouraged to follow the many suggestions of reviewers to strengthen their work.
The paper considers the problem of incorporating human physiological feedback into an autonomous driving system, where minimization of a predicted arousal response is used as an additional source of reward signal, with the intuition that this could be used as a proxy for training a policy that is risk averse.   Reviewers were generally positive about the novelty and relevance of the approach but had methodological concerns. In particular, concerns about the weighting of the intrinsic vs. extrinsic reward (why under different settings the optimal tradeoff parameter was different, how this affects the optimal policy if the influence of the intrinsic reward is not decreased with time). Additional baseline experiments were requested and performed, and the paper was modified to significantly incorporate other feedback such as drawing connections to imitation learning. A title change was proposed and accepted to reflect the focus on the application of risk aversion (I d ask that the authors update the paper OpenReview metadata to reflect this).  At a high level, I believe this is an original and interesting contribution to the literature. I have not heard from two of three reviewers regarding whether their concerns were addressed, but given that their concerns appear to me to have been addressed (and their initial scores indicated that the work met the bar for acceptance, if only marginally), I am inclined to recommend acceptance.
This paper is essentially an application of dual learning to multilingual NMT. The results are reasonable.  However, reviewers noted that the methodological novelty is minimal, and there are not a large number of new insights to be gained from the main experiments.  Thus, I am not recommending the paper for acceptance at this time.
This paper proposes a method for hierarchical reinforcement learning that aims to maximize mutual information between options and state action pairs. The approach and empirical analysis is interesting. The initial submission had many issues with clarity. However, the new revisions of the paper have significantly improved the clarity, better describing the idea and improving the terminology. The main remaining weakness is the scope of the experimental results. However, the reviewers agree that the paper exceeds the bar for publication at ICLR with the existing experiments.
The paper introduces a form of variational auto encoder for learning disentangled representations. The idea is to penalise synergistic mutual information. The introduction of concepts from synergy to the community is appreciated.   Although the approach appears interesting and forward looking in understanding complex models, at this point the paper does not convince on the theoretical nor on the experimental side. The main concepts used in the paper are developed elsewhere, the potential value of synergy is not properly examined.   The reviewers agree on a not so positive view on this paper, with ratings either ok, but not good enough, or clear rejection. There is a consensus that the paper needs more work.   
This paper proposes a regularization for IRL based on empowerment. The paper has some good results, and is generally well written. The reviewers raised concerns about how the approach was motivated; these concerns have largely been addressed from the reframing of the algorithm from the perspective of regularization. Now, all reviewers agree that the paper is somewhat above the bar for acceptance. Hence, I also recommend accept. There are several changes that the authors are strongly encouraged to incorporate in the final version of the paper (based on discussion between the reviewers):   The claim that empowerment acts as a regularizer in the policy update is a fairly complicated interpretation of the effect of the algorithm. It relies on an approximation derived in the appendix that relates the proposed objective with an empowerment regularized IRL formulation. The new framing makes much more sense. However, the one sentence reference to this section of the appendix in the main paper is not appropriate given that it is central to the claims of the paper s contribution. More discussion in the main text should be included.   There are still some parts of the implemented algorithm that could introduce bias (using a target network in the shaping term which differs from the theory in Ng et al. 1999), but this concern could be remedied by a code release. The authors are strongly encouraged to link to the code in the final non blind submission, especially since IRL implementations tend to be quite difficult to get right.   The authors said they would change the way they bold their best numbers in their rebuttal. The current paper does not make the promised change, and actually adopts different bolding conventions in different tables which is even more confusing. The numbers should be bolded in a consistent way, bolding the numbers with the best performance up to statistical significance.
The paper considers the task of incorporating knowledge expressed as rules into column networks. The reviewers acknowledge the need for such techniques, like the flexibility of the proposed approach, and appreciate the improvements to convergence speed and accuracy afforded by the proposed work.  The reviewers and the AC note the following as the primary concerns of the paper: (1) The primary concerned raised by the reviewers was that the evaluation is focused on whether KCLN can beat one with the knowledge, instead of measuring the efficacy of incorporating the knowledge itself (e.g. by comparing with other forms of incorporating knowledge, or by varying the quality of the rules that were introduced), (2) Even otherwise, the empirical results are not significant, offering slight improvements over the vanilla CLN (reviewer 1), (3) There are concerns that the rule based gates are introduced but gradients are only computed on the final layer, which might lead to instability, and (4) There are a number of issues in the presentation, where the space is used on redundant information and description of datasets, instead of focusing on the proposed model.  The comments by the authors address some of these concerns, in particular, clarifying that the forms of knowledge/rules are not limited, however, they focused on simple rules in the paper. However, the primary concerns in the evaluation still remain: (1) it seems to focus on comparing against Vanilla CLN, instead of focusing on the source of the knowledge, or on the efficacy in incorporating it (see earlier work on examples of how to evaluate these), and (2) the results are not considerably better with the proposed work, making the reviewers doubtful about the significance of the proposed work.  The reviewers agree that the paper is not ready for publication.
This paper studies deep convolutional architectures to perform compressive sensing of natural images, demonstrating improved empirical performance with an efficient pipeline.  Reviewers reached a consensus that this is an interesting contribution that advances data driven methods for compressed sensing, despite some doubts about the experimental setup and the scope of the theoretical insights. We thus recommend acceptance as poster. 
This work examines the AlphaGo Zero algorithm, a self play reinforcement learning algorithm that has been shown to learn policies with superhuman performance on 2 player perfect information games.  The main result of the paper is that the policy learned by AGZ corresponds to a Nash equilibrium, that and that the cross entropy minimization in the supervised learning inspired part of the algorithm converges to this Nash equillibrium, proves a bound on the expected returns of two policies under the and introduces a "robust MDP" view of a 2 player zero sum game played between the agent and nature.  R3 found the paper well structured and the results presented therein interesting. R2 complained of overly heavy notation and questioned the applicability of the results, as well as the utility of the robust MDP perspective (though did raise their score following revisions).  The most detailed critique came from R1, who suggested that the bound on the convergence of returns of two policies as the KL divergence between their induced distributions decreases is unsurprising, that using it to argue for AGZ s convergence to the optimal policy ignores the effects introduced by the suboptimality of the MCTS policy (while really interesting part being understanding how AGZ deals with, and whether or not it closes, this gap), and that the "robust MDP" view is less novel than the authors claim based on the known relationships between 2 player zero sum games and minimax robust control.   I find R1 s complaints, in particular with respect to "robust MDPs" (a criticism which went completely unaddressed by the authors in their rebuttal), convincing enough that I would narrowly recommend rejection at this time, while also agreeing with R3 that this is an interesting subject and that the results within could serve as the bedrock for a stronger future paper.
Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers  comments into consideration to improve your submission for the camera ready. 
The field of deep learning optimization suffers from a lack of standard benchmarks, and every paper reports results on a different set of models and architectures, likely with different protocols for tuning the baselines. This paper takes the useful step of providing a single benchmark suite for neural net optimizers.   The set of benchmarks seems well designed, and covers the range of baselines with a variety of representative architectures. It seems like a useful contribution that will improve the rigor of neural net optimizer evaluation.   One reviewer had a long back and forth with the authors about whether to provide a standard protocol for hyperparameter tuning. I side with the authors on this one: it seems like a bad idea to force a one size fits all protocol here.   As a lesser point, I m a little concerned about the strength of some of the baselines. As reviewers point out, some of the baseline results are weaker than typical implementations of those methods. One explanation might be the lack of learning rate schedules, something that s critical to get reasonable performance on some of these tasks. I get that using a fixed learning rate simplifies the grid search protocol, but I m worried it will hurt the baselines enough that effective learning rate schedules and normalization issues come to dominate the comparisons.  Still, the benchmark suite seems well constructed on the whole, and will probably be useful for evaluation of neural net optimizers. I recommend acceptance.  
This paper proposes a new optimization method for ReLU networks that optimizes in a scale invariant vector space in the hopes of facilitating learning. The proposed method is novel and is validated by some experiments on CIFAR 10 and CIFAR 100. The reviewers find the analysis of the invariance group informative but have raised questions about the computational cost of the method. These concerns were addressed by the authors in the revision. The method could be of practical interest to the community and so acceptance is recommended.
The paper proposes new methods for optimization of optimization of KL(student_model||teacher_model).   The topic is relevant. The paper also contains interesting ideas and the proposed methods are interesting; they are elegant and seems to work reasonably well on the tasks tried.  However, the reviewers do not all agree that the paper is well written. The reviewers have pointed out several issues that need to be addresses before the paper can be accepted.     
This paper presents the empirical relation between the task granularity and transfer learning, when applied between video classification and video captioning. The key take away message is that more fine grained tasks support better transfer in the case of classification captioning transfer on 20BN something something dataset.  Pros: The paper presents a new empirical study on transfer learning between video classification and video captioning performed on the recent 20BN something something dataset (220,000 videos concentrating on 174 action categories). The paper presents a lot of experimental results, albeit focused primarily on the 20BN dataset.  Cons: The investigation presented by this paper on the effect of the task granularity is rather application specific and empirical. As a result, it is unclear what generalizable knowledge or insights we gain for a broad range of other applications. The methodology used in the paper is relatively standard and not novel. Also, according to the 20BN something something leaderboard (https://20bn.com/datasets/something something), the performance reported in the paper does not seem competitive compared to current state of the art. There were some clarification questions raised by the reviewers but the authors did not respond.  Verdict:  Reject. The study presented by the paper is a bit too application specific with relatively narrow impact for ICLR. Relatively weak novelty and empirical results.
The paper proposes an transfer learning approach to reinforcement learning, where observations from a target domain are mapped to a source domain in which the algorithm was originally trained. Using unsupervised GAN models to learn this mapping from unaligned samples, the authors show that such a mapping allows the RL agent to successfully interact with the target domain without further training (apart from training the GAN models). The approach is empirically validated on modified versions of the Atari game breakout, as well as subsequent levels of Road Fighter, showing good performance on the transfer domain with a fraction of the samples that would be required for retraining the RL algorithm from scratch.   The reviewers and AC note the strong motivation for this work and emphasize that they find the idea interesting and novel. Reviewer 3 emphasizes the detailed analysis and results. Reviewer 2 notes the innovative idea to evaluate GANs in this application domain. Reviewer 1 identifies a key contribution in the thorough empirical analysis of the generalization issues that plaque current RL algorithms, as well as the comparison between different GAN models and finding their performance to be task specific.  The reviewers and AC noted several potential weaknesses: The proposed training based on images collected by an untrained agent focus the data on experience that agents would see very early on in the game, and may lead to generalization issues in more advanced parts of the game. Indeed these generalization issues are one possible explanation for the discrepancies between qualitative and quantitative results noted by reviewer 1. While the quantitative results indicate good performance on the target task, the image to image translation makes substantial errors, e.g., hallucinating blocks in breakout and erasing cars in Road Fighter. To the AC, the current paper does not provide enough insight into why the translation approach works even in cases where key elements are added or removed from the scene. The paper would benefit from a revision that thoroughly analyses such cases as well as the reason why the trained RL policy is able to generalize to them.  R1 further notes that the paper does not address the RL generalization issue, but rather presents an empirical study that shows that in specific cases it is easier to translate from a target to a source domain, than to learn a policy for the target domain. The AC shares this concern, especially given the limited error analysis and conceptual insights derived from the empirical study. There are further concerns about the experimental protocol and hyper parameter selection on the target tasks. Finally reviewer 1 questions the claim of whether data efficiency matters more than training efficiency in the proposed setting.  There is disagreement about this paper. Reviewers 2 and 3 gave high scores and positive reviews, but did not provide sufficient feedback to the concerns raised by reviewer 1, who put forward significant concerns.   The AC is particularly concerned about the experimental protocol and hyper parameter tuning directly on the test tasks. The authors counter this point by noting that "We agree that selecting configurations based on the test set is far from ideal, but we also note that this is the de facto standard in video game playing RL works, so we do not believe our work is any worse than others in the literature in this regard." The AC worries about the lack of motivation to identify a strong empirical setup to arrive at the strongest possible contribution. A key concern here is that the results seem to vary substantially by task, GAN model used, etc. and substantial tuning on the target domain seems to be required. This makes it hard to draw any generalizable conclusions. This concern can be alleviated by including additional analysis, e.g., error analysis of where a proposed approach fails, or additional experiments designed to isolate the factors that contribute to a particular performance level. However, the current paper does not go to this detail of empirical exploration. Given these concerns, I recommend not accepting the paper at the current stage.
The reviewers raised a number of concerns including the usefulness of the presented dataset given that the collected data is acted rather than naturalistic (and the large body of research in affective computing explains that models trained on acted data cannot generalise to naturalistic data), no methodological novelty in the presented work, and relatively uninteresting application with very limited real world application (it remains unclear whether having better empathetic dialogues would be truly crucial for any real life application and, in addition, all work is based on acted rather than real world data). The authors’ rebuttal addressed some of the reviewers’ concerns but not fully (especially when it comes to usefulness of the data). Overall, I believe that the effort to collect the presented database is noble and may be useful to the community to a small extent. However, given the unrealism of the data and, in turn, very limited (if any) generalisability of the presented to real world scenarios, and lack of methodological contribution, I cannot recommend this paper for presentation at ICLR.
With positive unlabeled learning the paper targets an interesting problem and proposes a new GAN based method to tackle it. All reviewers however agree that the write up and the motivation behind the method could be made more clear and that novelty compared to other GAN based methods is limited. Also the experimental analysis does not show a strong clear performance advantage over existing models. 
The authors propose a method to learn a neural network architecture which achieves the same accuracy as a reference network, with fewer parameters through Bayesian Optimization. The search is carried out on embeddings of the neural network architecture using a train bi directional LSTM. The reviewers generally found the work to be clearly written, and well motivated, with thorough experimentation, particularly in the revised version. Given the generally positive reviews from the authors, the AC recommends that the paper be accepted. 
The strength of the paper is that it designs an LP based algorithm for training neural networks with runtime exponential in the number of parameters and linear in the size of the datasets and the algorithm works for worst case datasets. As reviewer 2 and reviewer 3 pointed out, the cons include a) it s not clear why the algorithm provides any theoretical insights on how to design in the future polynomial time algorithm   it seems that the algorithms are inherently exponential time and b) it s not clear whether the algorithm is practically at all relevant. The AC also noted that brute force search algorithm is also exponential in # parameters and linear in size of datasets, and the authors agreed with it. This leaves the main contribution of the paper be that it works for the worst datasets. However, theoretically speaking, it s not clear this should be counted as a feature for algorithm design because we cannot go beyond the intractability without making assumptions on the data and in the AC s opinion, the big open question is how to make additional assumptions on the data (instead of removing them.) In summary, the drawback b) makes this a purely theoretical paper and the theoretical significance of the paper is unclear due to a). Therefore based on a), the AC decided to recommend reject, although the AC suggested the authors to re submit to other top theory or ML theory conferences which may better evaluate the theoretical significance of the paper. 
In this work, the authors propose a simple, under parameterized network architecture which can fit natural images well, when fed with a fixed random input signal. This allows the model to be used for a number of tasks without requiring that the model be trained on a dataset. Further, unlike a recently proposed related method (DIP; [Ulyanov et al., 18]), the method does not require regularization such as early stopping as with DIP.  The reviewers noted the simplicity and experimental validation, and were unanimous in recommending acceptance. 
This paper suggests the use of generative ensembles for detecting out of distribution samples.   The reviewers found the paper easy to read, especially after the changes made during the rebuttal. However, further elaboration in the technical descriptions (and assumptions made) could make the work seem more mature, as R2 and R1 point out.   The general feeling by reading the reviews and discussions is that this is promising work that, nevertheless, needs some more novel elements. A possible avenue for increasing the contribution of the paper is to follow R1’s advice to extract more convincing insights from the results.  
Paper studies an important problem   producing contrastive explanations (why did the network predict class B not A?). Two major concerns raised by reviewers   the use of one learned "black box" method to explain another and lack of human studies to quantify results   make it very difficult to accept this manuscript in its current state. We encourage the authors to incorporate reviewer feedback to make this manuscript stronger for a future submission; this is an important research topic. 
This paper focuses on the problem of detecting visual anomalies within textures. For that purpose, the authors consider several parametric texture models and train anomaly detection models on the corresponding outputs.   Reviewers were generally positive about the topic under study, but were unanimous in signaling a severe weaknesses in the experimental setup. In particular, in R2 words, "my main concern is that the performance evaluation is not suitable to achieve meaningful results", and "showing quantitative results from only two textures does not feel like a very comprehensive analysis". Moreover, the authors did not respond to reviewers feedback. Therefore, the AC recommends rejection at this time.
This paper proposes a new solution to the problem of domain generalization where the label distribution may differ across domains. The authors argue that prior work which ignores this observation suffers from an accuracy vs invariance trade off while their work does not.   The main contribution of the work is to 1) consider the case of different label distributions across domains and 2) to propose a regularizer extension to Xie 2017 to handle this.   There was disagreement between the reviewers on whether or not this contribution is significant enough to warrant publication. Two reviewers expressed concern of whether 1) naturally occurring data sources suffer substantially from this label distribution mismatch and 2) whether label distribution mismatch in practice results in significant performance loss for existing domain generalization techniques. Based on the experiments and discussions available now the answer to the above two points remains unclear. These key questions should be clarified and further justified before publication.
Reviewers largely agree that the proposed method for finetuning the deep neural networks is interesting and empirical results clearly show the benefits over finetuning only the last layer. I recommend acceptance. 
This paper conducts a study of the adversarial robustness of Bayesian Neural Network models. The reviewers all agree that the paper presents an interesting direction, with sound theoretical backing. However, there are important concerns regarding the significance and clarity of the work. In particular, the paper would greatly benefit from more demonstrated empirical significance, and more polished definitions and theoretical results. 
although the proposed method could be considered an interesting application to recently popular hypobolic space to word embeddings, it is unclear why this needs to be done so. experiments also do not support why or whether the application of hyperbolic space to word embedding is necessary.
The work proposes a method for smoothing a non differentiable machine learning pipeline (such as the Faster RCNN detector) using policy gradient. Unfortunately, the reviewers identified a number of critical issues, including no significant improvement beyond existing works. The authors did not provide a rebuttal for these critical issues. 
The paper describes the use of differentiable physics based rendering schemes to generate adversarial perturbations that are constrained by physics of image formation.  The paper puts forth a fairly novel approach to tackle an interesting question. However, some of the claims made regarding the "believability" of the adversarial examples produced by existing techniques are not fully supported. Also, the adversarial examples produced by the proposed techniques are not fully "physical" at least compared to how "physical" adversarial examples presented in some of the prior work were.  Overall though this paper constitutes a valuable contribution. 
The diffusion maps framework is used to embed a given collection of datasets into diffusion coordinates that capture intrinsic geometry. Then a correspondence map is constructed between datasets by  finding rotations that align these coordinates. The approach is interesting. The reviewers, however, found the empirical analysis somewhat simplistic with inadequate comparisons to other correspondence construction methods in the literature. 
The paper is proposed as probable accept based on current ratings with a majority accept (7,7,5).
After revision, all reviewers agree that this paper makes an interesting contribution to ICLR by proposing a new methodology for unbalanced optimal transport using GANs and should be accepted.
This paper proposes a new stochastic optimization scheme similar to Adam. The authors claim that Adam can be improved upon by decorrelating the second moment estimate v_t from gradient estimates g_t. This is done through the temporal decorrelation scheme, as well as block wise sharing of estimates v_t.  The reviewers agree that the paper is sufficiently well written, original and significant to be accepted for ICLR, although some unclarity remains after the reviews. A disadvantage of the method is mainly an increased computational cost (linear in  n , however this might be negligible when sharing v_t across blocks).
Important problem (visually grounded dialog); incremental (but not in a negative sense of the word) extension of prior work to an important new setting (GuessWhich); well executed. Paper was reviewed by three experts. Initially there were some concerns but after the author response and reviewer discussion, all three unanimously recommend acceptance.  
The reviewers and AC note the following potential weaknesses: 1) the proof techniques largley follow from previous work on linear models 2) it’s not clear how signficant it is to analyze a one neuron ReLU model for linearly separable data. 
The paper conveys interesting ideas but reviewers are concern about an incremental nature of results, choice of comparators, and in general empirical and analytical novelty.
Pros    Thorough analysis on a large number of diverse tasks   Extending the probing technique typically applied to individual encoder states to testing for presence of certain (linguistic) information based on pairs of encoders states (corresponding to pairs of words)   The comparison can be useful when deciding which representations to use for a given task  Cons    Nothing serious, it is solid and important empirical study  The reviewers are in consensus.
The reviewers have agreed this work is not ready for publication at ICLR.
The paper proposes a supervised adversarial method for disentangling the latent space of a VAE into two groups: latents z which are independent of the given attribute y, and \hat{y} which contains information about y. Since the encoder also predicts \hat{y} it can be used for classification and the paper shows competitive results on this task, apart from the attribute manipulation task. Reviewers had raised points about model complexity and connections to prior works which the authors have addressed and the paper is on the borderline based on the scores.   Though none of the reviewers explicitly pointed out the similarity of the paper with Fader networks (Lample et al., 2017), the adversarial setup for getting attribute invariant  z  is exactly same as in Fader networks, as also pointed out in an anonymous comment. The only difference is that encoder in the current paper also predicts the attribute itself (\hat{y}), which is not the case in Fader n/w, and hence the encoder can be used as a classifier as well (authors have also mentioned and discussed this difference in their response). However, the core idea of the paper as outlined in the title of the paper, ie, using adversarial loss for information factorization, is very similar to this earlier work, which diminishes the originality of the work.   With the borderline review scores, the paper can go in either of the half spaces (accept/reject) but I am hesitant to recommend an "accept" due to limited originality of the approach. However, if there is space in the program, the paper can be accepted. 
The paper proposes an additional module to train language models, adding a new loss that tries to predict the previous token given the next one, thus enforcing the model to remember the past. Two out of 3 reviewers recommend to accept the paper; the third one said it was misleading to claim SOTA since authors didn t try the mixture of softmax model that is actually currently SOTA. The authors acknowledged and modified the paper accordingly, and added a few more experiments. The reviewer still thinks the improvements are not important enough to claim significant novelty. Overall, I think the idea is simple and adds some structure to language modeling, but I also concur with the reviewer about limited improvements, which makes it a borderline paper. When calibrating with other area chairs, I decided to recommend to reject the paper.
The authors propose a technique for quantizing neural networks, which consist of repeated quantization/de quantization operations during training, and the second step learns scale factors. The method is simple, clearly presented, and requires no change in the training procedure. However, the authors noted that the work is somewhat incremental, and is similar to previously proposed approaches. As noted by the reviewers, the AC agrees that the work would be significantly strengthened by additional analysis of complexity in terms of computational time and memory relative to the other techniques.  
This paper provides a new family of untrained/randomly initialized sentence encoder baselines for a standard suite of NLP evaluation tasks, and shows that it does surprisingly well—very close to widely used methods for some of the tasks. All three reviewers acknowledge that this is a substantial contribution, and none see any major errors or fatal flaws.  One reviewer had initially argued the experiments and discussion are not as thorough as would be typical for a strong paper. In particular, the results are focused on a single set of word embeddings and a narrow class of architectures. I m sympathetic to this concern, but since there don t seem to be any outstanding concerns about the correctness of the paper, and since the other reviewers see the contribution as quite important, I recommend acceptance. [Update: This reviewer has since revised their review to make it more positive.]  (As a nit, I d ask the authors to ensure that the final version of the paper fits within the margins.)
The authors present a system for end to end multi lingual and multi speaker speech recognition. The presented method is based on multiple prior works that propose end to end models for multi lingual ASR and multi speaker ASR; the work combines these techniques and shows that a single system can do both with minimal changes.   The main critique from the reviewers is that the paper lacks novelty. It builds heavily on existing work, and  does not make any enough contributions to be accepted at ICLR. Furthermore, training and evaluations are all on simulated test sets that are not very realistic. So it is unclear how well the techniques would generalize to real use cases. For these reasons, the recommendation is to reject the paper.
The paper investigates a variant of the "cross entropy method" (CME) for heuristic combinatorial optimization, based on stochastically improving a search distribution via policy optimization in a surrogate objective.  Unfortunately, the reviewers unanimously recommended rejection, noting that the significance of the contribution over CME remains far from clear and insufficiently supported by the given evidence.  The experimental evaluation was unconvincing to all of the reviewers, particularly since only one artificial problem (clique finding) was considered in the paper (with an additional problem, k medoid clustering, briefly and incompletely considered in the appendix).  Several additional concerns were raised about the experimental evaluation, which triggered lengthy author responses but really need to be properly handled in the paper itself:    The sensitivity of performance to the optimization algorithm is a concern and requires more detailed understanding so that reasonable choices can be made in practice.    The independence assumption between search components is an extreme simplification that limits the appeal and applicability of the proposed approach.  Even after author response, it remains unconvincing that an independent search distribution over subcomponents can be effective in challenging combinatorial spaces.  Concrete evidence on challenging problems would be a more effective evidence than discussion.    The comparisons omitted any tailored algorithms for the specific problems.  Even if the authors insist on only comparing to more "general purpose" methods, there is a large space of evolutionary and Bayesian optimization strategies that have been neglected from the comparison.  A justification is needed for such an omission (if indeed it is even justifiable).
The paper studies RL based on data with confounders, where the confounders can affect both rewards and actions.  The setting is relevant in many problems and can have much potential.  This work is an interesting and useful attempt.  However, reviewers raised many questions regarding the problem setup and its comparison to related areas like causal inference.  While the author response provided further helpful details, the questions remained among the reviewers.  Therefore, the paper is not recommended for acceptance in its current stage; more work is needed to better motivate the setting and clarify its relation to other areas.  Furthermore, the paper should probably discuss its relation to (1) partially observable MDP; and (2) off policy RL.
This paper develops an active variable selection framework that couples a partial variational autoencoder capable of handling missing data with an information acquisition criteria derived from Bayesian experimental design. The paper is generally well written and the formulation appears to be natural, with a compelling real world healthcare application. The topic is relatively under explored in deep learning  and the paper appears to attempt to set a valuable baseline. However, the AC cannot recommend acceptance based on the fact that reviewer 2 has brought up concerns about the competitiveness of the approach relative to alternative methods reported in the experimental section, and all reviewers have found various parts of the paper to have room for improvement with regards to technical clarity. As such the paper would benefit from a revision and a stronger resubmission.
This paper introduces a new graph convolutional neural network, called LGNN, and applied it to solve the community detection problem. The reviewers think LGNN yields a nice and useful extension of graph CNN, especially in using the line graph of edge adjacencies and a non backtracking operator.  The empirical evaluation shows that the new method provides a useful tool for real datasets. The reviewers raised some issues in writing and reference, for which the authors have provided clarification and modified the papers accordingly.   
The paper proposes a novel sample based evaluation metric which extends the idea of FID by replacing the latent features of the inception network by those of a data set specific (V)AE and the FID by the mean FID of the class conditional distributions. Furthermore, the paper presents  interesting examples for which FID fails to match the human judgment while the new metric does not. All reviewers agree, that while these ideas are interesting, they are not convinced about the originality and significance of the contribution and believe that the work could be improved by a deeper analysis and experimental investigation.  
The paper provides a comprehensive study and generalisations of previous results on linear permutation invariant and equivariant operators / layers for the case of hypergraph data on multiple node sets. Reviewers indicate that the paper makes a particularly interesting and important contribution, with applications to graphs and hyper graphs, as demonstrated in experiments.   A concern was raised that the paper could be overstating its scope. A point is that the model might not actually give a complete characterization, since the analysis considers permutation action only. The authors have rephrased the claim. Following comments of the reviewer, the authors have also revised the paper to include a discussion of how the model is capable of approximating message passing networks.   Two referees give the paper a strong support. One referee considers the paper ok, but not good enough. The authors have made convincing efforts to improve issues and address the concerns.       
The paper s contribution lies in using cross lingual sharing of subword representations for improving document classification.  The paper presents interesting models and results.  While the paper is good (two out of three reviewers are happy about it), I do agree with the reviewer who suggests the experimentation with relatively dissimilar languages and showing whether or not the approach works for those cases.  I am also not very happy with the author response to the reviewer.  Moreover, I think the paper could improve further if the authors presented experiments on more tasks apart from document classification.
Dear authors,  The reviewers all appreciated the interest of studying properties of the latent representations rather than of the weights. The impact of the rank on the robustness to adversarial attacks is also of interest.  There were, however, two main issues raised. Due to the lack of confidence of some reviewers, I reviewed the paper myself and found the same issues:   Clarity could be improved. Some models are mentioned before being described (N LR) and some important details are missing. In particular, we sometimes lose track of the goal of the experiments. For instance, there are quite a few experiments on the further reduction of the rank of the representation but it is not clear what to extract from them.   More importantly, there are several important gaps in the analysis. In particular: a/ As many reviewers have pointed out, low rank constraints on the weight matrices induce low rank representations if the activation function is linear. As it is not, this might not be true but deserves a discussion. b/ You state that the rank constraint has little effect given that the actual rank is much less than the constraint. However, one would expect the resulting rank to be a smooth function of the rank of the constraint. Since there is a discrepancy between ResNet N LR and ResNet 1 LR, this should be investigated. c/ For the robustness to black box adversarial attacks, these attacks are constructed using the N LR models. Is is thus not too surprising that those models do not perform as well.  Thus, despite the lack of confidence of one reviewer (the question about the N LR models might stem from the fact that it is used before being introduced), I strongly encourage you to take their comments into account for a future submission.
All three reviewers feel that the paper needs to provide more convincing results to support their robustness claim, in addition to a number of other issues that need to be clarified/improved. The authors did not provide any response. 
although the problem of text infilling itself is interesting, all the reviewers were not certain about the extent of experiments and how they shed light on whether, how and why the proposed approach is better than existing approaches. 
The paper proposes and approach for model based reinforcement learning that adds a constraint to encourage the predictions from the model to be consistent with the observations from the environment. The reviewers had substantial concerns about the clarify of the initial submission, which has been significantly improved in revisions of the paper. The experiments have also been improved. Strengths: The method is simple, the performance is competitive with state of the art approaches, and the experiments are thorough including comparisons on seven different environments. Weaknesses: The main concern of the reviewers is the lack of concrete discussion about how the method compares to prior work. While the paper cites many different prior methods, the paper would be significantly improved by explicitly comparing and contrasting the ideas presented in this paper and those presented in prior work. A secondary weakness is that, while the results appear to be statistically significant, the improvement over prior methods is still relatively small. I do not think that this paper meets the bar for publication without an improved discussion of how this work is placed among the existing literature and without more convincing results.  As a side note, the authors should consider comparing to the below NeurIPS  18 paper, which significantly exceeds the performance of Nagabandi et al  17: https://arxiv.org/abs/1805.12114
This paper has received reviews from multiple experts who raise a litany of issues. These have been addressed quite convincingly by the authors, but I believe that ultimately this work needs to go through another round of reviewing, and this cannot be achieved in the context of ICLR s reviewing setup. I look forward to reading the final version of the paper in the near future.
The paper is a premature submission that needs significant improvement in terms of conceptual, theoretical, and empirical aspects.
The paper presents a simple and interesting idea to improve exploration efficiency, using the notion of action permissibility.  Experiments in two problems (lane keeping, and flappy bird) show that exploration can be improved over baselines like DQN and DDPG.  However, action permissibility appears to be very strong domain knowledge that limits the use in complex problems.  Rephrasing one of reviewers, action permissibility essentially implies that some one step information can be used to rule out suboptimal actions, while a defining challenge in RL is that the agent needs to learn/plan/reason over multiple steps to decide whether an action is suboptimal or not.  Indeed, the two problems in the experiments have such a property that a myopic agent can solve the tasks pretty well.  The paper would be stronger if the AP function can be defined for more common RL benchmarks, with similar benefits demonstrated.
This paper proposed a method to reduce the memory of training neural nets, in exchange for additional training time. The paper is simple and looks reasonable. It s a natural followup with previous work.  The improvement over previous work is not significant, with some overhead incurred in training time. This is a borderline paper but given the <30% acceptance rate, I need to downgrade the paper to reject. 
The proposed method introduces a method for unsupervised image to image mapping, using a new term into the objective function that enforces consistency in similarity between image patches across domains. Reviewers left constructive and detailed comments, which, the authors have made substantial efforts to address.  Reviewers have ranked paper as borderline, and in Area Chair s opinion, most major issued have been addressed:    R3&R2: Novelty compared to DistanceGAN/CRF limited: authors have clarified contributions in reference to DistanceGAN/CRF and demonstrated improved performance relative to several datasets.   R3&R1: Evaluation on additional datasets required: authors added evaluation on 4 more tasks   R3&R1: Details missing: authors added details.   
All reviewers still argue for rejection for the submitted paper. The AC thinks that this paper should be published at some point, but for now it is a "revise and resubmit".
The paper proposes the unique setting of adapting to multiple target domains. The idea being that their approach may leverage commonality across domains to improve adaptation while maintaining domain specific parameters where needed. This idea and general approach is interesting and worth exploring. The authors  rebuttal and paper edits significantly improved the draft and clarified some details missing from the original presentation.   There is an ablation study showing that each part of the model contributes to the overall performance. However, the approach provides only modest improvements over comparative methods which were not designed to learn from multiple target domains. In addition, comparison against the latest approaches is missing so it is likely that the performance reported here is below state of the art.   Overall, given the modest experimental gains combined with incremental improvement over single source information theoretic methods, this paper is not yet ready for publication.
Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers  comments into consideration to improve your submission for the camera ready. 
Dear authors,  Thank you for submitting your work to ICLR. The original goal of using smaller models to train a bigger one is definitely interesting and has been the topic of a lot of works.  However, the reviewers had two major complaints: the first one is about the clarity of the paper and the second one is about the significance of the tasks on which the algorith is tested. For the latter point, your rebuttal uses arguments which are little known in the ML community and so should be expanded in a future submission.
While the paper contains significant information, most insights have already been revealed in previous work as noted by R1.  The empirical novelty is therefore limited and the authors do not provide theoretical analysis to complement this.
This is an ambitious paper tackling the important and timely problem of controlling non annotated attributes in generated speech.   The reviewers had mixed opinions about the results. R1 asks for more convincing exposition of results but, nevertheless, acknowledging that it is difficult to evaluate TTS systems systematically. Besides, R2 and R3 find the results good.   Judging from the reviews and previous work, this paper does not seem to be very novel, although it certainly has intriguing new elements. Furthermore, it constitutes a mature piece of work.   
This paper presents a method for preventing exploding and vanishing gradients in LSTMs by stochastically blocking some paths of the information flow (but not others). Experiments show improved training speed and robustness to hyperparameter settings.  I m concerned about the quality of R2, since (as the authors point out) some of the text is copied verbatim from the paper. The other two reviewers are generally positive about the paper, with scores of 6 and 7, and R1 in particular points out that this work has already had noticeable impact in the field. While the reviewers pointed out some minor concerns with the experiments, there don t seem to be any major flaws. I think the paper is above the bar for acceptance. 
The paper proposes an approach for transfer learning by assigning weights to source samples and learning these jointly with the network parameters. Reviewers had a few concerns about experiments, some of which have been addressed by the authors. The proposed approach is simple which is a positive but it is not evaluated on any of the regular transfer learning benchmarks (eg, the ones used in Kornblith et al., 2018 "Do Better ImageNet Models Transfer Better?"). The tasks used in the paper, such as CIFAR noisy  > CIFAR and SVHN0 4  > MNIST5 9, are artificially constructed, and the paper falls short of demonstrating the effectiveness of the approach on real settings.   The paper is on the borderline with current scores and the lack of regular transfer learning benchmarks in the evaluations makes me lean towards not recommending acceptance. 
This paper describes an incorporation of attention into model agnostic meta learning. The reviewers found that the paper was rather confusing in its presentation of both the method and the tasks. While the results seemed interesting, it was difficult to frame them due to lack of clarity as to what the task is, and the relation between attention and MAML. It sounds like this paper needs a bit more work, and thus is not suitable for publication at this time.  It is disappointing that the reviews were so short, but as the authors did not challenge them, unfortunately the AC must decide on the basis of the first set of comments by reviewers.
The authors posit and investigate a hypothesis   the “lottery ticket hypothesis”   which aims to explain why overparameterized neural networks are easier to train than their sparse counterparts. Under this hypothesis, randomly initialized dense networks are easier to train because they contain a larger number of “winning tickets”. This paper received very favorable reviews, though there were some notable points of concern. The reviewers and the AC appreciated the detailed and careful experimentation and analysis. However, there were a couple of points of concern raised by the reviewers: 1) the lack of experiments conducted on large scale tasks and models, and 2) the lack of a clear application of the idea beyond what has been proposed previously.   Overall, this is a very interesting paper with convincing experimental validation and as such the AC is happy to accept the work.
The proposal is a scheme for using implicit matrix vector products to exploit curvature information for neural net optimization, roughly based on the adaptive learning rate and momentum tricks from Martens and Grosse (2015). The paper is well written, and the proposed method seems like a reasonable thing to try.  I don t see any critical flaws in the methods. While there was a long discussion between R1 and the authors on many detailed points, most of the points R1 raises seem very minor, and authors  response to the conceptual points seems satisfactory.  In terms of novelty, the method is mostly a remixing of ideas that have already appeared in the neural net optimization literature. There is sufficient novelty to justify acceptance if there were strong experimental results, but in my opinion not enough for the conceptual contributions to stand on their own.  There is not much evidence of a real optimization improvement. The per epoch improvement over SGD is fairly small, and (as the reviewers point out) probably outweighed by the factor of 2 computational overhead, so it s likely there is no wall clock improvement. Other details of the experimental setup seem concerning; e.g., if I understand right, the SGD training curve flatlines because the SGD parameters were tuned for validation accuracy rather than training accuracy (as is reported). The only comparison to another second order method is to K FAC on an MNIST MLP, even though K FAC and other methods have been applied to much larger scale models.   I think there s a promising idea here which could make a strong paper if the theory or experiments were further developed. But I can t recommend acceptance in its current form. 
The paper proposes to improve VAE by using a prior distribution that has been previously proposed for independent subspace analysis (ISA). The clarity of the paper could be improved by more clearly describing the proposed method and its implementation details. The originality is not that high, as the main change to VAE is replacing the usual isotropic Gaussian prior with an ISA prior. Moreover, the paper does not provide comparison to VAEs with other more sophisticated priors, such as the VampPrior, and it is unclear whether using the ISA prior makes it difficult to scale to high dimensional observations. Therefore, it is difficult to evaluate the significance of ISA VAE. The authors are encouraged to carefully revise their paper to address these concerns. 
Dear authors,  The topic of variance reduction in optimization is timely and the reviewers appreciated your attempt at circumventing the issues faced with the current popular methods.  They however had a concern about the significance of the results, which I echo:   First, there have been previous attempts at variance reduction which share some similarity with yours, for instance "No more pesky learning rate", "Topmoumoute online natural gradient algorithm" or even Adam (which does variance reduction without mentioning it).   The fact that previous similar methods exist is a non issue should yours perform better. However, the absence of stepsize tuning in the experimental evaluation is a big issue as the performance of an iterative algorithm is highly sensitive to it.  Finally, the link between flatness of the minimum and generalization is dubious, as mentioned for instance by Dinh et al. (2017).  As a consequence, I cannot accept this work for publication to ICLR but I encourage you to address the points of the reviewers should you wish to resubmit it to a future conference. 
Using volumetric convolutions, this paper focuses on learning in (rather than on) the unit sphere.  The novelty of the approach is debatable, and the mathematical analysis not strong enough to merit that.  In combination with good but not outstanding results, interest of the research community is doubted.  An extended experimental analysis of the method would greatly improve the paper.
Reviewers are in a consensus and weakly recommended to reject after engaging with the authors, with the reviewers updating their scores on Dec 11 after engagement. The authors answered most of the reviewers  concerns, however from further discussions with the  reviewers there are still some points which lead them to rank the paper lower than others. I thus lean to reject. Please take reviewers  comments into consideration to improve submission should you choose to resubmit.
The paper received mixed ratings. The proposed idea is quite reasonable but also sounds somewhat incremental. While the idea of separating foreground/background is reasonable, it also limits the applicability of the proposed method (i.e., the method is only demonstrated on aligned face images). In addition, combining AdaIn with foreground mask is a reasonable idea but doesn’t sound groundbreakingly novel. The comparison against StarGAN looks quite anecdotal  and the proposed method seems to cause only hairstyle changes (but transfer with other attributes are not obvious). In addition, please refer to detailed reviewers’ comments for other concerns. Overall, it sounds like a good engineering paper that might be better fit to computer vision venue, but experimental validation seems somewhat preliminary and it’s unclear how much novel insight and general technical contributions that this work provides. 
This paper addresses data sanitization, using a KL divergence based notion of privacy. While an interesting goal, the use of average case as opposed to worst case privacy misses the point of privacy guarantees, which must protect all individuals. (Otherwise, individuals with truly anomalous private values may be the only ones who opt for the highest levels of privacy, yet this situation will itself leak some information about their private values).  
The proposed “input forgetting” problem is interesting, and the reflective likelihood can come to be seen as a natural solution, however the reviewers overall are concerned about the rigor of the paper. Reviewer 2 pointed out a technical flaw and this was addressed, however the reviewers remain unconvinced about the theoretical justification for the approach. One suggestion made by reviewer 1 is to focus on simpler models that can be studied more rigorously. Alternatively, it could be useful to focus on stronger empirical results. The method works in the experiments given, but for example in the imbalanced data experiments, only MLE is compared to as a baseline. I think it would be more convincing to compare against stronger baselines from the literature. If they are orthogonal to the choice of estimator, then it would be even better to show that these baselines + RLL outperforms the baselines + MLE. Alternatively, you mention some challenging tasks like seq2seq, where a convincing demonstration would greatly strengthen the paper. While the paper is not yet ready in its current form, it seems like a promising approach that is worth further exploration.
* Strengths  The paper addresses an important topic: how to bound the probability that a given “bad” event occurs for a neural network under some distribution of inputs. This could be relevant, for instance, in autonomous robotics settings where there is some environment model and we would like to bound the probability of an adverse outcome (e.g. for an autonomous aircraft, the time to crash under a given turbulence model). The desired failure probabilities are often low enough that direct Monte Carlo simulation is too expensive. The present work provides some preliminary but meaningful progress towards better methods of estimating such low probability events, and provides some evidence that the methods can scale up to larger networks. It is well written and of high technical quality.  * Weaknesses  In the initial submission, one reviewer was concerned that the term “verification” was misleading, as the methods had no formal guarantees that the estimated probability was correct. The authors proposed to revise the paper to remove reference to verification in the title and the text, and afterwards all reviewers agreed the work should be accepted. The paper also may slightly overstate the generality of the method. For instance, the claim that this can be used to show that adversarial examples do not exist is probably wrong adversarial examples often occupy a negligibly small portion of the input space. There was also concern that most comparisons were limited to naive Monte Carlo.  * Discussion  While there was initial disagreement among reviewers, after the discussion all reviewers agree the paper should be accepted. However, we remind the authors to implement the changes promised during the discussion period.
This is an interesting paper that shows how improved off policy estimation (and optimization) can be improved by explicitly estimating the data logging policy.  It is remarkable that the estimation variance can be reduced over using the original logging policy for IPW, although this result depends on the (somewhat impractical) assumption that the parametric form for the true logging policy is known.  The reviewers unanimously recommended the paper be accepted.  However, there remain criticisms of the theoretical analysis that the authors should take into account in preparing a final version (namely, motivating the assumptions needed to obtain the results, and providing stronger intuitions behind the reduced variance).
The paper presents a new deep learning approach for combinatorial optimization problems based on the Transformer architecture. The paper is well written and several experiments are provided. A reviewer asked for more intuition to the proposed approach and authors have responded accordingly. Reviewers are also concerned with scalability and theoretical basis. Overall, all reviewers were positives in their scores, and I recommend accepting the paper.
The paper presents an algorithm for audio super resolution using adversarial models along with additional losses, e.g. using auto encoders and reconstruction losses, to improve the generation process.   Strengths   Proposes audio super resolution based on GANs, extending some of the techniques proposed for vision / image to audio.   The authors improved the paper during the review process by including results from a user study and ablation analysis.  Weaknesses   Although the paper presents an interesting application of GANs for the audio task, overall novelty is limited since the setup closely follows what has been done for vision and related tasks, and the baseline system. This is also not the first application of GANs for audio tasks.    Performance improvement over previously proposed (U Net) models is small. It would have been useful to also include UNet4 in user study, as one of the reviewers’ pointed out, since it sounds better in a few cases.   It is not entirely clear if the method would be an improvement of state of the art audio generative models like Wavenet.  Reviewers agree that the general direction of this work is interesting, but the results are not compelling enough at the moment for the paper to be accepted to ICLR. Given these review comments, the recommendation is to reject the paper.
The reviewers found that the paper needs more compelling empirical study.
The paper proposes methods to deal with estimating classification confidence on unseen data distributions.   The reviewers and AC note the following potential weaknesses: (1) limited novelty and (2) the authors  new comparison with Guo et al. (2017) asked by Reviewer 2 is not convincing enough.  AC thinks the proposed method has potential and is interesting, but decided that the authors need new ideas to meet the high standard of ICLR.
The authors propose a scheme to compress models using student teacher distillation, where training data are augmented using examples generated from a conditional GAN. The reviewers were generally in agreement that 1) that the experimental results generally support the claims made by the authors, and 2) that the paper is clearly written and easy to follow. However, the reviewers also raised a number of concerns: 1) that the experiments were conducted on small scale tasks, 2) the use of the compression score might be impractical since it would require retraining a compressed model, and is affected by the effectiveness of the compression algorithm which is an additional confounding factor. The authors in their rebuttal address 2) by noting that the student training was not too expensive, but I believe that this cost is task specific. Overall, I think 1) is a significant concern, and the AC agrees with the reviewers that an evaluation of the techniques on large scale datasets would strengthen the paper. 
This paper shows local convergence results for gradient descent on one hidden layer network with Gaussian inputs and sigmoid activations. Later it shows global convergence by using spectral initialization. All the reviewers agree that the results are similar to existing work in the literature with little novelty. There are also some concerns about the correctness of the statements expressed by some reviewers. 
This paper investigates learning to represent edit operations for two domains: text and source code. The primary contributions of the paper are in the specific task formulation and the new dataset (for source code edits). The technical novelty is relatively weak.  Pros: The paper introduces a new dataset for source code edits.    Cons: Reviewers raised various concerns about human evaluation and many other experimental details, most of which the rebuttal have successfully addressed. As a result, R3 updated their score from 4 to 6.   Verdict: Possible weak accept. None of the remaining issues after the rebuttal is a serious deal breaker (e.g., task simplification by assuming the knowledge of when and where the edit must be applied, simplifying the real world application of the automatic edits). However, the overall impact and novelty of the paper is relatively weak.
All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.
The paper studies an interesting problem with a reasonable solution.  However, reviewers feel that the technical contributions are somewhat incremental.  Furthermore, the empirical study would have been stronger with more proper baselines (simple adaptation to the multitask setting), and on problems beyond the simple grid worlds.  In addition, reviewers also find the presentation should be improved substantially.
This manuscript proposes an architectural improvement for generative adversarial network that allows the intermediate layers of a generator to be modulated by the input noise vector using conditional batch normalization. The reviewers find the paper simple and well supported by extensive experimental results. There were some concerns about the impact of such an empirical study. However, the strength and simplicity of the technique means that the method could be of practical interest to the ICLR community.
This paper presents a model for question answering, where the idea is to have a collaborative model that aligns queries and sentences on a small supervised dataset and also uses semi supervised information from a weakly supervised corpus to answer open domain questions resulting in short answer spans.  The main criticism of the paper is regarding its novelty, and reviewers cite the similarities with prior work such as Chen et al. and Min et al.  There is relative consensus between the reviewers that further work using the semi supervised outlook with stronger results could strengthen the paper further.
Reviewers are in a consensus and recommended to reject after engaging with the authors. Please take reviewers  comments into consideration to improve your submission should you decide to resubmit. 
The authors propose using a SVM, trained as a last layer of a neural network, to identify exemplars (support vectors) to save and use to prevent forgetting as the model is trained on further tasks. The method is effective on several supervised benchmarks and is compared to several other methods, including VCL, iCARL, and GEM. The reviewers had various objections to the initial paper that centered around comparisons to other methods and reporting of detailed performance numbers, which the authors resolved convincingly in their revised paper. However, the AC and 2 of the reviewers were unconvinced of the contribution of the approach. Although no one has used this particular strategy, of using support vectors to prevent forgetting, the approach is a simplistic composition of the NN and the SVM which is heuristic, at least in how the authors present it. Most importantly, the approach is limited to supervised classification problems, yet catastrophic forgetting is not commonly considered to be a problem for the supervised classifier setting; rather it is a problem for inherently sequential learning environments such as RL (MNIST and CIFAR are just commonly used in the literature for ease of evaluation).
The paper presents an approach to mitigate the presence of noisy labels during training by trying to forget wrong labels. Reviewers pointed out a few concerns, including lack of novelty, lack of enough experimental support, and lack of theoretical support. Authors have added some experiments and details about the experimental section, but reviewers still think it s not enough for acceptance. I concur with the reviewers to reject the paper.
The reviewers are unanimous in their assessment that the paper lacks originality in its current form to be publishable at ICLR 2018.
This is a well executed paper that makes clear contributions to the understanding of unrolled iterative optimization and soft thresholding for sparse signal recovery with neural networks.
While there was disagreement on this paper, reviewers remained unconvinced about the scalability and novelty of the presented work. While it was universally agreed that many positive points exist in this paper, it is not yet ready for publication. 
The paper proposes a new distributed DQN algorithm that combines recurrent neural networks with distributed prioritized replay memory. The authors systematically compare three types of initialization strategies for training the recurrent models. The thorough investigation is cited as a valuable contribution by all reviewers, with reviewer 1 noting that the study would be of interest to "anyone using recurrent networks on RL tasks". Empirical results on Atari and DMLab are impressive.  The reviewers noted several weaknesses in their original reviews. These included issues of clarity, a need for more detailed ablation studies, and need to more carefully document the empirical setup. A further question was raised on whether the empirical results could be complemented with theoretical or conceptual insights.  The authors carefully addressed all concerns raised during the reviewing and rebuttal period. They took exceptional care to clarify their writing, document experiment details, and ran a large set of additional experiments as suggested by the reviewers. The AC feels that the review period for the paper was particularly productive and would like to thank the reviewers and authors.  The reviewers and AC agree that the paper makes a significant contribution to the field and should be accepted.
Learning on Riemannian manifolds can be easily done  with this Python package.  Considering the recent work on these in latent variable models, the package can be quite a useful approach.  But its novelty is disputed.  In particular Pymanopt is a package that does mostly the same, even though that may be computationally more expensive.  The merits of Geomstats vs. Pymanopt is not clarified.  But be that as it may, there is interest amongst the reviewers for the software package.  In the end, too, it s not uniformly agreed upon that a software describing paper fits ICLR.
This paper studies the task of learning a binary classifier from only unlabeled data. They first provide a negative result, i.e., they show it is impossible to learn an unbiased estimator from a set of unlabeled data. Then they provide an empirical risk minimization method which works when given two sets of unlabeled data, as well as the class priors.   The four submitted reviews were unanimous in their vote to accept. The results are impactful, and might make for an interesting oral presentation.
This paper is concerned with solving Online Combinatorial Optimization (OCO) problems using reinforcement learning (RL). There is a well established traditional family of approaches to solving OCO problems, therefore the attempt itself to solve them with RL is very intriguing, as this provides insights about the capabilities of RL in a new but at the same time well understood class of problems.   The reviewers agree that this approach is not entirely new. While past similar efforts take away some of the novelty of this paper, the reviewers and AC believe that still the setting considered here contains novel and interesting elements.   All reviewers were unconvinced that this work can provide strong claims about using RL to learn any primal dual algorithm. This takes away some of the paper’s impact, but thanks to discussion the authors managed to clarify some “hand wavy” claims and toned down the claims that were not convincing. Therefore, it was agreed that the new revision still provides some useful insight into the RL and primal dual connection, even without a complete formal connection.  
The paper introduces a new and convincing method for video frame prediction, by adding prediction uncertainty through VAEs.  The results are convincing, and the reviewers are convinced.  It s unfortunate however that the method is only evaluated on simulated data.  Letting it loose on real data would cement the results and merit oral representation; in the current form, poster presentation is recommended.
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.     The paper tackles an interesting and challenging problem with a novel approach.   The method gives improves improved performance for the surface reconstruction task.  2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.  The paper   lacks clarity in some areas   doesn t sufficiently explain the trade offs between performing all computations in the spectral domain vs the spatial domain.   3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  Reviewers had a divergent set of concerns. After the rebuttal, the remaining concerns were:   the significance of the performance improvements. The AC believes that the quantitative and qualitative results in Table 3 and Figures 5 and 6 show significant improvements with respect to two recent methods.   a feeling that the proposed method could have been more efficient if more computations were done in the spectral domain. This is a fair point but should be considered as suggestions for improvement and future work rather than grounds for rejection in the AC s view.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers did not reach a consensus. The final decision is aligned with the more positive reviewer, AR1, because AR1 was more confident in his/her review and because of the additional reasons stated in the previous section. 
This paper studies the problem of heterogeneous domain transfer, for example across different data modalities.  The comments of the reviewers are overlapping to a great extent. On the  one hand, the reviewers and AC agree that the problem considered is very interesting and deserves more attention.  On the other hand, the reviewers have raised concerns about the amount of novelty contained in this manuscript, as well as convincingness of results. The AC understands the authors’ argument that a simple method can be a feature and not a flaw, however this work still does not feel complete. Even within a relatively simple framework, it would be desirable to examine the problem from multiple angles and "disentangle" the effects of the different hypotheses – for example the reviewers have drawn attention to end to end training and comparison with other baselines. The points raised above, together with improving the manuscript (as commented by reviewers) would make this work more complete.
This paper explores the addition of feedback connections to popular CNN architectures. All three reviewers suggest rejecting the paper, pointing to limited novelty with respect to other recent publications, and unconvincing experiments. The AC agrees with the reviewers.  
Following the unanimous vote of the four submitted reviews, this paper is not ready for publication at ICLR. Among other concerns raised, the experiments need significant work.
Reviewers are in a consensus and recommended to reject after engaging with the authors. Please take reviewers  comments into consideration to improve your submission should you decide to resubmit. 
The reviewers all agree that the idea is interesting, the writing clear and the experiments sufficient.   To improve the paper, the authors should consider better discussing their meta objective and some of the algorithmic choices. 
This paper describes the development of a large scale continuous visual speech recognition (lipreading) system, including an audiovisual processing pipeline that is used to extract stabilized videos of lips and corresponding phone sequences from YouTube videos, a deep network architecture trained with CTC loss that maps video sequences to sequences of distributions over phones, and an FST based decoder that produces word sequences from the phone score sequences. A performance evaluation shows that the proposed system outperforms other models described in the literature, as well as professional lipreaders. A number of ablation experiments compare the performance of the proposed architecture to the previously proposed LipNet and "Watch, Attend, and Spell" architectures, explore the performance differences caused by using phone  or character based CTC models, and some variations on the proposed architecture. This paper was extremely controversial and received a robust discussion between the authors and reviewers, with the primary point of contention being the suitability of the paper for ICLR. All reviewers agree that the quality of the work in the paper is excellent and that the reported results are impressive, but there was strong disagreement on whether or not this was sufficient for an ICLR paper. One reviewer thought so, while the other two reviewers argued that this is insufficient, and that to appear in ICLR the paper either (1) should have focused more on the preparation of the dataset, included public release of the data so other researchers could build on the work, and put forth the V2P model as a (very) strong baseline for the task; or (2) done a more in depth exploration of the representation learning aspects of the work by comparing phoneme and viseme units and providing more (admittedly costly) ablation experiments to shed more light on what aspects of the V2P architecture lead to the reported improvements in performance. The AC finds the arguments of the two negative reviewers to be persuasive. It is quite clear at this point that many supervised classification tasks (even structured classification tasks like lipreading) can be effectively tackled by a combination of a sufficiently flexible learning architecture and collection of a massive, annotated dataset, and the modeling techniques used in this paper are not new, per se, even if their application to lipreading is. Moreover, if the dataset is not publicly available, it is impossible for anyone else to build on this work. The paper, as it currently stands, would be appropriate in a more applications oriented venue.
The paper introduces a version of approximate policy iteration (API), called Autodidactic Iteration (ADI), designed to overcome the problem of sparse rewards.  In particular, the policy evaluation step of ADI is trained on a distribution of states that allows the reward to easily propagate from the goal state to states farther away.  ADI is applied to successfully solve the Rubik s Cube (together with other existing techniques).  This work is an interesting contribution where the ADI idea may be useful in other scenarios.  A limitation is that the whole empirical study is on the Rubik s Cube; a controlled experiment on other problems (even if simpler) can be useful to understand the pros & cons of ADI compared to others.  Minor: please update the bib entry of Bottou (2011).  It s now published in MLJ 2014.
The paper considers the problem of knowledge distillation from a few samples. The proposed solution is to align feature representations of the student network with the teacher by adding 1x1 convolutions to each student block, and learning only the parameters of those layers. As noted by Reviewers 1 and 2, the performance of the proposed method is rather poor in absolute terms, and the use case considered (distillation from a few samples) is not motivated well enough. Reviewers also note the method is quite simplistic and incremental.
This paper introduces a class of deep neural nets that provably have no bad local valleys. By constructing a new class of network this paper avoids having to rely on unrealistic assumptions and manages to provide a relatively concise proof that the network family has no strict local minima. Furthermore, it is demonstrated that this type of network yields reasonable experimental results on some benchmarks. The reviewers identified issues such as missing measurements of the training loss, which is the actual quantity studied in the theoretical results, as well as some issues with the presentation of the results. After revisions the reviewers are satisfied that their comments have been addressed. This paper continues an interesting line of theoretical research and brings it closer to practice and so it should be of interest to the ICLR community.
The authors propose an algorithm for enhancing noisy speech by also accounting for the phase information. This is done by adapting UNets to handle features defined in the complex space, and by adapting the loss function to improve an appropriate evaluation metric.  Strengths   Modifies existing techniques well to better suit the domain for which the algorithm is being proposed. Modifications like extending UNet to complex Unet to deal with phase, redefining the mask and loss are all interesting improvements.   Extensive results and analysis.  Weaknesses   The work is centered around speech enhancement, and hence has limited focus.   Even though the paper is limited to speech enhancement, the reviewers agreed that the contributions made by the paper are significant and can help improve related applications like ASR. The paper is well written with interesting results and analysis. Therefore, it is recommended that the paper be accepted. 
This paper focuses on neural network models for source code edits. Compared to prior literature that focused on generative models of source codes, this paper focuses on the generative models of edit sequences of the source code. The paper explores both explicit and implicit representations of source code edits with experiments on synthetic and real code data.  Pros: The task studied has a potential real world impact. The reviewers found the paper is generally clear to read.  Cons: While the paper doesn t have a major flaw, the overall impact and novelty of the paper are considered to be relatively marginal. Even after the rebuttal, none of the reviewers felt compelled to increase their score. One point that came up multiple times is that the paper treats the source code as flat text and does not model the semantic and syntactic structure of the source code (via e.g., abstract syntax tree). While this alone would have not been a deal breaker, the overall substance presented in the paper does not seem strong. Also, the empirical results are reasonable but not impressive given that the experiments are focused more on the synthetic data, and the experiments on the real source code are weaker and less clear as has been also noted by the fourth reviewer.   Verdict: Possible weak reject. No significant deal breaker per say but the overall substance and novelty are marginal.
A new regularized graph CNN approach is proposed for semi supervised learning on graphs.  The conventional Graph CNN is concatenated with a Transposed Network, which is used to supplement the supervised loss w.r.t. the labeled part of the graph with an unsupervised loss that serves as a regularizer measuring reconstruction errors of features. While this extension performs well and was found to be interesting in general by the reviewers,  the novelty of the approach (adding a reconstruction loss),  the completeness of the experimental evaluation, and the presentation quality have also been questioned consistently. The paper has improved during the course of the review, but overall the AC evaluates that paper is not upto ICLR 2019 standards in its current form. 
While there was some support for the ideas presented, the majority of the reviewers did not think the submission is ready for publication at ICLR. Significant concerns were raised about clarity of the exposition.
This paper proposed Selective Convolutional Unit (SCU) for improving the 1x1 convolutions used in the bottleneck of a ResNet block. The main idea is to remove channels of low “importance” and replace them by other ones which are in a similar fashion found to be important. To this end the authors propose the so called expected channel damage score (ECDS) which is used for channel selection. The authors also show the effectiveness of SCU on CIFAR 10, CIFAR 100 and Imagenet.  The major concerns from various reviewers are that the design seems the over complicated as well as the experiments are not state of the art. In response, the authors add some explanations on the design idea and new experiments of DenseNet BC 190 on CIFAR10/100. But the reviewers’ major concerns are still there and did not change their ratings (6,5,5). Based on current results, the paper is proposed for borderline lean reject.   
This paper introduces a variant of the CycleGAN designed to optimize molecular graphs to achieve a desired quality.  The work is reasonably clear and sensible, however it s of limited technical novelty, since it s mainly just combining two existing techniques.  Overall its specificity and incrementalness make it not meet the bar.
This paper proposes a document classification algorithm based on partitioned word vector averaging. I agree with even the most positive reviewer. More experiments would be good. This is a very developed old area.
The paper studies the problem of uncertainty estimation of neural networks and proposes to use Bayesian approach with noice contrastive prior.  The reviewers and AC note the potential weaknesses of experimental results: (1) lack of sufficient datasets with moderate to high dimensional inputs, (2) arguable choices of hyperparameters and (3) lack of direct evaluations, e.g., measuring network calibration is better than active learning.  The paper is well written and potentially interesting. However, AC decided that the paper might not be ready to publish in the current form due to the weakness.
The paper provides a simple method for regularising and robustifying GAN training. Always appreciated contribution to GANs. : )
The authors present a method using a VAE to model segmentation masks directly. Errors in reconstruction of masks by the VAE indicate that the mask may be outside the distribution of common mask shapes, and are used to predict poor quality segmentation scenarios that fall outside the distribution of common segmentations.   Pros: + R2: Technical idea is interesting, and a number of baselines used to compare.  + R1 & R4: Method is novel.   Cons:   R3 & R4: The method ignores the original input in its prediction, making the method wholly reliant on shape priors. In situations where the shape prior is weak, the method may be expected to fail. Authors have confirmed this, but not added any experiments to quantify its effect.    R4: The baseline regressor method is missing key details, which makes it impossible to judge if the comparison is fair (i.e. at minimum, number of learned parameters for each model, number of convolutional layers, structure of network, etc.). Authors have not provided these details. Authors have not investigated datasets with weak shape prior to see how methods compare in this setting.   R2: GANs can be used as a baseline. Authors confirmed, but did not supply results.   Reviewers generally agree that the idea is novel, but the value of the approach cannot be determined due to missing baseline experiments, and missing details of baselines. Recommend reject in current form, but encourage authors to complete experiments. 
Interesting and novel approach of modeling context (mainly external documents with information about the conversation content) for the conversational question answering task, demonstrating significant improvements on the newly released conversational QA datasets. The first version of the paper was weaker on motivation and lacked a clearer presentation of the approach as mentioned by the reviewers, but the paper was updated as explained in the responses to the reviewers. The ablation studies are useful in demonstration of the proposed FLOW approach. A question still remains after the reviews (this was not raised by the reviewers): How does the approach perform in comparison to the state of the art for the single question and answer tasks? If each question was asked in isolation, would it still be the best?   
This paper examines the relationship between attention and alignment in NMT. The reviewers all agreed that this is a valuable topic that is worth thinking about.  However, there were concerns both about the clarity of the paper and the framing with respect to previous work. First, it was hard for some reviewers to understand exactly what the paper was trying to do due to issues of the paper structure, etc. Second, there are a number of previous works that also examine similar concepts, and the description of how the proposed method differs seemed lacking.  Due to these issues, I cannot recommend it for acceptance in its current form.
This work examines how to craft adversarial examples that will lead trained seq2seq models to generate undesired outputs (here defined as, assigning higher than average probability to undesired outputs). Making a model safe for deployment is an important unsolved problem and this work is looking at it from an interesting angle, and all reviewers agree that the paper is clear, well presented, and offering useful observations. While the paper does not provide ways to fix the problem of egregious outputs being probable, as pointed out by reviewers, it is still a valuable study of the behavior of trained models and an interesting way to "probe" them, that would likely be of high interest to many people at ICLR.
All of the reviewers find this paper to contain interesting ideas. Originally, clarity was a major issue, although a few issues remain (see the comments of reviewer 3). The reviewers believe that the paper has been substantially improved from its original form, however there is still room for improvement: more comprehensive comparisons to existing work (reviewer 1), careful ablations (reviewer 3), etc. With a little bit of polish, this paper is likely to be accepted at another venue.  I am certainly not penalizing you for anonymously sharing your code on Github, as this was specifically requested by reviewer 1. 
The paper presents LEAPS, a hybrid model based and model free algorithm that uses a Bayesian approach to reason/plan over semantic features, while low level behavior is learned in a model free manner. The approach is designed for human made environments with semantic similarity, such as indoor navigation, and is empirically validated in a virtual indoor navigation task, House3D. Reviewers and AC note the interesting approach to this challenging problem. The presented approach can provide an elegant way to incorporate  domain knowledge into RL approaches.   The reviewers and AC note several potential weaknesses. The reviewers are concerned about the very low success rate, and critiqued the use of success rate as a key metric itself, given that random search with a sufficiently high cut off could solve the task. The authors added additional results in a metric that incorporates path length, and provided clarifying details. However, key concerns remained given the low success rates. The AC notes that e.g., results in the top and middle row of figure 4 show very similar results for LEAPS and the reported baselines. Further, "figure 5" shows no confidence / error bars, and it is not possible to assess whether any differences are statistically significant. Overall, the questions of whether something substantial has been learned, should be addressed with a detailed error analysis of the proposed approach and the baselines, to provide insight into whether and how the approaches solve the task. At the moment, the paper presents a potentially valuable approach, but does not provide convincing evidence and conceptual insights into the approach s effectiveness.
The paper studies learning from complementary labels – the setting when example comes with the label information about one of the classes that the example does not belong to. The paper core contribution is an unbiased risk estimator for arbitrary losses and models under this learning scenario, which is an improvement over the previous work, as rightly acknowledged by R1 and R2.  The reviewers and AC note the following potential weaknesses: (1) R3 raised an important concern that the core technical contribution is a special case of previously published more general framework which is not cited in the paper. The authors agree with R3 on this matter; (2) the proposed unbiased estimator is not practical, e.g. it leads to overfitting when the cross entropy loss is used, it is unbounded from below as pointed out by R1; (3) the two proposed modifications of the unbiased estimator are biased estimators, which defeats the motivation of the work and limits its main technical contributions; (4) R2 rightly pointed out that the assumption that complementary label is selected uniformly at random is unrealistic – see R2’s suggestions on how to address this issue.  While all the reviewers acknowledged that the proposed biased estimators show advantageous performance on practice, the AC decides that in its current state the paper does not present significant contributions to the prior work, given (1) (3), and needs major revision before submitting for another round of reviews.   
This paper addresses the problem of recommendations within user sessions from a reinforcement learning perspective. The problem is naturally modeled as an RL problem, given its sequential nature and inherent uncertainty of any model over user preferences. The problem suffers from delayed and sparse rewards, which the authors propose to address using self supervised prediction. The approach is empirically validated in a simulated setting, using data from the 2015 ACM RecSys Challenge.  The reviewers and AC note that the problem studied is an important application area where RL has high potential to improve over current research results and industry practice. The proposed idea is interesting, and the strong empirical evaluation on a publicly available data set is highlighted. R1 also commends the authors  decision to address the challenging cold start problem.  The reviewers and AC also note several potential weaknesses. The choice of addressing the problem from a reinforcement learning perspective is not clearly motivated. This is needed, as many supervised learning (and other types) approaches to the problem exist. A performance comparison to current state of the art RL baselines is missing. The proposed approach is related to both imagination augmented (I2A, Racaniere et al. 2017) and agents with auxiliary rewards (UNREAL, Jaderberg et al. 2016), but does not compare to either method. Neither does the related work section sufficiently clarify why the proposed approach is expected to improve over these prior approaches. A thorough comparison to these baselines in a real world application like session based recommendation would be a strong contribution in itself, but without the contributions of the paper are hard to assess. Reviewers also noted lack of clarity. Some concerns are addressed by the authors, but the consensus is that the paper would benefit from a major revision to clearly work out the method, as well as it s conceptual and empirical differences from existing reinforcement learning approaches. R3 mentions missing related work, some of which the authors include in the revision. The AC recommends also following up on references in cited papers to ensure a future revision of the paper is well placed in the context of prior work on recommender systems, especially when modeled as a reinforcement learning problem.  Overall, the paper was assessed as borderline by the reviewers. The ACs view is that there are too many concerns for acceptance at ICLR in the present form, and that the paper will benefit from a thorough revision.
The proposed notion of star convexity is interesting and the empirical work done to provide evidence that it is indeed present in real world neural network training is appreciated.  The reviewers raise a number of concerns. The authors were able to convince some of the reviewers with new experiments under MSE loss and experiments showing how robust the method was to the reference point. The most serious concerns relate to novelty and the assumptions that individual functions share a global minima with respect to which the path of iterates generated by SGD satisfies the star convexity property. I m inclined to accept the authors rebuttal, although it would have been nicer had the reviewer re engaged. Overall, the paper is on the borderline.
The paper proposes a probabilistic training method for binary Neural Network with stochastic versions of Batch Normalization and max pooling.  The reviewers and AC note the following potential weaknesses: (1) limited novelty and (2) preliminary experimental results.  AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.
This paper develops a generative density model based on continuous time flows on a potential field.   Strengths:  The paper contains interesting ideas and connections to physics, in particular the enforcement of symmetry in a computationally cheap way.  Weaknesses:  The main quantitative results of this paper are undercut by the numerical error introduced by the approximate, fixed step integrator used.  In the paper, the authors did not check the degree of numerical error (or to what extend their reported likelihoods do not normalize) as a function of the step size.  This was partially addressed in a comment below.  There does seem to be some novelty but the lack of concrete experiments is a letdown. One could e.g. verify that the samples have similar properties (e.g. moments) to the ground truth, which are known for the Ising model. Regarding clarity, the symmetry constraints are never clearly specified.  This paper contains many ideas that would have been novel, but were scooped by [1] which was put on arXiv 3 months before the ICLR submission date.  The authors have added appropriate references to this paper, but this still undercuts the originality of the contribution.  The explanation of how and which symmetries are enforced is a little bit buried and unclear.  Points of contention:  Two of the reviewers didn t seem to be aware that the main mathematical results of the model are special cases of results from [1].  Consensus:  All reviewers agreed that there were interesting ideas in the paper, and that it was close to the bar.  [1] Chen, Tian Qi, et al. "Neural Ordinary Differential Equations."
The manuscript describes a procedure for prioritizing the contents of an experience replay buffer in a UVFA setting based on a density model of the trajectory of the achieved goal states. A rank based transformation of densities is used to stochastically prioritize the replay memory.  Reducing the sample complexity of RL is a worthy goal and reviewers found the overall approach is interesting, if somewhat arbitrary in the implementation details. Concerns were raised about clarity and justification, and the restriction of experiments to fully deterministic environments.  After personally reading the updated manuscript I found clarity to still be lacking. Statements like "... uses the ranking number (starting from zero) directly as the probability for sampling"   this is not true (it is normalized, as confusingly laid out in equation 2 with the same symbol used for the unnormalized and normalized densities), and also implies that the least likely trajectory under the model is never sampled, which doesn t seem like a desirable property. Schaul s "prioritized experience replay" is cited for the choice of rank based distribution, but the distribution employed in that work has rather different form. The related work section is also very poor given the existing body of literature on curiosity in a reinforcement learning context, and the new "importance sampling perspective" section serves little explicatory purpose given that an importance re weighting is not performed.   Overall, I concur most strongly with AnonReviewer1 that more work is needed to motivate the method and prove its robustness applicability, as well as to polish the presentation.
mnist and small picture variants are not that impressive. it is a minor extension of VAEs which also are not common in sota systems.
The paper provides a novel attack method and contributes to evaluating the robustness of neural networks with recently proposed defenses. The evaluation is convincing overall and the authors have answered most questions from the reviewers. We recommend acceptance. 
There is no author response for this paper. The paper presents a multi task learning framework as a unified view on the previous methods for tackling catastrophic forgetting in continual learning. In light of this framework, the authors propose to minimize the KL divergence between the predictions of the previous optimal model and the current model using some stored samples from the previous tasks.   The consensus among all three reviewers and AC is that the paper lacks (1) novelty, as the proposed approach is similar if not identical to Learning without forgetting (LwF)[Li&Hoiem 2017] with the difference that the KL divergence is computed on samples kept from the previous tasks (and LwF uses samples from the current task). Methodological and experimental comparison to LwF is crucial to assess the benefits and novelty of the proposed approach.   Also the reviewers address other potential weaknesses and give suggestions for improvement: (2) empirical evaluations can be substantially improved with sensitivity analysis of the hyper parameters on the validation data (R3), indicating errors and error bars for all results (R3 and R2), using more challenging and realistic experimental setting where the data comes from different domains (R1), justifying the results better   see R2’s questions; (3) lack of clarity and motivation in Section 3.1   see R2’s and R1’s suggestions for how to improve clarity and potentially take advantage of the current task to probably correct the previous models prediction when it was wrong.  AC suggests, in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper. 
The authors present a theoretical and practical study on low precision training of neural networks. They introduce the notion of variance retention ratio (VRR) that determines the accumulation bit width for precise tailoring of computation hardware.  Empirically, the authors show that their theoretical result extends to practical implementation in three standard benchmarks.  A criticism of the paper has been certain hyperparameters that a reviewer found to be chosen rather arbitrarily, but I think the reviewers do a reasonable job in rebutting it.   Overall, there is consensus that the paper presents an interesting framework and does both practical and empirical analysis, and it should be accepted.
 This is an interesting direction and all reviewers thought the idea has merit, but pointed out some significant limitations. The authors did an admirable job at addressing some of these but some remain, including R1’s point 2 which is a significant issue. The authors are encouraged to submit a revised version of their work which addresses all the discussed limitations and will likely be a competitive submission to another top ML conference. 
The paper presents a large scale empirical comparison between different prominent losses, regularization and normalization schemes, and neural architectures frequently used in GAN training. Large scale comparisons in this field are rare and important and the outcome of the experimental analysis is clearly of interest for practitioners. However, as two of the reviewers point out, the significance of the new insights is limited, and after rebutal all reviewers agree that the paper would profit from a clearer write up and presentation of the main findings. I see the paper therefore, as lying slightly under the acceptance trashhold.
Adversarial defense is a tricky subject, and the authors are to be commended for their novel approach to this problem. The reviewers all agree that there is promise in this approach. However, after reviewing the discussion, they have all come to the conclusion that the robustness of your generative model needs to be more thoroughly explored. Regarding gradient masking, there are other attacks like a manifold attack that use gradients that can be explored as well. Regarding SPSA, it would be helpful perhaps to also include other numerical gradient attacks to ensure that SPSA is stronger and working as intended.  Essentially, the reviewers would all like to see a more streamlined version of this paper that removes any doubt about the efficacy of the generative approach. Once that is established, additional properties and features can be explored.  Also note that for the purposes of these reviews and discussion, Schott et al. was considered as concurrent work and not prior work. 
The paper proposes a new method for training generative models by minimizing general f divergences. The main technical idea is to optimize f divergence between joint distributions which is rightly observed to be the upper bound of the f divergence between the marginal distributions and address the disjoint support problem by convolving the data with a noise distribution.  The basic ideas in this work are not completely novel but are put together in a new way.   However, the key weakness of this work, as all the reviewer noticed, is that the empirical results are too week to support the usefulness of the proposed approach. The only quantitive results are in table 2, which is only a simple Gaussian example. It essential to have more substantial empirical results for supporting the new algorithm.  
This paper extends the single source H divergence theory for domain adaptation to the case of multiple domains. Thus, drawing on the known connection between H divergence and learning the domain classifier for adversarial adaptation, the authors propose a multi domain adversarial learning algorithm. The approach builds upon the gradient reversal version of adversarial adaptation proposed by Ganin et al 2016.   Overall, multi domain learning and limiting the worst case performance on any single domain is an interesting problem which has been relatively underexplored. Though this work does not have the highest performance on all datasets across competing methods, as noted by reviewers, it proposes a useful theoretical result which future research may build on. I would encourage the reviewers to compare against and discuss the missing prior work cited by Rev 3. 
The reviewers raised a number of concerns including the appropriateness of the chosen application and the terms in which social dilemmas have been discussed, the lack of explanations and discussions, missing references, and the extent of the evaluation studies. The authors’ rebuttal addressed some of the reviewers’ concerns but not fully. Overall, I believe that the work is interesting and may be useful to the community (though to a small extent., in my opinion). However, the paper would benefit from additional explanations, experiments and discussions pointed in quite some detail by the reviewers. AS is, the paper is below the acceptance threshold for presentation at ICLR.
The paper proposes to combine three methods of quantization and apply them to neural network compression.  The methods are known in the literature.  There is a lack of theoretical contribution, and experimental results show variable speedups that may not be competitive with the current state of the art in neural network compression.  The majority of reviewers recommend that this paper be rejected.  The authors have not provided a response.
There reviewers unanimously recommend rejecting this paper and, although I believe the submission is close to something that should be accepted, I concur with their recommendation.  This paper should be improved and published elsewhere, but the improvements needed are too extensive to justify accepting it in this conference. I believe the authors are studying a very promising algorithm and it is irrelevant that the algorithm is a relatively obvious one. Ideally, the contribution would be a clear experimental investigation of the utility of this algorithm in realistic conditions. Unfortunately, the existing experiments are not quite there.  I agree with reviewer 2 that the method is not particularly novel. However, I disagree that this is a problem, so it was not a factor in my decision. Novelty can be overrated and it would be fine if the experiments were sufficiently insightful and comprehensive.  I believe experiments that train for a single epoch on the reduced dataset are absolutely essential in order to understand the potential usefulness of the algorithm. Although it would of course be better, I do not think it is necessary to find datasets traditionally trained in a single pass. You can do single epoch training on other datasets even though it will likely degrade the final validation error reached. This is the type of small scale experiment the paper should include, additional apples to apples baselines just need to be added. Also, there are many large language modeling datasets where it is reasonable to make only a single pass over the training set. The goal should be to simulate, as closely as is possible, the sort of conditions that would actually justify using the algorithm in practice.  Another issue with the experimental protocol is that, when claiming a potential speedup, one must tune the baseline to get a particular result in the fewest steps. Most baselines get tuned to produce the best final validation error given a fixed number of steps. But when studying training speed, we should fix a competitive goal error rate and then tune for speed. Careful attention to these experimental protocol issues would be important. 
Reviewers have expressed concerns about clarity/writing of the paper and technical novelty, which the authors haven t responded to. The paper is not suitable for publication at ICLR.
The idea of the paper   imposing a GAN type loss on the latent interpolations of an autoencoder   is interesting. However there are strong concerns from R2 and R3 about limited experimental evaluation of the proposed method which falls short of demonstrating its advantages over latent spaces learned by existing GANs. Another point of concern was the use of only one real dataset (CelebA). Authors made substantial revisions to the paper in addressing many of the reviewers  points but these core concerns still persist with the current draft and it s not ready for publication at ICLR. Authors are encouraged to address these concerns and resubmit to another venue. 
This paper is very close to the decision boundary and the reviewers were split about whether it should be accepted or not. The authors updated the paper with additional experiments as request by the reviewers. The area chair acknowledges that there is some novelty that leads to (moderate) empirical gains but does not see these as sufficient to push the paper over the very competitive acceptance threshold. 
The paper gives a novel algorithm for transfer learning with label distribution shift with provably guarantees. As the reviewers pointed out, the pros include: 1) a solid and motivated algorithm for a understudied problem 2) the algorithm is implemented empirically and gives good performance. The drawback includes incomplete/unclear comparison with previous work. The authors claimed that the code of the previous work cannot be completed within a reasonable amount of time. The AC decided that the paper could be accepted without such a comparison, but the authors are strongly urged to clarify this point or include the comparison for a smaller dataset in the final revision if possible. 
This paper proposes a latent variable approach to the neural module networks of Andreas et al, whereby the program determining the structure of a module network is a structured discrete latent variable. The authors explore inference mechanisms over such programs and evaluate them on SHAPES.  This paper may seem acceptable on the basis of its scores, but R1 (in particular) and R3 did a shambolic job of reviewing: their reviews are extremely short, and offer no substance to justify their scores. R2 has admirably engaged in discussion and upped their score to 6, but continue to find the paper fairly borderline, as do I. Weighing the reviews by the confidence I have in the reviewers based on their engagement, I would have to concur with R2 that this paper is very borderline. I like the core idea, but agree that the presentation of the inference techniques for V NMN is complex and its presentation could stand to be significantly improved. I appreciate that the authors have made some updates on the basis of R2 s feedback, but unfortunately due to the competitive nature of this year s ICLR and the number of acceptable paper, I cannot fully recommend acceptance at this time.  As a complete side note, it is surprising not to see the Kingma & Welling (2013) VAE paper cited here, given the topic.
All three reviewers argue for rejection on the basis that this paper does not make a sufficiently novel and substantial contribution to warrant publication. The AC follows their recommendation.
This paper suggests that noise regularized estimators of mutual information in deep neural networks should be adaptive, in the sense that the variance of the regularization noise should be proportional to the range of the hidden activity. Two adaptive estimators are proposed: (1) an entropy based adaptive binning (EBAB) estimator that chooses the bin boundaries such that each bin contains the same number of unique observed activation levels, and (2) an adaptive kernel density estimator (aKDE) that adds isotropic Gaussian noise, where the variance of the noise is proportional to the maximum activity value in a given layer. These estimators are then used to show that (1) ReLU networks can compress, but that compression may or may not occur depending on the specific weight initialization; (2) different nonsaturating noninearities exhibit different information plane behaviors over the course of training; and (3) L2 regularization in ReLU networks encourages compression. The paper also finds that only compression in the last (softmax) layer correlates with generalization performance. The reviewers liked the range of experiments and found the observations in the paper interesting, but had reservations about the lack of rigor in the paper (no theoretical analysis of the convergence of the proposed estimator), were worried that post hoc addition of noise distorts the function of the network, and felt that there wasn t much insight provided on the cause of compression in deep neural networks. The AC shares these concerns, and considers them to be more significant than the reviewers do, but doesn t wish to override the reviewers  recommendation that the paper be accepted.
The reviewers are reasonably positive about this submission although two of them feel the paper is below acceptance threshold. AR1 advocates large scale experiments on ILSVRC2012/Cifar10/Cifar100 and so on. AR3 would like to see more comparisons to similar works and feels that the idea is not that significant. AR2 finds evaluations flawed. On balance, the reviewers find numerous flaws in experimentation that need to be improved.   Additionally, AC is aware that approaches such as  Convolutional Kernel Networks  by J. Mairal et al. derive a pooling layer which, by its motivation and design, obeys the sampling theorem to attain anti aliasing. Essentially, for pooling, they obtain a convolution of feature maps with an appropriate Gaussian prior to sampling. Thus, on balance, the idea proposed in this ICLR submission may sound novel but it is not. Ideas such as  blurring before downsampling  or  low pass filter kernels  applied here are simply special cases of anti aliasing. The authors may also want to read about aliasing in  Invariance, Stability, and Complexity of Deep Convolutional Representations  to see how to prevent aliasing. On balance, the theory behind this problem is mostly solved even if standard networks overlook this mechanism. Note also that there exist a fundamental trade off between shift invariance plus anti aliasing (stability) and performance; this being a reason why max pooling is still preferred over anti aliasing (better performance versus stability). Though, this is nothing new for those who delve into more theoretical papers on CNNs: this is an invite for the authors to go thoroughly first through the relevant literature/numerous prior works on this topic.
The paper proposes a new method to improve exploration in sparse reward problems, by having two agents competing with each other to generate shaping reward that relies on how novel a newly visited state is.  The idea is nice and simple, and the results are promising.  The authors implemented more baselines suggested in initial reviews, which was also helpful.  On the other hand, the approach appears somewhat ad hoc.  It is not always clear why (and when) the method works, although some intuitions are given.  One reviewer gave a nice suggestion of obtaining further insights by running experiments in less complex environments.  Overall, this work is an interesting contribution.
This paper was on the borderline. I am sympathetic to the authors  point about computational resources. It is helpful to demonstrate performance gains that offer "jump start" performance benefits, as the authors argue. However, the empirical results even on this part are still somewhat mixed  for example, the proposed approach struggles on Private Eye (doing far worse than DQN) in Table 2. In addition, while it is beneficial to remove the need for training a density model, it would be good to show a place where a density model fails (perhaps because it is so hard to find a good one) compared to their proposed approach. 
The paper extends capsule networks with a pairwise learning objective and evaluates on small face verification datasets. The authors do a great job describing prior work, but lack clarity when articulating their contribution and proposed method. In addition, some important implementation details, such as hyperparameter selection, are missing causing further confusion as to the final approach. Overall, according to the experiments shown, the approach offers modest improvements over prior work.  The approach offers an interesting and promising direction. We encourage the authors to revise the manuscript to clarify their approach and contribution and to improve their evaluation by including the relevant metrics and implementation details. 
The paper considers the problem of imitating multi modal expert demonstrations using a variational auto encoder to embed demonstrated trajectories into a structured latent space. The problem is important, and the paper is well written. The model is shown to work well on toy examples. However, as pointed out by the reviewers, given that multi modal has been studied before, the approach should have been compared both in theory and in practice to existing methods and baselines (e.g., InfoGAIL). Furthermore, the technical contribution is somewhat limited as it using an existing model on a new application domain.
This paper proposes a new framework which combines pruning and model distillation techniques for model acceleration. The reviewers have a consensus on rejection due to limited novelty.
The paper considers a procedure for the generation of adversarial examples under a black box setting. The authors claim simplicity as one of the main selling points, with which reviewers agreed, while also noting that the results were impressive or "promising". There were concerns over novelty and some confusion over the contribution compared to Guo et al, which I believe has been clarified.  The highest confidence reviewer (AnonReviewer2), a researcher with significant expertise in adversarial examples, raised issues of inconsistent threat models (and therefore unfair comparisons regarding query efficiency), missing baselines. A misunderstanding about comparison against a concurrent submission to ICLR 2019 was resolved on the basis that the relevant results are mentioned but not originally presented in the concurrent submission.   While I disagree with AnonReviewer2 that results on attacking a particular image from previous work (when run against the Google Cloud Vision API) would be informative, the reviewer has remaining unaddressed concerns about the fairness of comparison (comparing against results reported in previous work rather than re run in the same setting), and rightly points out that as many variables should be controlled for as possible when making comparisons. Running all methods under the same experimental setting with the same *collection* of query images is therefore appropriate.   The authors have not responded to AnonReviewer2 s updated post rebuttal review, and with the remaining sticking point of fairness of comparison with respect to query efficiency I must recommend rejection at this point in time, while noting that all reviewers considered the method promising; I thus would expect to see the method successfully published in the near future once issues of the experimental protocol have been solidified.
The paper addresses the problem of large scale fine grained classification by estimating pairwise potentials in a CRF model. The reviewers believe that the paper has some weaknesses including (1) the motivation for approximate learning is not clear (2) the approximate objective is not well studied and (3) the experiments are not convincing. The authors did not submit a rebuttal. I encourage the authors to take the feedback into account to improve the paper. 
The reviewers reached a consensus that the paper is not ready for publication in ICLR. (see more details in the reviews below. )
This paper studies training of the generative adversarial networks (GANs), specifically the discriminator, as a continual learning problem, where the discriminator does not forget previously generated samples. This model can be potentially used for improving GANs training and for generating synthetic datasets for evaluating continual learning methods. All the reviewers and AC agree that showing how continual learning techniques applied to the discriminator can alleviate mode collapse in GANs training is an important direction to study.  There is reviewer disagreement on this paper. AC can confirm that all three reviewers have read the author responses and have contributed to the final discussion.  While acknowledging that continual learning setting is potentially useful, the reviewers have raised several important concerns: (1) low technical novelty in light of EWC++ and online EWC methods (R1 and R3)   methodological and empirical comparison to these baselines is required to assess the difference and benefits of the proposed approach; the authors response to these concerns (and also R2’s comments in the discussion) were insufficient to assess the scope of the contribution. (2) More diverse/convincing empirical findings would strengthen the evaluation (e.g. assessing whether or not generator could help to overcome forgetting; showing that memory replay strategy by storing sufficient fake examples from previously generated samples cannot prevent mode collapse in GANs training – see the R3’s comment; showing the benefits of the generated samples for evaluating continual learning methods). (3) R1 left unconvinced that GAN training can be improved via continual learning training, as the relation between the proposed view and the minimax optimization difficulties in GANs is not addressed – see R1’s comment about this. The authors briefly discussed in their response to the review that the proposed approach is orthogonal to these works. However, a better (possibly theoretical) analysis of GANs training and continual learning would indeed help to evaluate the scope of the contribution of this work.  Regarding the available datasets that exhibit a coherent time evolution   see the Continuous Manifold Based Adaptation for Evolving Visual Domains by Hoffman et al, CVPR 2014.   Among (1) (3):  (2) and (3) did not have a substantial impact on the decision, but would be helpful to address in a subsequent revision. However, (1) makes it very difficult to assess the benefits of the proposed approach, and was viewed by AC as a critical issue.   AC suggests that in this current state the paper can be considered for a workshop and recommend to prepare a major revision before resubmitting it for the second round of reviews.    
This paper discusses the promising idea of using RL for optimizing simulators’ parameters.   The theme of this paper was very well received by the reviewers. Initial concerns about insufficient experimentation were justified, however the amendments done during the rebuttal period ameliorated this issue. The authors argue that due to considered domain and status of existing literature, extensive comparisons are difficult. The AC sympathizes with this argument, however it is still advised that the experiments are conducted in a more conclusive way, for example by disentangling the effects of the different choices made by the proposed model. For example, how would different sampling strategies for optimization perform? Are there more natural black box optimization methods to use?  The reviewers believe that the methodology followed has a lot of space for improvement. However, the paper presents some fresh and intriguing ideas, which make it overall a relevant work for presentation at ICLR.
This work develops a method for learning camouflage patterns that could be painted onto a 3d object in order to reliably fool an image based object detector.  Experiments are conducted in a simulated environment.  All reviewers agree that the problem and approach are interesting.  Reviewers 1 and 3 are highly positive, while Reviewer 2 believes that real world experiments are necessary to substantiate the claims of the paper.  While such experiments would certainly enhance the impact of the work, I agree with Reviewers 1 and 3 that the current approach is sufficiently interesting and well developed on its own.
The paper presents a lot of empirical evidence that fine tuning pruned networks is inferior to training them from scratch. These results seem unsurprising in retrospect, but hindsight is 20 20.  The reviewers raised a wide range of issues, some of which were addressed and some which were not. I recommend to the authors that they make sure that any claims they draw from their experiments are sufficiently prescribed. E.g., the lottery ticket experiments done by Anonymous in response to this paper show that the random initialization does poorer than restarting with the initial weights (other than in resnet, though this seems possibly due to the learning rate). There is something different in their setting, and so your claims should be properly circumscribed. I don t think the "standard" versus "nonstandard" terminology is appropriate until the actual boundary between these two behaviors is identified. I would recommend the authors make guarded claims here.
This paper examines a concept (also coined by the paper) of "search discrepancies" where the search algorithm behaves differently with large beam sizes. It then proposes heuristics to help prevent the model from performing worse when the size of the beam is increased.  I think there are some interesting insights in this paper with respect to how search works in modern neural models, but most reviewers (and me) were concerned by the heuristic approach taken to fix these errors. I still think that within a search paper, a clear separation between modeling errors and search errors is useful, and adding heuristics on top has a potential to making things more complicated down the road when, for example, we change our model or we change our training algorithm.  It would be nice if the nice insights in the paper could be turned into a more theoretically clean framework that could be re submitted to a future conference.
This was an extremely difficult case. There are many positive aspects of Graph2Seq, as detailed by all of the reviewers, however two of the reviewers have issue with the current theory, specifically the definition of k local gather and its relation to existing models. The authors  and reviewers have had a detailed and discussion on the issue, however we do not seem to have come to a resolution. I will not wade into the specifics of the argument, however, ultimately, the onus is on the authors to convince the reviewers of the merits/correctness, and in this case two reviewers had the same issue, and their concerns have not been resolved. The best advice I can give is to consider the discussion so far and why this misunderstanding occurred, so that it might lead the best version of this paper possible.
This paper proposes a new method for speeding up convolutional neural networks. It uses the idea of early terminating the computation of convolutional layers. It saves FLOPs, but the reviewers raised a critical concern that it doesn t save wall clock time. The time overhead is about 4 or 5 times of the original model. There is not any reduced execution time but much longer. The authors agreed that "the overhead on the inference time is certainly an issue of our method". The work is not mature and practical. recommend for rejection. 
While the paper has good quality and clarity and the proposed idea seems interesting, all three reviewers agree that the paper needs more challenging experiments to justify the proposed idea. The authors are not able to include additional experiments (such as these based on different transformations) into their revision to better convince the reviewers. In addition, the AC feels that the technical novelty of the paper is rather minor (some incremental change to VAE). In particular, related to some concerns of Reviewer 3, the AC feels the proposed idea is not too much different than introducing certain kind of side information for supervision; the main novelty seems to be distorting the data itself somehow to provide these side information (which does not seems to be that novel). 
This paper presents a new approach for posing control as inference that leverages Sequential Monte Carlo and Bayesian smoothing. There is significant interest from the reviewers into this method, and also an active discussion about this paper, particularly with respect to the optimism bias issue. The paper is borderline and the authors are encouraged to address the desired clarifications and changes from the reviewers.  
This paper describes a graph convolutional network (GCN) approach to capture relational information in natural language as well as knowledge sources for goal oriented dialogue systems. Relational information is captured by dependency parses, and when there is code switching in the input language, word co occurrence information is used instead. Experiments on the modified DSTC2 dataset show significant improvements over baselines. The original version of the paper lacked comparison to some SOTA baselines as also raised by the reviewers, these are included in the revised version. Although the results show improvements over other approaches, it is arguable BLEU and ROUGE scores are not good enough for this task. Inclusion of human evaluation in the results would be very useful. 
The reviewers and AC note the potential weaknesses of the paper in various aspects, and decided that the authors need more works to publish. 
This work proposes to use the MAML meta learning approach in order to tackle the typical problem of insufficient demonstrations in IRL.  All reviewers found this work to contain a novel and well motivated idea and the manuscript to be well written. The combination of MAML and MaxEnt IRL is straightforward, as R2 points out, however the AC does not consider this to be a flaw given that the main novelty here is the high level idea rather than the technical details.  However, all reviewers agree that for this paper to meet the ICLR standards, there has to be an increase in rigorousness through (a) a more close examination of assumptions, sensitivity of parameters and connections to imitation learning (b) expanding the experimental section.
This paper proposed a LBPNet for character recognition, which introduces the LBP feature extraction into deep learning. Reviewers are confused on implementation and not convinced on experiments. The only score 6 reviewer is also concerned "Empirically weak, practical advantage wrt to literature unclear". Only evaluating on MNIST/SVHN etc is not convincing to demo the effectiveness of the proposed method.
The paper proposes a framework at the intersection of programming and machine learning, where some variables in a program are replaced by PVars   variables whose values are learned using machine learning from data. The paper presents an API that is designed to support this scenario, as well as three case studies: binary search, quick sort, and caching   all implemented with PVars.  The reviewers and the AC agree that the paper presents and potentially valuable new idea, and shows concrete applications in the presented case studies. They provide example code in the paper, and present a detailed analysis of the obtained results.  The reviewers and AC also not several potential weaknesses   the AC will focus on a subset for the present discussion. The paper is unusual in that it presents a programming API rather than e.g., a thorough empirical comparison, a novel approach, or new theoretical insights. Paper at the intersection of systems and machine learning can make valuable contributions to the ICLR community, but need to provide a clear contributions which are supported in the paper by empirical or theoretical results. The research contributions of the present paper are vague, even after the revision phase. The main contribution claimed is the introduction of the API, and that such an API / system is feasible. This is an extremely weak claim. A stronger claim would be if e.g., the present approach would advance the state of the art beyond an existing such framework, e.g., probabilistic programming, either conceptually or empirically. I want to particularly highlight probabilistic programming here, as it is mentioned by the authors   this is a well developed research area, with existing approaches and widely used tools. The authors dismiss this approach in their related work section, saying that probabilistic programming is "specialized on working with distributions". Many would see the latter as a benefit, so the authors should clearly motivate how their approach improves over these existing methods, and how it would enable novel uses or otherwise provide benefits. At the current stage, the paper is not ready for publication.
The paper proposes a new RL algorithm (MIRL) in the control as inference framework that learns a state independent action prior.  A connection is provided to mutual information regularization.  Compared to entropic regularization, this approach is expected to work better when actions have significantly different importance.    The algorithm is shown to beat baselines in 11 out of 19 Atari games.  The paper is well written.  The derivation is novel, and the resulting algorithm is interesting and has good empirical results.  A few concerns were raised in initial reviews, including certain questions about experiments and potential negative impacts of the use of nonuniform action priors in MIRL.  The author responses and the new version were quite helpful, and all reviewers agree the paper is an interesting contribution.  In a revised version, the authors are encouraged to   (1) include a discussion of when MIRL might fail, and   (2) improve the related work section to compare the proposed method to other entropy regularized RL (sometimes under a different name in the literature), for example the following recent works and the references therein:     https://arxiv.org/abs/1705.07798     http://proceedings.mlr.press/v70/asadi17a.html     http://papers.nips.cc/paper/6870 bridging the gap between value and policy based reinforcement learning     http://proceedings.mlr.press/v80/dai18c.html