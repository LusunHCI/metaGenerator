This paper presents a theoretical characterization of the impact of noise in causal and non causal features on model generalization, through the lens of counterfactual data augmentations with toy data and models, and demonstrates that the predictions of this characterization bear out in several experiments on language counterfactually augmented language data with substantial models.  Pros:   Spurious features and their relationship out of domain generalization are a practically issue in modern applied ML, and this work helps to coalesce our understanding of this area.   Fairly extensive experimental work.  Cons:   Reviewers didn t find the connection between the theoretical analysis, which focused on a simplified setting, and the experimental work, to be especially clear. In particular, reviewers worried that the predictions that were tested experimentally were fairly intuitive ones that could reasonably be derived from a number of starting assumptions, so it s not clear that they offer strong support for the specific account given here.   Reviewers found the presentation, especially of the empirical work, confusing.  Overall, this paper makes a legitimate and sound contribution to an important research area. That contribution is small, and somewhat easy to misinterpret, but after some discussion, reviewers agreed that the paper should still be a worthwhile net positive for the field.  
The paper studies learning from complementary labels – the setting when example comes with the label information about one of the classes that the example does not belong to. The paper core contribution is an unbiased risk estimator for arbitrary losses and models under this learning scenario, which is an improvement over the previous work, as rightly acknowledged by R1 and R2.  The reviewers and AC note the following potential weaknesses: (1) R3 raised an important concern that the core technical contribution is a special case of previously published more general framework which is not cited in the paper. The authors agree with R3 on this matter; (2) the proposed unbiased estimator is not practical, e.g. it leads to overfitting when the cross entropy loss is used, it is unbounded from below as pointed out by R1; (3) the two proposed modifications of the unbiased estimator are biased estimators, which defeats the motivation of the work and limits its main technical contributions; (4) R2 rightly pointed out that the assumption that complementary label is selected uniformly at random is unrealistic – see R2’s suggestions on how to address this issue.  While all the reviewers acknowledged that the proposed biased estimators show advantageous performance on practice, the AC decides that in its current state the paper does not present significant contributions to the prior work, given (1) (3), and needs major revision before submitting for another round of reviews.   
While all reviewers agree the problem of TEEs for model training is well motivated, the reviewers remain divided on whether the concept of randomly selecting computations to verify has sufficient novelty, and whether the proposed gradient clipping method is well motivated. 
The initial reviews were mixed for this paper. On one hand, some of the reviewers highlighted that the proposed datasets could be useful to researchers. On the other, reviewers found a few important flaws with the current manuscript including missing baselines, issues with the proposed tasks, and possibly inaccurate/imprecise statements.  Our discussion after the author s response focussed on whether the positives aspects of the current paper outweighed some of the perceived weaknesses of the paper. In particular, while some of the initial criticisms from the reviewers were successfully addressed by the authors (including possible imprecisions and to a certain extent motivation), all the reviewers remained convinced that standard continual learning baselines could be adapted to this setting. They also conjectured that these missing baselines might not allow readers to appreciate the strength of the proposed datasets.   In their response, the authors argued that adapting models would require research. The reviewers are under the impression that it would be useful to test baselines more or less "as is" even if the authors do not think these baselines will be competitive. For example, in the discussion, a reviewer suggested that "an experience replay baseline could [...] have been implemented" where the replay buffer includes the hidden states of an LSTM. It might also be useful to study baselines that do not strictly obey the proposed setting, again to get a better understanding of the proposed tasks (including how difficult it is).  Overall, having some of these baselines would be one way to better connect the proposed work to the current continual learning literature. 
This paper presents a new pipeline for nn compression that extends that of Han et. al, but show that it reduces parameters further, maintains higher accuracy and can be applied to methods behind classification (semantic segmentation). While the authors found the paper clearly written, excepting for some typos, and potentially useful, there were questions about originality, and significance.    Reviewers were not completely convinced the method was different enough from deep compression: "The overall pipeline including the last two stage looks quite similar to Han[1].", or that enough focus was paid to the differences inherent with classification focused work: "The paper in the title and abstract refers to segmentation as the main area of focus. However, there does not seem to be much related to it except an experiment on the CityScapes dataset."    In terms of impact, the additional benefits from pruning seem to require a significant amount of computation, and the reviewers were not convinced these were worth a small gain in compression. Furthermore, authors felt that this approach was not being applied to the most state of the art approaches to demonstrate their use.
This paper considers convex optimization problems whose solutions involve the solution of linear systems defined in terms of the Hessian. It presents algorithms that reduce the runtime of standard iterative approaches to solving these problems by iteratively sketching the Hessian; the novelty lies in the fact that the authors use the idea of learned sketches which have been used prior for problems in data mining. In particular, the authors use the approach to learning sketches of Liu et al., 2020 to learn the entries in sparse sketching matrices for the Hessian, and propose using the Iterative Hessian Sketch algorithms of Pilanci and Wainright, 2016 to iteratively solve the concerned optimization problem. The advantages of learned sketches are that they may allow using smaller sketch sizes while making progress on the problem, as they are learned to work well on the distribution of Hessians from which the problem instance is drawn.  The consensus of the reviews is that the idea of using learned sketches for convex optimization seems to be novel, and this paper is an interesting attempt, but falls short of the level of contribution required for publication in ICLR. The main concern is that the theory provided for the use of the learned sketches is incremental: the analysis does not reflect the fact that the sketches are learned; instead, the algorithm builds in a safeguard by using both a random sketch and a learned sketch, and the analysis uses the properties of the random sketch to proceed. The empirical results are suggestive, but the convergence rates of the learned and random sketches do not vary much, so the benefits seem marginal for most of the problems considered (with the exception of a standard least squares problem, for which we know learned sketching performs well).   The paper is recommended to be rejected, as the theory is weak, and the empirical results are borderline. 
This paper received mixed reviews.  One reviewer is positive, while the remaining three reviewers are either negative or feel that the paper is below the threshold for acceptance.  The ideas presented in the paper are interesting and novel   this was acknowledged by three of the reviewers, even those who did not recommend acceptance.  The AC also recognizes the technical novelty presented.  However, as all the reviewers pointed out to varying degrees, the experimentation is problematic and the AC in agreement with this.  In particular, the heavy focus on improvement on top of SLIC makes it the applicability of the proposed approach highly limited and also not so convincing.  Recommendation for the paper is to reject and resubmit with improved experimentation.
While the reviewers were somewhat split on this paper, they all found some strengths, and pointed out some weaknesses. Among these the main seems to be the somewhat incremental nature of the work, in particular with respect to PCL. As the authors point out, the differences w.r.t. PCL are meaningful and include the main thrust of the paper (removal of false negatives), and the results do indicate usefulness of the proposed approach. Given the wide interest in self supervision I think the paper is above bar for acceptance.
The paper is acknowledged by all the reviewers as making a novel contribution   the proposal to reweight state action pairs depending on the variation in their Q value estimates during learning. However, despite its extensive reporting of numerical experiments, its arguments in favor of the proposed approach are found to be wanting on both empirical and theoretical fronts. Reviewer 3 points out (correctly, in my opinion) that 5 (or even 10 in the updated version) independent trials are not sufficient to establish the validity of the approach up to statistical significance, and that even a well reasoned heuristic explanation of why reweighting is expected to work in terms of reducing Q value estimation error is missing. I agree with this point, which also struck me when reading the submission myself, that at the very least, the submission ought to contain a basic (and not necessarily rigorous) argument as to why the variance reduction ostensibly achieved due to reweighting should lead the estimation algorithm to the right Q function in a general function approximation setting. For instance, even in the simplest multi armed bandit setting, it is of interest to ask why this procedure should perform consistently without introducing unwanted bias in an unforeseen sense, and a clear explanation offered for this would be interesting. Another important concern that most reviewers are left with is about the lack of sufficient insight into the action of the UCB mechanism against the backdrop of the reweighting procedure (reviewers 1, 2, 3). I hope that the author(s) assimilate the feedback to strengthen the paper s main pitch further and make a strong case in the near future. 
This paper proposes a new method for code generation based on structured language models.  After viewing the paper, reviews, and author response my assessment is that I basically agree with Reviewer 4. (Now, after revision) This work seems to be (1) a bit incremental over other works such as Brockschmidt et al. (2019), and (2) a bit of a niche topic for ICLR. At the same time it has (3) good engineering effort resulting in good scores, and (4) relatively detailed conceptual comparison with other work in the area. Also, (5) the title of "Structural Language Models for Code Generation" is clearly over claiming the contribution of the work   as cited in the paper there are many language models, unconditional or conditional, that have been used in code generation in the past. In order to be accurate, the title would need to be modified to something that more accurately describes the (somewhat limited) contribution of the work.  In general, I found this paper borderline. ICLR, as you know is quite competitive so while this is a reasonably good contribution, I m not sure whether it checks the box of either high quality or high general interest to warrant acceptance. Because of this, I m not recommending it for acceptance at this time, but definitely encourage the authors to continue to polish for submission to a different venue (perhaps a domain conference that would be more focused on the underlying task of code generation?)
AR1 seeks the paper to be more standalone and easier to read. As this comment comes from the reviewer who is very experienced in tensor models, it is highly recommended that the authors make further efforts to make the paper easier to follow. AR2 is concerned about  the manually crafted role schemes and alignment discrepancy of results between these schemes and RNNs. To this end, the authors hypothesized further reasons as to why this discrepancy occurs. AC encourages authors to make further efforts to clarify this point without overstating the ability of tensors to model RNNs (it would be interesting to see where these schemes and RNN differ). Lastly, AR3 seeks more clarifications on contributions.  While the paper is not ground breaking, it offers some starting point on relating tensors and RNNs. Thus, AC recommends an accept. Kindly note that tensor outer products have been used heavily in computer vision, i.e.:   Higher Order Occurrence Pooling for Bags of Words: Visual Concept Detection by Koniusz et al. (e.g. section 3 considers bi modal outer tensor product for combining multiple sources: one source can be considered a filter, another as role (similar to Smolensky at al. 1990), e.g. a spatial grid number refining local role of a visual word. This further is extended to multi modal cases (multiple filter or role modes etc.) )   Multilinear image analysis for facial recognition (e.g. so called tensor faces) by Vasilescu et al.   Multilinear independent components analysis by Vasilescu et al.   Tensor decompositions for learning latent variable models by Anandkumar et al.  Kindly  make connections to these works in your final draft (and to more prior works).  
This paper presents a novel RNN algorithm based on unfolding a reweighted L1 L1 minimization problem. Authors derive the generalization error bound which is tighter than existing methods.  All reviewers appreciate the theoretical contributions of the paper, particularly the derivation of generalization error bounds. However, at a higher level, the overall idea is incremental because RNN by unfolding L1 L1 minimization problem (Le+,2019) and reweighted L1 minimization (Candes+,2008) are both known techniques. The proposed method is essentially a simple combination of them and therefore the result seems somewhat obvious. Also, I agree with reviewers that some experiments are not deep enough to support the theory. For example, for over parameterization (large model parameters) issue, one can compare the models with the same number of parameters and observe how they generalize.  Overall, this is the very borderline paper that provides a good theoretical contribution with limited conceptual novelty and empirical evidences. As a conclusion, I decided to recommend rejection but could be accepted if there is a room. 
This paper presents a system which exploits semantic information of partial programs during program synthesis, and ensembling of synthesisers. The idea is general, and admirably simple. The explanation is clear, and the results are impressive. The reviewers, some after significant discussion, agree that this paper makes an import contribution and is one of the stronger papers in the conference. While some possible improvements to the method and experiment were discussed with the reviewers, it seems these are more suitable for future research, and that the paper is clearly publishable in its current form.
The authors propose the 2 Wasserstein barycenter problem between measures. The authors propose a novel formulation that leverages a condition (congruence) that the optimal transport (Monge) maps, here parameterized as potentials, must obey at optimality. The introduce various regularizers to encourage that property. The idea is demonstrated on convincing synthetic experiments and on a simple color transfer problem. Although experiments are a bit limited, I do believe, and follow here the opinion of all reviewers, that there is novelty in this approach, and that this paper is a worthy addition to the recent line of work trying to leverage ICNNs/Brenier s theorem to solve OT problems.
The paper proposes a model of agent collaboration to improve outcomes for any participating agent in a setting where every agent does not always benefit from collaborating with all other agents. The reviewers did find some of the theoretical results interesting, however, in its current (revised) form, they still argued during the discussion post rebuttal that: (i) the game theoretic formulation of this problem is not entirely new and has been studied in various forms before and (ii) the particular application of the results to federated learning comes after making various (questionable) assumptions. I would encourage the authors to take into account (i ii) for preparing a revised version of their paper and resubmit to another conference.
Three of the reviewers are significantly concerned about this submission, while R1 is positive. Meanwhile, R1 is also concerned about some details in the paper, including space and time complexity etc. The authors provided detailed feedback to these comments, but R1 does not provide support to this work during discussions. Thus a reject is recommended.
Thanks for the submission. This paper leverages the stability of differential privacy for the problems of anomaly and backdoor attack detection. The reviewers agree that this application of differential privacy is novel. The theory of the paper appears to be a bit weak (with very strong assumptions on the private learner), although it reflects the basic underlying idea of the detection technique. The paper also provides some empirical evaluation of the technique.
This paper proposes a meta RL algorithm that learns an objective function whose gradients can be used to efficiently train a learner on entirely new tasks from those seen during meta training. Building off policy gradient based meta RL methods is challenging, and had not been previously demonstrated. Further, the demonstrated generalization capabilities are a substantial improvement in capabilities over prior meta learning methods. There are a couple related works that are quite relevant (and somewhat similar in methodology) and overlooked   see [1,2]. Further, we strongly encourage the authors to run the method on multiple meta training environments and to report results with more seeds, as promised. The contributions are significant and should be seen by the ICLR community. Hence, I recommend an oral presentation.  [1] Yu et al. One Shot Imitation from Observing Humans via Domain Adaptive Meta Learning [2] Sung et al. Meta critic networks
The paper conveys interesting study but the reviewers expressed concerns regarding the difference of this work compared to existing approaches and pointed a room for more thorough empirical evaluation.
Well written paper that proposes a flow based model for categorical data, and applies it to graph generation with good results. Extending flow models to handle types of data that are not continuous is a useful contribution, and graph generation is an important application. Overall, the reviewers were positive about the paper, and only few negative points were raised, so I m happy to recommend acceptance.
The paper proposes a method to learn cross lingual representations by aligning monolingual models with the help of a parallel corpus using a three step process: transform, extract, and reorder. Experiments on XNLI show that the proposed method is able to perform zero shot cross lingual transfer, although its overall performance is still below state of the art jointly trained method XLM.  All three reviewers suggested that the proposed method needs to be evaluated more thoroughly (more datasets and languages). R2 and R4 raise some concerns around the complexity of the proposed method (possibly could be simplified further). R3 suggests a more thorough investigation on why the model saturates at 250,000 parallel sentences, among others.  The authors acknowledged reviewers  concerns in their response and will incorporate them in future work.  I recommend rejecting this paper for ICLR.
This paper proposes a method to improve alignments of a multilingual contextual embedding model (e.g., multilingual BERT) using parallel corpora as an anchor. The authors show the benefit of their approach in a zero shot XNLI experiment and present a word retrieval analysis to better understand multilingual BERT.  All reviewers agree that this is an interesting paper with valuable contributions. The authors and reviewers have been engaged in a thorough discussion during the rebuttal period and the revised paper has addressed most of the reviewers concerns.  I think this paper would be a good addition to ICLR so I recommend accepting this paper.
Dear Authors,  This paper eventually received mostly negative reviews (scores 5, 3, 5), with one mildly positive review (score 6). All reviews were particularly informative, offering detailed and expert feedback. I was hoping for author engagement, but unfortunately, no rebuttal was submitted.   In general, the reviewers and me found the paper well written, on a timely topic, but of a very limited theoretical novelty. Well articulated details of this can be found in the reviews and I would recommend the authors to consider them carefully in their revision. I have no option but to reject this work.   The main reason for rejection in this case is therefore limited theoretical novelty. However, this is a solid paper that is of publishable quality, albeit perhaps in a somehow lesser venue, at least in its current form.   Kind regards,  Area Chair
This work concerns Automatic Music Transcription (AMT)   transcribing notes given the audio of the music. The paper demonstrates that a single general purpose transformer model can perform AMT for many instruments across several different transcription datasets. The method represents the first unified AMT model that can transcribe music audio with an arbitrary number of instruments.  All reviewers rated this paper highly and are excited about seeing it at the conference. One reviewer noted that "This paper seems to be a great milestone in the AMT research. It is probably the first unified AMT model that can take music audio with an arbitrary number of instruments."  The reviewers had some suggestions and comments, which appear to be addressed by the authors.
Authors provide an empirical evaluation of batch size and learning rate selection and its effect on training and generalization performance. As the authors and reviewers note, this is an active area of research with many closely related results to the contributions of this paper already existing in the literature. In light of this work, reviewers felt that this paper did not clearly place itself in the appropriate context to make its contributions clear. Following the rebuttal, reviewers minds remained unchanged. 
The paper seeks to answer the question on the necessity of the self attention matrix in Transformers and whether it is possible to synthesize it by alternate means other than pairwise attention.   The reviewers appreciated the main general idea and the wide range of experiments conducted.  However, there are some concerns on clarity and evidence supporting main claims. While authors tried to address certain concerns through revision and response, the results suggests that self attention matrix is still needed for strong performance and cannot be fully replaced by synthesizers. While authors also acknowledge this in discussions, If we do not consider the combined models (R+V or D+V), the empirical results do not look very convincing on the competitiveness of Synthesizers. They are only competitive on MT/Dialogue while Failing quite considerably on GLUE and Summarization. Overall, I felt very positive about the direction the paper pursues, but the empirical results doesn t seem to fully support the claims.   Quoting some points from reviewer discussions:  > `Comment: Moving towards an analytic framing would necessitate having the bare minimum set of experiments/comparisons before running additional analyses, but the bare minimum is still needed for this paper.  > Comment: I think the paper needs a round of revision and experiments need additional, carefully chosen baselines to adequately present synthetic attention in the context of existing solutions.  > Comment: this is not a reason that some random explored idea should be viewed as a great contribution, given that there are already several theory grounded papers appeared in ICML (linear attention..), NIPS (Linformer, follow up work from linear attention..), and ICLR (Random feature attention, Performer..) this year. Compared to those theoretically well motivated attention modification papers, this work is not that solid.
This paper proposes a principled solution to the problem of joint source channel coding. The reviewers find the perspectives put forward in the paper refreshing and that the paper is well written. The background and motivation is explained really well.  However, reviewers found the paper limited in terms of modeling choices and evaluation methodology. One major flaw is that the experiments are limited to unrealistic datasets, and does not evaluate the method on a realistic benchmarks. It is also questioned whether the error correcting aspect is practically relevant.    
Thank you for your submission to ICLR.  The reviewers and I unanimously felt, even after some of the clarifications provided, that while there was some interesting element to this work, ultimately there were substantial issues with both the presentation and content of the paper.  Specifically, the reviewers largely felt that the precise problem being solved was somewhat poorly defined, and the benefit of the proposed preimage technique wasn t always clear.  And while the ACAS system was a nice application, it seems to be difficult to quantify the real benefit of the proposed method in this setting (especially given that other techniques can similarly be used to verify NNs for this size problem).  The answer that this paper provides seems to be something along the lines of "ease of visual interpretation" of the pre image conditions, but this needs to be quantified substantially more to be a compelling case.
This paper introduces a new graph convolutional neural network, called LGNN, and applied it to solve the community detection problem. The reviewers think LGNN yields a nice and useful extension of graph CNN, especially in using the line graph of edge adjacencies and a non backtracking operator.  The empirical evaluation shows that the new method provides a useful tool for real datasets. The reviewers raised some issues in writing and reference, for which the authors have provided clarification and modified the papers accordingly.   
All of the reviewers agree that this is a well written paper with the novel perspective of minimizing energy consumption in neural networks, as opposed to maximizing sparsity, which does not always correlate with energy cost. There are a number of promised clarifications and additional results that have emerged from the discussion that should be put into the final draft. Namely, describing the overhead of converting from sparse to dense representations, adding the Imagenet sparsity results, and adding the time taken to run the projection step.
This paper extends past work on kNN augmentation for language modeling to the task of machine translation: a classic parametric NMT model is augmented with kNN retrieval from an external datastore. Decoder internal token level representations are used to index and retrieve relevant contexts (source + target prefix) that weigh in during the final probability calculation for the next target word. Results are extremely positive across a range of MT setups including both in domain evaluation and domain transfer. Reviews are thorough, but quite divergent. There is general agreement that the proposed approach is reasonable, well motivated, and clearly described   and further, that experimental results are both solid and relatively extensive. However, the strongest criticism concerns the paper s relationship with past work.  In terms of ML novelty, everyone agrees (including the paper itself) that the proposed methodology is a relatively simple extension of past work on non conditional language modeling. However, two of the four reviewers strongly feel that, in light of the potentially prohibitive decoding costs, the positive experimental results are not sufficient to make this paper relevant to an ICLR audience given the lack of ML novelty. In contrast, another reviewer strongly takes an opposite stand point:  rather, that the results will be extremely impactful to the MT subcommunity at ICLR since they are unexpected (i.e. that a non parametric model might compete with highly tuned NMT systems) and very positive across a range of domains and settings (i.e. in domain, out of domain, multilingual)   further, that the approach has substantial novelty in the context of MT where parametric models are the norm and that it might inspire substantial future work  (e.g. on efficient decoding techniques and further non parametric techniques) given that it so drastically breaks the current MT mold. The final reviewer shares the concern of the former two about novelty, but is swayed by the experimental results and potential uses for the model (given kNN augmentation is possible without further training) and therefore votes for a marginal accept. After thorough, well reasoned, and well intentioned discussion between all four reviewers, the reviews land just barely in favor of acceptance, but with substantial divide. After considering the paper, reviews, rebuttal, and discussion I am swayed by the argument that (a) these experimental results are largely unexpected, (b) they are both extremely positive and offer a new trade off between test and train compute in MT, and (c) that the paper may therefore inspire substantial discussion and follow up work in the community. Thus I lean in favor of acceptance overall.
Two reviewers suggested to reject and the other reviewer also thought it below the threshold.
The paper introduces a setting called high fidelity imitation where the goal one shot generalization to new trajectories in a given environment. The authors contrast this with more standard one shot imitation approaches where one shot generalization is to a task rather than a precise trajectory. The authors propose a technique that works off of only state information, which is coupled with an RL algorithm that learns from a replay buffer that is populated by the imitator. The authors emphasize that their approach can leverage very large deep learning models, and demonstrate strong empirical performance in a (simulated) robotics setting.   A key weakness of the paper is its clarity. All reviewers were unclear about the precise setting as well as relation to prior work in one shot imitation learning. As a result, there were substantial challenges in assessing the technical contribution of the paper. There were many requests for clarification, including for the motivation, difference between the present setting and those addressed in previous work, algorithmic details, and experiment details.  I believe that a further concern was the lack of a wide range of baselines. The authors construct several baselines that are relevant in the given setting, but did not consider "naive baseline" approaches proposed by the reviewers. For example, behavior cloning is mentioned as a potential baseline several times. The authors argue that this is not applicable as it would require expert actions. Instead of considering it a baseline, BC could be used as an "oracle"   performance that could be achieved if demonstration actions were known. As long as the access to additional information is clearly marked, such a comparison with a privileged oracle can be properly placed by the reader. Without including such commonly known reference approaches, it is very challenging to assess the proposed method s performance in the context of the difficulty of the task. Generally, whenever a paper introduces both a new task and a new approach, a lot of care needs to be taken to build up insights into whether the task appropriately reflects the domain / challenge the paper claims to address, how challenging the task is in comparison to those addressed in prior work, and to place the performance of the novel proposed method in the context of prior work. In the present paper, on top of the task and approach being novel, the pure RL baseline D4PG is not yet widely known in the community and it s performance relative to common approaches is not well understood. Including commonly known RL approaches would help put all these results in context.  The authors took great care to respond to the reviewer comments, providing thorough discussion of related work and clarifications of the task and approach, and these were very helpful to the AC to understand the paper. The AC believes that the paper has excellent potential. At the same time, a much more thorough empirical evaluation is needed to demonstrate the value of the proposed approach in this novel setting, as well as to provide additional conceptual insights into why and in what kinds of settings the algorithm performance well, or where its limitations are.  
The paper studies the Lipschitz properties of neural networks — in particular, two layer neural networks that interpolate generic datasets. It conjectures a “size robustness tradeoff”: in this setting, the number of neurons required to interpolate with an O(1) Lipschitz function is proportional to the number of data points n, while the number of neurons required for interpolation alone is proportional to n/d, where d is the data dimension. More precisely, the conjecture is that the best achievable Lipschitz constant is proportional to $\sqrt{n/k}$, where $k$ is the number of neurons. The paper proves weaker versions of both sides of this conjecture: it proves that a spectral upper bound on the Lipschitz constant is lower bounded by $\sqrt{n/k}$ and that there exist networks achieving this Lipschitz constant when $k ~ n/d$ and $k ~n$. The paper also provides experiments supporting its claims, with the caveat that the actual Lipschitz constant is a worst case quantity that cannot be directly observed.   Pros and cons:  [+] The paper identifies a novel (conjectured) phenomenon involving the dependence of the Lipschitz constant of an interpolating network on the degree of overparameterization. In words, Lipschitz interpolation requires significantly more neurons than mere interpolation. This observation seems likely to stimulate future work.   [+] The paper provides relatively simple and rigorous proofs of simplified versions of its conjectures (both upper and lower bounds on the achievable Lipschitz constant).   [+] The exposition is technically clean, and the paper is clear on the limitations of its analyses.   [ ] The setting of the paper’s analysis seems somewhat mismatched with the practice of deep learning. The data are assumed to be generic, where neural networks excel in fitting structured data. Several reviewers noted this mismatch and raised concerns about whether this conjectured/proved tradeoff on generic data carries over to structured datasets.   [ ] A technical limitation is the shallowness of the network: controlling the Lipschitz properties of deep networks is much more challenging at a technical level, because one needs to argue that for the worst input, features propagate in a “generic” fashion. It is technically challenging to avoid exponential dependence on depth.   [ ] The paper obtains only partial progress towards proving its conjectures — for example, it shows that it is possible to interpolate with a Lipschitz constant of $n \log n / k$, where the conjectured bound is $\sqrt{n/k}$.   [ ] Comparing to kernel methods would help to better contextualize the results, since in a similar setting, kernel methods could also potentially be analyzed via localization arguments.   Overall, the paper conjectures a novel phenomenon around size/robustness tradeoffs in interpolating neural networks. While the paper s conjectures have the potential to stimulate further empirical and theoretical work, the reviewers (in particular R1 and R2) note a number of significant limitations to the paper’s analysis. In light of these issues, the paper falls below the par for acceptance. 
This paper furthers recent work by Tian et al. 2021 to explain how representation learning with non contrastive self supervision works. The paper accomplishes this by analyzing a family of algorithms in which DirectPred from Tian et al. (2021) is a special case. Their theoretical analysis is performed with linear networks. Overall, the reviewers questioned the added value relative to Tian et al. 2021, noting that  "The analysis of DirectSet and DirectCopy succeeds at proving that it can successfully learn a projection matrix onto an invariant feature space subspace, but essentially boils down to a similar approach as DirectPred (albeit more efficient)"  The authors in their reply state "how the representation is related to the data distribution and augmentation process," however the relative contribution and why its important isn t transparent.
This paper proposes 1) using neural guided Monte Carlo Tree Search to search for expressions that match a dataset and 2) Augments the loss to match the asymptotics of the true function when these are given.  The use of MCTS sounds more sensible than standard evolutionary search.  The augmented loss could make sense but seems extremely niche, requiring specific side information about the problem being solved.  Overall, the task is so niche that I don t think it ll be of wide interest.  It s not clear that it s solving a real problem.
The article introduces a Bayesian approach for online learning in non stationary environments. The approach, which bears similarities with weighted likelihood estimation methods, associate a binary weight to each past observation, indicating if this observation should be including or not to compute the posterior. The weights are estimated via maximum a posteriori.   The paper is well written, the approach is novel and its usefulness demonstrated on a number of different experiments. The original submission missed some relevant references that have been added in the revision. The approach has some limitations, highlighted by the reviewers: * it requires to solve a binary optimisation problem whose complexity scales exponentially with the size of the dataset; although the greedy procedure proposed by the authors seems to work fine on the examples shown, the approach may not be applicable to larger datasets * it requires to store all the data * it requires the traceability of the marginal likelihood  Despite these limitations, there was a general agreement that this paper offers a novel and useful contribution, and I recommend acceptance.   As noted by reviewer o4TK, I also think that the title is not very accurate. Bayesian methods naturally allow recursive updates of one s beliefs, and therefore have "memory". Maybe change the title for "Bayes with augmented selective/adaptive memory"?
This paper tackles the problem of safe exploration in RL. The proposed approach uses an imaginative module to construct a connectivity graph between all states using forward predictions. The idea then consists in using this graph to plan a trajectory which avoids states labelled as "unsafe".  Several concerns were raised and the authors did not provide any rebuttal. A major point is that the assumption that the approach has access to what are unsafe states, which is either unreasonable in practice or makes the problem much simpler. Another major point is the uniform data collection about every state action pairs. This can be really unsafe and defeats the purpose of safe exploration following this phase. These questions may be due to a miscomprehension, indicating that the paper should be clarified, as demanded by reviewers. Finally, the experiments would benefit from additional details in order to be correctly understood.  All reviewers agree that this paper should be rejected. Hence, I recommend reject.
Reviews are somewhat mixed, but all are below the acceptance threshold. Reviewers praise the overall application and the presentation (though there is some variance in response to this aspect), but have concerns about lack of certain comparisons and technical novelty.
This paper studies non smooth and non convex optimization and provides a global analysis for orthogonal dictionary learning. The referees indicate that the analysis is highly nontrivial compared with existing work.   The experiments fall a bit short and the relation to the loss landscape of neural networks could be described more clearly.   The reviewers pointed out that the experiments section was too short. The revision included a few more experiments. The paper has a theoretical focus, and scores high ratings there.   The confidence levels of the reviewers is relatively moderate, with only one confident reviewer. However, all five reviewers regard this paper positively, in particular the confident reviewer. 
This paper proposed to apply emsembles of high precision deep networks and low precision ones to improve the robustness against adversarial attacks while not increase the cost in time and memory heavily.  Experiments on different tasks under various types of adversarial attacks show the proposed method improves the robustness of the models without sacrificing the accuracy on normal input.  The idea is simple and effective.  Some reviewers have had concerns on the novelty of the idea and the comparisons with related work but I think the authors give convincing answers to these questions.
In this paper, the problem of identifying a low dimensional latent space for high dimensional Bayesian optimization (BO) is considered. In particular, the authors focus on the problem of collision, where different points in the original space become identical in the latent space, and propose a regularization method to avoid this problem. Latent space identification for high dimensional Bayesian optimization is an interesting and the authors  approach sounds reasonable. However, many reviewers pointed out that the discussion and results in the paper do not provide sufficient evidence for the authors  claims. Therefore, we have to conclude that the paper cannot be accepted at this time.
There is definite consensus on this paper, with all reviewers expressing very favorable opinions. The author responses are very well articulated and address the main concerns expressed by the reviewers. The paper is very well written and the ablation study well executed. Some recent related work was missed in the original submission, but this was adequately addressed in rebuttal. The proposed approach is novel technique for feature representation learning. The clarifications to the manuscript and the new analyses are especially appreciated. 
This paper proposes a method to train autoregressive model that takes advantage of a well designed energy based learning objective model. With the importance sampling, the model can be trained efficiently without requiring an MCMC sampling. Experiments are conducted to verify the effectiveness of the proposed method.  The idea is interesting and well motivated, but the experiments need to be improved. Reviewer FnWE’s major concerns include limited novelty, lack of discussion with closely related works, and insufficient experiments, and recommend rejecting the paper by assigning a rating of 3. Rebuttal doesn’t address his/her concerns. Reviewer in11 is concerned with the computational cost and training instability due to the extra EBM module and has a few unclear technical details that need to be clarified. The author’s reply along with additional experiments during rebuttal partially addresses the concerns of Reviewer in11, who eventually increases the rating to 6. Reviewer AQxn’s major concern is also about the lack of sufficient comparison with other relevant energy based models.  Reviewer DZsJ pointed out that the more insightful analysis about the model is missing in the experiments. Even though the authors provide additional experiments for Reviewer DZsJ, they are not satisfied with the feedback because the additional results are not supportive of the claims made in the paper, and end up with a rating of 6. Reviewer SjXn’s concerns include the lack of comparison with relevant works and the unclear motivation of the design of the joint distribution. After the rebuttal, Reviewer SjXn’s concerns remain and assign a rating of 5 to the paper. The overall rating of the paper after rebuttal is marginally below the acceptance rate. Even though this paper proposes an interesting idea, the reviewers’ comments are not well addressed. As a result, AC cannot recommend accepting the paper.  The AC urges the authors to revise their paper according to the comments from the reviewers, and resubmit their work in a future venue.
The paper proposes a very simple but thoroughly evaluated and investigated idea for improving generalization in GCNs. Though the reviews are mixed, and in the post rebuttal discussion the two negative reviewers stuck to their ratings, the area chair feels that there are no strong grounds for rejection in the negative reviews. Accept.
The paper analyzes neural network with hidden layer of piecewise linear units, a single output, and a quadratic loss. The reviewers find the results incremental and not "surprising", and also complained about comparison with previous work. I think the topic is very pertinent, and definitely more relevant compared to studying multi layer linear networks. Hence, I recommend the paper be presented in the workshop track.
This manuscript presents a method built on top of CLIP which transforms language embeddings to understand new phrases while maintaining the original abilities of CLIP.  1. The technical novelty of this work is limited. That being said, if the task is of wide interest, a straightforward approach that performs well is not just good, it s even preferable to one that is complex. The authors bring up the fact that reviewers are asked to rate both technical and empirical novelty. This is true. Yet, reviewers were unconvinced both by the method and the setting.    The manuscript does not explain why this setting provides additional challenges or value compared to the many other continual learning or zero shot settings that exist in the literature. What the authors say "we are not aware of any previous work that learns a direct transformation of the representations themselves to accomplish both continual learning and retained zero shot use of those representations with the same model" seems to be undisputed by the reviewers. But is it critically important to future ML research that a single model does both? Or that a model learns a direct transformation to do so? Overall, this task seems very constrained and tailored to this one approach, while usually the more general a setting is, the more it is valued by the community because it will be more likely to stand the test of time and lead to new advances. Reviewers also could not point to a compelling immediate practical need for such a model, which would be another reason for considering a novel setting.    While in the responses the authors acknowledge that they do not consider their method to be the ultimate solution, that the method has significant limitations, and that this really should be considered a strong baseline, this is not how the work is presented. Relatively little is said in the manuscript about any of these topics.  2. In response to requests for experiments (such as exploring the space of transformations and exploring alternate models to CLIP) the authors put forward that space limitations preclude such experiments. I would encourage authors not to rely on this argument going forward as it does not serve their cause well. Between the unlimited appendix and the possibility of linking to an anonymized website space cannot be a constraint. Science is complex these days and it s not unusual to have to report extensive additional experiments outside the main body of the manuscript. I encourage authors to consider that these requests by multiple reviewers are likely going to be the first questions that the readers of their work will also want answers to. Exploring other transformations and models is critical to understanding the value and impact of the work.  Minor point: I did not see this in the reviews but Figure 1(a) has the labels for CLIP text and image flipped.  If the authors round out the experiments and demonstrate either that their idea is more general purpose, i.e., that it can be applied to other settings and problems, or that this setting is of great value on its own, this could be a strong contribution.
This paper presents a theoretical analysis of the approximation properties of linear recurrent encoder decoder architectures, obtaining universal approximation results and subsequently showing approximation rates of targets for RNN encoder decoders. It introduces a notion of "temporal products," which helps to characterize the types of temporal relationships that can be efficiently learned in this setting.  Overall, the reviewers and I all agree that this paper makes important theoretical contributions to the important problem of the approximation capabilities of encoder decoder architectures. The main weaknesses involve the rather simplified linear problem setup, but this limitation is easily forgiven in this first of its kind rigorous analysis. I recommend acceptance.
The reviews were a bit mixed, with a general consensus towards acceptance. The authors were one of the first to extend lookahead to minimax optimization, and demonstrated its potential through thorough experiments. The theoretical results were not as strong or at least not very well presented. Overall, the authors made interesting contributions and this work is of general interest to the ICLR audience. Please consider further polishing the draft according to the reviewers  comments. The AC would also like to draw the authors  attention to the following issues discovered in an independent assessment:  (a) As the reviewers mentioned, how lookahead minimax addresses rotational dynamics is not clearly presented. The current justification is a bit handwaving and speculative.   (b) Please consider rewriting Section 3. If there is some new results on the minimization problem, state the results in a theorem and include all assumptions clearly and precisely. This is also useful for other people to reference your result. As the authors themselves pointed out, this result falls quite short of explaining or motivation lookahead.   (c) Theorem 1, add e.g. in the citation before (Bertsekas, 1999). Theorem 2, in its current form, is quite weak in two aspects: (a) without checking its proof one can already see how to derive it in 1 line or 2. (b) if the base optimizer already converges, what is the point of having lookahead to converge as well? The potentially different convergence rate should be one s target here. It is certainly fine for the authors to not fully justify their proposed algorithm, as long as the authors (hopefully) are at least aware of the issues.  (d) Section 4 is a bit disappointing as one would have expected the authors to derive some qualitative results here (also raised by some reviewers).
This paper combines existing models to detect topics and generate responses, and the resulting model is shown to be slightly preferred by human evaluators over baselines. This is quite incremental and the results are not impressive enough to stand on their own merit.
The paper presents a nice analysis of the spectrum of a matrix that is obtained by applying non linear functions to a random matrix. The paper is mostly well written, the result is novel and interesting, and has clear implications for ML problems like spectral clustering.  So I would enthusiastically recommend the paper for acceptance at ICLR.  It would be important for authors to take into account reviewer comments. In particular, instantiating the theorems for simple ML centric examples would be very useful.    
The paper addresses the problem of out of distribution detection for helping the segmentation process.  The reviewers and AC note the critical limitation of novelty of this paper to meet the high standard of ICLR. AC also thinks the authors should avoid using explicit OOD datasets (e.g., ILVRC) due to the nature of this problem. Otherwise, this is a toy binary classification problem.  AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.
The paper proposed Trained ML oracles to find the decent direction and step size in optimization. The process they call grafting. Reviewers raised several concerns about the reliability of ML oracles in general settings which is valid. The rebuttal could not convince the reviewers to change their opinion.  Ideally for an empirical only paper with heavy reliability on ML for critical decisions, to meet the high bar of ICLR there must be several experiments (5 10 datasets or more) on diverse datasets and settings. Also, there should be discussions on when and how the method fails and related discussions. In that sense the paper does not meet the bar for publication.
The authors use Empowerment for morphology optimisation, a quite novel idea. After initial unclarities and various improvements on the submission, the reviewers unanimously voted for acceptance of the paper.  
The reviewers generally agreed that the ideas presented in the paper are interesting and novel. However, all reviewers also agreed that the paper is quite preliminary in its current form: the particular approach, while sensible, appears to be somewhat heuristic, and the evaluations are not as complete as necessary to fully evaluate the proposed approach.  Generally, my sense is that there is something quite interesting in this work, but the present paper is too preliminary for publication. I would encourage the authors to take the reviewer comments into account and improve the work into a more complete submission for a future venue.
This work proposes to improve trust region policy search (TRPO) by using normalizing flow policies. This idea is a straightforward combination of two existing techniques and is not super surprising in terms of novelty. In this case, really strong experiments are needed to support the work; this is , unfortunately, is the not the case.  For example, it was notice by the reviewers that the Mujoco TRPO experiments does not use the best implementation of TRPO, which makes it difficult to judge the strength of the work compared with state of the art. 
Dear Authors,  Thank you very much for your very detailed feedback and also updating the manuscript in the rebuttal phase. Your effort has highly contributed to clarifying some of the concerns raised by the reviewers and improving our understanding of your work.   On the other hand, we still think that the current work has rather limited novelty, and motivation and theoretical justification need to be further enhanced to be accepted for ICLR.  For these reasons, I suggest rejection of this paper, in comparison with many other strong submissions. The reviewers added further comments after receiving your feedback. I hope their comments are useful for improving your work for future publication.
The authors propose to tackle the problem of catastrophic forgetting in continual learning by adopting the generative replay strategy with the generator network as an extendable memory module.   While acknowledging that the proposed model is potentially useful, the reviewers raised several important concerns that were viewed by AC as critical issues: (1) poor presentation clarity of the manuscript and incremental technical contribution in light of prior work by Serra et al. (2018); (2) rigorous experiments and in depth analysis of the baseline models in terms of accuracy, number of parameters, memory demand and model complexity would significantly strengthen the evaluation – see R1’s and R3’s suggestions how to improve; (3) simple strategies such as storing a number of examples and memory replay should not be neglected and evaluated to assess the scope of the contribution.  Additionally R1 raised a concern that preventing the generator from forgetting should be supported by an ablation study on both, the discriminator and the generator, abilities to remember and to forget.  R1 and R3 provided very detailed and constructive reviews, as acknowledged by the authors. R2 expressed similar concerns about time/memory comparison of different methods, but his/her brief review did not have a substantial impact on the decision.  AC suggests in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper. 
As the reviewer confidence of the reviews of 2.8 or lower, I made a full and detailed pass of submission as it was requested by the PCs.   The paper studies how to parallelization of MCTS affects its performance and provides the analysis of the excess regret   how much  "error" we incur by  parallelizing as opposed to single threaded execution.  The submission however does not give convincing arguments on why existing parallel methods perform empirically under their sequential counterparts. This would be particularly useful for the settings described in the submission, which are only resembling the actual parallel solutions.   Furthermore, the paper analysis of a "cumulative regret" in which it counts the errors made for the exploration   but the setting here is a MCTS (search, planning), when we access an oracle (a model of the environment) and we should be rather interested in the sample complexity, studied for example in https://www.cis.upenn.edu/~mkearns/papers/sparsesampling journal.pdf that gave early results and many follow ups until OR  we should study how good the final policy it   and in your parallel setting (which is indeed very timely), how small the excess error of this final policy is. [Or you only focus on a single action recommended by MCTS and you study some pure exploration measure: best arm identification, simple regret, \eps BAI, ... ]  From the theoretical side modeling practice,  I don t see the theoretical framework provided to be suited with practical considerations that an MCTS is facing.  Specifically the need of condition 1 for the consequence of Theorem 1 to hold is very worrying.  Briefly it states that if  the value and the counts are not the same as reference method, then the results obtained from the that parallel executions would not match the results reference method.  While this true, it simply tells us that it would be good if the parallel threads came up with the same value as the reference method   the main issue that this does not tell the practitioner how to assure the condition states in Eq 8. It would be much more interesting to characterize that, i.e., under which conditions (in particular, that the practitioner can influence OR  at least verify prior to the execution) is the property stated in the Eq. 8 satisfied.   Theorem 2 bring additional concerns. A book of Munos on MCTS (https://hal.archives ouvertes.fr/hal 00747575v4/document) has Section 2.3 devoted why vanilla UCT has poor performance. There is work by  Kocsis and Szepesvári studying under which conditions UCT can have favorable regret    there is at least a discussion missing why Theorem 2 would be able to bypass known hardness for UCT for higher depths, and if not I question its utility. Finally, from finite time result excess I would expect more finite time lessons learned beyond "regret term that converges to zero as n increases".  
This paper addresses the extension of path space based SGD (which has some previously acknowledged advantages over traditional weight space SGD) to handle batch normalization. Given the success of BN in traditional settings, this is a reasonable scenario to consider.  The analysis and algorithm development involved exploits a reparameterization process to transition from the weight space to the path space.  Empirical tests are then conducted on CIFAR and ImageNet.  Overall, there was a consensus among reviewers to reject this paper, and the AC did not find sufficient justification to overrule this consensus.  Note that some of the negative feedback was likely due, at least in part, to unclear aspects of the paper, an issue either explicitly stated or implied by all reviewers.  While obviously some revisions were made, at this point it seems that a new round of review is required to reevaluate the contribution and ensure that it is properly appreciated.
This paper introduces a novel pruning algorithm for neural networks, gently regularizing the weights away (through weight decay) and using Hessian information instead of simple magnitude. All in all an idea that is simple and effective, and could be of interest to a large audience.   AC
The authors have proposed 3 continual learning variants which are all based on MNIST and which vary in terms of whether task ids are given and what the classification task is, and they have proposed a method which incorporates a symmetric VAE for generative replay with a class discriminator. The proposed method does work well on the continual learning scenarios and the incorporation of the generative model with the classifier is more efficient than keeping them separate. The discussion of the different CL scenarios and of related work is nice to read. However, the authors imply that these scenarios cover the space of important CL variants, yet they do not consider many other settings, such as when tasks continually change rather than having sharp boundaries. The authors have also only focused on the catastrophic forgetting aspect of continual learning, without considering scenarios where, e.g., strong forward transfer (or backwards transfer) is very important. Regarding the proposed architecture that combines a VAE with a softmax classifier for efficiency, the reviewers all felt that this was not novel enough to recommend publication.
This paper studies the robustness of NeuralODE, as well as propose a new variant. The results suggest that the neuralODE can be used as a building block to build robust deep networks. The reviewers agree that this is a good paper for ICLR, and based on their recommendation I suggest to accept this paper.
This paper tackles hard exploration RL problems. The idea is to learn separate exploration and exploitation strategies using the same network (representation). The exploration is driven by intrinsic rewards, which are generated using an episodic memory and a lifelong novelty modules. Several experiments (simple and Atari domains) show that the proposed approach compares favourably with the baselines.  The work is novel both in terms of the episodic curiosity metric and its integration with the life long curiosity metric, and the results are convincing. All reviewers being positive about this paper, I therefore recommend acceptance.
The reviewers are in agreement that while the paper is interesting, both the clarity of presentation and experimental rigor could be improved. The committee feels this paper is not ready for publication at ICLR 2018 inits current form.
In this paper, the authors propose the incremental RNN, a novel recurrent neural network architecture that resolves the exploding/vanishing gradient problem. While the reviewers initially had various concerns, the paper has been substantially improved during the discussion period and all questions by the reviewers have been resolved. The main idea of the paper is elegant, the theoretical results interesting, and the empirical evaluation extensive. The reviewers and the AC recommend acceptance of this paper to ICLR 2020.
The paper analyzes the behavior of VAEs in modeling data lying on a low dimensional manifold. It formally proves some of the conjectures/informal statements in an earlier work by Dai and Wipf (2019) in the case of linear VAE and linear manifold, and disproves the same for the nonlinear case. In particular, it proves, by analyzing the objective and its gradient flow dynamics, that VAE captures the intrinsic dimension of data distribution correctly. For nonlinear cases, the paper shows a counterexample to the conjecture in (Dai & Wipf; 2019) where the support of VAE generators is a superset of that of data distribution.   Two of the reviewers had raised following specific concerns   (i) Theory only considers linear encoders and linear/1 hidden layer nonlinear decoders, (ii) The convergence behavior during training is only provided for linear VAEs, (iii) Some statements in the introduction/abstract misrepresent the results in (Dai & Wipf; 2019). However the authors have adequately addressed (i) and (ii) in their response   the paper shows that the correct manifold will only be recovered in the linear case; in the nonlinear case, even for simple manifolds (1 hidden layer) the correct manifold is not recovered as shown by the counterexamples. Authors have also promised to modify the statements in the abstract and introduction to address the concern in (iii). Other two reviewers are largely positive about the paper. The paper makes an important contribution to the VAE literature in further clarifying VAEs  behavior while modeling low dimensional manifolds, and will be a good addition to the conference program.
The reviewers have raised several important concerns about the paper that the authors decided not to address.
The paper presents an improvement to the core set active learning algorithm by leveraging distance measures weighted by uncertainty scores and using beam search instead of greedy search.   The reviewers agreed that the paper provides a nice theoretical analysis as well as motivation for the proposal, as well an ablation that shows the proposal indeed empirically outperforms the original core set algorithm. However, the reviewers also agreed that additional important comparisons would make the paper more convincing, including Bayesian core set algorithms as well as other recent proposals based on the original core set algorithm.
The paper presents a mathematical analysis of the discrepancy between GD and GF trajectories. Following the discussion period, a knowledgeable R3 updated his/her initial rating from 4 to 6,  a knowledgeable R4 raised his/her score from 5 to 6. Finally, a very confident R1 considers this a good paper that should be accepted. He/she indicates that this paper provides a unique and very illuminating perspective on gradient descent through an extremely simple idea. The topic is very timely. I agree with R1 that the paper contributes a refreshing perspective with important elements which should be of interest to a good number of researchers. Taking into account the discussion, confidence levels and ratings of the reviewers, I am recommending the paper to be accepted. I would like to encourage the authors to take the reviewers  comments carefully into consideration when preparing the final version of the article. 
The paper presents a new method for generation of backdoor attacks against deep networks. The new method uses global warping instead of noise patches which makes the attack much more stealthy than previous approaches. The attack effectiveness is demonstrated on 3 benchmark datasets. A small user study is carried out to demonstrate that the attack is stealthier than conventional backdoor attacks.   The new attack is a novel and original contribution which is likely to advance the understanding of backdoor attacks. There were some issues with respect to clarity in the original manuscript but the authors adequately addressed the critical remarks raised by the reviewers. 
This paper proposes a hypergraph representation learning based on multiset encoding, which  covers most existing propagation methods for hypergraph neural networks. The authors provide theoretical proofs that both CE based and tensor based propagation rules can be represented as a composition of two multiset functions, and propose two different multiset encoding functions, based on DeepSets and SetTransformer. The authors validate their method for its semi supervised node classification performance on multiple benchmark datasets, showing that it is superior or comparable to a large number of existing works on hypergraph representation learning.   The following is the summary of the pros and cons of the paper mentioned by the reviewers:  Pros   The proposed framework generalizes existing message passing methods for hypergraphs, and authors provide theoretical proofs on how it can generalize to two different types of propagation rules for hypergraph representation learning.    The paper is well organized and is clearly written, and the code is provided for reproduction.    The experiments consider a wide range of hypergraph datasets and baselines, and the proposed method either outperforms them or at least achieves comparable performance.  Cons   It is still unclear where the benefits come from, due to lack of ablation studies and deeper analysis.    Experiments are only restricted to the semi supervised node classification task.   While the initial reviews were split due to these negative points, all reviewers unanimously recommended for acceptance after the discussion period, as they found the responses from the authors satisfactory.    In summary, this is a well written paper that proposes a neat framework for hypergraph representation learning that generalizes to most existing methods, backed up by compelling performance on benchmark datasets, which will make it a strong addition to ICLR. However, as mentioned by the reviewers there should be more ablation studies and in–depth analysis of what makes the proposed multiset function more effective, as this is lacking in the current version of the paper. It would be worthwhile to also validate the proposed framework on other tasks (e.g. graph classification tasks).    One minor thing that I want to point out is regarding the claim that this is the first attempt to connect the problem of learning multiset functions with hypergraph neural networks.  [Jo et al. 21], which is a hypergraph based framework for edge representation learning, utilized GMT [Baek et al. 21], which performs multiset encoding using SetTransformer for hypergraph representation learning, and thus I suggest the authors to tone down on the claim that this is the first work that connects multiset encoding with hypergraph neural networks, and properly acknowledge this.   [Jo et al. 21] Edge Representation Learning with Hypergraphs, NeurIPS 2021
The paper presents an adversarial learning framework for reading comprehension.  Although the idea is interesting and presents an approach that ideally would make reading comprehension approaches more robust, the results are not substantially solid (see reviewer 3 s comments) compared to other baselines to warrant acceptance.  Comments from reviewer 2 are also noteworthy where they mention that adversarial perturbations to a context around an answer can alter the facts in the context, thus destroying the actual information present there, and the rebuttal does not seem to satisfy the concern.  Addressing these issues will strengthen the paper for a potential future venue.
Most reviewers did not feel that this paper was ready for publication. I thank the authors for answering all the concerns of the reviewers, running new experiments and submitting a revised version, however, this was not not enough to alleviate the reviewers  concerns, notably relating to the handling of the ethical consideration in the writing of the manuscript.
The proposed relational reasoning algorithm is basically a fairly standard graph neural network, with a few modifications (e.g., the prediction loss at each layer   also not a new idea per se).   The claim that previously reasoning has not been considered in previous applications of graph neural networks (see discussion) is questionable.  It is not even clear what is meant here by  reasoning  as many applications of graph neural networks may be regarded as performing some kind of inference on graphs (e.g., matrix completion tasks by Berg, Kipf and Welling; statistical relational learning by  Schlichtkrull et al).  So the contribution seems a bit over stated.  Rather than introduces a new model, the work basically proposes an application of largely known model to two (not so hard) tasks which have not been studied in the context of GNNs. The claim that the approach is a general framework for dealing with complex reasoning problems is not well supported as both problems are (arguably) not complex reasoning problems (see R2).  There is a general consensus between reviewers that the paper, in its current form, does not quite meet acceptance criteria.  Pros:   an interesting direction   clarity Cons:   the claim of generality is not well supported   the approach is not so novel   the approach should be better grounded in previous work   
The paper proposes a method for structured representation learning using autoencoders. The method has two primary ingredients: (i) encourage independence in latent blocks by feeding different blocks of the latent representation to different depths of the decoder by injecting noise in an Ada IN inspired block, (ii) a so called hybrid sampling, that samples each block from a fixed learned set of k latent vectors, similar to the codebook used in VQ VAE (Oord et al 2017). The method is claimed to result in higher fidelity reconstruction and generation while also learning representations that are more disentangled compared to VAE and $\beta$ VAE.   Some limitations that came up in the reviews and later in the discussion among the reviewers are (i) lack of comparison with more advanced disentangled VAEs, which would be helpful in establishing the claim of the paper on better reconstruction but comparable disentanglement performance to regularization based methods (ii) high level similarity to VLAE and other methods that also use hierarchical latent variables that limits the claims on novelty. Current draft also emphasizes disentanglement which the reviewers found lacking in justification and rigor. The paper is currently not suitable for publication at ICLR but taking into account the comments from reviewers on the presentation aspects will help improve the paper.
This paper proposes a simple plug and play language model approach to the problem of controlled language generation. The problem is important and timely, and the approach is simple yet effective. Reviewers had some discussions whether  1) there is enough novelty, 2) evaluation task really shows effectiveness, and 3) this paper will inspire future research directions.   After discussions of the above points, reviewers are leaning more positive, and I reflect their positive sentiment by recommending it to be accepted. I look forward to seeing this work presented at ICLR.
This paper presents a case study of training a video classifier and subsequently analyzing the features to reduce reliance on spurious artifacts. The supervised learning task is zebrafish bout classification which is relevant for biological experiments. The paper analyzed the image support for the learned neural net features using a previously developed technique called Deep Taylor Decomposition. This analysis showed that the CNNs when applied to the raw video were relying on artifacts of the data collection process, which spuriously increased classification accuracies by a "clever Hans" mechanism. By identifying and removing these artifacts, a retrained CNN classifier was able to outperform an older SVM classifier. More importantly, the analysis of the network features enabled the researchers to isolate which parts of the zebrafish motion were relevant for the classification.  The reviewers found the paper to be well written and the experiments to be well designed. The reviewers suggested a some changes to the phrasing in the document, which the authors adopted. In response to the reviewers, the authors also clarified their use of ImageNet for pre training and examined alternative approaches for building saliency maps.  This paper should be published as the reviewers found the paper to be a good case study of how model interpretability can be useful in practice. 
Summary of discussion: Three reviewers rated the paper Good (7) while Reviewer2 disagreed. R2 s criticism was focussed on how this work is placed within existing/related literature, and no technical problem was identified. The authors have addressed some of R2 s comments/concerns, R2 has not participated in the discussion.  Novelty and contributions: Overall the reviews seem consistent with an incremental paper which is technically valid, improves the state of the art on a reasonably difficult task. However, it does not appear from the reviews that the paper substantially advances our understanding of machine learning more broadly beyond this specific application.  Experiments: There is some disagreement among reviewers on the adequacy of the experiments, with at least two reviewers calling for experiments involving  natural photos . I believe the author s responses adequately address these concerns: they pointed out that the key selling point of their paper is the ability to model structured noise which is less relevant in natural photos.  On the balance of things, I think this paper should be accepted, but I wouldn t argue if it did not make the cut due to its narrow scope. For this reason, I recommended poster presentation.
The reviewers have the following concerns: 1. The theoretical results for the proposed method are weak. Theorem 4.2 cannot be considered as a convergence result, because the bound depends on some random variables $r_{T,i}$. The reviewers agree that a proper analysis would require some knowledge on the lower bound of these variables. Although there is some empirical explanation for this, the lower bounded assumption of  $r_{T,i}$ is not theoretically justified. The authors acknowledge that this is the main challenge for the present algorithm. In addition, the analysis requires bounded gradient and bounded function value, which is also strong for nonconvex settings.  2. The empirical performance is not strong. In most experiments, the proposed method is not better than the baseline AEGD. The novelty and contribution of SGEM over AEGD is quite limited, since the idea of adding momentum is not new.   The suggestions to improve this paper are as follows 1. Since the lower bounded assumption on $r_{T,i}$ is not standard and hard to verify, the authors might consider analyzing a theoretical guarantee for it. On the other hand, they could verify more experiments with various data sets to have some sense whether this assumption may be true or not. Next, please try to relax the strong assumptions as discussed.  2. It is better if the authors can show the performance of SGEM for convex settings, and for other deep learning tasks (e.g. NLP) as suggested by the reviewers.  The authors should consider to improve the paper based on the reviewers  comments and suggestions and resubmit this paper in the future venues.
The authors propose two new benchmark datasets CIFAR 10 N and CIFAR 100 N, variants of CIFAR 10 and CIFAR 100 with real world human annotation noise. The benchmark datasets are more realistic (e.g. instance dependent noise) than some existing synthetic benchmarks for label noise. The authors also benchmark several popular baselines on the proposed benchmark  All the reviewers thought that this is an useful contribution to the community and appreciated the detailed author response. The consensus decision leaned towards accept. I recommend acceptance & encourage the authors to address any remaining concerns in the final version.  Please clarify the license (e.g. MIT license) when you release the dataset.
This paper is concerned with solving Online Combinatorial Optimization (OCO) problems using reinforcement learning (RL). There is a well established traditional family of approaches to solving OCO problems, therefore the attempt itself to solve them with RL is very intriguing, as this provides insights about the capabilities of RL in a new but at the same time well understood class of problems.   The reviewers agree that this approach is not entirely new. While past similar efforts take away some of the novelty of this paper, the reviewers and AC believe that still the setting considered here contains novel and interesting elements.   All reviewers were unconvinced that this work can provide strong claims about using RL to learn any primal dual algorithm. This takes away some of the paper’s impact, but thanks to discussion the authors managed to clarify some “hand wavy” claims and toned down the claims that were not convincing. Therefore, it was agreed that the new revision still provides some useful insight into the RL and primal dual connection, even without a complete formal connection.  
The presented paper gives a differentiable product quantization framework to compress embedding and support the claim by experiments (the supporting materials are as large as the paper itself). Reviewers agreed that the idea is simple is interesting, and also nice and positive discussion appeared. However, the main limiting factor is the small novelty over Chen 2018b, and I agree with that. Also, the comparison with low rank is rather formal: of course it would be of full rank , as the authors claim in the answer, but looking at singular values is needed to make this claim. Also, one can use low rank tensor factorization to compress embeddings, and this can be compared.  To summarize, I think the contribution is not enough to be accepted.
This paper proposes an interesting approach to leveraging crowd sourced labels, along with an ML model learned from the data itself.   The reviewers were unanimous in their vote to accept.
Pros:   Use of Bloomier filters for lossy compression of nets is novel and well motivated, with interesting compression performance. Cons:   Does lossy compression for transmission, doesn’t address FLOPS required for runtime execution. A lot of times, client devices do not have enough cpu to run large networks (title should be udpated to mean compression and transmission)   Missing results for full network, larger deeper network.  Overall, the content is novel and interesting, so I would encourage the authors to submit to the workshop track. 
The reviewers generally had concerns that the goal of recovering only the model architecture was unmotivated (given that knowing the architecture is not a large threat on its own, and there are existing attacks that work without knowledge of the model architecture). Moreover, given the strength of the assumed attack model, recovering model architecture is a fairly unambitious goal (again, more serious attacks have already been demonstrated under weaker attack models). Finally, though less seriously, the analysis is fairly preliminary, e.g. it is unclear if the attack can generalize to nearby architectures that were outside the training set.
The paper proposes to use multiple discriminators to stabilize the GAN training process. Additionally, the discriminators only see randomly projected real and generated samples.  Some valid concerns raised by the reviewers which makes the paper weak:    Multiple discriminators have been tried before and the authors do not clearly show experimentally / theoretically if the random projection is adding any value.   Authors compare only with DCGAN and the results are mostly subjective. How much improvement the proposed approach provides when compared to other GAN models that are developed with stability as the main goal is hence not clear.
The paper addresses an important unsolved problem, i.e. deriving explainable features for use in graph classification. It does it by providing: i)  a simple to implement (local) node aggregation approach; ii) some theoretical support to the proposed approach; iii) empirical evidence that the proposed approach could be effective.  Notwithstanding the above merits, the reported work seems to still be in a preliminary phase. In fact: i) reference to literature is missing some important recent contributions to the addressed problem (e.g.  Gated Graph Sequence Neural Networks, GNNExplainer);  if possibile, also experimental comparisons vs those approaches is desirable; ii) experimental results do not provide a solid evidence that the proposed approach can really help to provide a clear explanation of the output, and the overall performance in classification is mostly below SOTA models; adding more datasets could help to give a more solid support to the main statement about explainability/performance; iii) presentation needs to better highlight the original contribution w.r.t. relevant literature (which is not completely clear in the current version of the paper), to improve the explanation of proofs, to discuss (both from a theoretical and empirical perspective) some important issues, such as computational scalability with the increase of size of local structures, and robustness to noise of the proposed (local) aggregation method.  In summary, although the proposed approach seems to be of some value, more work is needed to better place the proposed approach in the context of current literature and to gain a stronger experimental support to the main claim of the paper w.r.t. explainability.
The authors propose a method for associative learning as an alternative to back propagation based learning. The idea is to interesting. The coupling between layers are broken down into local loss functions that can be updated independently. The targets are projected to previous layers and the information is preserved using an auto encoder loss function. The projections from the target side are then compared with the projections from input side using a bridge function and a metric loss. The method is evaluated on text and image classification tasks. The results suggest that this is a promising alternative to back propagation based learning.  Pros + A novel idea that seems promising + Evaluated on text and image classification tasks and demonstrated utility  Cons   The impact of the number of additional parameters and the computation is not clarified (even though epoch s are lower)  The authors utilized the discussion period very well, running additional experiments that were suggested (especially ablation studies). They  also clarified all the questions that were raised. In all, the paper has improved substantially from the robust discussion.
The paper is very clear.  It provides a good overview of the problem, making it easy to follow even for researchers outside the area.    This work provides a novel approach for extrapolating the expected accuracy on a larger set of classes from a training set with smaller number of classes with a creative, simple and elegant solution through reversed ROC.  Such an approach will be useful for extreme classification settings.  In real world settings, classifiers are often trained on a pilot set of data, and then deployed where the classes are much larger.  It is useful to have a mechanism to estimate how the classification performance will change with larger number of classes.    The reviewers all agree that this work provides a novel contribution to predicting classification accuracy.  The authors have satisfactorily addressed the reviewers’ comments and provided sufficient clarification to the questions.  We also appreciate the edits that the authors have made.   
At this time, this work is not yet ready for publication. The core idea influence functions was poorly explained in the initial submission, and although major changes to the paper were made to rectify this, at least some reviewers of the remain unconvinced and it is unclear that the paper has been fully evaluated with this confusion resolved. There are a sufficient number of other concerns around the paper, that having rectified these more fully and outside the tight time constraints of the rebuttal period, I hope for an interesting resubmission in future.
This paper interprets batch norm in terms of normalizing the backpropagated gradients. All of the reviewers believe this interpretation is novel and potentially interesting, but that the paper doesn t make the case that this helps explain batch norm, or provide useful insights into how to improve it. The authors have responded to the original set of reviews by toning down some of the claims in the original paper, but haven t addressed the reviewers  more substantive concerns. There may potentially be interesting ideas here, but I don t think it s ready for publication at ICLR.  
This paper sits right at the borderline: the reviewers agree that it is interesting and addresses a relevant problem. On the negative side, the presentation could be improved (including some incorrect claims), and the experiments could be strengthened (both in terms of baselines and datasets used). Ultimately, the paper will probably require another round of reviews before it is ready for publication.
The submission proposes to use GANs to learn a generative model of fMRI scans that can then be used for downstream classification tasks.  Although there was some appreciation from the reviewers of the approach, there were several important remaining concerns:  1) From Reviewer 1: "Generating high resolution images with GANs even on faces for which there is almost infinite data is still a challenge. Here a few thousand data points are used. So it raises too concerns: First is it enough?"  and  2) R1 and R2 both raised concerns about the significance of the improvements.  Looking through the tables, there are many reported differences that are reasonably small, and no error bars or significance are given.  This should be a requirement for an empirical paper about fMRI.
Maintaining the privacy of membership information contained within the data used to train machine learning models is paramount across many application domains.  Moreover, this risk can be more acute when the model is used to make predictions using out of sample data.  This paper applies a causal learning framework to mitigate this problem, motivated by the fact that causal models can be invariant to the training distribution and therefore potentially more resistant to certain privacy attacks.  Both theoretical and empirical results are provided in support of this application of causal modeling.  Overall, during the rebuttal period there was no strong support for this paper, and one reviewer in particular mentioned lingering unresolved yet non trivial concerns.  For example, to avoid counter examples raised the reviewer, a deterministic labeling function must be introduced, which trivializes the distribution p(Y|X) and leads to a problematic training and testing scenario from a practical standpoint.  Similarly the theoretical treatment involving Markov blankets was deemed confusing and/or misleading even after careful inspection of all author response details.  At the very least, this suggests that another round of review is required to clarify these issues before publication, and hence the decision to reject at this time.
The paper proposes a new learning algorithm for deep neural networks that first reformulates the problem as a multi convex and then uses an alternating update to solve. The reviewers are concerned about the closeness to previous work, comparisons with related work like dlADMM, and the difficulty of the dataset. While the authors proposed the possibility of addressing some of these issues, the reviewers feel that without actually addressing them, the paper is not yet ready for publication. 
The reviewers all agreed that the paper is a solid contribution.  Pros:   A simple and reasonable extension to adaptive prediction sets that performs well empirically.   The procedure presented is versatile (i.e. can be applied to general scores or be used to improve base conformal prediction methods).   A very thorough experimental analysis, including large datasets (i.e. Imagenet) and a wide range of model architectures including ResNet 152.   Some formal theoretical guarantees are provided for the procedure, although they appear to be straightforward.  Cons:   Limited technical novelty.  Overall, I recommend a spotlight because the reviewers felt that the topic of predictive uncertainty is of interest to the broader ML and computer vision community, and the paper can have a potentially large impact in popularizing conformal methods as a viable uncertainty estimation method.
The paper proposes a new teacher student framework where the teacher network guides the student network in learning useful information from trajectories of a dynamical system. The proposed framework is inspired by the Knowledge Distillation method. The teacher learns what information should be used from the trajectories and distills this information for the student in the form of target activations. In a nutshell, the framework allows the student to interpolate between model based and model free approaches in an automated fashion. Experimental evaluation on both the hand crafted and simulated tasks demonstrate the effectiveness of the proposed framework. The reviewers had borderline scores in their initial reviews and raised several questions for the authors. The reviewers appreciated the rebuttal, which helped in answering their key questions   I want to thank the authors for engaging with the reviewers during the discussion phase. The reviewers have an overall positive assessment of the paper, and believe that the proposed teacher student framework is novel and potentially useful for many real world problems. The reviewers have provided detailed feedback in their reviews, and I would like to strongly encourage the authors to incorporate this feedback when preparing the final version of the paper.
The authors attempt to tackle the problem of compositional generalization, i.e., the problem of generalizing to  novel combinations of familiar words or structures. The authors propose a transfer learning strategy based on pretraining language models. The idea is to introduce a pre finetuning task where a model is first trained on compositional train test splits from other datasets, before transferring to fine tuning on the training data from the target dataset. Although the technique brings some improvements, and the authors do their best the address the reviewers  questions, it is still unclear:  a) Why the method should work in principle, whether there is a theoretical backing and how it formally relates to meta learning b) How the approach compares to data augmentation methods since pre finetuning requires more data, albeit from a different dataset. See for example: https://openreview.net/forum?id PS3IMnScugk c) The whole approach would be more convincing if the authors could articulate *how* their method renders a model more robust to distribution shifts (e.g., based on GOGS results it does not help structural generalization, do the gains  come from lexical generalization?) d) it would also be interesting whether this method works on larger scale or more realistic datsets like CFQ, ATIS or machine translation https://arxiv.org/pdf/1912.09713.pdf https://arxiv.org/abs/2010.11818
This work has generated a lot of discussion between authors and reviewers and among reviewers. Overall it is reported that the results on EEG are not conclusive and directly relevant for this field. Besides the theoretical contribution is not reported as a strong point of the work and the comparison with alternative baseline methods is judged too limited.  For all these reasons the paper cannot be endorsed for publication at ICLR this year.
This paper proposes a load balanced hashing called AHash that balances the load of hashing bins to avoid empty bins that appear in some minwise hashing methods. Reviewers found the work interesting and well motivated. Authors addressed some clarity issues in their rebuttal. However the impact appeared quite limited, and the experimental validation limited to few realistic experiments that did not alleviate this concern. We thus recommend rejection.
This paper proposes a novel application of generative adversarial networks to model neural spiking activity.  Their technical contribution, SpikeGAN, generates neural spikes that accurately match the statistics of real recorded spiking behavior from a small number of neurons.  The paper is controversial among the reviewers with a 4, a 6 and an 8.  The 6 is short and finds the idea exciting but questions the utility of the proposed approach in terms of actually studying neural spiking.  The 4 and 8 are both quite thorough reviews.  4 seems to mostly question the motivation of using a GAN over a MaxEnt model and demands empirical comparison to other approaches.  8 applauds the paper as a well executed pure application paper, applying recent innovations in machine learning to an important application with some technical innovation.  Overall the reviewers found the paper clear and easy to follow and agree that the application of GANs to neural spiking activity is novel.  In general, I find that such high variance in scores (with thorough reviews) indicate that the paper is exciting, innovative and might stir up some interesting discussion.  As such, and under the belief that ICLR is made stronger with interesting application papers, I feel inclined to accept as a poster.  Pros:   A novel application of GANs to neural spiking data   Addresses an important and highly studied application area (computational neuroscience)   Clearly written and well presented   The approach appears to model well real neural spiking activity from salamander retina  Cons:   Known pitfalls of GANs aren t really addressed in the paper (mode collapse, etc.)   The authors don t compare to state of the art models of neural spiking activity (although they compare to an accepted standard approach   MaxEnt)   Limited technical innovation over existing methods for GANs
This paper proposes to use a generative adversarial network to train a substitute that replicates (imitates) a learned model under attack. It then shows that the adversarial examples for the substitute can be effectively used to attack the learned model. The proposed approach leads to better success rates of attacking than other substitute training approaches that require more training examples. The condition to get a well trained imitation model is that a sufficient number of queries are obtained from the target model. This paper has valuable contributions by developing an imitation attacker. However, some key issues remain. In particular, I agree with R1 that the average number of queries per image is relatively high, even during training. In the rebuttal, the authors made the assumption that “suppose their method could make an infinite number of queries for target models”, which is unfortunately not realistic. Another point that I found confusing: at testing, I don’t see how you can use the imitation model D to generate adversarial samples (D is a discriminative model, not a generator); it should be G, right? 
While the authors made a strong rebuttal, none of the reviewers were particularly enthusiastic about the contributions of this paper and we unfortunately have to reject borderline papers. Concerns were expressed about the presentation, as well as the scalability of the approach. The AC encourages the authors to "revise and resubmit".
The reviewers agree that this is an interesting and promising paper, although it is on the theoretical side, without even satisfying toy examples to demonstrate its usefulness. This itself is not a fatal problem (ICLR can and should welcome theoretical papers), however including such experiments would significantly strengthen the impact of this paper, and make it more competitive with other ICLR submissions.
While there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at ICLR in its present form.  The most significant concerns raised were about the strength of the experiments, and choice of appropriate baselines.
This is a novel, simple, and experimentally well supported new idea for entity linking.  The key insight is to perform entity linking by producing meaningful entity names with seq2seq approaches, and the big surprise is how well this works experimentally (at least for wikipedia style entities).  Very nice paper! 
This paper proposes an optimization algorithm based on  weak contraction mapping . The paper is written poorly without clear definitions and mathematical rigor. Reviewers doubt both the correctness and the usefulness of the proposed method.  I strongly suggest authors to rewrite the paper addressing all the reviews before submitting to a different venue. 
I thank the authors for their submission and active participation in the discussions. This papers is borderline with three reviewers leaning towards acceptance [3c96,7T33,Zhvq] and one leaning towards rejection [o38w]. Reviewer o38w s main concerns are around the lack of details about how the baselines were tuned and missing training details (specifically the connectivity test used to reject candidate environments). During discussion both, reviewers Zhvq and 7T33, agree that the paper requires substantial restructuring/rewriting to properly address the reviewer s feedback which is currently mostly addressed in the appendix. Based on the discussion with reviewers, my assessment is that this paper is not ready for publication at this point and that it will benefit greatly from another iteration. I want to very strongly encourage the authors to further improve their paper based on the reviewer feedback.
This paper presents a memory architecture for RL based on reservoir sampling, and is meant to be an alternative to RNNs. The reviewers consider the idea to be potentially interesting and useful, but have concerns about the mathematical justification. They also point out limitations in the experiments: in particular, use of artificial toy problems, and a lack of strong baselines. I don t think the paper is ready for ICLR publication in its current form.  
This paper presents a very interesting study of using an artificial language (generated using a specific algorithm via a transformer model) and training SOTA transformer and LSTM language models on that language;  the authors show that these LMs underestimate the probability of sequences from this language and overestimate the probability of ill formed sentences, among other observations.  This is a very interesting study that captures the behavior of recent LMs.  All reviewers are supportive of accepting this paper and it is good to see the engagement between reviewers and authors of this paper.
This paper combines Prolog like reasoning with distributional semantics, applied to natural language question answering. Given the importance of combining neural and symbolic techniques, this paper provides an important contribution. Further, the proposed method complements standard QA models as it can be easily combined with them.  The reviewers and AC note the following potential weaknesses: (1) The evaluation consisted primarily on small subsets of existing benchmarks,  (2) the reviewers were concerned that the handcrafted rules were introducing domain information into the model, and (3) were unconvinced that the benefits of the proposed approach were actually complementary to existing neural models.   The authors addressed a number of these concerns in the response and their revision. They discussed how OpenIE affects the performance, and other questions the reviewers had. Further, they clarified that the rule templates are really high level/generic and not "prior knowledge" as the reviewers had initially assumed. The revision also provided more error analysis, and heavily edited the paper for clarity. Although these changes increased the reviewer scores, a critical concern still remains: the evaluation is not performed on the complete question answering benchmark, but on small subsets of the data, and the benefits are not significant. This makes the evaluation quite weak, and the authors are encouraged to identify appropriate evaluation benchmarks.   There is disagreement in the reviewer scores; even though all of them identified the weak evaluation as a concern, some are more forgiving than others, partly due to the other improvements made to the paper. The AC, however, agrees with reviewer 2 that the empirical results need to be sound for this paper to have an impact, and thus is recommending a rejection. Please note that paper was incredibly close to an acceptance, but identifying appropriate benchmarks will make the paper much stronger.
This paper proposes a measure of inherent difficulty of datasets. While reviewers agree that there are good ideas in this paper that is worth pursuing, several concerns has been risen by reviewers, which are mostly acknowledged by the authors. We look forward to seeing an improved version of this paper soon! 
This paper studies efficient robust training. The key idea is to use backward smoothing as an advanced random initialization to improve a model s adversarial robustness. The approach is sound, well grounded, and quite logical. Results demonstrate the effectiveness.  However, there exists some limitations:  1) Andriushchenko & Flammarion, 2020 gives a better and more fundamental explanation on how to address fast adversarial training.  2) Backward smoothing does not generalize to standard adversarial training. In other words, it only works for KL divergence loss rather than cross entropy loss, and it seems that backward smoothing does not address the fundamental problem of fast adversarial training.  3) If we compare the performance of Fast TRADES and Backward Smoothing since Backward Smoothing intends to improve Fast TRADES, there is always a tradeoff between clean accuracy and adversarial robustness, e.g., Table 4 and Table 8.   4) Randomized smoothing is helpful for one step adversarial training and randomized smoothing in general seems to be orthogonal to the proposed method. Moreover, 2 step PGD training can perform similarly well to backward smoothing while being much simpler conceptually.  In the end, I think that this paper may not be ready for publication at ICLR, but the next version must be a strong paper if above limitations can be well addressed.
The paper presents a method that unifies classification based approaches for outlier detection and (one class) anomaly detection. The paper also extends the applicability to non image data.  In the end, all the reviewers agreed that the paper makes a valuable contribution and I m happy to recommend acceptance.
The paper develops kernel functions in Banach spaces. However the results seem to be preliminary and further development is needed before the manuscript can be published. Reviewers point out several errors and also author/authors have graciuously agree with the suggestion  that they will incorporate all the feedback in future submissions.
The paper leverages concepts coming from hindsight relabelling methods to define a novel "iterated" supervised learning procedure to learn policies to reach different goals. The algorithmic solution is well supported in terms of intuition, preliminary theoretical guarantees, as well as strong empirical validation.   There is a general consensus among the reviewers that this is a strong submission and the rebuttal helped in clarifying some aspects of the paper (e.g., the comparison with Go Explore) and reinforced the empirical analysis. This is a clear accept.
The authors provide an empirical and theoretical exploration of Nesterov momentum, particularly in the over parametrized settings. Nesterov momentum has attracted great interest at various times in deep learning, but its properties and practical utility are not well understood. This paper makes an important step towards shedding some light on this approach for training models with a large number of parameters. 
The paper proposes to combine a VAE model with the Optimal Transport to approximate some components of the model. The authors evaluate their approach on semi supervised problems and claim to obtain very competitive results compared to literature. Unfortunately, the paper would benefit substantially from revisions to make it easier to follow. For this reason, the paper is not ready for publication in this venue at this time.
After reading the paper, reviews and authors’ feedback. The meta reviewer agrees with the reviewers that the paper touches an interesting topic (reversible computing) but could be improved in the area of presentation and evaluation. Therefore this paper is rejected.  Thank you for submitting the paper to ICLR.  
This paper provides a new family of untrained/randomly initialized sentence encoder baselines for a standard suite of NLP evaluation tasks, and shows that it does surprisingly well—very close to widely used methods for some of the tasks. All three reviewers acknowledge that this is a substantial contribution, and none see any major errors or fatal flaws.  One reviewer had initially argued the experiments and discussion are not as thorough as would be typical for a strong paper. In particular, the results are focused on a single set of word embeddings and a narrow class of architectures. I m sympathetic to this concern, but since there don t seem to be any outstanding concerns about the correctness of the paper, and since the other reviewers see the contribution as quite important, I recommend acceptance. [Update: This reviewer has since revised their review to make it more positive.]  (As a nit, I d ask the authors to ensure that the final version of the paper fits within the margins.)
This paper extends previous work on searching for good neural architectures by iteratively growing a network, including energy aware metrics during the process. There was discussion about the extent of the novelty of this work and how well it was evaluated, and in the end the reviewers felt it was not quite ready for publicaiton.
All reviewers suggested rejection of the paper. This is based on concerns regarding novelty of results, clarity of presentation, simplicity of conducted experiments and missing ablation studies (and several other points raised in the reviews). The authors also did not submit a rebuttal. Hence I am recommending rejection of the paper.
This paper introduces an unsupervised concept learning and explanation algorithm, as well as a concept of "completeness" for evaluating representations in an unsupervised way.  There are several valuable contributions here, and the paper improved substantially after the rebuttal.  It would not be unreasonable to accept this paper.  But after extensive post review discussion, we decided that the completeness idea was the most valuable contribution, but that it was insufficiently investigated.  To quote R3, who I agree with: " I think the paper could be strengthened considerably with a rewrite that focuses first on a shortcoming of existing methods in finding complete solutions. I also think their explanations for why PCA is not complete are somewhat speculative and I expect that studying the completeness of activation spaces in invertible networks would lead to some relevant insights"  
This paper presents an end to end technique for named entity recognition, that uses pre trained models so as to avoid long training times, and evaluates it against several baselines. The paper was reviewed by three experts working in this area. R1 recommends Reject, giving the opinion that although the paper is well written and results are good, they feel the technique itself has little novelty and that the main reason the technique works well is using BERT. R2 recommends Weak Reject based on similar reasoning, that the approach consists of existing components (albeit combined in a novel way) and suggest some ablation experiments to isolate the source of the good performance. R3 recommends Weak Accept but feels it is "unsurprising" that BERT allows for faster training and higher accuracy. In their response, authors emphasize that the application of pretraining to named entity recognition is new, and that theirs is a methodological advance, not purely a practical one (as R1 suggests and other reviews imply). They also argue it is not possible to do a fair ablation study that removes BERT, but make an attempt. The reviewers chose to keep their scores after the response. Given the split decision, the AC also read the paper. It is clear the paper has significant merit and significant practical value, as the reviews indicate. However, given that three expert reviewers   all of whom are NLP researchers at top institutions   feel that the contribution of the paper is weak (in the context of the expectations of ICLR) makes it not possible for us to recommend acceptance at this time. 
This paper proposes quantum inspired methods for increasing the parametric efficiency of word embeddings. While a little heavy in terms of quantum jargon, and perhaps a little ignorant of loosely related work in this sub field (e.g. see the work of Coecke and colleagues from 2008 onwards), the majority of reviewers were broadly convinced the work and results were of sufficient merit to be published.
This paper proposes a prototypical contrastive predictive coding by combining the prototypical method and contrastive learning, and presents its efficient implementation for three distillation tasks: supervised model compression, self supervised model compression, and self supervised learning via self distillation. The paper is well written, and the effectiveness of the proposed method is validated through extensive experiments.  Reviewers generally agree the paper has clear merits despite some weaknesses for improvement. Overall, I would like to recommend it for acceptance and encourage authors to incorporate all the review comments and suggestions in the final version.
In this paper the authors demonstrate the use of meta learning in plastic recurrent neural networks with an evolutionary approach, avoiding gradients. They show that this approach can be used to develop networks that can solve problems like sequence prediction and simple navigation.  The reviews for this paper all had scores below the acceptance threshold (3,5,3,3). The principal concerns were:  (1) The lack of novelty. Other papers have taken very similar approaches (e.g. Najarro & Risi, 2020 or Miconi et al., 2019), and fundamentally this paper simply ties together different elements in one package.  (2) Lack of demonstration of the approach beyond some very simple tasks.  (3) Lack of connection to the related literature on neuro evolution and ML.   (4) General clarity and style of writing issues.  The authors responded to the reviewers, but the responses did not convince the reviewers enough to increase their scores past threshold. Given this, a reject decision was reached.
All the reviewers pointed out issues with the experiments, which the rebuttal did not address. The paper seems interesting, and the authors are encouraged to improve it.
This paper has generated a lot of great discussion and it presents a very different way of doing image recognition at scale compared to current state of the art practices.  All reviewers rated this paper as an accept. This work is interesting enough that in my view it really deservers further exposure and discussion and an oral presentation at ICLR would be a good way to achieve that.
The paper proposes a new curriculum learning framework by parameterizing data partitioning and weighting schemes. Extensive experiments are performed on three different datasets to demonstrate the effectiveness of the proposed framework. The reviewers acknowledged that the proposed framework is interesting as it encompasses several existing curriculum learning methods. However, the reviewers pointed out several weaknesses in the paper and shared concerns, including the scalability of the framework to larger datasets and the significance of the improvements over baselines. I want to thank the authors for their detailed responses. Based on the reviewers’ concerns and follow up discussions, there was a consensus that the work is not ready for publication. The reviewers have provided detailed feedback to the authors. We hope that the authors can incorporate this feedback when preparing future revisions of the paper.
This paper proposes a self attention based approach for learning representations for the vertices of a dynamic graph, where the topology of the edges may change. The attention focuses on representing the interaction of vertices that have connections. Experimental results for the link prediction task on multiple datasets demonstrate the benefits of the approach. The idea of attention or its computation is not novel, however its application for estimating embeddings for dynamic graph vertices is new. The original version of the paper did not have strong baselines as noted by multiple reviewers, but the paper was  revised during the review period. However, some of these suggestions, for example, experiments with larger graph sizes and other related work i.e., similar work on static graphs are left as a future work.
The chief contribution of this paper is to show that a single set of policy parameters can be optimized in an alternating fashion while the design parameters of the body are also optimized with policy gradients and sampled. The fact that this simple approach seems to work is interesting and worthy of note. However, the paper is otherwise quite limited   other methods are not considered or compared, incomplete experimental results are given, and important limitations of the method are not addressed. As it is an interesting but preliminary work, the workshop track would be appropriate.
The paper proposes a principled modeling framework to train a stochastic auto encoder that is regularized with mutual information maximization. For unsupervised learning, this auto encoder produces a hybrid continuous discrete latent representation. While the authors  response and revision have partially addressed some of the raised concerns on the technical analyses, the experimental evaluations presented in the paper do not appear adequate to justify the advantages of the proposed method over previously proposed ones, and the clarity (in particular, notation) needs further improvement. The proposed framework and techniques are potentially of interest to the machine learning community, but the paper of its current form fells below the acceptance bar. The authors are encouraged to improve the clarify of the paper and provide more convincing experiments (e.g., on high dimensional datasets beyond MNIST).
The paper studies the behavior cloning based strategies of offline RL algorithms in different type of environments and reports that performance primarily depends on model size and regularization. The results contradict some of the earlier claims, and the authors conjecture that model size and regularization characteristics can explain past results.    During the review period, the reviewers agreed that the paper has certain merits, and on the other hand, they also raised some concerns, regarding some missing technical details, whether the empirical finding could be trusted, the generalization of the findings to more scenarios, and the comparison with some highly related papers. The authors did a good job in their rebuttal, which removed many of the above concerns (although not all) and convinced the reviewers to raise their scores. As a result, we believe it is fine to accept the paper (although somehow like a weak accept).
The paper considers the problem of incorporating human physiological feedback into an autonomous driving system, where minimization of a predicted arousal response is used as an additional source of reward signal, with the intuition that this could be used as a proxy for training a policy that is risk averse.   Reviewers were generally positive about the novelty and relevance of the approach but had methodological concerns. In particular, concerns about the weighting of the intrinsic vs. extrinsic reward (why under different settings the optimal tradeoff parameter was different, how this affects the optimal policy if the influence of the intrinsic reward is not decreased with time). Additional baseline experiments were requested and performed, and the paper was modified to significantly incorporate other feedback such as drawing connections to imitation learning. A title change was proposed and accepted to reflect the focus on the application of risk aversion (I d ask that the authors update the paper OpenReview metadata to reflect this).  At a high level, I believe this is an original and interesting contribution to the literature. I have not heard from two of three reviewers regarding whether their concerns were addressed, but given that their concerns appear to me to have been addressed (and their initial scores indicated that the work met the bar for acceptance, if only marginally), I am inclined to recommend acceptance.
Initially, two reviewers gave high scores to this paper while they both admitted that they know little about this field. The other review raised significant concerns on novelty while claiming high confidence. During discussions, one of the high scoring reviewers lowered his/her score. Thus a reject is recommended.
The paper scores low on novelty. The experiments and model analysis are not very strong.
After reading the author s rebuttal, the reviewers still think that this is an incremental work, and the theory and experiments .are inconsistent. The authors are encouraged to consider the the reivewer s comments to improve the paper.
Observing that in contrast to classical information bottleneck, the deep variational information bottleneck (DVIB) model is not invariant to monotonic transformations of input and output marginals, the authors show how to incorporate this invariance along with sparsity in DVIB using the copula transform. The revised version of the paper addressed some of the reviewer concerns about clarity as well as the strength of the experimental section, but the authors are encouraged to improve these aspects of the paper further.
Predicting graphs is an interesting and important direction, and there exist essentially no (effective) general purpose techniques for this problem.  The idea of predicting nodes one by one, though not entirely surprising, is interesting and the approach makes sense. Unfortunately, I (and some of reviewers) less convinced by evaluation:     For example, evaluation on syntactic parsing of natural language is very weak. First of all, the used metric   perplexity and exact match are non standard and problematic (e.g., optimizing exact match would largely correspond to ignoring longer sentences where predicting the entire tree is unrealistic).  Also the exact match scores are very low (~30% whereas 45+ were achieve by models back in 2010).    A reviewer had, I believe, valid concerns about comparison with GrammarVAE, which were not fully addressed.  Overall, I believe that it is interesting work, which regretfully cannot be published as a conference paper in its current form.  + important / under explored problem + a reasonable (though maybe not entirely surprising / original) approach   issues with evaluation  
The paper proposes a method to sample the parameters of the generator and discriminator in a BayesGAN (Saatci and Wilson, 2017) setting. The main innovation is a modified Hamiltonian Monte Carlo sampling scheme. Unfortunately the method is not clearly presented, to the point that all reviewers had difficulties understanding how the method works. The revision is making progress but still does not clearly explain the method. While the paper cannot be accepted for publication in its present form, the experimental results are encouraging so I encourage the authors to keep improving their manuscript.
This paper considers the analysis of momentum for nonconvex problems. While this is a worthy direction, as some reviewers pointed out, the examples considered are rather specialized and one can argue they are mostly "convex like". Therefore it is not clear that these results are generalizable and whether the analysis offers insights about how momentum helps (including faster escape of saddle in nonconvex regime, and faster convergence in the convex regime). In the current form, these results have limited scopes. 
The reviewers agree that the technical innovations presented in this paper are not great enough to justify acceptance.  The authors correctly point out to the reviewers that the ICLR CFP states that the topics of "implementation issues, parallelization, software platforms, hardware” are acceptable.  I would point out that most papers in these spaces describe *technical innovations* that enable improvements in "parallelization, software platforms, hardware" rather than implementations of these improvements.   However, it is certainly true that a software package is an acceptable (although less common) basis for a publication, provided is it sufficiently unique and impactful.  After pointing this out to the reviewers and collecting opinions, the reviewers do not feel the combined technical and software contributions of this paper are enough to justify acceptance.   
I thank the authors and reviewers for the lively discussions. Reviewers found the work to be interesting but some concerns were raised regarding the significance of the results. In particular, two reviewers mentioned that authors did not fully address their concerns in the rebuttal period. Given all, I think the paper still needs a bit of work before being accepted. I recommend authors to address comments raised by the reviewers to improve their work.   AC 
This paper studies differentially private, communication efficient training methods for federated learning. While the problem studied in this paper is well motivated and interesting, the reviewers raised several concerns about the paper. Despite the authors  reconstruction protection explanation, the concern over large values of epsilon at the scale of 400 persists. There is not too much technical novelty since the main technique is given by prior work. 
All reviewers recommend accept.  Discussion can be consulted below.
This paper extends multi agent imitation learning to extensive form games. There is a long discussion between reviewer #3 and the authors on the difference between Markov Games (MGs) and Extensive Form Games (EFGs). The core of the discussion is on whether methods developed under the MG formalism (where agents take actions simultaneously) naturally can be applied to the EFG problem setting (where agents can take actions asynchronously). Despite the long discussion, the authors and reviewer did not come to an agreement on this point. Given that it is a crucial point for determining the significance of the contribution, my decision is to decline the paper. I suggest that the authors add a detailed discussion on why MG methods cannot be applied to EFGs in the way suggested by reviewer #3 in the next version of this work and then resubmit.
This paper analyzes a problem with the convergence of Adam, and presents a solution. It identifies an error in the convergence proof of Adam (which also applies to related methods such as RMSProp) and gives a simple example where it fails to converge. The paper then repairs the algorithm in a way that guarantees convergence without introducing much computational or memory overhead. There ought to be a lot of interest in this paper: Adam is a widely used algorithm, but sometimes underperforms SGD on certain problems, and this could be part of the explanation. The fix is both principled and practical. Overall, this is a strong paper, and I recommend acceptance. 
This work aims at giving a systematic evaluation of different unsupervised domain adaptation methods on time series classification tasks under a fair setting. By providing extensive experiments on various datasets, competitive baselines, and model selection approaches, this paper has the potential to facilitate future research on this topic if the mentioned concerns are well addressed.  After rebuttal and discussion, the final scores were 3/5/5/5/5. AC considered all reviews, author responses, and the discussions, as well as reading through the paper as a neutral referee, and reject the paper based on the following concerns: + *Model Selection Criterion*: As stated by the authors, employing labeled target data for model selection will violate the fundamental assumption of unsupervised domain adaptation. However, the proposed Few Shot Target Risk (FST Risk) also requires labeling a few target domain samples. If it is possible, why not directly conduct semi supervised domain adaptation? + *Experiment Details*: As a benchmark paper, it is extremely important to carefully design the experiment details to attain promising results. Among these details, a suitable network backbone for time series classification (Is CNN or ResNet 18 the best choice?  Or TCN mentioned by Reviewer f7Xp), large scale datasets with considerable domain gap, and evaluation metrics are the first consideration to attain insightful findings. + *Novelty or Interesting Findings*: As pointed out by reviewers, it is obvious that the technical novelty is limited but it may be okay for a benchmark paper if solid/interesting experimental results are observed. However, some of the findings are also fragile and the experiments should be carefully conducted to make them more solid.  In summary, this paper studies a promising research direction of domain adaptation, but the work cannot be accepted before addressing the reviewers  comments. The weaknesses mentioned above will have a high probability of being asked by the reviewers of the next conference. So the authors need to make sure that they substantially revise their work before submitting it to another venue.
The paper considers learning a fair classifier under distribution shift. The proposal involves an additional MMD penalty between the model curvatures on the data subgroups defined by the sensitive attribute. Reviewers generally found the problem setting to be well motivated, and the paper to have interesting ideas. Some concerns were raised in the initial set of reviews:  (1) _Relation between local curvatures and fairness robustness_. The concern was that the paper does not make sufficiently clear how similarity of the distributions of local curvatures ensures fairness robustness, and that there is no explicit definition of fairness robust to distribution shift.  (2) _Comparison to related work_. The concern was that works such as FARF as also considering the issue of distribution shift.  (3) _Technical novelty_. The concern was that technical depth of the proposal may be limited, as it builds on existing ideas (e.g., adversarial learning, Hessian to measure curvature).  (4) _Significance of results_. The concern was that the improvements of the proposed method are not significant, statistically and/or practically.  For point (1), the response clarified that the proposal is to ensure that the local curvature (and hence robustness) across data subgroups is similar. The relevant reviewer was still unclear as to whether this ensures what one might intuitively consider "robust fairness". On my review of the paper, I do concur that from the Introduction, and the para preceding Eqn 4, it appears that one natural notion is  $ \sup_{\mathbb{Q} \in \mathcal{U}( \mathbb{P} )} \Delta( \mathbb{Q}( \hat{Y}, Y \mid A   0 ), \mathbb{Q}( \hat{Y}, Y \mid A   1 ) ) $  where $\mathbb{P}$ is the observed data distribution, $\mathcal{U}$ is some uncertainty set, and $\Delta$ is some fairness measure (e.g., DP). Assuming this is indeed the ideal, it would be useful to mathematically contrast it to the proposal adopted in the present paper. The para preceding Eqn 4 correctly notes that the above notion would require specifying $\mathcal{U}$. This may be challenging, but an apparently reasonable strategy that follows the distributionally robust optimization literature would be to use a specific ball around the training distribution (e.g., all distributions with bounded KL divergence against $\mathbb{P}$). Further, it is of interest to ask whether the proposed objective in any way approximate this one; put another way, is there any implicit assumption made as to which class of distributions one is likely to encounter?  Further discussion would also be useful on the following alternative to the objective presented in the paper: rather than match the curvatures for the subgroups, simply minimise their unweighted average. This ought also to ensure robustness under the two different distributions; page 2 hints that this might not work owing to the different scales of these terms (i.e., the minority subgroup being much less robust), but the point does not seem to be discussed very explicitly subsequently.  For point (2), the response noted that FARF is designed for online learning, whereas the present paper involves a single, static training set drawn iid from a single distribution. In the present paper, the drift happens at test time, and the learner has no access to samples from this distribution. The authors argued that FARF can be applied as is to this setting. From my reading of this and the FARF paper, I agree that while the latter should be cited, it is not clearly applicable to the present setting.  This said, the present paper primarily focusses on the covariate shift setting, for which there have been some relevant recent works; see:  Singh et al., "Fairness Violations and Mitigation under Covariate Shift", FAccT  21.  Rezaei et al., "Robust Fairness under Covariate Shift", AAAI  21.  The former uses tools from joint causal graphs, while the latter assumes access to an unlabelled sample for the target distribution. The present work is certainly different in technical details, but at a minimum it seems prudent to acknowledge that there are relevant works on ensuring fairness outside the observed training distribution, and thus tone down statements such as "As a pioneer work...". There also seems scope to compare against the latter, e.g., to see how valuable having a few samples from the target domain are.  Another work relevant to the spirit of ensuring fairness beyond the observed data is  Mandal et al., "Ensuring Fairness Beyond the Training Data", NeurIPS 2020.  This is in line with the distributionally robust objective suggested in point (1), where one considers test distributions that can be arbitrary re weightings of the training distribution.  For point (3), from my reading, the technical content is reasonable. I would however have liked more mathematical discussions on point (1) above, which is important as it is the foundation of the strategy followed.  For point (4), the response asserts their improvements are significant practically and statistically. From my reading, I am inclined to agree with this claim. I would however note that another reviewer raised the question of whether Gaussian and uniform noise are reflective of real world distribution shifts. I concur with this concern; this part of the paper seems a little disappointing. The response mentioned results on a new setting with more realistic shift, which we suggest is incorporated into future versions of the paper.  Overall, the paper has some interesting ideas for a topical and important problem. At the same time, there is scope for tightening the work per the comments above, particularly on points (1) and (2), and to some extent (4). We believe that addressing these would help properly situate the work, and thus increase its clarity and potential impact. We thus encourage the authors to consider incorporating these for a future submission.
This work proposes a novel, interesting and simple technique to improve the model robustness to distribution shift. The proposed method is called Adversarial Batch Normalization (AdvBN) which is based on adversarial perturbation of BN statistics. Authors provide extensive experiments to show the effectiveness of AdvBN. All reviewers agree that the proposed method is interesting and novel. The main concern of reviewers is about the some of the details of the empirical evaluation of the proposed methods which makes its effectiveness less clear. In particular, the following concerns are shared among the reviewers:  1  Authors give different treatments to Stylized ImageNet compare to other tasks by using auxiliary BN at inference time instead of standard BN and further results provided by authors show that the improvement over previous methods disappear if they use standard BN for inference on Stylized ImageNet. I think authors could mitigate this issue by further investigation or providing a better explanation on why they have a different treatment for Stylized ImageNet (other than the fact that auxiliary BN has a better performance on that task). The other potential remedy is to come up with an automatic way to decide which one to use at the inference time using a batch of "unlabeled" validation data.  2  The improvement of AdvBN over AugMix and AdvProp (which was added during the rebuttal) is not clear. In particular, both methods improve over AdvBN on ImageNet C. If standard BN is used for AdvBN on Stylized ImageNet, then both AugMix and AdvProp improve over AdvBN. That only leaves ImageNet Instagram as an ImageNet variant where AdvBN shows a clear improvement over AdvBN and AugMix. A potential solution is to try combining AugMix and AdvBN (not sure if AdvProp could be combined effectively) to see if there is a way to get maximum benefit out of these methods.  3  The empirical section could be improved by doing experiments in a systematic way. That is for any choices made in the experiment design, there should be a reason that is explained clearly. For example: 1) applying the same type of data augmentation on all methods (or reporting all methods with and without data augmentation). 2) compare to all baselines on ResNet 50 and then pick the top 2 baselines (say AugMix and AdvProp) and then compare them on DenseNet and EfficientNet. 3) comparing with the same baselines as (2) on the segmentation task.  Finally, I want to thank authors for engaging with reviewers, running many experiments during the rebuttal period and updating the paper accordingly. I also want to reassure authors that my final evaluation of the paper is based on: 1) reading all reviews and responses 2) weighing the reviews based on their substance, quality and engagement of reviewers 3) looking at the initial and final revision of the paper. In particular, even though the average score of this paper is low, in my opinion it is a borderline paper. After taking all of the above into account, my decision is to recommend rejection. Even though the proposed method is very interesting, there are three clear valid concerns all of which can be addressed as I suggested above. Without addressing those concerns, the empirical advantage of the proposed method is not demonstrated properly. I think after addressing those concerns this paper will be in a much better shape, more useful for ML community and hence receives the attention it deserves. I sympathize with authors that their efforts during the rebuttal period did not result in improving reviewers  scores but I want to emphasize that I did take all those updates into account when making my recommendation for this paper. 
The paper proposes a boosting algorithm for RL based on online boosting. The main advantage of the result is that the sample complexity does not explicitly depend on number of states. Post rebuttal, some of the reviewers have changed their opinion on the paper. However, overall the reviewers still seem to be on the fence about this paper. Seems like the paper combines the techniques from Hazan Singh’21 along with a frank wolfe algorithm to deal with non convex sets but the reviewers seem to view this as not as significant a new contribution.   I see the paper as being interesting but do agree with some of the comments of the reviewers and am leaning to a reject.
The paper presents a method for modeling videos with object centric structured representations. The paper is well written and clearly motivated. Using a Graph Neural Network for modeling latent physics is a sensible idea and can be beneficial for planning/control. Experimental results show improved performance over the baselines. After the rebuttal, many questions/concerns from the reviewers were addressed, and all reviewers recommend weak acceptance.
Dear authors,  While the reviewers appreciated the idea, the significant loss of accuracy was a concern. Even though you made significant changes to the submission, it is unfortunately unrealistic to ask the reviewers to do another review of a heavily modified version in such a short amount of time.  Thus, I cannot accept this paper for publication but I encourage you to address the reviewers  concerns and resubmit at a later conference.
The paper discusses an extension of BERT for learning user representations based on activity patterns in a self supervised setting. All reviewers have concerns about the validity of the claims and the significance of the experimental results. Overall, I agree with the reviewers that the paper needs more work to be published at ICLR. I recommend rejection.
The paper focuses on large scale multi agent reinforcement learning and proposes Learning Structured Communication (LSC) to deal issues of scale and learn sample efficiently. Reviewers are positive about the presented ideas, but note remaining limitations. In particular, the empirical validation does not lead to sufficiently novel insights, and additional analysis is needed to round out the paper.
Pros:   new multi objective approach to IRL   new algorithm   strong results   real world dataset  Cons:   straightforward theoretical extensions   unclear motivation   inappropriate empirical assessment metrics   weak rebuttal  All the reviewers feel that the paper needs further improvements, and while the authors comment on some of these concerns, their rebuttal and revised paper does not address them sufficiently. So at this stage it is a (borderline) reject.
This paper was reviewed by 4 experts in the field. The reviewers raised their concerns on lack of novelty, unconvincing experiment, and the presentation of this paper, While the paper clearly has merit, the decision is not to recommend acceptance. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere. 
The main contribution of this work is introducing large and carefully curated datasets for benchmarking morality judgments of language models. First of all, I d like to thank the reviewers for their detailed and thoughtful reviews and for being engaged in discussions with the authors. We believe that the paper is now much stronger than the initial submission.  The reviewers judged this work as important and largely well executed.  Some of them have initially raised concerns that the claims are too bold but these seem to have been addressed in the revisions and the rebuttal. R4 is still concerned that the ICLR format is not suitable / optimal for presenting a dataset. While we agree that journal format could be more suitable for this work, we do not see that as enough reason to reject the paper, especially given that the author invested much effort in providing extra details about the annotation and the underlying theories.   There are also suggestions to expand error analysis but this also seems to have been mostly addressed.     
This method makes a connection between evolutionary and variational methods in a particular model.  This is a good contribution, but there has been little effort to position it in comparison to standard methods that do the same thing, showing relative strengths and weaknesses.  Also, please shorten the abstract.
The authors propose a scale invariant sparsity measure for deep networks. The experiments are extensive and convincing, according to reviewers. I recommend acceptance.
The authors propose a network expandable approach to tackle NAS in the continual learning setting. More specifically, they use a RNN controller to decide which neurons to use (for a new task) and the additional capacity required (i.e., number of new neurons to add). This work can be viewed as an extension of RCL and as such suffers from the large runtime. This was a concern for most reviewers. While reviewers highlighted the gains in the experiments conducted, several questions remained regarding the efficiency of the proposed approach and how it compares to other strategies. The practical relevance of the proposed approach was also a concern as its application requires to restrict it to models of modest size. 
This paper studies different inductive biases that would improve OOD generalization (and in particular under translation, rotation and scaling) for image tasks. The study is focused on a toy dataset which allows authors to have more control over the data generation process and the transformations. Authors further show that iterative training using an auto encoder and presenting data in log polar space helps with rotation and scaling transformations on their toy dataset.   Strong Points:   The paper is well written and easy to follow.   The data generation process and the resulting toy dataset are novel and interesting.   The experiment design and evaluations are solid.  Weak Points:   No natural image datasets: While using a toy dataset has several benefits it does not grant that the conclusions would generalize to realistic settings. Reviewers have suggested several realistic datasets and I encourage authors to evaluate their findings on some of these datasets.   Limited Baselines: As reviewers have pointed, comparison with baselines can be improved by including stronger baselines as well as more clear discussion about other techniques such as augmenting the data with the transformations.   Related Work: A proper discussion of related work to set the context and highlight the contributions of this paper is missing. In particular, reviewers have pointed to prior work on the benefits of presenting the image in log polar space.  Unfortunately, authors did not engage with reviewers during the discussion period. Given the prior work and lack of any natural image dataset, I think the novelty and significance of this work is limited. Therefore, I recommend rejecting the paper. However, I encourage authors to improve the paper by addressing the points raised by the reviewers.
In spite of some slightly mixed scores (with one borderline positive review), scores are ultimately lukewarm and tend toward negative (and furthermore, reviews are broadly in agreement as to the issues they raise). Main issues center around low significance of the results, and issues with the presentation that need to be addressed.
The paper has two contributions. A novel benchmark for clinical multi modal multi task learning based on the already released MIMIC III and a multi modal multi task machine learning model. While the paper does show value in providing a curated benchmark and combining/unifying existing approaches to a timely problem, the reviewers agree that the paper provides insufficient novelty to warrant publication.
The authors propose a neural module based approach for reasoning about video grounding.  The goal is to provide performance and interpretability.  Unfortunately, the reviewers found the paper opaque, the results confusing, and expressed repeated concerns about the novelty, fairness of comparisons and concerns that the surprising results were not sufficiently well justified by the paper (or the author s response).
This paper received three recommendations of accept and one recommendation of reject.   The paper is mixed.  The results presented are both compelling and will have impact on the community.  The AC does not agree with R2 s views that the paper requires proposal of a novel method for acceptance.  At the same time, the AC also does not agree with the views of the other reviewers that the current experiments alone are enough to carry the paper without more conclusive statements.  As hinted by R3, simply pointing out the problems is not enough without proposing how to adjust our models and experimentation protocols in the future is insufficient.    In its current state, the paper would make for a good workshop submission.  Alternatively, the AC suggests to the authors to expand on the SimpleView baseline and or propose alternative solutions or protocols.
This paper proposes to combine RL and imitation learning, and the proposed approach seems convincing.    As is typical in RL work, the evaluation of the method is not strong enough to convince the reviewers.  Increasing community criticism on RL methods not scaling must be taken seriously here, despite the authors  disagreement. 
This paper establishes the guarantee for the generalization of fairness aware learning in binary classification under PAC learning and a more practical asymptotic framework. The paper is nicely written, and theorems and proofs are well organized. However, novelty of the contribution seems to be insufficient. A future version of the paper may benefit from additional theoretical results or more diverse experiments.
This paper proposes a framework for few shot font generation. Reviewers thought that the model was well suited to the task. They were split on clarity, with some saying that the paper was easy to follow and others saying that it lacked sufficient detail. Overall reviewers found the technical novelty limited, saying that the approach was a “transformer variant”, while it was the first to apply a Transformer like model on few shot font generation, there wasn’t sufficient novelty in this task to have broad appeal to the ICLR community. The reviewers also pointed out some deficiency in the evaluation, concerning the chosen metrics (multiple reviewers requesting fidelity metrics) and missing baselines. Some reviewers posed questions to the authors but the authors did not respond to the reviews. There is a clear consensus to reject the paper.
As one of the reviewers concisely summarized: This paper investigates maximum entropy (MaxEnt) inference and compares it to a Bayesian estimator and regularized maximum likelihood for finite models.   Two reviewers specifically question whether they have learned anything new after reading. This combined with various other drawbacks described during the review phase led to strong agreement among the reviewers about a variety of deficiencies in this paper. One reviewer initially gave a relatively high score but has since revised his/her opinion in light of the other reviews and discussion. I find that the significance of this work is not high enough to warrant acceptance at this time, but the authors would do well to incorporate the reviewers suggestions to improve the paper. 
The paper provides a unique contribution that uses Padde approximations to approximate non linear operators for solving initial value problems in PDEs. The paper contains also a non trivial experiment with a real world dataset that showcase the impact of the proposed model. The authors have provided a strong rebuttal and therefore I recommend Accept.
All reviewers except for AnonReviewer1 were in favour of accept.  AnonReviewer1 was strongly in favour of reject, but AnonReviewer2 argued against some of AnonReviewer1 s opinion.  The authors also gave a coherent, well argued statement of their contribution.  Nevertheless, there are some improvements still needed.  Position:   the scope of the uncertainty estimation to Dirichlet based uncertainty estimation techniques was limited.  Sticking to Dirichlet based uncertainty is limited, although the coverage of methods within the Dirichlet based family is OK but could be improved. Note (from AnonReviewer1 s comments) Joo Chung and Seo, ICML 2020, is one paper that should be included and Chan, Alaa, and van der Schaar, ICML2020 is also relevant.  While its not about adversial attacks it covers a related idea with a good technique.   Finally, these papers cite Ovadia, Fertig Ren etal. NeurIPS 2019, which is an excellent summary of calibration and estimation under shift, not exactly adversarial attacks but surely related.  The big winner is deep ensembles (Lakshminarayanan etal, NeurIPS 2017).  I think using deep ensembles directly would be a good complement to the Dirichlet methods in this paper. Note, also, the authors already included additional works mentioned by AnonReviewer2.  Critique:   The authors proposed a robust training strategy but this didn t lead to uniform improvement.  Position:  The scope of the adversarial attacks is limited.  The attacks covered are a good though basic range.  But because these show problems, the argument is that more sophisticated attacks do not need to be studied.  Position:  The datasets covered is limited.  Certainly, there are problems with extending experiments to text data.  But the argument is that if things don t work well for the smaller datasets given, then that is still a problem, so why bother extending the evaluation to larger datasets.  Arguably, the latter two positions have been addressed by the authors, but not the first two.  This makes the paper marginal. So this is a good publishable paper, but comparatively marginal.
This paper proposes a model parallelism scheme (CMP) for training differentiable NAS with large supernets, which performs the forward and backward passes for multiple tasks at the same time, to increase hardware utilization. Moreover, since CMP consumes large GPU memory due to having multiple computational graphs in memory at the same time, the authors further propose binary neural architecture search (NASB), which binarizes the parameters and gradients to reduce memory footprints and computations. The experimental validation shows that the proposed parallelization technique is more efficient than prior parallelization techniques and can significantly reduce the search cost of differentiable NAS methods. Specifically, the proposed NASB CMP yields architectures with competitive performance on CIFAR 10, at lower search cost compared with baselines that have memory reduction mechanisms such as PC DARTS.  This paper received split reviews, with the majority of the reviewers leaning toward rejection (three 5’s) and one leaning positive (6). The reviewers in general agreed that the problem of achieving resource and time efficiency for NAS is important, and that the proposed idea of consecutive model parallelism is novel and may have some practical impact. The reviewers also found the paper to be mostly clear and well written.  However, reviewers had a common concern that the experimental validation is weak, since 1) the improvement in search cost seems less meaningful with small workloads such as CIFAR 10, 2) some important baselines are missing, and 3) unclear contribution of CMP and NASB due to a missing ablation study. During the interactive discussion period, the authors provided results on additional baselines (NAO and AlphaX), and intermediate results of ImageNet experiments which shows that the proposed method is faster than the baseline. Yet, the reviewers kept their original ratings even after the internal discussion, as they found the incomplete experiments and missing ablation study unsatisfactory.  In summary, this is a well written paper proposing a novel idea to tackle the resource and time efficiency of differentiable NAS, which is a practically important problem. Yet, the experimental validation is too weak to validate the effectiveness and practicality of the proposed method, and thus it seems like a preliminary work not yet ready for publication. However, the work is well motivated and promising, and addressing the reviewers’ common concerns on missing large scale experiments and ablation study will make the paper stronger and significantly increase its chance of getting accepted in the next submission. 
The paper proposes the physics informed neural operator. It combines the operating learning and function optimization frameworks, which improves convergence rates and accuracy over traditional methods. While the paper was well written, several reviewers raised their concerns on the novelty of the paper, especially regarding the difference from PINN DeepONet (Wang et. al.). Following this, there have been a long discussion between the authors and the reviewers, as well as among the reviewers. As a consequence, we think the authors somehow overclaimed their contributions on combining PINN and operator learning, and there are some important references missing and baselines not compared empirically. With this, the conclusion is that we cannot accept this paper in its current form, and we hope that authors can take all the review feedback into consideration and better position the novelty and impact of their work in the future submissions.
This paper addresses the problem of rotation estimation in 2D images. The method attempted to reduce the labeling need by learning in a semi supervised fashion. The approach learns a VAE where the latent code is be factored into the latent vector and the object rotation.   All reviewers agreed that this paper is not ready for acceptance. The reviewers did express promise in the direction of this work. However, there were a few main concerns. First, the focus on 2D instead of 3D orientation. The general consensus was that 3D would be more pertinent use case and that extension of the proposed approach from 2D to 3D is likely non trivial. The second issue is that minimal technical novelty. The reviewers argue that the proposed solution is a combination of existing techniques to a new problem area.   Since the work does not have sufficient technical novelty to compare against other disentanglement works and is being applied to a less relevant experimental setting, the AC does not recommend acceptance.  
 This paper addresses a crucial problem with graph convolutions on meshes.  The authors identify the issues related to existing networks and devise a sensible approach. The work presents a novel message passing GNN operator for meshes that is equivariant under gauge transformations. The reviewers unanimously agree on the both the importance of the problem and the impact the proposed work could have.   Suggestions for next version:   The paper is unreadable without the appendix and somehow it would be better to make it self contained   Additional references as suggested in the reviews.   Expanded experiments as suggested by R4, will also improve reader s confidence in the method.   I would recommend acceptance. I would request the authors to release a sufficiently documented and easy to use implementation. This not only allows readers to build on this work but also increase the overall impact of this method.
The paper presents a method to regularize the discriminator in  GAN training with a ranking loss based on the user preference for a desired set within a larger dataset. The tradeoff between GAN loss and preference loss dependence on the distance of the set to the full dataset and the authors consider two regimes : "small and major correction". A major correction is needed when the targeted set is very different from the whole density, authors propose in this scenario to replace samples from the data by samples from the generator. The setting in the paper is interesting and can be useful in practice.   There was a lengthy discussions between the authors and the reviewers, the discussion pinpointed issues , some of them were addressed in the rebuttal . Some issues remain unanswered regarding the clarity and some claims in the paper.  The clarity of the paper needs further improvement and  1)  clarify section 3  the setup and the background section   2)  justify claims about the method, in  the strong correction scenario  when fresh generated samples are introduced how  is this an effective procedure? (conceptually / theoretically).  
This paper proposes to modify DETR, a recent Transformer based architecture for object detection. More precisely,  they propose to sparsify input feature maps by learning an extra classifier to select which input features (few of them) will be used in the attention module. The supervision of this classifier is guided by second extra module  coming from the Transformer decoder attention weights. The resulting framework, called Sparse DETR, is an efficient end to end object detection architecture that allows to overcome the main computational bottleneck of DETR. Sparse DETR can use  only 10% 50% of the original encoder query while achieving DETR comparable results.  Authors tried to answer to all the questions raised during the rebuttal.  Even if the final scores are still contrasted, most of reviewers are very positive.  Overall, this paper provides valuable insight into reducing computational complexity (by reducing the number of queries) for DETR like detectors. The proposed method is novel and technically sound. Even if there are some tricks to make the whole working well, this work is more effective and efficient than previous propositions to handle this complexity problem,  bringing us some new insight of how to sparsify queries without performance dropping.   All these elements lead me to propose this paper for publication at ICLR.
This paper investigated using bio inspired cumulative fatigue model to improve bipedal locomotion via deep RL. The proposed method marginally improved bipedal locomotion behavior. The size of the experiments should be improved, together with generalization to other symmetric walkers. AC agrees with the reviewers that the empirical performance is not significant enough. The paper may fit the scope of a bipedal locomotion journal/conference better than ICLR.
The paper presents a novel view on adversarial examples, where models using ReLU are inherently sensitive to adversarial examples because ReLU activations yield a polytope of examples with exactly the same activation. Reviewers found the finding interesting and novel but argue it is limited in impact. I also found the idea interesting but the paper could probably be improved as all reviewers have remarked. Overall, I found it borderline but probably not enough for acceptance.
This paper shows how to implement a low rank version of the Adagrad preconditioner in a GPU friendly manner. A theoretical analysis of a "hard window" version of the proposed algorithm demonstrates that it is not worse than SGD at finding a first order stationary point in the nonconvex setting. Experiments on CIFAR 10 classification using a ConvNet and Penn Treebank character level language modeling using an LSTM show that the proposed algorithm improves training loss faster than SGD, Adagrad, and Adam (measuring time in epochs) and has better generalization performance on the language modeling task. However, if wall clock time is used to measure time, there is no speedup for the ConvNet model, but there is for the recurrent model. The reviewers liked the simplicity of the approach and greatly appreciated the elegant visualization of the eigenspectrum in Figure 4. But, even after discussion, critical concerns remained about the need for more focus on the practical tradeoffs between per iteration improvement and per second improvement in the loss and the need for a more careful analysis of the relationship of this method to stochastic L BFGS. A more minor concern is that the term "full matrix regularization" seems somewhat deceptive when the actual regularization is low rank. The AC also suggests that, if the authors plan to revise this paper and submit it to another venue, they consider the relationship between GGT and the various stochastic natural gradient optimization algorithms in the literature that differ from GGT primarily in the exponent on the Gram matrix.
This work presents a distributed SVGD (DSVGD) algorithm as a new non parametric Bayesian framework for federated learning. The reviewers concerned with the practical advantages of the proposed method, including the communication cost and the constraint of updating one agent per time. The authors rebuttal helped addressing some of the concerns, including proposing a new Parallel DSVGD algorithm. This is very much appreciated. However, given the significant modification needed over the original version, we think it is better for the authors to further improve the work and submit to the next conference. 
The paper studies a high order discretization of the ODE corresponding to Nesterov s accelerated method, as introduced by Su Boyd Candes. The main claim of the paper is that the more complex discretization scheme leads to a method that is more stable and faster. However, the theoretical claims do not seem sufficiently supported, and the experimental results are insufficient to judge the usefulness of the proposed approach. Thus, the reviews could not recommend acceptance, and I concur. The authors are advised to revise the paper to provide more theoretical and experimental evidence for usefulness/competitiveness of the proposed approach, and resubmit to a different venue. 
This paper studies the impact of using momentum to escape saddle points. They show that a heavy use of momentum improves the convergence rate to second order stationary points. The reviewers agreed that this type of analysis is interesting and helps understand the benefits of this standard method in deep learning. The authors were able to address most of the concerns of the reviewers during rebutal, but is borderline due to lingering concerns about the presentation of the results. We encourage the authors to give more thought to the presentation before publication.
In this paper, a defense method against test time adversarial ML attacks is proposed. Unfortunately, it is not clear whether the proposed method is practically useful or not, because the types of attacks assumed in this paper are too simple and heuristic. Also, the position of the proposed method in the vast amount of existing research on adversarial attacks is not clear. Although the proposed method is conceptually interesting, no evidence is provided that the proposed method is significantly superior to existing approaches.
This paper uses a variant of parallel tempering to tune the subset of neural net hyperparameters which control the amount of noise and/or rate of diffusion (e.g. learning rate, batch size). It s certainly an appealing idea to run multiple chains in parallel and periodically propose swaps between them. However, I m not persuaded about the details. The argumentation in the paper is fairly informal, and it uses ideas from optimization and MCMC somewhat interchangeably. Since the individual chains aren t sampling from any known stationary distribution, it s not clear to me what MH based swaps will achieve.   The authors are upset with one of the reviews and think it misrepresents their paper. However, I find myself agreeing with most of the reviewer s points. Furthermore, as a general principle, the availability of code doesn t by itself make a paper reproducible. One should be able to reproduce it without the code, and one shouldn t need to refer to the code for important details about the algorithm.  Another limitation (pointed out by various reviewers) is that there aren t any comparisons against prior work on hyperparameter optimization. Overall, I think there are some promising and appealing ideas in this submission, but it needs to be cleaned up before it s ready for publication at ICLR. 
This work considers an apparent problem with current approaches to compositional generalisation (CG) in neural networks. The problem seems to be roughly: 1. prior work in CG aims to extract  compositional representations  from the training distribution 2. work on CG, the training set and the test set are drawn from different distributions therefore 3. we don t know whether these models can also extract compositional representations from the test distribution  All four expert reviewers were, to differing degrees, confused by this problem framing, largely because they consider the premise (1) to be false.   I am also aware of a large body of recent work on CG in neural networks (see those papers listed by R2) and, as far as i know, none of it involves extracting  compositional representations  from the training set. Rather, it involves learning something (from the training set) that enables strong performance on a test set that differs from the training set in a way that is informed by ideas of compositionaity.   As far as I know, there are very few  studies that try to identify compositionality by considering the internal representations of neural networks, so it feels incorrect to claim this is standard practice. Any work that goes down this route ought to have a very thorough treatement of the various thorny philosophical and theoretical treatments of compositionality in the literature. As pointed out by R4, the work in its current form does not do this.   In summary, this work attempts to solve a problem that none of the four expert reviewers consider to be in need of a solution. 
The manuscript proposes a meta attention based mechanism for improving off policy actor critic algorithms. Instead of introducing attention into networks at the level of pixels or multiple sources of information, this work focuses on using attention between features from the actor network (which become queries and values) and features from the critic network (which act as keys). Attention produces new features that are given as input to the action net, enabling it to potentially improve it s action selection. The attention is trained using a meta learning objective that encourages outputting features that help other parts of the architecture to learn. Reviewer note that there are some positive features of the paper. It is relatively well written and the figures are useful for spelling out the approach. In addition, so felt that the basic idea is an interesting one. However, there was general agreement that the manuscript is not ready for publication.  Most reviewers noted that, the newly proposed architecture and learning rules were not well motivated by the manuscript. Why was this particular approach pursued? Is there any better theoretical justification than can be offered? These questions on their own are not problematic. However, the empirical work does not robustly demonstrate that the algorithm yields a clear performance gain over baseline actor critic methods in the literature. Most of the tasks are relatively simple and those gains that are observed are marginal. This is especially difficult for the manuscript given the increased compute and complexity of implementation required by the method. Finally, several reviewers were concerned about the presentation of some of the technical aspects of the works, potentially making it difficult to replicate important aspects of the work. In sum, the manuscript is not ready for publication.
The paper considers a learning problem to determine the best low precision configuration within the memory budget.  It is an interesting problem that could be of interest to the community.  Overall, the reviewers were fairly positive on the paper and believe the paper give interesting insights into how to use limited memory for learning.
This paper proposes to jointly learn a mapper and planner for navigation or manipulation in a 2D space represented by an MxM grid, with the mapper taking raw observations as inputs and producing a 2D MxM occupancy and goal location map, and the planner   pretrained on generic 2D maps of same size MxM   produces an MxM action distance image (the plan). The whole system is trained end to end, with the mapper trained on the specific navigation or manipulation task, and the planner frozen. The Spatial Planning Transformer network can predict the MxM action plans faster than baselines (Value Iteration Networks and Gated Path Planning Networks) because of the attention mechanism in the transformer architecture, as opposed to the local information propagation through the convolutional encoding of the Bellman equation in VIN or GPPN. The differentiable approach is motivated by exploiting regularities in 2D maps and a faster inference time (as opposed to classical Dijkstra planners or VIN / GPPN), and is demonstrated on a simple room navigation task in the Gibson environment.  Reviewers have praised how a simple idea (transformer based planning à la VIN) can be applied as a unifying approach to 2 DOF manipulation and 2D navigation, and the out of sample evaluation. The critique was about: * the lacunary explanation of the mapper (corrected by the authors), * the limits of non recurrent mappers that cannot handle occlusions in the observations when building maps, * confusion about where the full MxM image of action labels (necessary for supervision) can come from (the authors added experiments with sparse labels), * claims about this approach being preferable to A* (countered by the authors, who conducted extensive experiments at the request of R4), * focus on two problems at once (mapping and planning) rather than more ablation analysis of the planner, with a potential comparison to Active Neural SLAM, * missing evaluation using navigation specific metrics, such as SPL.  The authors have a point in their defense of differentiable / learning based methods for planning (including VIN and GPPN) as opposed to classical planning such as A*, and there is value in investigating how an end to end differentiable method for mapping and planning could be designed. At the same time, several reviewers raised concerns about scalability and about the pertinence of combining two learnable modules (mapper and planner) rather than investigating and demonstrating the advantage of transformers for planning.  Given these reviews, rebuttal, and remaining concerns, I am sorry to reject this paper. I hope that with the proposed modifications it will be quickly accepted at another venue.
This paper analyzes and extends learning methods based on Policy Spaced Response Oracles (PSRO) through the application of alpha rank.  In doing so, the paper explores connections with Nash equilibria, establishes convergence guarantees in multiple settings, and presents promising empirical results on (among other things) 3 to 5 player poker games.  Although this paper originally received mixed scores, after the rebuttal period all reviewers converged to a consensus. A revised version also includes new experiments from the MuJoCo soccer domain, and new poker results as well.  Overall, this paper provides a nice balance of theoretical support and practical relevance that should be of high impact to the RL community. 
This paper tackles the problem of transferring learning between tasks when performing Bayesian hyperparameter optimization. In this setting, tasks can correspond to different datasets or different metrics. The proposed approach uses Gaussian copulas to synchronize the different scales of the considered tasks and uses Thompson Sampling from the resulting Gaussian Copula Process for selecting next hyperparameters.  The main weakness of the paper resides in the concerns raised about the experiments. First, the results are hard to interpret, leading to a misunderstanding of performances. Moreover, the considered baselines may not be adapted (they may be trivial). This might be due to a misunderstanding of the paper, which would align with the third major concern, that is the lack of clarity. These points could be addressed in a future version of the work, but it would need to be reviewed again and therefore would be too late for the current camera ready.  Hence, I recommend rejecting this paper.
The paper proposes a method to predict protein functions from Gene Ontology (GO) and protein sequences. The protein sequences are embedded with a pretrained protein language model (SeqVec) and the GO network is modelled with a graph convolutional neural network.  Reviewers found the paper well written and structured. At the same time, they found the novelty of the paper limited. Two reviewers pointed out that the paper is very similar to DeepGOA, which the authors cite but don t compare against. Overall, there is consensus among the reviewers that the paper is not suitable for ICLR.  The authors didn t submit a rebuttal.  We encourage the authors to take into account reviewer comments to improve the paper. Since it is more on the application side, perhaps a computational biology conference / workshop would be more appropriate for this paper.
The consensus recommendation is that the paper is not ready for publication at this time.
The paper receives a mixed rating, with R3 rates the paper above the bar, R1 and R2 rates marginally above the bar, and R4 recommends rejection. The cited positive points include 1) decomposing image generation into first synthesizing segmentation masks and then converting segmentation masks to images, and 2) good results comparing to Progressive GAN and BigGAN. R4 raises several concerns, including the novelty concern and unconvincing experimental validation. After analyzing the papers, the reviews, and the rebuttal, the AC finds the arguments made by R4 more convincing. Decomposing image generation to a two step approach has been illustrated in the prior work [Wang & Gupta ECCV 2016, Hong, Yang, Choi, Lee CVPR 2018]. The proposed method does not provide additional insights. The provided experimental results are not very convincing, either. As the proposed setting assuming the availability of segmentation masks, it is not surprising that it outperforms the unconditional baselines. Overall, the AC believes the paper does not have enough novelty to justify its acceptance and would recommend rejection of the paper.
This paper offers a disentangled pose and identity representation for image to image translation.  The reviewers are borderline, but the AC finds the discussion by reviewer SuTz compelling, and agrees that the authors missed key references in the submitted manuscript.  In their rebuttal, the authors acknowledged the references were relevant, but believed their paper is not in the same area.  Overall this paper is borderline, but just below the threshold for acceptance in the opinion of this AC.
This work proposes a new strategy for prioritized experience replay. It is based on the argument that the TD error itself may not be a good indicator for priority, so we should rely on other factors that are easier and more reliable to learn. The new method is based on two modifications: (1) modifying the critic s objective so that it learns a good model of the environment (reward and transition dynamics) as well, and (2) use the combination of the TD loss and the model loss in order to define the priorities in the ER queue.  The majority of reviewers are positive about this work. They believe the method is novel and the experiments are extensive. The authors improved the paper during the discussion phase, so many of the questions have already been answered.  There are some concerns though, some of them shared by reviewers and some after my own reading of the paper:  One concern is about the justifications for the method, which are based on the heuristic and intuitive arguments, rather than principled approach. Currently it is not clear, at least to me, why adding a model error to the objective is a good idea.  Another concern, which is not shared by the reviewers, is that there is much overlap in the confidence band of figures and confidence intervals of tables. For example, in many of the subfigures of Figure 2 or 4, there is a significant overlap in the shaded areas. Or many of the numbers in Table 1 (with and without MaPER) are within each others  confidence intervals. Are the results statistically significant?  Another comment, again not shared by reviewers, is regarding how the loss functions are defined. Consider the loss in Eq. (1):   Is there a squared missing? I assume that it is missing. Although it does not matter at this stage, when you add other terms to the loss, it would, i.e, the minimizer of $f(x) + g(x)$ is not necessarily the same as the minimizer of $f^2(x) + g^2(x)$.    Is the target value based on a fixed parameter (not optimized), or do you actually consider the expected of the TD error, which would be equal to the empirical Bellman error. If the latter, it would be a biased estimate of the Bellman error. And it is not what DQN or the TD method optimizes (that s why Sutton and Barto s textbook calls them pseudo gradient).  These requires some clarifications.  Another question related to the model: Is it assumed that the model $T_\theta$ is deterministic and predicts a next state, as opposed to predicting a distribution over them? (cf. equations after (5) )?  All strengths and concerns considered together, I believe this is a good paper overall, and can be accepted at ICLR. Hopefully we get a better understanding of what this method is actually doing in the future research.  I have the following suggestions to the authors:   Perform statistical significant tests on your results. In some cases, it might be helpful to increase the number of runs from five to a larger number. It may also be more visually clear to provide standard error instead of standard deviation.   Clarify issues about the definition of the loss function.   Please consider improving and clarifying your argument of why you method works.   Please consider the remaining comments by reviewers in order to improve your paper.
There is significant discussion on this paper and high variance between reviewers:  one reviewer gave the paper a low score.  However the committee feels that this paper should be accepted at the conference since it provides a better framework for reproducibility, performs more large scale experiments than prior work.  One small issue the lack of comparison in terms of empirical results between this work and Zhang et al s work, but the responses provided to both the reviewers and anonymous commenters seem to be satisfactory.
This paper describes how multi agent reinforcement learning at scale leads to the evolution of complex behaviors. Actually, "at scale" may be an understatement   a lot of computing power was used here. But the amount of compute used is not the point, rather the point is that complex and fascinating behavior can emerge from a long co evolutionary process (though gradient based RL is used here, the principle is the same) where the arms race forms an implicit curriculum. This is the existence proof that people in artificial life and adaptive behavior have been looking for for so long.   Two reviewers were positive about the paper, with a third being negative because the paper does not give any new insights about how to do RL at scale. But that was not the stated aim of the paper, as the authors clarify in a response.  This paper will draw quite some attention and deserves an oral presentation.
The work presents a modification to existing approaches of automatic learning rate adaptation (called TLR) via a second order approximation of the function mapping step size to the change in loss when optimized with SGD. This was easily the most controversial paper in the AC s stack, with 4 reviewers advocating accept and 2 reviewers strongly arguing for reject. The authors also went through considerable effort to address reviewer and AC concerns and uploaded multiple additional experiments and ablations to support the robustness and efficacy of the proposed method. Despite a long discussion and rebuttal period, reviewers were unable to reach a consensus. There were several different aspects of the work whose merits were thoroughly debated during the rebuttal period.  The first aspect regarded what the exact contribution of the work was. Initial reviewers who were very high on the work believed that the entire derivation from equation (1) to equation (9) was novel. However, as other reviewers correctly pointed out (1) to (7) is a standard derivation of adaptive learning rates and has appeared in several prior works. Instead it is primarily equation (9) that is the contribution. Given that multiple reviewers initially believed that (1  > 7) was a novel contribution, I feel it is safe to say that the authors do not adequately discuss their contributions with respect to prior work. However, all reviewers in the end agreed that equation (9) is novel and potentially interesting (though some remain skeptical of it s utility).  The second topic of debate regarded the short horizon bias raised by reviewer hkZ3. The short horizon bias presents a fundamental barrier to meta optimization of the learning rate. To summarize, greedily selecting the step size to minimize the loss will result in the optimizer taking too small of steps in the flat directions of an ill conditioned loss surface. This results in faster training in the short term but slower training in the long run. The presented method seeks to greedily optimize the loss over short time scales and thus will be subject to the short horizon bias. The initial draft of the work did not include any discussion of this prior work. During the rebuttal, the authors initially argued that their method can help mitigate the short horizon bias before later concluding that it is a limitation of the method. There was debate between the AC and reviewers regarding whether or not existing methods of adaptive learning rate schedules were already at a fundamental barrier presented by the short horizon bias. One reviewer even mentioned that in their own research they have abandoned the general approach of adaptive learning rates because they cannot overcome this issue. This debate was never resolved, it s plausible to the AC that there is room for increasing the robustness of existing approaches while not addressing the short horizon issue. It is the AC s opinion however that the work would be significantly strengthened with experiments directly addressing the short horizon issue.  The final item of debate regarded the strength of the considered baselines. The authors claim that the second order term in (eq 9) largely removes the need for tuning relative to Baydin et. al. and that the method outperforms multiple baselines across multiple workloads, including Adam (Kingma et. al.), SLS (Vaswani et. al.), and SPS (Loizou et. al.). Indeed multiple plots are given showing that the authors have found a configuration of their method that consistently outperforms certain fixed configurations of the considered baselines. Furthermore, ablations are presented which suggest that indeed it is the addition of equation (9) that is responsible for this strong performance. Despite all of this presented evidence some reviewers remained skeptical, and believed that they could produce a different but fixed configuration of say Baydin et. al. (or even Adam) which matched the proposed method on all of the considered workloads. There are compelling reasons for reviewers to consider the presented experiments with skepticism. Indeed the deep learning optimization literature has for years struggled to make progress despite publishing hundreds of papers see for example the results of [1] which perform an independent comparison of 100 s of published methods and found that none convincingly outperform Adam. Given this, it is clear that the current standard for evaluating optimizers in the literature is inadequate if we are to reliably make progress.  To give a more relevant example of the difficulty of comparing optimization methods, suppose for the sake of argument that we were not evaluating the efficacy of TLR but instead the method of Vaswani et. al. (SLS). Vaswani et. al. makes many similar claims as the proposed method, namely the method consistently outperforms Adam across multiple workloads and enjoys a similar robustness to hyperparameters (e.g. their Figure 6). However, in Figure 3 and 4 of this work we see SLS no longer outperforms Adam on the considered workloads. If Vaswani et. al. had argued for acceptance based on the author s Figure 3 and 4, I don t think any reviewer would have recommended acceptance. This begs the obvious question, why does SLS consistently outperform Adam in the experiments run by Vaswani et. al., but not in the experiments run in the considered paper? There are at least two possible answers here, both of which are concerning. Either Vaswani et. al. is yet another method that generally doesn t outperform Adam or in comparing SLS with TLR in this work the authors did not properly tune SLS in their baselines. Furthermore, what is going to happen if future work tries to compare against TLR? Will TLR still look better than Adam or will independent review find that TLR is yet another method that on average performs about as well as Adam? As a reviewer trying to compare the two papers I see very similar evidence given supporting the two methods and thus I am left with an unresolved contradiction.  Given all of this, I am forced to conclude that there is insufficient evidence presented in this work that the proposed method generally outperforms related methods such as SLS and Adam. A natural question thus is, what would have been sufficient evidence? Indeed the presented experiments seem to be about as convincing as what is shown in previous published methods such as SLS. In a sense the AC is also arguing that Vaswani et. al. presented insufficient evidence that SLS generally outperforms Adam (looking at Figure 3 and 4 perhaps SLS in fact isn t as useful as Vaswani et. al. claim). In looking at the experiments presented in this work, related prior works, and the 100 s of methods considered in [1] a common recurring theme is that when comparing with prior work authors consistently run their own implementation of baselines on workloads of their choosing, rather than directly comparing with published results. In doing so, this leaves open the question regarding whether or not the authors (perhaps inadvertently!) are only considering workloads and hyperparameter settings which favor their own method rather than giving a realistic assessment of the efficacy of their own methods relative to others. Thus, if the authors wish to argue that TLR generally outperforms SLS, a strong piece of evidence the authors could provide is to run TLR directly on the open sourced code provided by Vaswani et. al. Show the reviewer how TLR compares when added directly to (for example) Vaswani et. al. Figure 4. In doing so, the authors will have addressed any concerns reviewers may have about how well represented SLS is, as the authors will be comparing against SLS in a setting where there were actual incentives to make SLS look good.  1. Schmidt et. al. Descending in a crowded valley – Benchmarking Deep Learning Optimizers, https://arxiv.org/abs/2007.01547
The pros and cons of the paper are summarized below:  Pros: * The proposed tweaks to the dynamic evaluation of Mikolov et al. 2010 are somewhat effective, and when added on top of already strong baseline models improve them substantially  Cons: * Novelty is limited. This is essentially a slightly better training scheme than the method proposed by Mikolov et al. 2010. * The fair comparison against Mikolov et al. 2010 is only shown in Table 1, where a perplexity of 78.6 turns to a perplexity of 73.5. This is a decent gain, but the great majority of this is achieved by switching the optimizer from SGD to an adaptive method, which as of 2018 is a somewhat limited contribution. The remainder of the tables in the paper do not compare with the method of Mikolov et al. * The paper title, abstract, and introduction do not mention previous work, and may give the false impression that this is the first paper to propose dynamic evaluation for neural sequence models, significantly overclaiming the paper s contribution and potentially misleading readers.  As a result, while I think that dynamic evaluation itself is useful, given the limited novelty of the proposed method and the lack of comparison to the real baseline (the simpler strategy of Mikolov et al.) in the majority of the experiments, I think this papers till falls short of the quality bar of ICLR.  Also, independent of this decision, a final note about perplexity as an evaluation measure to elaborate on the comments of reviewer 1. In general, perplexity is an evaluation measure that is useful for comparing language models of the same model class, but tends to not correlate well with model performance (e.g. ASR accuracy) across very different types of models. For example, see "Evaluation Metrics for Language Models" by Chen et al. 1998. The method of dynamic evaluation is similar to the cache based language models that existed in 1998 in that it reinforces the model to choose similar vocabulary to that it s seen before. As you can see from this paper that the quality of perplexity of an evaluation measure falls when cache based models are thrown into the mix, and one reason for this is that cache models, while helping perplexity greatly, tend to reinforce previous errors when errors do occur.
The focus on this paper s proposed FILIP method is to perform word patch alignment by token wise similarity matrix through cross modal late interaction by modifying only contrastive loss, leading to training and inference efficiency. The authors also collected FILIP300M, a large scale cleaned image captioning dataset for FILIP’s V L pre training. FILIP achieves strong performance on zero shot image classification and image text matching tasks, and the paper also visualizes the ability of fine grained (visual textual token) classification and localization. Overall most of the reviewers appreciated the idea and the generalization results, but had some concerns about not enough technical novelty over the Khattab and Zaharia Colbert paper, which this paper adopts for multimodal tasks. Some reviewers also had concerns about the dataset release but the authors promise to address this. Some reviewers were also not fully convinced about the high storage requirements and scalability for some of the retrieval tasks that the authors tested.  NOTE: The authors are also asked to describe any ethical considerations or issues that arise in their large scale dataset collection in the camera ready version of the paper, see https://arxiv.org/abs/2110.01963 for examples.
The authors propose a new method for deepfake detection (ENST) which relies on high frequency information, low level/shallow features, and optical flow. In particular, EfficientNet B5 is used to extract the high frequency info and shallow features, and a Swin Transformer to capture discrepancies between optical flows. Empirical validation on FaceForensics++ and Celeb DF shows some improvements over the baselines.  The reviewers found this to be a relevant and timely topic. The reviewers also found that integrating information from the frequency domain, the spatial domain, and optical flow is a promising approach. There were three reviewers suggesting rejection, and one suggesting acceptance. After the rebuttal and discussion phase, the following remaining issues were highlighted:    **Limited technical novelty** (nearly all components used in this work were already expired in other work).   Underwhelming empirical improvements given the fact that the model uses EfficientNet B5 and the SwinTransformer.    Many claims are still not supported by empirical evidence. For instance, to claim generalisation, an extensive analysis, including more datasets as well as competing methods should be carried out.
The submission presents an approach that leverages machine learning to optimize the placement and scheduling of computation graphs (such as TensorFlow graphs) by a compiler. The work is interesting and well executed. All reviewers recommend accepting the paper.
This paper studies the landscape of linear networks and its critical point. The authors utilize geometric properties of determinantal varieties to derive interesting results on the landscape of linear networks. The reviewers raised some concerns about the fact that many of the results stated here can already be achieved using other techniques and therefore had some concerns about the novelty of these results. The authors provided a detailed response addressing these concerns. One reviewer however still had some concerns about the novelty. My own understanding of the paper is that while some of these results can be obtained using other approaches the proof techniques (brining ideas from algebraic geometry) is novel and could be rather useful. While at this point it is not clear that the techniques generalize to the nonlinear case I think algebraic geometry perspective have a good potential and provide some diversity in the theoretical techniques. As a result I recommend acceptance if possible.
The authors supplied an updated paper resolving the most important reviewer concerns after the deadline for revisions. In part, this was due to reviewers requesting new experiments that take substantial time to complete.  After discussion with the reviewers, I believe that if the revised manuscript had arrived earlier, then it should be accepted. Without the new results I would recommend rejecting since I believe the original submission lacked important experiments to justify the approach (inductive setting experiments are very useful).  The community has an interest in uniform application of the rules surrounding the revision process. It is not fair to other authors to consider revisions past the deadline and we do not want to encourage late revisions. Better to submit a finished piece of work initially and not assume it will be possible to use up a lot of reviewer time and fix during the review process.  We also don t want to encourage shoddy, rushed experimental work. However, the way we typically handle requests from reviewers that require a lot of work to complete is by rejecting papers and encouraging them to be resubmitted sometime in the future, typically to another similar conference.  Thus I am recommending rejecting this paper on policy grounds, not on the merits of the latest draft. I believe that we should base the decision on the state of the paper at the same deadline that applies to all other authors.  However, I am asking the program chairs to review this case since ultimately they will be the final arbiters of policy questions like this.
This paper has initially received mixed reviews, with two favorable and two unfavorable reviews. Several serious issues have been raised, in particular on experiments and validation; limited novelty; limited performance improvement; on the preliminary stage of the paper, in particular presentation and writing, and on the justification of key choices.  The authors provided responses to some of these issues, but in the discussion phase the reviewers (and the AC) judged the the responses did not sufficiently address the weaknesses of the paper, in particular:   The experimental setup does not assess the key innovation of the paper.    Several contributions claimed by the authors (in the paper and in the response) are judged not to be novel.   Concerns regarding the metric chosen to measure smoothness. and other issues.  The reviewers and AC agreed, that the paper has potential and merits, but that at at this point it is not yet ready for publication.
This paper studies the problem of certified robustness to adversarial examples. It first demonstrates that many existing certified defenses can be viewed under a unified framework of regularization. Then, it proposes a new double margin based regularizer to obtain better certified robustness.  Overall, it has major technical issues and the rebuttal is not satisfying.
The paper proposes a graph convolution operator (BankGCN) to be used in graph neural networks. The reviewers mainly raised concerns about the limited of novelty in the light of numerous previous works that are similar or address similar problems as well as lacking evaluation. While the rebuttal addressed some of the concerns, the overall impression is that the paper is not of sufficient methodological or experimental significance for the conference.
This work applies deep kernel learning to the problem of few shot regression for modeling biological assays. To deal with sparse data on new tasks, the authors propose to adapt the learned kernel to each task. Reviews were mixed about the method and experiments, some reviewers were satisfied with the author rebuttal while others did not support acceptance during the discussion period. Some reviewers ultimately felt that the experimental results were too weak to warrant publication. On the binding task the method is comparable with simpler baselines, and some felt that the gains on antibacterial were unconvincing.  Other reviewers felt that there remained simpler baselines to compare with, for example ablating the affects of learning the kernel with simple hand picking one. While authors commented they tried this, there were no details given on the results or what exactly they tried.   Based on the reviewer discussion, the work feels too preliminary in its current form to warrant publication in ICLR. However, given that there are clearly some interesting ideas proposed in this work, I recommend resubmitting with stronger experimental evidence that the method helps over baselines.
This paper proposes PAC_Bayesian bounds for negative log likelihood loss function. A few reviewers raised concerns around 1) distinguish their contributions better from prior work (eg Alquier). 2) confounders in their experiments. Both reviewers agreed that the paper, as it is written, does not provide sufficient evidence of significance. In addition, experiments shown in the paper varies two things   # parameters (therefore expressiveness and potential generalizability) and depth at each setting. As pointed out, this isn’t right   in order to capture the effect, one has to control for all confounders carefully. Another concerned raised were around Theorem 2   that it contains data distribution on the right hand side, which isn’t all that useful to calculate generalization bounds (we don’t have access to the distribution). We highly encourage authors to take another cycle of edits to better distinguish their work from others before future submissions.  
Paper https://arxiv.org/abs/1802.10026 (Garipov et. al, NeurIPS 2018) shows that one can find curves between two independently trained solutions along which the loss is relatively constant. The authors of this ICLR submission claim as a key contribution that they show the weights along the path correspond to different models that make different predictions ("Note that prior work on loss landscapes has focused on mode connectivity and low loss tunnels, but has not explicitly focused on how diverse the functions from different modes are, beyond an initial exploration in Fort & Jastrzebski (2019)"). Much of the disagreement between two of the reviewers and the authors is whether this point had already been shown in 1802.10026.  It is in fact very clear that 1802.10026 shows that different points on the curve correspond to diverse functions. Figure 2 (right) of this paper shows the test error of an _ensemble_ of predictions made by the network for the parameters at one end of the curve, and the network described by \phi_\theta(t) at some point t along the curve: since the error goes down and changes significantly as t varies, the functions corresponding to different parameter settings along these curves must be diverse. This functional diversity is also made explicit multiple times in 1802.10026, which clearly says that this result shows that the curves contain meaningfully different representations.  In response to R3, the authors incorrectly claim that  "Figure 2 in Garipov et al. only plots loss and accuracy, and does not measure function space similarity, between different initializations, or along the tunnel at all. Just by looking at accuracy and loss values, there is no way to infer how similar the predictions of the two functions are." But Figure 2 (right) is actually showing the test error of an average of predictions of networks with parameters at different points along the curve, how it changes as one moves along the curve, and the improved accuracy of the ensemble over using one of the endpoints. If the functions associated with different parameters along the curve were the same, averaging their predictions would not help performance.   Moreover, Figure 6 (bottom left, dashed lines) in the appendix of 1802.10026 shows the improvement in performance in ensembling points along the curve over ensembling independently trained networks. Section A6 (Appendix) also describes ensembling along the curve in some detail, with several quantitative results. There is no sense in ensembling models along the curve if they were the same model.  These results unequivocally demonstrate that the points on the curve have functional diversity, and this connection is made explicit multiple times in 1802.10026 with the claim of meaningfully different representations: “This result also demonstrates that these curves do not exist only due to degenerate parametrizations of the network (such as rescaling on either side of a ReLU); instead, points along the curve correspond to meaningfully different representations of the data that can be ensembled for improved performance.”  Additionally, other published work has built on this observation, such as 1907.07504 (UAI 2019), which performs Bayesian model averaging over the mode connecting subspace, relying on diversity of functions in this space; that work also visualizes the different functions arising in this space.   It is incorrect to attribute these findings to Fort & Jastrzebski (2019) or the current submission.  It is a positive contribution to build on prior work, but what is prior work and what is new should be accurately characterized, and currently is not, even after the discussion phase where multiple reviewers raised the same concern. Reviewers appreciated the broader investigation of diversity and its effect on ensembling, and the more detailed study regarding connecting curves. In addition to the concerns about inaccurate claims regarding prior work and novelty (which included aspects of the mode connectivity work but also other works), several reviewers also felt that the time accuracy trade offs of deep ensembles relative to standard approaches were not clearly presented, and comparisons were lacking. It would be simple and informative to do an experiment showing a runtime accuracy trade off curve for deep ensembles alongside FGE and various Bayesian deep learning methods and mc dropout. It s also possible to use for example parallel MCMC chains to explore multiple quite different modes like deep ensembles but for Bayesian deep learning. For the paper to be accepted, it would need significant revisions, correcting the accuracy of claims, and providing such experiments.
This paper proposes a meta learning algorithm for reinforcement learning. The work is very interesting for the RL community, it is clear and well organized. The work is impressive and it contributes to the state of the art. 
This paper combines considers the task of finding a minimal set of inputs that explain predictions of trained neural models. The authors propose a method that they refer to as "scaling symbolic methods using gradients" (SMUG). This method use integrated gradients methods to score first layer neurons on the degree to which they influence the prediction and then produces and solve an SMT problem (restricted to first layer activations) that finds the minimal mask that changes these influential neurons.   Reviewers had somewhat mixed perspectives on this submission. All reviewers were broadly in agreement that the paper is clearly written and presents an interesting combination of symbolic (i.e. SMT based) and gradient based methods for model explanation. R2 questions the need for sparsity (and therefore the SMT component) in model explanations, and R3 similarly notes that SMUG does not necessarily rely on SMT at all. That said, no reviewers raise major concerns with the quality of exposition, experimental evaluation, or the level of technical contributions in this work. The metareviewer is inclined to say that this work is above the bar for acceptance, and represents a reasonable approach to integrating SMT based and gradient based methods for model explanation.
All reviewers unanimously recommending rejecting this submission and I concur with that recommendation. However, many reviewers were quite pleased with the premise and basic concept of the submission and would have liked to see a clearer version with a bit more in terms of experiments.   I agree with the submission that the most interesting architecture search research is about the search space, not the search algorithm. The submission uses measurements of the data Jacobian matrix at different points to construct an extended data Jacobian matrix that then is projected and serves as input to a contrastive embedding learning algorithm. The resulting architecture embeddings can be used for many different things, including architecture search.  Ultimately, I am recommending rejecting this submission not because of one single overriding weakness, but because the totality of issues the reviewers raised make it clear the submission is not strong enough to publish in its current form. I encourage the authors to continue this line of work and produce a stronger submission in the future to ICLR or another venue.
There is a clear consensus over all reviewers that this is a very strong empirical analysis, with actionable insights that should prove quite useful both to researchers and practitioners. I have no doubt that many will use it as a reference when implementing and using RL algorithms (especially since the authors said they would release their code).  This is thus a clear accept, that in my opinion would deserve an oral presentation, so as to better disseminate its key findings.
All reviewers recommended accept after discussion. I am happy to accept this paper.
The paper proposes a method for time series forecasting based on a hierarchical deep learning approach. Three reviewers submitted reviews, with two marginally accept and one marginally reject. The paper was therefore borderline, but the issues raised by the marginal reject reviewer on the justification for the design choice of a deep latent model and the experimental setup appear worth addressing in a revision resubmitted to another conference.
The presented method essentially builds a model that remaps features into a new space that optimizes nearest neighbor classification. The model is a neural network, and the optimization is carried out through a genetic algorithm.  Pros:    One major issue with neural network classification is that of a lack of explainability. Many networks are currently "black box" approaches. By moving to the optimization problem to that of building a feature space for nearest neighbor classification, one can, to a degree, alleviate the "black box" issue by providing the discovered nearest neighbor instances as "evidence" of the decision.   Authors use established datasets.  Cons:   Authors do not properly cite previous work, as brought up by reviewers. There is much literature on optimization of feature spaces (such as the entire field of metric learning), as well as prior approaches using genetic optimization. The originality and significance here is therefore not clear. 
This paper proposes a new two stage second order unsupervised feature selection method via knowledge contrastive distillation. In the first stage, a sparse attention matrix that represents second order statistics is learned. In the second stage, a relational graph based on the learned attention matrix is constructed to perform graph segmentation for feature selection.   This proposed method contains some new and interesting ideas and is novel in the unsupervised feature selection setting, though some components such as the second order affinity matrix are not totally new. The proposed method is technically sound. The authors compared their method with 10 methods including several recent deep methods on 12 datasets and demonstrated consistent improvements.   However, there are some concerns from the reviewers, even after the discussion phase. 1) The computational efficiency of the proposed method seems to be low. Since one goal of feature selection is to speed up downstream tasks, the efficiency of feature selection itself should also be considered. I suggest the authors analyze the computational bottleneck of the proposed method and improve the efficiency. 2) More ablation studies can be added to illustrate how the proposed method removes the redundancy issues of the selected features. 3) Some metrics like supervised classification accuracy can be potentially used as a metric. Though supervised classification is impossible in the unsupervised learning setting, running the experiments on some datasets that have labels by pretending having on label is one way to evaluate the method.  Overall, the paper provides some new and interesting ideas. However, given the above concerns, the novelty and significance of the paper will degenerate. Although we think the paper is not ready for ICLR in this round, we believe that the paper would be a strong one if the concerns can be well addressed.
The submission is concerned with providing a transport based formulation for generative modeling in order to avoid the standard max/min optimization challenge of GANs. The authors propose representing the divergence with a fluid flow model, the solution of which can be found by discretizing the space, resulting in an alignment of high dimensional point clouds.   The authors disagreed about the novelty and clarity of the work, but they did agree that the empirical and theoretical support was lacking, and that the paper could be substantially improved through better validation and better results   in particular, the approach struggles with MNIST digit generation compared to other methods.  The recommendation is to not accept the submission at this time.
This paper proposes a method to improve the sample efficiency of the HER algorithm by sampling goals from a distribution that is learned from human demonstrations. Empirical results on a simulated robotic insertion task show that the proposed method enjoys a better sample efficiency compared to HER.  The reviewers find the paper well written overall and the proposed idea reasonable. However, there are concerns regarding the limited novelty of the proposed method, which seems incremental. Also, the empirical evaluation suffers from a lack of diversity. The considered tasks are virtually all equivalent to an insertion task. The paper would benefit from further empirical evaluations that include tasks such as those considered in the original HER paper.
This paper presents a zero shot incremental learning approach that does not store past samples for experience replay. The idea is novel and well motivated, and the paper is well written. Reviewers  comments were mainly about missing baselines, missing ablation studies, and  clarifications about the proposed method. In the revised paper, the authors provided more justifications and added new experimental results on large benchmark datasets as well as ablation studies. After discussion, all the reviewers are positive about this submission.   Thus, I recommend to accept this paper. I encourage the authors to take the review feedback into account in the final version.
This paper proposes a CNN based text classification model that uses words, characters, and labels as its input. It also presents an attention block to replace the pooling operation that is typically used in a CNN. The proposed method is evaluated on six benchmark classification datasets, achieving reasonably good results.  While the proposed method performs reasonably well compared to baselines in the papers, all reviewers pointed out that there is no discussion or comparison with existing SotA based on pretrained models (e.g., BERT, XLNet), which would strengthen the main claim of the paper. All three reviewers also suggested that the writing of the paper could be improved. The authors did not respond to these reviews, so there was little discussion needed to arrive at a consensus.  I agree with all reviewers and recommend to reject the paper.
This paper describes a few practically relevant extensions of the conformal prediction framework, that has recently become popular in the ML community for providing (marginally valid) prediction sets without making distributional assumptions. The conceptual contributions are not major, given existing work   without recalibration, the main idea of optimizing over two parameters was explored by Yang and Kuchibhotla (and is well understood even before YK, albeit not fleshed out). The current paper generalizes YK, and with the additional recalibration dataset, it is again simply an instance of standard conformal prediction. The optimization via Lagrangians is a nice addition, but it is ultimately a heuristic that performs well in practice. Nevertheless, the paper is well written, and the experiments are well done, making this a good contribution for practitioners. I recommend acceptance, and congratulate the authors on a nice work.   As a minor note, Remark 1 should not be attributed to [AB21], since it is a well known fact and deserves an earlier reference.
This paper offers likely novel schemes for image resizing.  The performance improvement is clear.  Unfortunately two reviewers find substantial clarity issues in the manuscript after revision, and the AC concurs that this is still an issue.  The paper is borderline but given the number of higher ranked papers in the pool is unable to be accepted unfortunately. 
The paper examines a sum over paths representation of ReLU networks, for which learning can be broken into two parts: learning the gates, and learning the weights given the gates, the latter of which being described by the Neural Path Kernel. The paper introduces a dual architecture, Deep Linear Gated Networks (DLGN) that parameterizes these two processes separately. The DLGN is argued to aid in interpretability of ReLU networks, with a main conclusion being that the neural network is learned path by path instead of layer by layer.  The reviewers generally found strength in the motivation and perspective and thought that the DLGN could serve as a useful architecture for aiding interpretability. Some reviewers found the presentation hard to follow, and others were not entirely convinced by the ultimate conclusions. Overall, the reviewers opinions were mixed.  I believe the ICLR community would generally find interest in the DLGN and the interpretations it might afford to deep ReLU networks. However, the number and strength of the conclusions obtained in the current analysis are rather weak. The conclusion that networks learn path by path instead of layer by layer was emphasized but the implications were not highlighted, and it remains unclear to me and at least some reviewers what the concrete significance of this observation actually is. Another major claim is that the DLGN recovers more than 83.5% of the performance of state of the art DNNs, but a priori it is not obvious what this number means, or if it is even good or bad performance. A more detailed analysis with additional common baselines, ablations, etc., would really help readers understand the significance of the performance gap.  Overall, this is an interesting direction with significant potential, but for the above reasons I cannot recommend the current version for acceptance.
A method for pruning neural networks is proposed.  Reviewers raised several concerns, including poor technical presentation and insufficient experimental validation with respect to both baseline methods and ablation studies.  All reviewer ratings lean toward reject and the authors did not provide a response.
This paper presents a refreshening insight into the classical idea of using external memory for reinforcement learning agents that learn and act in partially observable environments. The authors investigate a number of different memory architectures (Ok, OAk, Kk) and provide an insightful discussion on why we want to restrict the structure of the memory.   Reviewers generally appreciated the technical contribution of the paper, although not very convinced that this work will have a significant impact on future work. AC is also not sure about the conclusion drawn from the paper, where policies with external memory could have better sample complexity compared to rnn based policies. BTTT is computationally expensive, but it shall give better direction of which state to jump to, compared to the authors approach where the gradients are stopped at every timestep. So there should be pros and cons about this approach, and AC suspects that the sample complexity improvement actually comes from the fact that authors are explicitly limiting what can be stored in the memory, e.g. O or OA. This advantage can be broken in some other domains. AC admits that this is only a speculation at this point, but the motivation to use the external memory framework proposed in the paper needs to be more carefully investigated. 
This is a thought provoking paper which describes a significant problem that plausibly occurs in deployed ML/RL models. The paper is clearly written, describing claims using examples and developing small unit tests to probe models. However, as the reviews and discussion show,  the exposition should be substantially re worked so that the core contributions are more understandable   the core message in the revised manuscript is still very nuanced and easy to mis understand.  Let s say we train an ML model using supervised learning to minimize a loss function on a dataset. Several models may have near optimal loss as measured on a validation set   a learning algorithm is free to return any one of them. Now in a deployed system, these ML models are not merely generating passive predictions; these predictions are driving system operation and potentially influencing future states/contexts/inputs that the model will be invoked on. It is well known that supervised learning makes an iid assumption between training and deployment which is violated in this setting   that is not the main point of this paper. Consider again the set of models with near optimal loss. Some of them, when deployed, may cause the distribution mismatch between training and deployment to be miniscule, while other models may introduce a vast mismatch. We may choose a learning algorithm which just so happens to pick models from the former category; and we may conclude that feedback effects induced by the ML model are not substantial. We then change some unrelated detail in the learning algorithm (but not the objective, datasets, validation criteria, etc.) which just so happens to pick models from the latter category and suddenly witness a large distribution shift. What happened? And could we have developed tests to detect that our learning algorithms have these tendencies? The paper attempts to articulate such questions, and design the first step in answering them.  Moving to RL, where we routinely consider distribution shifts in states visited by different policies, does not fundamentally fix all these issues because the reward function is typically an engineered proxy to elicit desired behavior   and we may again find that some RL algorithms have a tendency to find reward maximizing policies that exploit gaps in reward specification as opposed to following intended behavior.  The core question studied in this paper, scoped to the supervised learning setting, is very related to that of strategic classification (see e.g., https://arxiv.org/pdf/1910.10362.pdf Strategic Classification is Causal Modeling in Disguise). The following sketch is inspired by that literature.  We might hope to augment the training objective of ML/myopic RL/strategic RL to address the Auto induced distribution shift problem as follows. [Supervised learning for content recommendation] Let the training/validation data distribution be D. Assume for now that there is no exogenous factor in the environment that causes any distribution shifts in deployment   so, the only shift is due to feedback effects from the predictions made by the model. For an ML model f, let the corresponding recommendation policy be pi_f, and let the long term distribution of data seen from user interactions with pi_f be D[pi_f]. Then, what we want is: f*   argmin_F Expectation over D [ L(f) ] subject to constraint that D ~  D[pi_f].  For a contextual bandit/myopic RL formulation of the problem, we could similarly constrain the learning problem as pi*   argmax_Pi Expectation over D [ Reward of pi] subject to constraint that D \approx D[pi]. Essentially, both supervised ML and contextual bandit algorithms are assuming that context distribution is unchanged   so let us enforce that the context distribution is indeed unchanged as a consequence of the policy s actions. It is unclear how to generalize this kind of thinking to situations where environmental changes also contribute to distribution shift.  The authors call out precisely this flaw using the cryptic comment    not trying to change X  is not the same as  trying to not change X . The formulation above does  trying to not change X , but that is an insufficient band aid in situations when environment changes X. It s also unclear how one might estimate D[pi] or D[pi_f] and appropriately constrain the learning algorithm   but these are all interesting questions to study.  The paper in its current form is asking an important question. In supervised learning, the desired solution might actually coincide with strategic classification solution concepts. The paper may be asking a generalization of the phenomenon for myopic RL and RL. It may spark interesting discussions and follow up work, but is not yet mature beyond a workshop poster. Generalizing the unit tests, articulating the scope of situations where context swapping may be a useful strategy, and even formalizing the problem and desired goal (as attempted above for the content recommendation example) will substantially strengthen the paper.
This manuscript proposes an approach for estimating cross correlations between model outputs, related to deep CCA. Authors note that the procedure improves results when applied to supervised learning problems.  The reviewers have pointed out the close connection to previous work on deep CCA, and the author(s) have agreed. The reviewers agree that the paper has promise if properly expanded both theoretically and empirically.
The manuscript develops new insights into how catastrophic forgetting takes place in the context of continual learning. The authors develop a new method based on this insight and demonstrate that it performs better than or as well as previously developed baselines, as well as showing that it is more widely applicable than close competitors (e.g. to cases where task boundary are unknown).   The manuscript starts by pointing to evidence that catastrophic forgetting at task boundaries is due at least in large part to abrupt representation drift (e.g. in the penultimate layer of a network) caused by gradients coming from new class examples. Most reviewers found this novel and interesting. Reviewers also tended to be happy with the writing, motivation, and experimental results supporting the conclusions. One of the reviewers recommends against publishing (3   Reject): mCT3 cites positives in the novel explanation of forgetting and the development of new metrics (Averaged Anytime Accuracy) and Total FLOPs, which they say help make the analysis more rigorous. However, fundamentally, they believe that the relationship between the insights and the proposed methods are not strong enough and that the methods do not provide more than marginal improvements empirically. They point to work such as SS IL as a baseline which, in their opinion, is not improved upon significantly enough to adjust their recommendation. The authors provide multiple effective rebuttals to the concerns, as well as detailed experimental analysis of SS IL: 0. The new method is shown to be as good or better than SS IL, 1. That their method are more computationally and memory efficient than SS IL, 2. SS IL requires task ids, whereas they do not, 3. Detail analysis (Appendix B) shows that SS IL mostly fails to learn the current task in the online setting in the miniImageNet case that the reviewer worries about, a fact that is obscured by the simple Acc metric. Reviewer mCT3 does not respond to the rebuttals in any substantive or compelling fashion, and leaves their score at 3/Reject. While I believe that the reviewers concerns should be thoroughly addressed in a final version of the manuscript, I am in agreement with the 3 of 4 reviewers who recommend publication.
This paper considers that the model s training data may be not accessible when learning the attacking model, and thus a more practical blackbox attack scheme, Beyond ImageNet Attack (BIA) framework, is designed. All the reviewers agreed that the setting in this paper is important and helpful when designing attack methods. However, the method is not totally new. Nevertheless, considering the importance of the problem investigated in this paper, the nice design of the overall framework, and the extensive experiments, the AC recommends accept for this paper.
The paper proposes an autoencoder for sets, an interesting and timely problem.  The encoder here is based on prior related work (Vinyals et al. 2016) while the decoder uses a loss based on finding a matching between the input and output set elements.  Experiments on multiple data sets are given, but none are realistic.   The reviewers have also pointed out a number of experimental comparisons that would improve the contribution of the paper, such as considering multiple matching algorithms and more baselines.  In the end the idea is reasonable and results are encouraging, but too preliminary at this point.
This paper proposes a unified approach for performing state estimation and future forecasting for agents interacting within a multi agent system. The method relies on a graph structured recurrent neural network trained on temporal and visual (pixel) information.   The paper is well written, with a convincing motivation and a set of novel ideas.   The reviewers pointed to a few caveats in the methodology, such as quality of trajectories (AnonReviewer2) and expensive learning of states (AnonReviewer3). However, these issues do not discount much of the papers  quality. Besides, the authors have rebutted satisfactorily some of those comments.  More importantly, all three reviewers were not convinced by the experimental evaluation. AnonReviewer1 believes that the idea has a lot of potential, but is hindered by the insufficient exposition of the experiments. AnonReviewer3 similarly asks for more consistency in the experiments.  Overall, all reviewers agree on a score "marginally above the threshold". While this is not a particularly strong score, the AC weighted all opinions that, despite some caveats, indicate that the developed model and considered application fit nicely in a coherent and convincing story. The authors are strongly advised to work further on the experimental section (which they already started doing as is evident from the rebuttal) to further improve their paper.
Main content:  Blind review #3 summarizes it well:  This paper presents results on Dictionary Learning through l4 maximization. The authors base this paper heavily off of the formulation and algorithm in Zhai et. al. (2019) "Complete dictionary learning via l4 norm maximization over the orthogonal group". The paper draws connections between complete dictionary learning, PCA, and ICA by pointing out similarities between the objectives functions that are optimized as well as the algorithms used. The paper further presents results on dictionary learning in the presence of different types of noise (AWGN, sparse corruptions, outliers) and show that the l4 objective is robust to different types of noise. Finally the authors apply different types of noise to synthetic and real images and show that the dictionaries that they learn are robust to the noise applied.     Discussion:  Reviews agree about the interesting work, including the connections of complete dictionary learning with classic PCA and ICA (after further clarification during the rebuttal period). Additional empirical strengthening during the rebuttal period also addressed a reviewer concern.     Recommendation and justification:  As review #3 wrote, "Overall this paper makes significant contributions by extending the work in [Zhai et. al s (2019) "Complete dictionary learning via l4 norm maximization over the orthogonal group"] to noisy dictionary learning settings".
 The authors presents a technique for training neural networks, through dynamic sparse reparameterization. The work builds on previous work notably SET (Mocanu et al., 18), but the authors propose to use an adaptive threshold for and a heuristic for determining how to reparameterize weights across layers.  The reviewers raised a number of concerns on the original manuscript, most notably 1) that the work lacked comparisons against existing dynamic reparameterization schemes, 2) an analysis of the computational complexity of the proposed method relative to other works, and that 3) the work is an incremental improvement over SET. In the revised version, the authors revised the paper to address the various concerns raised by the reviewers. To address weakness 1) the authors ran experiments comparing the proposed approach to SET and DeepR, and demonstrated that the proposed method performs at least as well, or is better than either approach. While the new draft is in the ACs view a significant improvement over the initial version, the reviewers still had concerns about the fact that the work appears to be incremental relative to SET, and that the differences in performance between the two models were not very large (although the author’s note that the differences are statistically significant). The reviewers were not entirely unanimous in their decision, which meant that the scores that this work received placed it at the borderline for acceptance. As such, the AC ultimately decide to recommend rejection, though the authors are encouraged to resubmit the revised version of the paper to a future venue. 
The paper attempts at generating two types of summaries for scientific papers: summary of contribution and summary of background context. Most reviewers appreciated the motivation and found this research to be quite interesting and useful, however all reviewers had concerns regarding both execution/presentation of the ideas. While authors try to address some of the concerns, there are many clarifying questions and points raised by reviewers. Addressing all these points requires rather a major revision. Therefore the paper is not quite ready yet and would benefit from another iteration.
The paper shows that active learning is an emergent property of pre trained models. They show that simple uncertainty sampling improves sample efficiency by 6 times (up to 6x fewer samples for the same accuracy). This is an interesting and important observation that has practical implications.   Initially, there were various concerns regarding the message of the paper, including the tile and use of uncertainty function in AL and lack of enough experiments that were addressed through rebuttal period.   However, there are still remaining concerns that lead to the paper not being ready for publication. Namely,    (1) Clear discussion on how much of the gains are due to active learning vs pre training with respect to different cases. it is also worth investigating additional causes for the failure cases.     (2) there are many observations here without a clear narrative or theory. Moreover, making the story more cohesive will strengthen the paper.
The authors have proposed a new method for exploration that is related to parameter noise, but instead uses Gaussian dropout across entire episodes, thus allowing for temporally consistent exploration. The method is evaluated in sparsely rewarded continuous control domains such as half cheetah and humanoid, and compared against PPO and other variants. The method is novel and does seem to work stably across the tested tasks, and simple exploration methods are important for the RL field. However, the paper is poorly and confusingly written and really really needs to be thoroughly edited before the camera ready deadline. There are many approaches which are referred to without any summary or description, which makes it difficult to read the paper. The three reviewers all had low confidence in their understanding of the paper, which makes this a very borderline submission even though the reviewers gave relatively high scores. 
This paper investigates the dereverberation problem from the audio visual perspective.  The geometry of the environment is represented by RGB and depth images.  The authors propose a so called visually informed dereverberation of audio (VIDA) model and also create a dataset consisting of both synthetic and real data to verify the effectiveness of the model.  Experiments are conducted on speech enhancement, speech recognition and speaker identification tasks.  The authors compare VIDA with audio only dereverberation as well as various established baseline systems in the community.    The audio visual way of coping with dereverberation using visual representation of the acoustic environment seems to be interesting. The authors  rebuttal has cleared most of the concerns raised by the reviewers but there are still numerous lingering concerns which affect its acceptance.  First of all,  most of the reviewers consider the novelty not overwhelmingly significant.  Second, the contribution of the visual input seems to be only marginal compared to the audio only dereverberation. Results on real data are also mixed.  Some of the reported p values are extremely small, which raises questions whether it is due to the size of the test set.  Third, there are noticeable artifacts in some of the samples in the demo.  Fourth,  there are numerous issues in the paper that are worth further in depth investigation. For instance, it would be helpful to show in which way exactly the RGB and depth images helps.
The authors replace the large filtering step in the permutohedral lattice with a spatially varying convolutional kernel. They show that inference is more efficient and training is easier.  In practice, the synthetic experiments seem to show a greater improvement than appears in real data.  There are concerns about the clarity, lack of theoretical proofs, and at times overstated claims that do not have sufficient support.  The ratings before the rebuttal and discussion were 7 4 6.  After, R1 adjusted their score from 6 to 4.  R2 initially gave a 7 but later said "I think the authors missed an opportunity here. I rated it as an accept, because I saw what it could have been after a good revision. The core idea is good, but fully agree with R1 and R3 that the paper needs work (which the authors were not willing to do). I checked the latest revision (as of Monday morning). None of R3 s writing/claims issues are fixed, neither were my additional experimental requests, not even R1 s typos." There is therefore a consensus among reviewers for reject. 
The paper analyses the behaviour of Neural Processes in the frequency domain and, in particular, how it suppresses high frequency components of the input functions. While this is entirely intuitive, the paper adds some theoretical analysis via the Nyquist Shannon theorem. But the analysis remains too generic and it is not clear it will be of broad interest to the community. 
This work presents a straightforward and easy to understand method for using hypernetworks to adapt existing models to be able to increase their output space. The method itself is also interesting and is detailed enough for reproducibility. However, the experiments and results should be improved by expanding the demonstration of CHNs beyond the narrow P VAE application and comparing against relevant baselines in the recommendation system literature.  Pros   Clear writing.   Detailed hyperparameters to aid reproducibility.   Straightforward model.  Cons   Lack of sufficient comparison to related work, especially to existing recommendation systems that handle the cold start issue and to Vartak, 2017.   Limited results that only demonstrate application to P VAE meaning it s still unknown if CHNs work well with other models. The result on the synthetic dataset is also less persuasive.   Lack of sufficient ablations, i.e. training a SVM/linear regression model until convergence.
The authors study a practical problem of selecting/combining existing multi label classification APIs under a budget constraint for a specific problem instance on hand. The task can be viewed as an (online) integer programming problem when given an accuracy estimator for the combination performance. The authors relax the integer constraints and propose a framework to solve the task in the dual form. They also run experiments to validate that the proposed framework is advantageous (cost or accuracy wise) over the best single API.  Most of the reviewers are positive about the practical value and the potential impacts of the work in applications/products/services. There are several disputes between the authors and some reviewers that cannot be fully resolved during the rebuttal. In the end, no reviewers express willingness to strongly champion for the acceptance of the paper, making the paper a borderline case. The decision is based on a careful examination of the current manuscript and every side s opinions.  * Novelty: Some reviewers question about the novelty of the work. There are two aspects about novelty: one is on whether the problem itself is novel (are the authors trying to propose a new multi label method?) In this aspect, the authors  response, which states that they are not aiming at proposing a new method, but at solving an automation task for MLaaS users, appears believable. The other aspect is whether the solution technique, namely the relaxed integer programming and other techniques, are sufficiently novel. Some reviewers find the novelty aspect satisfactory, while others believe that the proposed optimization technique have been widely used in machine learning community. The authors did not clarify the similarity/difference of the proposed technique to existing ones during the rebuttal. In this sense, the technical novelty is not well justified.  * Speed: Some reviewers are concerned about different aspects of the running time and other costs. The authors emphasized the rapid speed in inference phase, particularly in Figure 3. Less is discussed about the time needed for the training phase (although the authors claim to be much smaller than the inference time) somehow even the most positive reviewers have some questions about this aspect. The authors could add more clarification about the different "time" costs to the discussion. One dispute between some reviewers and the authors is about the *complexity* analysis of time, which is indeed missing in the current manuscript and can be a nice to have for future todos.  * Theoretical Guarantee: One major dispute between some reviewers and the authors is on the theoretical guarantee provided. The reviewers suggest a regret style bound, which compares the solution to the worst case sequence; the authors provide an optimization style bound, which compares the solution to the absolute optimal solution. Different bounds have their different roles for supporting the framework. Given that the authors have provided some reasonable bounds, the lack of regret bound is not taken against the authors.  * Specialty: One concern raised by some reviewers is that the technique does not seem particularly tailored for multi label classification (except some minor parts). In this sense, it is nice to have for the authors to discuss more on the wider applicability of the technique, and/or include some more specialty of the multi label classification problem into the technique design.  After taking all the factors above into account, and calibrating the received scores to the distribution across the papers, it seems that the paper could use some more revision before being mature enough as an impactful work.
This paper studies group equivariant neural network representations by building on the work by [Cohen and Welling,  14], which introduced learning of group irreducible representations, and [Kondor 18], who introduced tensor product non linearities operating directly in the group Fourier domain.   Reviewers highlighted the significance of the approach, but were also unanimously concerned by the lack of clarity of the current manuscript, making its widespread impact within ICLR difficult, and the lack of a large scale experiment that corroborates the usefulness of the approach. They were also very positive about the improvements of the paper during the author response phase. The AC completely agrees with this assessment of the paper. Therefore, the paper cannot be accepted at this time, but the AC strongly encourages the authors to resubmit their work in the next conference cycle by addressing the above remarks (improve clarity of presentation and include a large scale experiment). 
This paper presents to integrate the codes based on multiple hashing functions with Transformer networks to reduce vocabulary sizes in input and output spaces. Compared to non hashed models, it enables training more complex and powerful models with the same number of overall parameters, thus leads to better performance.  Although the technical contribution is limited considering hash based approach itself is rather well known and straightforward, all reviewers agree that some findings in the experiments are interesting. On the cons side, two reviewers were concerned about unclear presentation regarding the details of the method. More importantly, the proposed method is only evaluated on non standard tasks without comparison to other previous methods. Considering that the main contribution of the paper is in empirical side, I agree it is necessary to evaluate the method on more standard benchmarking tasks in NLP where there should be many other state of the art methods of model compression. For these reasons, I’d like to recommend rejection. 
This paper is a systematic study of how assumptions that are present recent theoretical meta learning bounds are satisfied in practical methods, and whether promoting these assumptions (by adding appropriate regularization terms) can improve performance of existing methods. The authors review common themes in theoretical frameworks for a meta learning setting that involves a feature learning step, based on which linear predictors for a variety of tasks are trained. Statistical guarantees for such a framework (that is, statistical guarantees for the performance of trained on an additional target task) are based on the assumption that the set of weight vectors of the linear predictors span the space (ie exhibit variety) and that the training tasks all enjoy a similar margin separability (that is, that the representation is not significantly better suited for some of the tasks than others).  The current submission, cleanly reviews the existing literature, distills out these two properties and then proposes a regularization framework (that could be added to various meta learning algorithms) to promote these properties in the learned feature representation.   Finally, the authors experimentally evaluate to what degree the properties are already observed by some meta learning methods, and whether the proposed additions will improve performance. It is established that adding the regularization terms improves performance on most tasks. The authors thus argue that incorporating insights obtained form recent theoretical frameworks of analysis, can lead to improved performance in practice. Naturally, the purpose of the presented results is not to establish a new state of the art on a set of benchmark tasks, but to systematically study and compare the effect of adding regularization terms that will promote the properties that are desirable for a  feature representation based on statistical bounds.  I would argue that the research community should support this type of studies. The work is well presented and conducted. Most importantly, the study has a clear and general message, that will be valuable for researchers and practitioners working in on meta learning.    However, the reviewers did not recommend publishing this type of study for ICLR. The authors are encouraged to resubmit their work to a different venue.
This an interesting new contribution to construction of random features for approximating kernel functions. While the empirical results look promising, the reviewers have raised concerns about not having insights into why the approach is more effective;  the exposition of the quadrature method is difficult to follow; and the connection between the quadrature rules and the random feature map is never explicitly stated. Some comparisons are missing (e.g., QMC methods). As such the paper will benefit from a revision and is not ready for ICLR 2018 acceptance.
The paper proposes a meta gradient boosting framework to tackle the model agnostic meta learning problem. The idea is to use a base learn that learns shared information across tasks, and gradient boosted modules to capture task specific modules. The experiments show that the proposed meta gradient boosting framework (with 5 gradient boosting modules) achieves better or competitive results compared to the baselines. However, there were several issues that the author feedback did not addressed properly. For instance, R2 were not satisfied by discussing briefly the suggested baselines without adding the comparison, or R1 pointed out that the claim “the learning and updating strategy proposed in the method ensured a weak base learner” because clear separable datasets could convergence quickly and weak is not anymore applicable. Besides these two specific concerns, the reviewers expected a large revision of the paper due to several cons about the paper. All reviewers agreed a mayor revision is needed before acceptance. Therefore I recommend rejection.
This paper presents a novel approach to grammar induction. Like older work by Klein and Manning, the paper finds benefit in jointly inducing both constituency and dependency structure. However, unlike most approaches to grammar induction, the model is not generative   rather, it is a transformer based architecture that is trained to optimize a masked language modeling objective. The resulting parses appear to beat non trivial baselines, but direct comparisons with several relevant state of the art systems are not drawn. Reviewers overall found the approach interesting and novel. However, nearly all reviewers raised serious concerns about experimental comparisons with related work and brought up several missing state of the art baselines that, like the proposed system, do not require gold POS. Reviewers also pointed out issues with clarity in several sections. In rebuttal, authors provide a substantial update to the original draft. So substantial that all reviewers mentioned in discussion that the new draft would effectively require an entirely new review. While I applaud authors for the substantial revisions, and while ICLR guidelines do not explicitly limit the amount change to a draft allowed in rebuttal, in this case the revisions are sufficiently drastic that I agree with reviewers that a new review process is required. Thus, I recommend rejection but strongly encourage authors to resubmit. 
This paper provides a rigorous analysis of feedback alignment under two restrictions 1) that all, except the first, layers are constrained to realize monotone functions and 2) the task is binary classification. Overall, all reviewers agree that this is an interesting submission providing important results on the topic and as such all agree that it should feature at the ICLR program. Thus, I recommend acceptance. However, I ask the authors to take into account the reviewers  concerns and include a discussion about limitations (and general applicability) of this work.
For many problems such as ligand protein binding, quantitative structure activity prediction (QSAR), predicting protein function from structure, etc., the 3D geometry of the molecules is of great importance.  One way to represent this is simply to assign locations to all atoms in 3 dimensional space.  If using graph convolutional kernels or other relational representations such that aligning molecules is not necessary, these approaches with 3D geometry can be efficient and far more effective than 1D or 2D representations.  The contribution of the paper is to make this point and to produce a resource with this kind of 3D data.  Such a resource would be of high value.  Nevertheless, reviewers feel provision of such a resource is perhaps not a major contribution to the ICLR and ML communities.  There is a sense that more innovative and substantial contribution would come from addressing also the challenge that 3D geometry can changes and that there may be multiple low energy conformations of biomolecules that should be considered.  The authors contend that unlike ligands which are small and may have many low energy conformations, large biomolecules have a much more constrained conformational space.  This meta reviewer is sympathetic to the authors  point and appreciates the importance of the resource.  Nevertheless, even large biomolecules often have some portions of flexible conformation and high 3D structure variation that should be considered.  And indeed addressing the kind of multiple instance problem that arises by considering multiple conformations of large molecules or of ligands binding to large molecules would certainly require and likely yield bigger ICLR/ML innovations.  In the end the paper contributes a useful resource but does not excite the reviewers substantially enough, without those extensions or others, for a recommendation of acceptance at this time.
In my opinion, the main strength of this work is the theoretical analysis and some observations that may be of great interest to the NLP community in terms of better analyzing the performance of RL (and "RL like") methods as optimizers. The main weakness, as pointed out by R3, the limited empirical analysis.  I would urge the authors to take R3 s advice and attempt insofar as possible to broaden the scope of the empirical analysis in the final. I believe that this is important for the paper to be able to make its case convincingly.  Nonetheless, I do think that the paper makes a significant contribution that will be of interest to the community, and should be presented at ICLR. Therefore, I would recommend for it to be accepted.
This paper develops a method for decomposing scenes into object specific neural radiance fields.  After the discussion phase, two reviewers support acceptance.  Empirical results on multiple synthetic datasets and benchmarks appear convincing; the rebuttal also added an initial demonstration of generalization to real images.
Two knowledgeable reviewers and one fairly confident reviewer were positive (7) about this submission. The authors  response clarified a few questions and comments from the initial reviews. The paper provides exact bounds that close the gap between lower and upper bounds, and that helps us understand these networks better. With the unanimously positive feedback, I am recommending the paper to be accepted. 
Thank you for your submission to ICLR.  There is some disagreement about this paper, and several of the reviews are of relatively low confidence.  While I appreciate the effort that the authors have put into addressing the concerns of the reviewers, after going through the paper and the responses myself, I m ultimately coming down on the side of the less positive reviews.  My reasoning, honestly, is that I think the authors are vastly overestimating the knowledge that the ICLR audience will have about numerical methods for PDE solutions.  Reading through the paper, I honestly have very little idea about how the actual numerical techniques are carried out, and it s unclear to me precisely where this method falls in between a traditional numerical solver an actual neural network.  Reading through the reviews, even the more positive ones, I don t think I m alone in this perception (and the authors will hopefully believe me that these reviewers _are_ indeed emblematic of the subgroup of ICLR that is most experience with differential equations).  I really feel like either a substantial rewrite of the paper is needed, to make clear the full extent of the numerical methods being applied; or alternatively, the work may really be better suited for a numerical methods venue.
The paper received 3 reviews with positive ratings: 7,6,7. The reviewers appreciated overall quality of the manuscript, thoroughness of the evaluation, and practical importance of this work (mentioning though that the technical novelty is still not high). They also acknowledged impressive empirical performance. The authors provided detailed responses to each of the reviews separately, which seemed to have resolved the remaining concerns. As a result, the final recommendation is to accept this work for presentation at ICLR as a poster.
The paper presents a mathematical framework for encoding and decoding continuous valued signals from spiking neurons, proving both mathematical theory and simulation results. Two reviewers had serious concerns about the mathematical correctness and exactness of some of the presented mathematical results, and the AC raised questions about relationship to (un cited) prior work. The authors aimed to address these shortcomings in the discussion phase, but neither me nor the reviewers were convinced that it provides a correct, and substantial, advance over prior approaches. I do hope that the feedback from the process will useful to the reviewers, and will help them in clarifying their contributions.
This paper presents a model for building sentence embeddings using a generative transformer model that encoders separately semantic aspects (that are common across languages)  and language specific aspects. The authors evaluate their embeddings in a non parametric way (i.e., on STS tasks by measuring cosine similarity) and find their method to outperform other sentence embeddings methods. The main concern that both reviewers (and myself) have about this work relates to its evaluation part. While the authors present a set of very interesting difficult evaluation and probing splits aiming at quantifying the linguistic behaviour of their model, it is unsatisfying the fact that the authors do not evaluate their model extensively in standard classification embedding benchmarks (e.g., as in GLUE). The authors comment: “[their model in producing embeddings] it isn’t as strong when using classification for final predictions. This indicates that the embeddings learned by our approach may be most useful when no downstream training is possible”. If this is true, why is it the case and isn’t it quite restrictive? I think this work is interesting with a nice analysis but the current empirical results are borderline  (yes, the model is better on STS, but this is quite limited of an idea compared to using these embeddings as features in a classification tasks). As such, I do not recommend this paper for acceptance but I do hope that authors will keep improving their method and will make it work in more general problems involving classification tasks.
This paper proposes a method to learn graph features by means of neural networks for graph classification. The reviewers find that the paper needs to improve in terms of novelty and experimental comparisons. 
The paper presents AdvGAN: a GAN that is trained to generate adversarial examples against a convolutional network. The motivation for this method is unclear: the proposed attack does not outperform simpler attack methods such as Carlini Wagner attack. In white box settings, a clear downside for the attacker is that it needs to re train its GAN everytime the defender changes its convolutional network.  More importantly, the work appears preliminary. In particular, the lack of extensive quantitative experiments on ImageNet makes it difficult to compare the proposed approach to alternative attacks methods such as (I )FGSM, DeepFool, and Carlini Wagner. The fact that AdvGAN performs well on MNIST is nice, but MNIST should be considered for what it is: a toy dataset. If AdvGANs are, as the authors state in their rebuttal, fast and good at generating high resolution images, then it should be straightforward to perform comprehensive experiments with AdvGANs on ImageNet (rather than focusing on a small number of images on a single target, as the authors did in their revision)?
This is a solid paper that proposes and analyzes a sound approach to zero order optimization, covering a variants of a simple base algorithm.  After resolving some issues during the response period, the reviewers concluded with a unanimous recommendation of acceptance.  Some concerns regarding the necessity for such algorithms persisted, but the connection to adversarial examples provides an interesting motivation.
The paper proposed an interesting method to improve the robustness of DARTS and hence to alleviate the mode collapse. The idea consists of adding an auxiliary skip connection branch that complements the output of the cell function together with a depth analysis about the effect of the auxiliary branch. The proposed approach is validated on a few benchmarks showing the effectiveness of the proposed approach. All reviewers agreed that the idea is simple, efficient and interesting. author response satisfactorily addressed most of the points raised by the reviewers, and most of them increased their original score accepting the paper. Therefore, I recommend acceptance.
The paper aims to study what is learned in the word representations by comparing SkipGram embeddings trained from a text corpus and CNNs trained from ImageNet.  Pros: The paper tries to be comprehensive, including analysis of text representations and image representations, and the cases of misclassification and adversarial examples.   Cons: The clarity of the paper is a major concern, as noted by all reviwers, and the authors did not come back with rebuttal to address reviewers  quetions. Also, as R1 and R2 pointed out the novelty over recent relevant papers such as (Dharmaretnam & Fyshe, 2018) is not clear.  Verdict: Reject due to weak novelty and major clarity issues.
While there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at ICLR in its present form.  Concerns raised included the need for better motivation of the practicality of the approach, versus its computational cost. The need for improved evaluations was also raised.
This submission considers the problem of learning disentangled representations from data in which there are correlations between underlying factors of variation (FoVs). Much of the work on learning disentangled representations has considered simulated datasets in which the FoVs are conditionally independent. The authors perform an extensive experimental evaluation (4000+ trained models). The main findings of this evaluation are that:    Existing methods fail to disentangle when ground truth FoVs are correlated, in the sense that learned factors will reflect the correlations in the training data   Metrics for disentanglement do not necessarily reveal correlations in underlying factors   Semi supervision and weak supervision can be used to induce learned factors that align with true FoVs  Reviewers expressed diverging opinions on this paper:    R2 is a favor of acceptance, but does note that the paper is somewhat difficult to follow, owing to the fact that it presents a large number of results and does not quite arrive at a streamlined narrative.    R4 was initially critical and posted detailed comments relating to framing, interpretation of existing work, and the conclusions that can be drawn from the presented experiments. The authors were able to address a number of these concerns in a detailed discussion with the reviewer.     R3 is critical of the experimental setup, which considers linear correlations between two underlying factors, and feels the semi supervised and weakly supervised experiments have limited novelty. This reviewer did not respond to author feedback.   The metareviewer has carefully read the reviews, author feedback, and subsequent discussion. Owing to the fact that reviews are diverging the meteareviewer also read the paper.   As R4 notes, the results in this paper are in some sense unsurprising – we would expect correlations between underlying factors to lead to correlated learned factors. In fact one could even argue that dimensions in the latent space should reflect correlations in the training data. That said, the metareviewer feels that a paper need not present results that are surprising, as long as the experimental evaluation is rigorous and there are no major problems with framing and exposition.   In this context, the metareviewer would like to express their appreciation to R4 for taking the time to follow up in detail with the authors, and for checking their revisions. The metareviewer feels that the fact that these revisions have a fairly large edit distance should not in itself not impede acceptance, as long as reviewers are agree that the edits improve the paper.   The metareviewer is not entirely convinced by the criticisms presented by R3. The reviewer is of course correct that real world datasets will not just have linear correlations between two factors. That said, an experimental evaluation of how correlations affect the degree of disentanglement has to start somewhere, and even these comparatively simple experiments represent a substantial effort on the part of the authors.   Having read the submission, the metareviewer agrees with R2s assessment that the exposition is difficult to follow, even for readers who know the relevant literature. As the reviewer notes, the overall narrative could be clearer. Extracting a clear narrative is challenging when there are many experiments to report, but it is nonetheless something that the authors should spend additional time and thought on. Another factor that hurts clarity is that experiments are described in long paragraphs that often would benefit from an equation or two, for example to describe the substitution function in Section 4.1, or to more precisely describe the form of weak supervision that the authors employ in Section 4.2.   As a final note, the metareviewer would suggest more explicit discussion on what authors think a VAE *should* do when factors are correlated. Arguably learning factors that reflect correlations is the "correct" when the training data exhibits such correlations. Currently the authors do not provide much of an arguments for *why* they think a VAE should learn a representation that ignores these correlations. A possible argument is that train time correlations might not be representative of test time correlations. Here, testing to what extent a learned representation generalizes to test time data with a shift in correlation would also strengthen results.  On balance, the metareviewer s assessment is that this paper falls narrowly below the threshold for acceptance. While the experimental evaluation represents a substantial effort that in itself is above the bar, there are problems with narrative and the clarity of  writing that rise above the level of minor revisions that could be addressed by camera ready without additional review. Based on this, the metareviewer will recommend rejection. With a little bit more work on writing and exposition, this will make a great paper at a future conference. 
The paper formulates the problem of unsupervised one to many image translation and addresses the problem by minimizing  the mutual information.  The reviewers and AC note the critical limitation of novelty and comparison of this paper to meet the high standard of ICLR.   AC decided that the authors need more works to publish.
This submission has generated sufficient debate, including some messages that, in my viewpoint, have the wrong tone. It may well be that different colleagues see the work in different ways. It is very hard to evaluate submissions in a short time and mistakes can happen. In this case, I think there were and still are misunderstandings and unclarity wrt very crucial points of the paper. This does not mean that the work overall is weak not that there is no contribution. If the content is so interesting (as discussed by authors and multiple reviewers) in some way (which it seems to be), then a better presentation and argumentation will lead to a publication elsewhere soon, but based on all the data that I have here, I recommend rejection. I see to reason to list details about the content and possible concerns, as they should be clear from the multiple messages among authors and reviewers. Best of luck.
This paper studies an interesting problem: the landscape of neural networks. I agree with the authors  comment that this work improves our understanding of one aspect of neural networks, and I do find the result of this paper is of interest to some extent. Reviewer 5 pointed out the technique used in the paper is interesting, and Reviewer 3 has shown interest in the techniques (and indicated the possibility of increasing the score). Nevertheless, a few reviewers questioned the requirement of the large width; I do not think having a large width itself is necessarily an issue (even in the presence of convergence results on NTK), but it is necessary to clearly explain the context and the relation/differences with closely related works in the literature. In the current form, the paper probably has not reached the bar of acceptance, thus I recommend reject. 
The authors propose WARM, a novel method that actively queries a small set of true labels to improve the label function in weak supervision. In particular, the authors propose a methodology that converts the label function to "soft" versions that are differentiable, which are in term learnable with true labels using proper updates of parameters. Empirical results on several real world data sets demonstrate that the method yields a pretty strong performance.  The reviewers generally agree that the idea of making the labeling functions differentiable is conceptually interesting. They are also positive about the simplicity and the promising performance. They share joint concerns on whether the idea has been sufficiently studied in terms of the design choices and completeness of the experiments. For instance, the authors can conduct deeper exploration of the trade off for differentiable LFs. They can also study active learning strategies that are beyond basic uncertainty sampling. While the authors have provided more studies about those exploration and ablation studies during the rebuttal, generally the results are not sufficient to convince most of the reviewers. In future revisions, the authors are encouraged to clarify its position with respect to existing works that combine active learning and weakly supervised learning.  The authors position the paper as more empirical than theoretical. So the suggestion from some reviewers about more theoretical study is viewed as nice to have but not a must.
A version of GCNs of Kipf and Welling is introduced with (1) no non linearity; (2) a basic form of (softmax) attention over neighbors where the attention scores are computed as the cosine of endpoints  representations (scaled with a single learned scalar). There is a moderate improvement on Citeseer, Cora, Pubmed.  Since the use of gates with GCNs / Graph neural networks is becoming increasingly common (starting perhaps with GGSNNs of Li et al, ICLR 2016)) and using attention in graph neural networks is also not new  (see reviews and comments for references), the novelty is very limited.  In order to make the submission more convincing the authors could: (1) present results on harder datasets; (2)  carefully evaluate against other forms of attention (i.e. previous work).  As it stands, though it is interesting to see that such simple model performs well on the three datasets, I do not see it as an ICLR paper.  Pros:   a simple model, achieves results close / on par with state of the art  Cons:   limited originality   either results on harder datasets or / and evaluation agains other forms of attention (i.e. previous work) are needed   
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.    The problem is interesting and challenging   The proposed approach is novel and performs well.  2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    The clarity could be improved  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  Many concerns were clarified during the discussion period. One major concern had been the experimental evaluation. In particular, some reviewers felt that experiments on real images (rather than in simulation) was needed. To strengthen this aspect, the authors added new qualitative and quantitative results on a real world experiment with a robot arm, under 10 different scenarios, showing good performance on this challenging task. Still, one reviewer was left unconvinced that the experimental evaluation was sufficient.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  Consensus was not reached. The final decision is aligned with the positive reviews as the AC believes that the evaluation was adequate. 
The paper proposes the idea of using optimal transport to evaluate the semantic correspondence between two sets of words predicted by the model and ground truth sequences. Strong empirical results are presented which support the use of optimal transport in conjunction with log likelihood for training sequence models. I appreciate the improvements to the manuscript during the review process, and I encourage the authors to address the rest of the comments in the final version.
This paper proposes adding noise regularization, iteratively during training to word embeddings. The method is evaluated on CNN based text classification. Overall, there is novelty in the proposed method, however there are concerns about the experiments and analysis of the proposed approach.
This manuscript proposes an extension of convolution operations for manifold valued data. The primary contributions include the development and description of the approach and implementation and evaluation on real data.  The reviewers and AC expressed concern about the clarity of the presentation, particularly for a general ICLR audience. Though the contributions are primarily conceptual/theoretical, reviewers expressed concern about the breadth and quality of the presented experimental results. Some additional concerns related to missing proofs and details were addressed in the rebuttal.
This submission proposes a new encoding mechanism, i.e. a new quantum data loader for images with a reduced number of qubits, which is then used for image classification with off the shelf quantum neural networks (from TensorFlow Quantum). There are two major concerns raised by most reviewers. The first concern regards the novelty in the design of the quantum data loader and the use of off the shelf quantum neural networks (QNNs), the latter of which is neither novel nor close to state of art QNNs for the same purpose.  The quantum data loading procedure also assumes a binary representation of images that might not be enough for low contrast images. Although the number of qubits is reduced, the circuit tends to have a large depth which makes it hard for practical implementations.  The second concern regards the overall performance of the proposed solution for image classification, where a clear quantum benefit is missing, or in some cases, a quantum disadvantage shows up.  Based on these discussions, we believe that the submission requires substantial improvements before its publication.
The reviewers are unanonymous in their assessment that the paper is not ICLR quality in its current form.
This paper deals with the important practical problem of speeding up GNNs. Although the proposed method based on LSH may be considered to be a rather too simple preprocessing, it would be worthwhile to share the practical idea with the community as far as the proposed method is shown effective enough. However, as pointed out by several reviewers, it is concerned that the experimental validation of this paper is not sufficient. Further and deeper validations will make this paper stronger.
This paper presents a theoretical analysis of CNN compression using tensor methods. None of the three reviewers have strong opinion; there scores are 5, 6, and 5.  The attempt to understand the mechanism of how tensor decomposition compresses CNNs is meaningful and interesting. However, the main contribution of this work is not sufficiently distinct compared to the existing approaches and the analysis and proof is conduected only for simplified models as mentioned by reviewers. The practical benefit of this paper is not clear and the experimental validation is weak because only a small number of model architectures were tested on a few small datasets.  This is a borderline paper. However, this paper needs to extend its contribution by performing more comprehensive analysis for general CNNs. 
In this work, authors use proxy distributions learned by advanced generative models to improve adversarial robustness. In the discussion period, authors did a good job in addressing reviewers  questions and comments. All reviewers think the paper is above the accept threshold, so do I.
This paper focuses on the problem of detecting visual anomalies within textures. For that purpose, the authors consider several parametric texture models and train anomaly detection models on the corresponding outputs.   Reviewers were generally positive about the topic under study, but were unanimous in signaling a severe weaknesses in the experimental setup. In particular, in R2 words, "my main concern is that the performance evaluation is not suitable to achieve meaningful results", and "showing quantitative results from only two textures does not feel like a very comprehensive analysis". Moreover, the authors did not respond to reviewers feedback. Therefore, the AC recommends rejection at this time.
Main content:  Blind review #1 summarizes it well:  This paper first introduces a method for quantifying to what extent a dataset split exhibits compound (or, alternatively, atom) divergence, where in particular atoms refer to basic structures used by examples in the datasets, and compounds result from compositional rule application to these atoms. The paper then proposes to evaluate learners on datasets with maximal compound divergence (but minimal atom divergence) between the train and test portions, as a way of testing whether a model exhibits compositional generalization, and suggests a greedy algorithm for forming datasets with this property. In particular, the authors introduce a large automatically generated semantic parsing dataset, which allows for the construction of datasets with these train/test split divergence properties. Finally, the authors evaluate three sequence to sequence style semantic parsers on the constructed datasets, and they find that they all generalize very poorly on datasets with maximal compound divergence, and that furthermore the compound divergence appears to be anticorrelated with accuracy.     Discussion:  Blind review #1 is the most knowledgeable in this area and wrote "This is an interesting and ambitious paper tackling an important problem. It is worth noting that the claim that it is the compound divergence that controls the difficulty of generalization (rather than something else, like length) is a substantive one, and the authors do provide evidence of this."     Recommendation and justification:  This paper deserves to be accepted because it tackles an important problem that is overlooked in current work that is evaluated on datasets of questionable meaningfulness. It adds insight by focusing on the qualities of datasets that enable testing how well learning algorithms do on compositional generalization, which is crucial to intelligence.
This manuscript proposes an architectural improvement for generative adversarial network that allows the intermediate layers of a generator to be modulated by the input noise vector using conditional batch normalization. The reviewers find the paper simple and well supported by extensive experimental results. There were some concerns about the impact of such an empirical study. However, the strength and simplicity of the technique means that the method could be of practical interest to the ICLR community.
All reviewers agree in their assessment that this paper has merits but is not yet ready for acceptance into ICLR. The area chair commends the authors for their responses to the reviews.
The rationality of the proposed method, especially its implementation detail, is challenged by the reviewers. Additionally, the experimental part and the writing of the paper should be improved. According to the feedback of the reviewers, I don t think this work is qualified enough at its current status. 
The paper investigates hybrid NN architectures to represent programs, involving both local (RNN, Transformer) and global (Gated Graph NN) structures, with the goal of exploiting the program structure while permitting the fast flow of information through the whole program.  The proof of concept for the quality of the representation is the performance on the VarMisuse task (identifying where a variable was replaced by another one, and which variable was the correct one). Other criteria regard the computational cost of training and number of parameters.  Varied architectures, involving fast and local transmission with and without attention mechanisms, are investigated, comparing full graphs and compressed (leaves only) graphs. The lessons learned concern the trade off between the architecture of the model, the computational time and the learning curve. It is suggested that the Transformer learns from scratch to connect the tokens as appropriate; and that interleaving RNN and GNN allows for more effective processing, with less message passes and less parameters with improved accuracy.  A first issue raised by the reviewers concerns the computational time (ca 100 hours on P100 GPUs); the authors focus on the performance gain w.r.t. GGNN in terms of computational time (significant) and in terms of epochs. Another concern raised by the reviewers is the moderate originality of the proposed architecture. I strongly recommend that the authors make their architecture public; this is imo the best way to evidence the originality of the proposed solution.   The authors did a good job in answering the other concerns, in particular concerning the computational time and the choice of the samples. I thus recommend acceptance. 
This paper presents an approach for mitigating subgroup performance gap in images in cases when a classifier relies on subgroup specific features. The authors propose a data augmentation approach, where synthetically produced examples (by GANs) act as instantiations of the real samples in all possible subgroups. By matching the predictions of original and augmented examples, the prediction model is forced to ignore subgroup differences encouraging invariance. The proposed method of ‘controlled data augmentations’ (as precisely called by R4) is relevant and well motivated, the theoretical justifications support the main claims, and the experimental results are diverse and demonstrate merits of the proposed approach. As rightly pointed out by R3, ‘The appendices are also very thorough, and the code is organized well’.  In the initial evaluation, the reviewers have raised (in unison) concerns regarding overlapping subgroups per class, and an imbalance problem in the subgroups when training GANs. There were also questions reg. theoretical justifications, and empirical evaluations of the baseline methods. The authors have addressed all major concerns in the rebuttal. Pleased to report that based on the author respond with extra experiments and explanations, R2 has raised the score from 6 to 7. In conclusion, all four reviewers were convinced by the author’s rebuttal, and AC recommends acceptance of this paper – congratulations to the authors!  There is a colossal effort in the community addressing a goal similar to this work – learning invariant representations w.r.t. sensitive features by means of algorithmic fairness methods. (R1 and R3 relate to it). When preparing the final version, the authors are encouraged to elaborate more on the discussion/comparison to fairness based methods, ideally including empirical evidence where possible (where subgroups overlap, e.g. CelebA). The AC believes this will strengthen the final revision and will have an even broader impact in the community. 
The paper presents an approach to mitigate the presence of noisy labels during training by trying to forget wrong labels. Reviewers pointed out a few concerns, including lack of novelty, lack of enough experimental support, and lack of theoretical support. Authors have added some experiments and details about the experimental section, but reviewers still think it s not enough for acceptance. I concur with the reviewers to reject the paper.
Knowledge distillation (KD) has been widely used in practice for deployment.  In this paper, a variant of KD is proposed: given a student network, an auxiliary teacher architecture is temporarily generated via dynamic additive convolutions; dense feature connections are introduced to co train the teacher and student models. The proposed method is novel and interesting. Empirical results showed that the proposed method can perform better than several KD variants.  However, it is unclear why the proposed method works, although the authors tried to address this issue in their rebuttal.   Besides this,  a bigger concern on this work is that it missed a comparison with a recent approach in [1] which looks much simpler and performs significantly better on similar experiments.  In [1], their ResNet50 (0.5x) is smaller than the student model in this paper (which used more filters on the top) but showed much stronger performance on both relative and absolute improvements over the same baseline (training from scratch) for the ImageNet classification task. On the technical side, the method in [1] simply uses the original ResNet50 as the teacher model,  and the student model ResNet50 (0.5x) progressively mimics the intermediate outputs of the teacher model from layer to layer. [1] also contains a  theoretic analysis  (mean field analysis based) to support their method. Comparing with the method in [1], the proposed method here is more complicated, less motivated, and less efficient.   [1] D. Zhou, M. Ye, C. Chen, T. Meng, M. Tan, X. Song, Q. Le, Q. Liu and D. Schuurmans. Go Wide, Then Narrow: Efficient Training of Deep Thin Networks. ICML 2020.
The paper proposes a neural net based method for active localization in a known map using a learnt perception model (convnet) and a learnt control policy combined with a set belief state representation. The method compares well to baselines and has good accuracy in 2d and 3d envs. All three reviewers are in favor of acceptance due to the novelty and competitive performance of the approach.
The paper introduces "Concept Embeddings"  to  Prototypical Network, which are part based representations and are learnt by a set of independent networks (which can share weights).  The method first computes the concept embeddings of an input, and then takes the summation of the distances between those concept embeddings and their corresponding concept prototypes in each class to estimate the class probability. The experiments validates the proposed methods on 4 benchmarks in three different domains, including vision, language and biology. For the biology task, the authors also develop a new benchmark on cross organ cell type classification.  The key novel idea of transferable concepts results in significantly improved generalization ability over the existing few shot learning methods.  Although some reviewers raised concerns about not using other few shot image classification datasets such as MiniImageNet these are not appropriate benchmarks, as the method requires the “part based concepts” to reasonably span the space of all images which is a characteristic of fine grained image classification problem.   Although this does limit the scope of the method, the fact that it is applicable for multiple tasks is a strong counteragument to the claim that it is too limited, so overall I disagree with the assessment of one reviewer that the choice of benchmarks is insufficient.  
The authors propose an approach to Bayesian deep learning, by representing neural network weights as latent variables mapped through a Kronecker factored Gaussian process. The ideas have merit and are well motivated. Reviewers were primarily concerned by the experimental validation, and lack of discussion and comparisons with related work. After the rebuttal, reviewers still expressed concern regarding both points, with no reviewer championing the work.  One reviewer writes: "I have read the authors  rebuttal. I still have reservation regarding the gain of a GP over an NN in my original review and I do not think the authors have addressed this very convincingly   while I agree that in general, sparse GP can match the performance of GP with a sufficiently large number of inducing inputs, the proposed method also incurs extra approximations so arguing for the advantage of the proposed method in term of the accurate approximate inference of sparse GP seems problematic."  Another reviewer points out that the comment in the author rebuttal about Kronecker factored methods (Saatci, 2011) for non Gaussian likelihoods and with variational inference being an open question is not accurate: SV DKL (https://arxiv.org/abs/1611.00336) and other approaches (http://proceedings.mlr.press/v37/flaxman15.pdf) were specifically designed to address this question, and are implemented in popular packages. Moreover, there is highly relevant additional work on latent variable representations for neural network weights, inducing priors on p(w) through p(z), which is not discussed or compared against (https://arxiv.org/abs/1811.07006, https://arxiv.org/abs/1907.07504). The revision only includes a minor consideration of DKL in the appendix.   While the ideas in the paper are promising, and the generally thoughtful exchanges were appreciated, there is clearly related work that should be discussed in the main text, with appropriate comparisons. With reviewers expressing additional reservations after rebuttal, and the lack of a clear champion, the paper would benefit from significant revisions in these directions.   Note: In the text, it says: "However, obtaining p(w|D) and p(D) exactly is intractable when N is large or when the network is large and as such, approximation methods are often required." One cannot exactly obtain p(D), or the predictive distribution, regardless of N or the size of the network; exact inference is intractable because the relevant integrals cannot be expressed in closed form, since the parameters are mapped through non linearities, in addition to typically non Gaussian likelihoods.
Three reviewers provided negative reviews and the authors wrote detailed feedback. During the later discussion stages, the reviewers acknowledged that some concerns are alleviated (e.g. R1 raised score from 4 to 5), but two concerns still remain: i) the novelty is less clear to the reviewers; ii) the advantage over existing approaches is not strong enough. I personally think the first concern is somewhat subjective; for the second concern, the authors indeed added a few experiments on "comparison to Top K, SignSGD, EF SignSGD, and PowerSGD", but R1 still maintained that the comparison is "not well demonstrated", which I guess is because R1 views the two works mentioned in R1 s review as "concurrent or later" while R1 pointed out in the discussion that those two works appear before Oct 2019.   R2 pointed out that the paper could have shown the advantage in one of two aspects (see below) but did not.  See the discussions of the three reviewers below.  R1 "The advantage of the proposed compression method over existing approaches is not well demonstrated." R2: "To justify the significance of the contribution, I think the paper should show at least 1 of the followings: When approaching a very high compression ratio, the natural compressor has significantly better testing accuracy compared to the previous work, i.e., the proposed work could push the compression ratio higher than the previous work with nearly no accuracy loss. The authors could use the new compressor alone, or combine it with some other compressors such as top k, as the authors mentioned in the answers. (However, according to the extra experiments, the testing accuracy is slightly better than top k, and slightly worse than power k. None of them makes a significant difference.) The natural compressors can achieve similar accuracy compared to the previous work with the same compression ratio, but with much less computation overhead. (I think this one may work for this paper, as the authors mentioned in their answers. However, to justify such a contribution, we will need to check the training time compared to EF SGD with top k and power k, which seems not included in the extra experiments.)" R4:  "I stand with my point that the novelty is limited."  Overall, I think the paper might have presented an interesting idea, but unfortunately can not be accepted in the current form.  
This paper provides a careful and well executed evaluation of the code level details of two leading policy search algorithms, which are typically considered implementation details and therefore often unstated or brushed aside in papers. These are revealed to have major implications for the performance of both algorithms.  The reviewers are all in agreement that this paper has important reproducibility and evaluation implications for the field, and adds substantially to our body of knowledge on policy gradient algorithms. I therefore recommend it be accepted.  However, a serious limitation is that only 3 random seeds were used to get average performance in the first, key experiment. Experiments are expensive, but that result is not meaningful without more runs, and arguably could be misleading rather than informative. The authors should increase the number of runs as much as possible, at least to 10 but ideally more.
The aper introduces simplicial complex networks, a new class of neural networks based on the idea of the subdivision of a simplicial complex. The paper is interesting and brings ideas of algebraic topology to inform the design of new neural network architectures.   Reviewer 1 was positive about the ideas of this paper, but had several concerns about clarity, scalablity and the sense that the paper might still be in an early phase. Reviewer 2 had similar concerns about clarity, comparisons, and usefulness. Although there were no responses form the author, the discussion explored the paper further, but continued to think the idea is still in its early phase.  The paper is not currently ready for acceptance, and we hope the authors will find useful feedback for their ongoing reasearch. 
This paper describes the development of a large scale continuous visual speech recognition (lipreading) system, including an audiovisual processing pipeline that is used to extract stabilized videos of lips and corresponding phone sequences from YouTube videos, a deep network architecture trained with CTC loss that maps video sequences to sequences of distributions over phones, and an FST based decoder that produces word sequences from the phone score sequences. A performance evaluation shows that the proposed system outperforms other models described in the literature, as well as professional lipreaders. A number of ablation experiments compare the performance of the proposed architecture to the previously proposed LipNet and "Watch, Attend, and Spell" architectures, explore the performance differences caused by using phone  or character based CTC models, and some variations on the proposed architecture. This paper was extremely controversial and received a robust discussion between the authors and reviewers, with the primary point of contention being the suitability of the paper for ICLR. All reviewers agree that the quality of the work in the paper is excellent and that the reported results are impressive, but there was strong disagreement on whether or not this was sufficient for an ICLR paper. One reviewer thought so, while the other two reviewers argued that this is insufficient, and that to appear in ICLR the paper either (1) should have focused more on the preparation of the dataset, included public release of the data so other researchers could build on the work, and put forth the V2P model as a (very) strong baseline for the task; or (2) done a more in depth exploration of the representation learning aspects of the work by comparing phoneme and viseme units and providing more (admittedly costly) ablation experiments to shed more light on what aspects of the V2P architecture lead to the reported improvements in performance. The AC finds the arguments of the two negative reviewers to be persuasive. It is quite clear at this point that many supervised classification tasks (even structured classification tasks like lipreading) can be effectively tackled by a combination of a sufficiently flexible learning architecture and collection of a massive, annotated dataset, and the modeling techniques used in this paper are not new, per se, even if their application to lipreading is. Moreover, if the dataset is not publicly available, it is impossible for anyone else to build on this work. The paper, as it currently stands, would be appropriate in a more applications oriented venue.
Reviewers split on this paper with one arguing that it is an intriguing and significant paper for both neuroscience and deep learning, whereas others argued that it fails to answer some key questions and stops short of offering testable predictions or novel findings. In particular Reviewer 2 questioned the limited experimental predictions and their experimental backing, as well as the plausibility of gradient calculations.  Reviewer 4 raised more fundamental concerns about the significance of the paper s contributions. All reviewers appreciated the paper s clarity. Overall, though, Reviewers 2 and 4 raised enough significant concerns that I cannot recommend acceptance. 
There were several ambivalent reviews for this submission and one favorable one. Although this is a difficult case, I am recommending accepting the paper.  There were two main questions in my mind. 1. Did the authors justify that the limited neighborhood problem they try to fix with their method is a real problem and that they fixed it? If so, accept.  Here I believe evidence has been presented, but the case remains undecided.  2. If they have not, is the method/experiments sufficiently useful to be interesting anyway?  This question I would lean towards answering in the affirmative.  I believe the paper as a whole is sufficiently interesting and executed sufficiently well to be accepted, although I was not convinced of the first point (1) above. One review voting to reject did not find the conceptual contribution very valuable but still thought the paper was not severely flawed. I am partly down weighting the conceptual criticism they made. I am more concerned with experimental issues. However, I did not see sufficiently severe issues raised by the reviewers to justify rejection.  Ultimately, I could go either way on this case, but I think some members of the community will benefit from reading this work enough that it should be accepted.
In this work, the authors propose a simple, under parameterized network architecture which can fit natural images well, when fed with a fixed random input signal. This allows the model to be used for a number of tasks without requiring that the model be trained on a dataset. Further, unlike a recently proposed related method (DIP; [Ulyanov et al., 18]), the method does not require regularization such as early stopping as with DIP.  The reviewers noted the simplicity and experimental validation, and were unanimous in recommending acceptance. 
This paper proposes Restricted AutoEncoders (REAs) for unsupervised feature selection, and applies and evaluates it in applications in biology. The paper was reviewed by three experts. R1 recommends Weak Reject, identifying some specific technical concerns as well as questions about missing and unclear experimental details. R2 recommends Reject, with concerns about limited novelty and unconvincing experimental results. R3 recommends Weak Accept saying that the overall idea is good, but also feels the contribution is "severely undermined" by a recently published paper that proposes a very similar approach. Given that that paper (at ECMLPKDD 2019) was presented just one week before the deadline for ICLR, we would not have expected the authors to cite the paper. Nevertheless, given the concerns expressed by the other reviewers and the lack of an author response to help clarify the novelty, technical concerns, and missing details, we are not able to recommend acceptance. We believe the paper does have significant merit and hope that the reviewer comments will help authors in preparing a revision for another venue.
The paper gives a gradient free method for generating adversarial examples for the code2seq model of source code.  While the reviewers found the high level objectives interesting, the experimental evaluation leaves quite a bit to be desired. (Please see the reviews for more details.) As a result, the paper cannot be accepted in the current form. We urge the authors to improve the paper along the lines that the reviews suggest and resubmit to a different venue.
The paper studies the globally optimal solutions to deep network training problems using convex duality. It derives duals of training problems in which we attempt to fit a dataset while regularizing the network weights with weight decay (L2 regularization). The paper uses strong duality to characterize optimal solutions to several instances of this problem. For fitting deep linear networks, it proves that the globally optimal weights “align” across layers. For fitting ReLU networks, it studies two cases: rank one data and “whitened” data, which satisfy $X^T X   I$. It proves that the optimal weights satisfy certain alignment and orthogonality conditions.   Pros and cons:  [+] The paper uses the machinery of convex duality to characterize alignment of weights in optimal solutions to various neural network training problems. The extension of this approach from shallow networks to deep networks is potentially significant.   [+] The paper is well written and technically precise.   [ ] As noted by the reviewers, the assumptions required to analyze deep ReLU networks are somewhat restrictive. In particular, the paper assumes a form of whitening in which the observed data vectors are orthonormal. This is much a much stronger assumption than the whitening usually applied in statistics, in which a linear transformation is applied to ensure that $XX^T   n I$, i.e., the empirical covariance is the identity. While the paper and rebuttal are correct to argue that SGD often uses minibatches of size $n’ \ll d$, the paper’s main claims are about the globally optimal solutions to the overall training problem.   [+/ ] Several reviewers raise concerns about the significance of results on the rank one case for practice. The paper correctly notes that a number of previous works have studied rank one data, and that this paper generalizes those results to deep networks. The paper gives a very clear and explicit recipe for the optimal weights in this restricted setting.  Reviewers are split on the importance of this generalization — in particular, the extent to which results for the rank one case lead to insights that generalize to higher ranks.   [ ] Experiments verify the theory, in the sense that the theoretically derived weights are equal or better than those learned by SGD, in terms of the training loss. However, the learned networks do not seem to generalize (right panels of Figure 4), again raising concerns about the realism of the setting.   Reviewers evaluation of the paper is split, with most reviewers appreciating its technical rigor and clean resolution of the rank 1/linear/whitened cases. While the review generated enthusiasm for the paper and its results, there were also significant concerns about the relatively restricted setting and the strength of the paper’s implications for training realistic networks, some of which remain after the authors response. 
This is a fascinating paper, and representative of the sort of work which is welcome in our field and in our community. It presents a compiler framework for the development of DSLs (and models) for Deep Learning and related methods. Overall, reviewers were supportive of and excited by this line of work, but questioned its suitability for the main conference. In particular, the lack of experimental demonstrations of the system, and the disconnect between domain specific technical knowledge required to appreciate this work and that of the average ICLR attendee were some of the main causes for concern. It is clear to me that this paper is not suitable for the main conference, not due to its quality, but due to its subject matter. I would be happy, however, to tentatively recommend it for acceptance to the workshop as this topic deserves discussion at the conference, and this would provide the basis for a useful bridge between the compilers community and the deep learning community.
This paper provides a theoretical analysis of GANs, showing its advantages when the measure satisfies the disconnected support property. Its main theoretical results are interesting, but the reviews and discussion shows some misleading places.  It was also found some of the claims and proof are not mathematically rigorous. 
Description of paper content:  The paper proposes a strategy to train a “transition policy” that can connect two pre trained policies. The transition policy tries to reach state action pairs that are within the occupancy distribution of the second policy using Inverse RL. The technique was evaluated on robot manipulation and locomotion problems.   Summary of paper discussion:  The discussion was not lengthy. The reviewers felt the paper was quite well written, instructive, and novel, yet also implied the experimental results were less systematic than might be desired. All reviewers were weakly supportive of publication and made few critical comments. The salient ones concerned the experimental domains, the number of baselines, and the question of the generality of the approach.
The authors present GraphVAE, a method for fitting a generative deep model, a variational autoencoder, to small graphs.  Fitting deep learning models to graphs remains challenging (although there is relevant literature as brought up by the reviewers and anonymous comments) and this paper is a strong start.  In weighing the various reviews, AnonReviewer3 is weighed more highly than AnonReviewer1 and AnonReviewer2 since that review is far more thorough and the reviewer is more expert on this subject.  Unfortunately, the review from AnonReviewer1 is extremely short and of very low confidence.  As such, this paper sits just below the borderline for acceptance.  In general, the main criticisms of the paper are that some claims are too strong (e.g. non differentiability of discrete structures), treatment of related work (missing references, etc.) and weak experiments and baselines.  The consensus among the reviews (even AnonReviewer2) is that the paper is preliminary.  The paper is close, however, and addressing these concerns will make the paper much stronger.  Pros:   Proposes a method to build a generative deep model of graphs   Addresses a timely and interesting topic in deep learning   Exposition is clear  Cons:   Treatment of related literature should be improved   Experiments and baselines are somewhat weak   "Preliminary"   Only works on rather small graphs (i.e. O(k^4) for graphs with k nodes)
The paper presents a technique for learning RL agents to generalize well to unseen environments.  All reviewers and AC think that the paper has some potential but is a bit below the bar to be accepted due to the following facts:  (a) Limited experiments, i.e., consider more appealing baselines/scenarios and provide more experimental details. (b) The proposed method/idea is simple/reasonable, but not super novel, i.e., not enough considering the ICLR high standard (potentially enough for a workshop paper).  Hence, I think this is a borderline paper toward rejection.  
All the reviewers recommend accept, and the found the paper interesting and novel. 
This paper proposes a method of decentralized mechanism design to reduce the price of anarchy. Based on the detailed responses of the authors, all reviewers were satisfied by the technical contribution after the rebuttal period.   There was, however, a heavily engaged and lengthy discussion between most reviewers regarding the applicability of the method and how it links to the motivation given in the paper. The paper could be improved by (1) highlighting an exemplar real world use case in the paper motivation (there are a couple mentioned briefly in the introduction but one of these could be emphasized more); and (2) connecting the choices made in the design of the approach to the opening motivation sections and exemplar use case.  The level of engagement from most reviewers demonstrates a good level of interest from a representative sample of the ICLR community but demonstrated that their remained work outstanding to clarify the core message and significance of the contribution.
This paper proposes a new task domain for learning based AI agents, HALMA, a game that is designed to bring together multiple areas of research in AI. Perception, in the form of recognition of MNIST digits, learning mathematics   in the form of arithmetic operations on the natural numbers, and navigation and planning. When combined into a game, these elements are argued to require various important properties of human cognition, such as abstraction, analogy and affordance.   I commend the ambitious goals of this work, and its multidisciplinary motivations. I believe the benchmark can indeed eventually be an important challenge for the community. I think that the dynamic testing aspect is particularly interesting, where the environment produces trials designed to go beyond the agent s experience to that point. However, having considered the views of the reviewers and read the (main body) paper entirely myself, I unfortunately cannot recommend acceptance in its current form.   The main reason for the decision is simply that it is prohibitively challenging for me to grasp exactly how the game actually works after a thorough reading and considerable thought. The authors spend two pages motivating the approach with (arguably excessively grandiose) allusions to Marr s levels of analysis, Gibson s affordances, Holyoak s analogy and various other famous works from the history of AI and philosophy of mind; as well as to the board game HALMA. But as a reader I can t myself start to make any of these connections because the game proposed by the authors has not been explained to me! It is finally introduced on the fourth page   with reference to Figure 2 which is too small to consult and very hard to interpret. After consulting the appendix (where the idea is a bit clearer) I was able to decipher the way in which the numbers related to the maze itself, but was (and am still) unclear on the actions available to the agent. These are explained as follows:   ""The direction set is t , , , u. The primitive action set, in terms of the numberof moves, is t , , , u; this design of primitive numbers with a maximum of three aligns withthe doctrine of core knowledge in developmental psychology (Feigenson & Carey, 2003; Dehaene,2011). If an option is selected, consecutive hops as in Halma are simulated; all observations fromintermediate states will be skipped, and only the observation of the final state is provided. A movewould fail if a wall stops the agent, leaving the agent’s position unchanged; failure moves bringpenalties to the agent. The agent would receive a positive reward when reaching the goal""  From reading this I am left with the following questions:    Are directions primitive actions?    How can a primitive action also be a number of moves?    What does it mean to select an option?    Can I select an option and an action at the same timestep?   Most importantly, I still don t really know how the game works.   This example is intended to illustrate the difficulty faced by readers of this paper in general.    I note that the reviewers awarded this work scores that place it on the borderline for acceptance, but with consistently low confidence. On consulting with the reviewers it is clear that this is not because they lack expertise but because they too did not understand the full details of how this domain/task works. This is also clear from the lack of detail in their reviews; only reviewer 3 engaged with any of the details of the task itself.   To summarise, I think there is potentially a very interesting and important contribution in this dataset. However, the work will only have impact in the community if it can be understood and adopted after a single read of the paper. I therefore recommend that the authors resubmit this work to a different venue taking account of the following:     Explain how the game works *then* connect it to the literature on human learning *not* vice versa (from the concrete to the abstract)   Be very concrete, perhaps guide the reader through a single particular episode explaining the observations available to the agent and the options open to it at each important point   Get to the point of your contribution. Tenuous connections to cogsci etc can go in the discussion   Make all diagrams and illustrations extremely simple to interpret and large enough to easily read   Avoid use of subjective adjectives, and particularly describing one s own contributions as "ingenious solutions" and "impeccable"   Avoid rhetorical flourishes and latin   Submission to a journal may allow the authors greater space to draw the desired connections to disparate fields without compromising on readability or exposition of their methods
The paper proposes a series of zeroth order optimization approaches to stabilize DARTS training. Although the reviewers think that zeroth order approach is novel to the NAS community, they also point out several weaknesses. In particular, the method will introduce extra computation time and the results are not really standing out comparing with other state of the art methods. Therefore, despite some interesting ideas are presented in the paper, we decide to reject the paper and encourage the authors to address those weaknesses in their future revision.
This paper introduces a bag of techniques to improve contrastive divergence training of energy based models (EBMs), particularly a KL divergence term, data augmentation, multi scale energy functions, and reservoir sampling. The overall paper is well written and clearly presented.   In response to the major concerns from reviewers, the AC recognizes the authors  effort in expanding related work and adding ablation on the effects of the KL loss. However, reviewers remain unconvinced by the significance of the current results. In particular, the quality improvement by adding the KL terms is subtle compared to using reservoir sampling (as evidenced in the contrast of the last two rows in Table 2). Moreover, the authors are also encouraged to compare additionally with recent development in EBM, as pointed out by R2 & R4.  The AC does find the results on downstream tasks such as out of distribution quite promising and interesting. Perhaps it s worth expanding the discussion with formal reasoning on why KL loss helps in this case.   All four knowledgeable reviewers are leaning towards rejection, the AC respects and agrees with the decision.  
All three reviewers agreed that the paper should not be accepted. No rebuttal was offered, thus the paper is rejected.
This paper studies the generalization error of semi supervised learning, where the algorithm gradually pseudo labels the data throughout the learning process. Theoretically, an upper bound on the generalization error is shown to decompose into a term that vanishes with successive labeling and another that does not, leading to a plateau in performance. This is studied analytically for a mixture of two Gaussians. Experimentally, similar behavior is also observed to occur in more realistic scenarios. What reviewers struggled with is to understand what part of the results are, to some extent, obvious, and what offer deeper insight. What is obvious: even if a Bayes classifier were available for pseudo labeling, feature overlap means that there is a plateau of noise beyond which labeling cannot improve. What is not obvious: is it even worth pseudo labeling, or could we make things worse? The merit of the paper is in elucidating the latter. There are several concerns that remain, however, even after discussions. First, there is whether the insight is substantial or not. Here, some comparison and contrast with existing literature suggests otherwise. Second, there is whether the experimentally observed behavior is an instance of the phenomenon described by theory. Here, better structured experiments are needed to tie in with the theory. Overall, although the paper presents compelling insight, it is not yet ready to disseminate. It needs a stronger argument for its added theoretical contribution and clearer experiments to support that the presented theory is indeed behind the empirical behavior of these iterative algorithms.
The updated draft has helped to address some of the issues that the reviewers had, however the reviewers believe there are still outstanding issues. With regard to the technical flaw, one reviewer has pointed out that the update changes the story of the paper by breaking the connection between the generative and discriminative model in terms of preserving or ignoring conditional dependencies.  In terms of the experiments, the paper has been improved by the reporting of standard deviation, and comparison to other works. However it is recommended that the authors compare to NAS by fixing the number of parameters and reporting the results to facilitate an apples to apples comparison. Another reviewer also recommends comparing to other architectures for a fixed number of neurons.
This paper combines two different types of existing optimization methods, CEM/CMA ES and DDPG/TD3, for policy optimization. The approach resembles ERL but demonstrates good better performance on a variety of continuous control benchmarks.  Although I feel the novelty of the paper is limited, the provided promising results may justify the acceptance of the paper.
The paper provides a novel analysis of the robustness to adversarial attacks in network representation learning. It appears to be a useful contribution for important class of models; however,  the detailed reviews (1 and 2) raise some concerns that may require a bit of further work (though partially addressed in revised version).
The authors present a new framework to make deep ensembles provide better coverage of the posterior and be less reliant on initialisation. The authors generally did a good job presenting their approach, avoiding dubious claims that deep ensembles are non Bayesian, and instead focusing on ways in which deep ensembles can be improved, practically and theoretically. It is worth noting in a revised version, however, that many approximate inference procedures do not have theoretical guarantees. The claim that deep ensembles have "arbitrary bad approximation guarantees" is vague and appears to single them out in a way that could confuse the reader. Regarding priors, it is also worth noting that Wilson & Izmailov (2020) provide evidence that the prior in weight space induces a prior in function space with useful properties, although the prior can be improved.  The authors do a good job of responding to reviewers, and describing limitations. Ultimately, however, the general opinion was not swayed to accept. In addition to reviewer concerns, the experimental evaluation could be substantially improved. There are several procedures that build on deep ensembles to capture uncertainty within modes. How does this procedure compare? Why are no likelihood evaluations considered? What about accuracy? In its present form, it s unclear what practical value the contributions are providing, besides possibly better OOD detection, but even that direction is explored in a relatively limited way. It could also be interesting to measure the distance of the predictive distribution to a good proxy for the Bayesian model average. Overall, there are the raw ingredients of a good paper here, and the authors are encouraged to continue with this work.
This paper gives a scalable Laplace approximation which makes use of recently proposed Kronecker factored approximations to the Gauss Newton matrix. The approach seems sound and useful. While it is a rather natural extension of existing methods, it is well executed, and the ideas seem worth putting out there. 
The paper builds upon a recent paper BiQGEMM, providing a binary coding based post training quantization technique. The authors show how to combine magnitude based importance metrics to these techniques and achieve superior performance. The use of importance metrics for quantization and pruning is not new, and magnitude based metrics are among the more common metrics. With that in mind, the novelty of the paper is in the integration of importance metrics to the techniques of BiQGEMM. The provided methods lead to several hyper parameters and the task of tuning these can be non trivial and time consuming. Due to this the authors devote a detailed section showing how to properly tune these hyper parameters. This is appreciated and indeed alleviates the problem coming with new hyper parameters.  The paper received mixed opinions by the reviewers related to its overall novelty, but the resulting conclusion is that although the combination of binary coding based quantization with importance scores is not trivial, the challenges faced relate more to correct implementation as opposed to scientific novelty. Combined with other issues raised by the reviewers such as a need for further comparison with existing work, this lead me to recommend rejection for this paper. 
meta score: 4  This paper is primarily an application paper applying known RL techniques to dialogue.    Very little reference to the extensive literature in this area.  Pros:    interesting application (digital search)    revised version contains subjective evaluation of experiments  Cons:    limited technical novelty    very weak links to the state of the art, missing many key aspects of the research domain 
The meta learning framework based on learning the loss function for time series forecasting is an interesting and important topic. However, the reviewers think the literature, baselines, and experimental results need significant improvement.
The reviewers raised a number of concerns about the novelty of the paper and comparisons. The authors were able to address the concerns regarding the comparisons in the response, and the reviewers unanimously agree that the paper should be published. I do think however that this paper is quite borderline. I agree with the reviewers that the updated experiments are convincing in terms of the provided comparisons. However, the reservations I have about the work can perhaps best be stated as follows: There is quite a bit of work in the area of imitation from observations, which makes a range of different assumptions and utilizes a variety of different domain adaptation techniques. Much of this work is in the robotics domain (which is cited in the paper), and much of it demonstrates results in fairly realistic settings, often with real humans and real robots. In comparison, the experiments in this paper are quite simplistic, using toy domains and "demonstrations" obtained from a computational oracle (i.e., another policy). Given the maturity of this field and the current state of the art, I am skeptical of this evaluation, and I think TPIL is a very weak baseline. That said, I would  defer to the reviewers in this case   I do think the particular technical contributions that the paper makes are a valuable addition to the literature, though somewhat incremental. I am also sympathetic to the authors in that much of the more successful prior work in this area that does evaluate under realistic conditions makes subtly different assumptions, or utilizes different techniques for which it is difficult to provide an apples to apples comparison.  One thing I would request of the authors for the camera ready though is: Please tone down the claims. "Human like 7 DOF Striker" is not human like, it s a crudely simulated robotic arm that was recolored. It would of course be better to have a realistic evaluation (as many prior papers in this field indeed have), but in the absence of that, it is best not to overclaim and be upfront that the evaluation is on relatively simple simulated tasks under conditions that are not necessarily realistic (and have nothing to do with actual humans), but meant rather to evaluate in an apples to apples manner the particular algorithmic innovations in the method.
Authors present a method for disease classification and localization in histopathology images. Standard image processing techniques are used to extract and normalize tiles of tissue, after which features are extracted from pertained networks. A 1 D convolutional filter is applied to the bag of features from the tiles (along the tile dimension, kernel filter size equal to dimensionality of feature vector). The max R and min R values are kept as input into a neural network for classification, and thresholding of these values provides localization for disease / non disease.  Pro:    Potential to reduce annotation complexity of datasets while producing predictions and localization  Con:   Results are not great. If anything, results re affirm why strong annotations are necessary.   Several reviewer concerns regarding novelty of proposed method. While authors have made clear the distinctions from prior art, the significance of those changes are debated.  Given the current pros/cons, the committee feels the paper is not ready for acceptance in its current form.
This paper proposes an approach to allow a neural network to memorize and reason over a long time horizon. Experiments on synthetic datasets, question answering, and sequence recommendation are presented to evaluate the proposed method.   The paper addresses an important problem of processing long sequences. However,  all reviewers agree that the writing of the paper can be improved (i.e., motivation, details of experiment design/setup, and others below). Importantly, I think the authors need to elaborate the differences of continual memory with existing episodic memory methods. The authors added a paragraph about continual learning during the rebuttal period, and mentioned that their continual memory focuses on remembering infinite information stream without forgetting. Episodic memory models can be applied/adapted for this purpose, so the authors should at least compare with one of them (ideally more).
This paper presents a new method for clustering multiple graphs, without vertex correspondence, by combing existing approaches on graphon estimation and spectral clustering. All reviewers agree that this is a neat paper with new theoretical and empirical results. The main concerns were also properly addressed during rebutal. Overall, it is a good paper.
This paper proposes and studies a variant of policy optimization mirror descent policy optimization (MDPO) which was inspired by the mirror descent algorithm in the optimization literature. The proposed algorithm attempts to find a policy parameter that maximizes the expected regularized advantage function,  where the regularization term is based on the KL divergence between the new policy iterate and the current policy iterate. The main contributions are algorithmic and empirical, with detailed discussions provided to illuminate the connection between MDPO and other existing policy optimization paradigms like TRPO, PPO, etc. The paper provides an interesting and useful contribution to the growing literature of policy optimization.
The paper presents a hybrid architecture which combines WaveNet and LSTM for speeding up raw audio generation. The novelty of the method is limited, as it’s a simple combination of existing techniques. The practical impact of the approach is rather questionable since the generated audio has significantly lower MOS scores than the state of the art WaveNet model.
All reviewers agree that this paper is a useful and valuable contributions to ML engineering.    insightful analysis .. highly user friendly operator design    "useful and I can see it having large adoption in the community of scientific computing" ... "    "Personally I tend to buy these advantages of einops" ... "However, there is a lack of solid empirical study to validate the effectiveness and efficiency of the design"    "a useful and appealing new coding tool."  The negative reviewers appear fixated on the (true) observation that the paper does not look like a conventional ICLR paper, thati it "reads like a technical blog", and "lacks rigour". I belive it is fair and measured to state that these reviews may be considered to exhibit aspects of gatekeeping: requiring more "mathiness" that does not help the paper, or more "rigour" through user studies that are in fact less valuable than the reviewers  own observations "I could see myself...", "I tend to buy...".  This is a paper about design, not about models or algorithms (although the algorithmic work is good).  It is about the design of tools that we all use, and about the decisions and thought processes that led to that design.  A reviewer decries "many non rigorous claims".  These are claims about the usability of existing systems, and mostly appear in the discussion and footnotes, as the authors note in rebuttal.  Of course, one could have run user studies to back up each claim, but I am just as convinced by the examples shown in the paper.  It matters not to me what some users corralled into a user study thought.  It matters what I and my colleagues will think, and I am now sure to recommend einops to colleagues.  I would not have met it had the paper not been submitted to ICLR, and hence I am certain it should be accepted, so more can see that we care not just about mathiness, but actually enabling progress in our field.  The job of a conference like ICLR is to expose researchers and practitioners in machine learning to ideas and techniques that may advance their research and practice.  Programming, and the translation of mathematical ideas to efficient computer code, are fundamental to all of machine learning, and hence programming models are very much suitable for presentation to an ICLR audience.  I am certain that this paper, and the technology it describes, are more important to ICLR readers than knowing that if module A is co trained with module B, then combined with compression C, the SOTA on some arbitrary benchmark is increased by 0.31 +/  1.04.   Reviewer gRMH says "there is no code", but the code has been in the open for three years; it is an accident of our misapplication of the principles of blind review that the reviewer felt they could not search for the code, and that the authors felt they could not bring to bear the evidence that three years of real world usage have brought.    Reviewers say the work is just an extension of einsum, while noting that the extension is useful and nontrivial.  Yes, it is an extension, and the paper s examples show how it yields more compact code that is also more readable and maintainable.  I could add more examples, but in short, I tend to side with the authors  response at almost every point.  At the same time, the final version of the paper has been strengthened by this dialectic, and I expect further strenghtening through exposure to the ICLR community.  To the authors: Listing 1 is useful, but should be in an appendix.  Instead, add examples of ellipses on P5, and show more inline examples in general.  The paper would be strengthened by another pass over the English   after the decision is made I would be happy to volunteer to help.
This work starts from the observation that maximum likelihood estimation, while consistent, has a bias on a finite sample which is likely to hurt for small sample sizes. From this, they apply Firth bias reduction to the few shot learning setting and demonstrate its empirical benefits, notably relatively to L2 regularization or label smoothing alternatives.  After some discussion with the authors, all reviewers are supportive of this work being accepted. Two are also suggesting this work be featured as a spotlight.  The proposed method is simple, well motivated, and appears to be effective. Therefore, I m happy to recommend this work be accepted and receive a spotlight presentation.
This work aims at doing Bayesian inference via Langevin dynamics with data subsampling. This builds on previous work with "replica exchange" where parallel chains are run at different temperatures and can be swapped to encourage moving between modes. The main technical novelty here is a scheme to reduce variance. This is done in the style of SGRD by periodically computing the gradient on all data and then using those values as control variates. This is shown to reduce variance.  Reviewers generally felt that this represented a sensible combination of known ideas aimed at an important and timely problem with sufficient empirical evaluation. There was consensus the paper was clearly written. I concur that even if the combination is "expected" to work, the presence of guarantees for performance represent sufficient technical novelty. I particularly applaud the fact that the paper does not over claim and generously gives credit to related work. This is helpful to the reader and encourages the flow of ideas. For these reasons I recommend acceptance of the paper.  In reading the paper, I had a couple questions about the experiments:  1. It s not obvious to me from the experiments how specific the method is to the replica exchange setting. The main control variate idea appears to be applicable without replica exchange. I would very much like to see a "VR SGHMC" row in Table 1 unless there is a good reason that this cannot be done. It would be very beneficial to understand the contributions of these different algorithmic components.  2. The CIFAR experiments directly test variance. That s fine, the paper is aimed at reducing variance, after all. However, I would like to see more tests of the follow on improvements in optimization speed. It has been my experience that improvements in variance sometimes produce surprisingly small improvements in optimization speed. My intuition for this is that reduced variance mostly helps by making it possible to use a larger step size without the same penalty in the stationary dist. In practice, the step size typically ends up being imperfect, meaning that changes in variance have small changes. 
This paper tackles a very important topic in deep RL, namely automatic (non differentiable) hyper parameter tuning. It does so by combining ideas from genetic algorithms and neural architecture search with shared experience replay in order to obtain the key property of sample efficiency.  The proposed solution is communicated clearly, and the results are compelling (often 10x improvements), as well as qualitatively interesting.  Unfortunately for the authors, their original submission contained only part of the intended results, hence the borderline scores by some reviewers. In the meanwhile, a second suite of experiments have been added, which I think are compelling enough evidence to validate the paper s approach.
The paper puts forward a theoretical investigation of the learnability of                                                           tree structured Boolean circuits with neural networks.                                                                              The authors identify *local correlations*, ie correlation of each internal                                                          target circuit gate with the target output, as critical property for                                                                characterizing learnability by layerwise training.                                                                                                                                                                                                                      The reviewers agree that the paper is well written and content to be correct                                                        (to the best of their knowledge).                                                                                                   However, they have reservations about the strength of the assumptions about the                                                     target functions as well as the layerwise training procedure.                                                                                                                                                                                                           I think this paper is slightly below acceptance threshold for ICLR, which is a quite                                                applied conference.                                                                                                                 The assumptions are quite strong, ie local correlations and the topology of the                                                     circuit to be known as well as layerwise training, and possibly too far removed                                                     from current deep learning practice.
This paper proposes an active intervention targeting mechanism for causal structure discovery. After the discussion, there was a consensus among the reviewers that this paper needs another round of revision to address the lingering concerns. These concerns include providing a more fair experimental setup (e.g. by properly distinguishing and designing proper experiments for the observational, random intervention, and targeted intervention settings). Since the paper lacks theoretical guarantees (which is OK and not a requirement for acceptance), the merits rest on providing a thorough and fair experimental evaluation.
In order to evaluate the evidence lower bound (ELBO), VAEs typically use a parametric distribution based decoder $p(x|z)$. If the data is continuous, one often considers a Gaussian VAE, where the canonical setting is to assume a diagonal covariance matrix $p(x|z)   N(x; \mu(z), \sigma^2 \mathbf{I})$. In this paper, the authors suggest replacing the diagonal covariance matrix with a structured covariance matrix (low rank + diagonal). As this only amounts to a minor change to a canonical Gaussian VAE, strong empirical results are expected to justify its acceptance. However, the image generation results presented in the paper are not comparable to the state of the art VAE results (e.g., Arash Vahdat, and Jan Kautz. "NVAE: A Deep Hierarchical Variational Autoencoder." Neural Information Processing Systems (NeurIPS), 2020).
This work extends Druckmann and Chklowskii, 2012 and demonstrates some interesting properties of the new model. This would be of interest to a neuroscience audience, but the focus is off for ICLR.
This heavily disputed paper discusses a biologically motivated alternative to back propagation learning.   In particular, methods focussing on sign symmetry rather than weight symmetry are investigated and, importantly, scaled to large problems.  The paper demonstrates the viability of the approach.  If nothing else, it instigates a wonderful platform for debate.  The results are convincing and the paper is well presented.  But the biological plausibility of the methods needed for these algorithms can be disputed.  In my opinion, these are best tackled in a poster session, following the good practice at neuroscience meetings.  On an aside note, the use of the approach to ResNet should be questioned.  The skip connections in ResNet may be all but biologically relevant.
Dear authors,  I have read the reviewers and your careful rebuttals. I would have liked to see much more engagement from the reviewers. However, even after your rebuttal, no reviewer suggested acceptance, with two reviewers proposing reject (3) and two proposing weak reject (5).  The reviewers found the paper well written. I concur. The reviewers also notice that the contributions are very marginal compared to prior literature. Personalized FL formulation studied here was in a simpler form first proposed by Hanzely and Richtarik (Federated learning of a mixture of global and local models, 2020) and later generalized by Hanzely et al   a paper the authors cite. That work performed an in depth analysis, also including the nonconvex case, which the authors (claim) did not notice. Compared to that work, the authors perform an analysis in the partial participation regime. However, partial participation is by now a standard technique which can usually be combined with other techniques without much difficulty. The authors tried to argue that their analysis approach is unique, but the reviewers remained unconvinced.   In summary, I think this is a solid piece of work which is perhaps judged, looking at the raw scores, a bit too harshly. However, most verbal comments are indeed fair. I am also of the opinion that the paper in its current form does not reach the necessary bar for acceptance. I would encourage the authors to carefully revise the manuscript, taking into account all feedback that they find useful. I think the paper can be improved, with not too much effort perhaps, to a state in which the bar could be reached.   Kind regards,  Area chair
This paper contributes to the recently emerging literature about applying reinforcement learning methods to combinatorial optimization problems. The authors consider TSPs and propose a search method that interleaves greedy local search with Monte Carlo Tree Search (MCTS). This approach does not contain learned function approximation for transferring knowledge across problem instances, which is usually considered the main motivation for applying RL to comb opt problems.  The reviewers state that, although the approach is a relatively straight forward combination of two existing methods, it is in principle somewhat interesting.  However, the experiments indicate a large gap to SOTA solvers for TSPs.  No rebuttal was submitted.  In absence of both SOTA results and methodological novelty, as assessed by the reviewers and my owm reading, I recommend to reject the paper in its current form.
The paper deals with adversarial attacks on graph neural networks, a new and promising field in graph representation learning. The paper analyzes a new extreme setting of attack for a single node, and presents important insights, albeit not new algorithms.  The reviewers were not particularly enthusiastic and complained about   limited novelty in light of Zuegner et al   missing baselines   doubts about the attack setting with a selected attacker node  The authors provided an elaborate rebuttal, including specific responses to the above questions, however, the final scores are not quite above the bar, especially having in mind the sheer number of submissions on graph deep learning this year. We, therefore, recommend rejection and encourage the author to publish the paper elsewhere.  
This work proposes a robust variant of GAN, in which the generator and discriminator compete with each other in a worst case setting within a small Wasserstein ball. Unfortunately, the reviewers have raised some critical concerns in terms of theoretical analysis and empirical support. The authors did not submit rebuttals in time. We encourage the authors to improve the work based on reviewer s comments. 
The authors consider the impact of designing fair algorithms on adversarial robustness. The particular focus is on poisoning attacks. They show experimentally that for some datasets and models/algorithms that using "fair" algorithms increase adversarial vulnerability compared to the standard training procedure (ignoring fairness criteria). The reviews have raised questions about whether the experimental results are extensive enough and I share their concerns. Most importantly, the authors have not addressed the question regarding to the quality of approximation at all.
The paper examines the idea that real world data is highly structured / lies on a low dimensional manifold. The authors show differences in neural network dynamics when trained on structured (MNIST) vs. unstructured datasets (random), and show that "structure" can be captured by their new "hidden manifold" generative model that explicitly considers some low dimensional manifold.  The reviewers perceived a lack of actionable insights following the paper, since in general these ideas are known, and for MNIST to be a limited dataset, despite finding the paper generally clear and correct.  Following the discussion, I must recommend rejection at this time, but highly encourage the authors to take the insights developed in the paper a bit further and submit to another venue. E.g. trying to improve our algorithms by considering the inductive bias of structure of the hidden manifold, or developing a systematic and quantifiable notion of structure for many different datasets that correlate with difficulty of training would both be great contributions.
This paper introduces an approach for estimating the quality of protein models. The proposed method consists in using graph convolutional networks (GCNs) to learn a representation of protein models and predict both a local and a global quality score. Experiments show that the proposed approach performs better than methods based on 1D and 3D CNNs.  Overall, this is a borderline paper. The improvement over state of the art for this specific application is noticeable. However, a major drawback is the lack of methodological novelty, the proposed solution being a direct application of GCNs. It does not bring new insights in representation learning. The contribution would therefore be of interest to a limited audience, in light of which I recommend to reject this paper.
Earlier work suggests that adversarial examples exploit local features and that more robust models rely on global features. The authors propose to exploit this insight by performing data augmentation in adversarial training, by cutting and reshuffling image block. They demonstrate the idea empirically and witness interesting gains. I think the technique is an interesting contribution, but empirically and as a tool. 
This contribution investigates and takes a step back on an important problem in recent ML, namely the impact of the noise distribution in density estimation using Noise Contrastive Estimation. The work offers both theoretical insights and convincing experiments.  For these reasons, this work should be endorsed for publication at ICLR 2022.
This paper explores the memorization of tokens in prior context in LSTM and Transformer based language models. While all reviewers agree this is an interesting and important direction worth studying, they raise several concerns about the validity of the experimental setup and the conclusions drawn about LSTMs. Primarily the shallow depth of the LSTM architecture seems to confound the main conclusion about their inferiority (Reviewer WHFY). Further exploration about what makes transformers better (e.g. attention) is also important to provide a more complete picture (Reviewer ax86). Other concerns include the use of synthetic data (Reviewer tpb6), a limited number of noun lists (Reviewer r2TC) and the lack of discussion about the practical significance of verbatim recall (Reviewer M2A9). Overall, while the paper takes a step towards an important insight about pretained LMs, it needs to be polished further and hopefully can be published at a future conference.
This paper proposes a way to automatically weight different tasks in a multi task setting.  The problem is a bit niche, and the paper had a lot of problems with clarity, as well as the motivation for the experimental setup and evaluation.
Good clarity: a NAS benchmark for ASR and results transferable across datasets. Although this is more specific to speech domain, building such a benchmark for speech is important for general NAS research, especially the papers finds different behaviors compared to image classification benchmarks.   The main factor for the decision is the clarity and importance for NAS in speech domain. 
In this paper, the authors propose a theoretically principled neural network that inherently resists ℓ∞ perturbations without the help of adversarial training. Although the authors insist to focus on the novel design with comprehensive theoretical supports, the reviewers still concern the insufficient empirical evaluations despite the novel idea and theoretical analysis.
This paper studies extensions of the Scattering Graph Transform to spatio temporal domains. By exploring several design choices for spatio temporal wavelet filters, the authors provide a solid and broad study of such predefined represenatations, including stability analysis as well as extensive empirical evaluations.  Reviewers were generally favorable, and highlighted the importance of this method as providing a simple yet powerful baseline for spatio temporal graph prediction tasks that requires no training. Despite some concerns about lack of analysis of the empirical results, the AC believes this work will provide a valuable baseline for future research and therefore recommends acceptance as a poster. 
Overall, the committee finds this paper to be interesting, well written and proposes an end to end model for a very relevant task.  The comparisons are also interesting and well rounded.  Reviewer 2 is critical of the paper, but the committee finds the answers to the criticisms to be satisfactory.  The paper will bring value to the conference.
While this paper would be significantly improved with experiments on real data, the reviewers all agreed that there is value in the ideas and simple experiments in this paper and all voted for acceptance after the discussion period.  We encourage the authors to consider adding an experimental evaluation in more realistic settings (e.g. with real data) in the final version of the paper.
This paper presents a probabilistic framework that explains why models trained adversarially are robust generators. It received fairly high initial scores. The reviewers thought the work was novel and interesting. They liked that the analysis provided a way to derive a novel training method and sampling algorithms. Reviewers confirmed their support of acceptance and I think this paper is clearly above the bar. Respectfully, I’d prefer that the authors don’t ask the reviewers to “raise your score”. It is up to the reviewers to make that decision.
 This is an interesting direction and all reviewers thought the idea has merit, but pointed out some significant limitations. The authors did an admirable job at addressing some of these but some remain, including R1’s point 2 which is a significant issue. The authors are encouraged to submit a revised version of their work which addresses all the discussed limitations and will likely be a competitive submission to another top ML conference. 
This paper proposes a novel method called CCE for explaining mistakes by DNNs on image classification. It is built on top of two prior ideas: counterfactual explanations and concept activation vectors. CCE explains a mistake by assigning scores to a shot list of concepts, where a large positive score means that adding that concept to the image will increase the probability of correctly classifying the image, as will removing or reducing a concept with a large negative score. The strengths of the paper include novel combination of previous work, clear presentation, interesting experiments, convincing results on controlled settings.  The weaknesses include the lack of results on less controlled settings, the lack of more meaningful spurious correlations in the medical examples, and the lack of user studies. Although the reviewers have shown interests in this paper, they clearly do not support the paper strongly. In addition, the authors have missed the following paper that also combines counterfactual explanations and concept activation vectors: Akula, Arjun, Shuai Wang, and Song Chun Zhu. “Cocox: Generating conceptual and counterfactual explanations via fault lines.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 03. 2020.
This article proposes latent variable augmentation scheme for inference in nonlinear multivariate Hawkes processes. It combines existing approaches (Polya gamma and sparsity inducing variables) in a sensible way and is clearly written. Concerns were raised with respect to the comparison to alternative baselines, and answered by the authors. As a result, some reviewers have increased their score, and I recommend acceptance. 
This paper discusses the problem of cross domain lossy compression on the basis of its reformulation as an entropy constrained optimal transport. Two average distortion measures (without and with common randomness) are defined (Definitions 2 and 3), and some of their properties are investigated, as summarized in Theorems 1 3. The authors also demonstrated in Section 2.2 that in the Bernoulli Hamming case the common randomness can indeed improve the performance under some conditions. Results of the numerical experiments on super resolution and denoising are presented to illustrate the principles derived from the theoretical considerations.  This paper received 5 reviews, with score/confidence being 8/3, 6/3, 6/2, 8/3, 3/4, which exhibit a relatively large spread across the borderline. Upon reading the reviews and the author responses, as well as the paper itself, I think that this paper proposes an interesting framework of optimal transport with entropy bottleneck, as well as architectural designs supported by the theoretical development, with potential image processing applications. The authors have provided further numerical results in their response.  My main concern is that the arguments in this paper are somehow confusing in that they borrow several notions and terms from the context of lossy compression and rate distortion theory in the field of information theory, and use them in quite different meanings without explicitly stating so. (It seems to me that this would have been one major reason for the negative evaluation by Reviewer LfvG.) Examples are: 1. **Target distribution:** In rate distortion theory the target distribution $p_Y$ is not fixed, whereas it is fixed in this paper. 2. **Rate constraint:** In rate distortion theory the rate constraint is imposed in terms of the mutual information $I(X;Y)$ between the source $X$ and the target $Y$, in a form $I(X;Y)\le R$. The justification of this particular form of the rate constraint rather than any other forms is that it is compatible with the operational achievability/converse arguments via explicit construction of encoder/decoder pairs. In this paper, on the other hand, the authors consider a Markov chain $X\to Z\to Y$ and impose the rate constraint on the entropy $H(Z)$ of the intermediate random variable $Z$. Under the Markov assumption one has $I(X;Y)\le H(Z)$, so that the rate constraint in this paper is stronger than that in rate distortion theory, and one would have no control over how tight/loose the adopted constraint $H(Z)\le R$ is against $I(X;Y)\le R$. In relation to this, the expression "identify the tradeoff between the compression rate and minimum achievable distortion" (page 2, line 12) would be at best misleading, as the arguments in this paper might be suboptimal, not necessarily providing the theoretically best achievable results. 3. **Extension versus single shot:** In rate distortion theory one usually considers $n$th extension of a source and a block encoder/decoder pair with blocklength $n$. On the other hand, this paper considers what is called the "single shot" setting, in which one does not consider extension of sources. There are some pieces of work on lossy compression in the single shot setting [C1][C2], so that I would be interested in how such pieces of work and the development in this paper will be related, an issue not explored at all in this paper.  As a result, although the quantities $D_{\mathrm{ncr}}$ and $D_{\mathrm{cr}}$ defined in Definitions 2 and 3 would look quite like the distortion rate functions in rate distortion theory, they are actually not the distortion rate functions at all. Although the authors, perhaps carefully, did not call them distortion rate functions, there should still be some explicit explanation on the difference of their framework from the standard one in information theory.  [C1] Nir Elkayam and Meir Feder, "One shot approach to lossy source coding under average distortion constraints," IEEE International Symposium on Information Theory, 2389 2393, 2020. [link](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp &arnumber 9173943)  [C2] ibid., "One shot approach to lossy source coding under average distortion constraints," [Online]. Available: https://arxiv.org/abs/2001.03983  Another concern of mine, from the viewpoint of the theory of optimal transport, is regarding the optimal transport map. It is known that under fairly general conditions the optimal transport *plan* exists. However the optimal transport *map* is not guaranteed to exist, and even if it exists it can be highly irregular. (See, e.g., Villani, 2009) The descriptions in this paper, such as "computing the optimal transport map is not straightforward" and "learn approximately optimal mappings" on page 4 would be too naive in that they would assume existence and approximability of the optimal mapping.  Despite these concerns, most reviewers agree that this paper presents an interesting piece of work. I would therefore like to recommend acceptance of this paper, and would appreciate it if the authors consider addressing appropriately the above concerns of mine in the final version.
In this paper, a new learning scheme for minimizing the confidence set by conformal prediction is proposed. Most of the reviewers agree that the idea is interesting and novel. This is an important contribution to trustworthy ML, with theoretically sound considerations and thorough experimental validation.
The paper proposes a new optimization approach for neural nets where, instead of a fixed learning rate (often hard to tune), there is one learning rate per unit, randomly sampled from a distribution. Reviewers think the idea is novel, original and simple. Overall, reviewers found the experiments unconvincing enough in practice. I found the paper really borderline, and decided to side with the reviewers in rejecting the paper.
This paper presents a method for training sparse neural networks that also provides a speedup during training, in contrast to methods for training sparse networks which train dense networks (at normal speed) and then prune weights.  The method provides modest theoretical speedups during training, never measured in wallclock time.   The authors improved their paper considerably in response to the reviews.  I would be inclined to accept this paper despite not being a big win empirically, however a couple points of sloppiness pointed out (and maintained post rebuttal) by R1 tip the balance to reject, in my opinion.  Specifically:   1) "I do not agree that keeping the learning rate fixed across methods is the right approach."  This seems like a major problem with the experiments to me.  2) "I would request the authors to slightly rewrite certain parts of their paper so as not to imply that momentum decreases the variance of the gradients in general."  I agree.
This paper is enthusiastically supported by all three reviewers. Thus an accept is recommended.
This paper proposes verification algorithms for a class of convex relaxable specifications to evaluate the robustness of neural networks under adversarial examples.  The reviewers were unanimous in their vote to accept the paper. Note: the remaining score of 5 belongs to a reviewer who agreed to acceptance in the discussion.
The authors propose an adaptive block wise coordinate descent method and claim faster convergence and lower generalization error. While the reviewers agreed that this method may work well in practice, they had several concerns about the relevance of the theory and strength of the empirical results. After considering the author responses, the reviewers have agreed that this paper is not yet ready for publication. 
Paper presents an approach and evaluation setting for few shot learning in histology images. The approach leverages contrastive learning pretraining, and latent augmentation (LA) for data augmentation. The evaluation examines in domain few shot learning, mixed domain few shot learning, and out of domain few shot learning.    Latent augmentation is an approach to learn how categories vary between samples within unsupervised clusters in a base dataset, and transfer that variation to the few shot sampled classes.   Pros:   A couple reviewers have claimed as a strength the novelty of the proposed latent augmentation method, but as other reviewers point out, there is much work in this field, some of which wasn t cited (i.e. Delta Encoder, NeurIPS 2018).   The latent augmentation method is simple to implement, and outperforms standard input augmentation approaches.   The paper is rich in content and details of experiments.   Examining learning over a variety of domain shift settings is interesting.   Shows contrastive learning can outperform supervised pretraining for this application domain.  Cons:   Multiple reviewers raise concerns about technical novelty. This work applies mostly previously proposed methods, or variations thereof, to the domain of medical imaging. May be more suited to a medical imaging venue.    Some of the results are consistent with prior reports, such as finding that self supervised learning can outperform supervised pretraining. In that regard the results are not surprising.   One reviewer raised issues about lack of comparison to other relevant few shot works. Authors argue that fine tuning is a competitive baseline. Authors did add comparison to one other variation augmentation approach, distribution calibration. But as mentioned, delta encoder is a very related work, which has not been cited nor compared against. Biggest difference is that delta encoder uses labels, but the unsupervised clusters can trivially be supplied as labels in this setting. AC feels authors should have done a more comprehensive comparison to related learned augmentation works.    Authors initially did not address how latent augmentation is affected by random seeds, but authors have replied to reviewers with additional data.  Reviewer consensus, excluding 1 reviewer, favors accept, though significant concerns regarding technical novelty and comparisons to other relevant works persist (especially in regards to works that learn how to augment as the proposed LA method does).
This paper addresses the issue of numerical rounding off errors that can arise when using latent variable models for data compression,  e.g., because of differences in floating point arithmetic across different platforms (sender and receiver). The authors propose using neural networks that perform integer arithmetic (integer networks) to mitigate this issue. The problem statement is well described, and the presentation is generally OK, although it could be improved in certain aspects as pointed out by the reviewers. The experiments are properly carried out, and the experimental results are good. Thank you for addressing the questions raised by the reviewers. After taking into account the author s responds, there is consensus that the paper is worthy of publication. I therefore recommend acceptance. 
While the reviewers all seem to think this is interesting and basically good work, the Reviewers are consistent and unanimous in rejecting the paper. While the authors did provide a thorough rebuttal, the original paper did not meet the criteria and the reviewers have not changed their scores.
This paper proposes an extension of CTC by considering the wild card to adjust the label missing issues during training. The authors propose to minimize the loss over all possible sub segments of the input to automatically align the one that matches the available transcript. It is empirically proved to significantly improve performance over CTC even if up to 40 70% label sequence is missed (overall performance similar to the complete label case) across different tasks.   As agreed by the reviewers, the paper is well presented and the problem is interesting to a broad community. Dynamic time warping with unconstrained endpoints itself is not a new idea and a classical topic for speech recognition (e.g. word spotting). The contribution of the paper is the formal introduction of the approach to CTC and to give experimental results to confirm the effectiveness. Also the use of simulated data weakens the paper a bit.  The decision is mainly based on the clear presentation and fair experimental justification.
This paper proposes a personalized federated learning method using a hyper network to encode unlabeled data from new clients. At inference time, new clients can use unlabeled data as input to this hyper network in order to obtain a personalized version of the model. The key strength of the paper is that the idea is interesting and timely. Personalization has been studied for clients that participate from the beginning of training, but personalization of models for new clients that join later on has not been considered in most previous works. The experimental results also show a reasonable improvement over the baselines. However, the following concerns remain: 1) Novelty in comparison with reference [1]. Please add a detailed comparison when you revise the paper. 2) Explanation of the experimental results and comparison with baselines was deemed insufficient by some of the reviewers. 3) The generalization bound and the DP results seem standard extensions of existing works and do not add much novelty to the paper.  There wasn t much post rebuttal discussion and the reviewers decided to stick to their original scores. Therefore, I recommend rejection of the paper. I hope that the authors will take the reviewers  constructive comments into account when revising the paper for a future resubmission.
The authors propose an algorithm that learns sparse patterns of images that are highly predictive of a target class, even if added to a non target class. The reviewers agree that the algorithm is novel, is tested on a wide array of experiments, and the paper well written.  Unfortunately, it seems that some of the main claims, such as DNNs trained on clean data "learn abstract shapes along with some texture", resort to qualitative evaluation of the few examples shown in the paper. Furthermore, two reviewers were concerned with how one particular design choice in the algorithm might bias the authors  claims. In particular, pointed out that the patterns learned are highly to the initial canvas used, which is not necessarily strongly motivated.   As these two issues are integral parts of the paper, I hesitate to recommend Acceptance at this point.  That said, the approach looks very promising and I hope the authors continue to pursue this idea.
On the positive side, this is a quite nice empirical exploration of the interaction between data parallelism and sparsity for training neural networks. The experiments are broad and detailed. On the negative side, the empirical results recapitulate what would be expected and what has already been seen in the literature, as the authors themselves point out ("We note that our observation is consistent with the results of regular network training presented in (Shallue et al., 2019; Zhang et al., 2019)."). And the theory presented, while it does explain the results nicely, is a trivial reformulation of the standard convergence result given in Equation 2. So while this is an interesting paper and the reviewers rated it positively on average, the sparsity exploration is _much_ more novel than the data parallelism exploration, and there are significant novelty weaknesses that need to be taken into consideration.
This work is likely to lead to more connections between machine learning and neuroscience at a fine grained level where ML methods can help explain and understand neural circuits.  To encourage this, it would be helpful if authors described the biology of the PN KC APL network and the known constraints over possible formalizations of that network. The authors present one formalization, but little discussion is given toward the design space for such models. Are there other possible ways to describe the PN KC APL network? Are all alternate ways to do so equivalent to the model presented here? What properties are unknown and how could they affect the formalization presented here?  Overall, reviewers agree this is a good submission.
This was a borderline paper, with both pros and cons.  In the end, it was not considered sufficiently mature to accept in its current form.  The reviewers all criticized the assumptions needed, and lamented the lack of clarity around the distinction between reinforcement learning and planning.  The paper requires a clearer contribution, based on a stronger justification of the approach and weakening of the assumptions.  The submitted comments should be able to help the authors strengthen this work.
The paper presents an empirical study on the impact of pertained model on lifelong learning. It concludes that the generic pertaining can benefit the lifelong learning duet the flatter loss landscape and evaluates on CV and NLP tasks. The paper is well written with detailed analysis. However, there is concerns on its limited setting and the conclusion is known in the community and based on empirical studies.
This paper proposes an out of distribution detection (OOD) method without assuming OOD in validation.  As reviewers mentioned, I think the idea is interesting and the proposed method has potential. However, I think the paper can be much improved and is not ready to publish due to the followings given reviewers  comments:  (a) The prior work also has some experiments without OOD in validation, i.e., use adversarial examples (AE) instead in validation. Hence, the main motivation of this paper becomes weak unless the authors justify enough why AE is dangerous to use in validation.   (b) The performance of their replication of the prior method is far lower than reported. I understand that sometimes it is not easy to reproduce the prior results. In this case, one can put the numbers in the original paper. Or, one can provide detailed analysis why the prior method should fail in some cases.  (c) The authors follow exactly same experimental settings in the prior works. But, the reported score of the prior method is already very high in the settings, and the gain can be marginal. Namely, the considered settings are more or less "easy problems". Hence, additional harder interesting OOD settings, e.g., motivated by autonomous driving, would strength the paper.  Hence, I recommend rejection.
This paper proposes to re organize the training data in such a way that padding can be avoided. The novelty is somewhat limited and the results are what one would expect   a nice speed up of 2x but nothing really game changing. While the reviewer scores straddle the decision boundary, nobody is very strongly supportive of acceptance and the positive reviews actually have lower confidence.
This work brings improvement to contrastive learning method for text data by combining a Wasserstein objective with a "memory bank" strategy for getting (and updating) hard negative samples. The approach leads to small but consistent improvements across a variety of representation learning tasks in both supervised and unsupervised settings. While the paper makes a useful contribution and evaluates with some success on downstream tasks, the reviewers would like to see some intrinsic, qualitative discussion of the representation learning itself, and in comparison to more powerful contrastive learning methods. Overall the work falls below the acceptance threshold in a very competitive venue, so I cannot recommend acceptance.  Even after discarding one uninformative review, the consensus remains borderline. The reviewers are, however, appreciative of the additional clarifying experiments provided by the authors. In the internal discussion, a concern was raised that the BERT baselines may not have a fair chance to compete, as they are not fine tuned on the unsupervised data in the same way that the proposed method is, leading to possibly overestimating the improvement.  I encourage the authors to consider this.  Finally, while some concerns about clarity have been addressed, some remain in the current version. In particular, I spot at least three duplicate and inconsistent entries, for Hjelm et al, Lin et al, and McAllester and Stratos.)
The paper considers the effect of permutations in SGD   exploring the question of can we go beyond random permutations (which themselves have shown to be better than with replacement sampling)? The paper studies these questions from multiple viewpoints   showing that there is a one dimensional function for which the optimal permutation can be exponentially better in terms of rate than random. Further they show that for the general high dimensional the gap between random and optimal is non existent. Further they study a Flip Flop algorithm which flips the permutation every alternate epoch and for convex quadratics they show that this technique can lead to improved convergence rates for multiple base permutation schemes.   Overall the results of the paper was found to be interesting across the reviewers. The reviewers agree that the paper is well written. The paper initiates the analysis in a new direction for optimization, i.e. how can we leverage permutations to further improve the convergence of GD and how much further can we go beyond random permutations.   The only weakness highlighted by the reviewers is the limitation of scope to quadratic functions for the FlipFlop algorithm   while this is a significant restriction, given the new line of enquiry opened by the paper this can be discounted.
The paper proposes an additional module to train language models, adding a new loss that tries to predict the previous token given the next one, thus enforcing the model to remember the past. Two out of 3 reviewers recommend to accept the paper; the third one said it was misleading to claim SOTA since authors didn t try the mixture of softmax model that is actually currently SOTA. The authors acknowledged and modified the paper accordingly, and added a few more experiments. The reviewer still thinks the improvements are not important enough to claim significant novelty. Overall, I think the idea is simple and adds some structure to language modeling, but I also concur with the reviewer about limited improvements, which makes it a borderline paper. When calibrating with other area chairs, I decided to recommend to reject the paper.
The paper is a nice addition to the developing theory of implicit bias in neural training. While the results are somewhat expected, the technical aspects are fairly involved due to the adversarial component.
Overall it was decided to reject this paper mainly because it seemed to be a minor extension of known constructions. The reviewers agreed that it certainly is valuable that the paper presents the best known results for kernel k means, but the paper was viewed by the reviewers as more of an observation and primarily an off the shelf application of techniques in the coreset literature. Because of this, the novelty was thought to be a bit below the bar.   One suggestion for improving the presentation is that in the thesis of Melanie Schmidt, there seems to be such a construction for kernel k means which is exponential in 1/eps, and so much worse than what is in this submission. While that s great and certainly something to add and discuss in the paper, the reviewers still felt the technical novelty here was not quite enough to merit acceptance.
Reviewers have all agreed that this paper studied an important problem and made valuable contributions. The goal is to reduce the communication costs  of Federated learning where the data are stored in different parities based on subsets of features.  The paper developed the theory to show guaranteed convergence and provided empirical evaluations to validate the theory.    On the other hand, compared with existing literature, Reviewers feel that the novelty of this submission appears limited and the improvements seem to be incremental. Reviewers appreciate the Authors  efforts in conducting the detailed rebuttals and providing an improved manuscript. We hope the authors would continue to improve the paper based on reviews, when they prepare for their future submission.
The paper proposes an approach to learn the task specific weights in pretraining or mutli task learning. It provides theoretical guarantees to the algorithm, as well as strong empirical results on several NLP problems. All the reviewers agreed that the work is interesting and the paper is well written. During the discussion period, the authors committed to address in the revised version (relatively minor) concerns raised by reviewers, including providing additional clarifications and additional comparisons to related methods. Overall, this is a strong paper that merits an acceptance.
The paper has received all negative scores. Furthermore, one of the reviewers identified an anonymity violation. This is a reject.
This paper resulted in significant discussion   both between R2 and the authors, and between the AC, PCs, and other solicited experts.  The problem of language grounding (and instruction following) in virtual environments is clearly important, this work was one of the first in the recent resurgence, and the goal of understand what the agents have learned is clearly noble and important. In terms of raw recommendations, the majority reviewer recommendation is negative, but since concerns raised by R2 seemed subjective (which in principle is not a problem), out of abundance of caution, we solicited additional input. Unfortunately, we received feedback consistent with the concerns raised here:    The lack of generality of the behavior found. Even if we ignore the difficult question of why the agent prefers what it does, it s unclear how the conclusions here generalize much farther than the model and environment used; the manuscript does not provide any novel or transferable principals of the form "this kind of bias in the environment leads to this kind of bias in models with these properties".    We realize even providing that concrete a statement might be hard, but also missing are thorough comparisons to other kinds of models (e.g. non deep, as asked by R1) to establish that this is a general phenomenon.  Ultimately, there is a sense that this is too narrow an analysis, too soon. If there was one architecture for learning embodied agents in 3d environments that was clearly successful and useful, then studying its properties might be interesting (even crucial).  But the dust in this space isn t settled. Our current agents are fairly poor, and so the impact of understanding the biases of a specific model trained in a specific environment seems fairly low.  Finally   this not taken into consideration in making the decision   it is not okay to list personal homepage domains (that may reveal author identity to ACs) as conflict domains; those are meant for institutional conflicts/domains. 
This paper presents a hierarchical version of β TCVAE that promotes disentanglement in the latent space and improves the robustness of VAEs over adversarial attacks, without (much) degeneration on the quality of reconstructions. The analysis on the relationship between disentanglement and adversarial robustness is valuable and the method is new. The results look promising. The comments were properly addressed.
The paper presents a semi supervised learning approach to handle semantic classification (pixel level classification). The approach extends Hung et al. 18, using a confidence map generated by an auxiliary network, aimed to improve the identification of small objects.  The reviews state that the paper novelty is limited compared to the state of the art; the reviewers made several suggestions to improve the processing pipeline (including all images, including the confidence weights).  The reviews also state that the paper needs be carefully polished.   The area chair hopes that the suggestions about the contents and writing of the paper will help to prepare an improved version of the paper.  
This manuscript was the object of a rich and lengthy discussion. The AC also felt compelled to read the paper in details and discussed it further with the SAC.  The authors did a thorough job at addressing some of the reviewers points. The added results on cross entropy loss and additional discussion, as well as the points made in "Further Discussion on the Numerical Experiments" are very much appreciated.   However, significant concerns remain on establishing connections with prior work, including related ideas on invariance from the causality literature, so as to gain deeper understanding of the implications of the proposed objective. We also strongly encourage the authors to further work on strengthening their theoretical analysis to clearly demonstrate the value of the proposed approach.  The proposed formulation is certainly thought provoking and we urge the authors to pursue their work in view of the above comments.
This work introduces an autoregressive flow model that generates molecular geometries by placing one atom at the time.  In order to preserve the E(3) invariance of the density, successive atom locations are sampled relative to already placed atoms (in a coordinate system described by distance, angle and torsion). The paper is overall well written and experimental results are compelling.
The paper analyses the importance of different DNN modules for generalization performance, explaining why certain architectures may be much better performing than others. All reviewers agree that this is an interesting paper with a novel and important contribution. 
This paper proposes a method to address the covariate shift and label shift problems simultaneously.   The paper is an interesting attempt towards an important problem. However, Reviewers and AC commonly believe that the current version is not acceptable due to several major misconceptions and misleading presentations. In particular:   The novelty of the paper is not very significant.   The main concern of this work is that its shift assumption is not well justified.   The proposed method may be problematic by using the minimax entropy and self training with resampling.   The presentation has many errors that require a full rewrite.  Hence I recommend rejection.
This submission received four high quality reviews. After the discussion period, all reviewers agreed that this submission is not strong enough to be accepted. Concerns include the novelty of the proposed method wrt related work and the limited experiments. The AC agrees. The AC also finds it disappointing that the authors didn t address the concerns on novelty or even discuss the related papers suggested by the reviewers in the revision.   The recommendation is reject.
The paper studies, theoretically and empirically, the problem when generalization error decreases as $n^{ \beta}$ where $\beta$ is not $\frac{1}{2}$. It analyses a Teacher Student problem where the Teacher generates data from a Gaussian random field. The paper provides a theorem that derives $\beta$ for Gaussian and Laplace kernels, and show empirical evidence supporting the theory using MNIST and CIFAR.  The reviews contained two low scores, both of which were not confident. A more confident reviewer provided a weak accept score, and interacted multiple times with the authors during the discussion period (which is one of the nice things about the ICLR review process). However, this reviewer also noted that ICLR may not be the best venue for this work.  Overall, while this paper shows promise, the negative review scores show that the topic may not be the best fit to the ICLR audience.
After communicating with each reviewer about the rebuttal, there seems to be a consensus that the paper contains a number of interesting ideas, but the motivation for the paper and the relationship to the literature needs to be expanded.  The reviewers have not changed their scores, and so there is not currently enough support to accept this paper.
This paper considers the question of whether recent concept based learning algorithms, as well disentangled representation learning algorithms, result in high quality representations. In particular, the authors consider what high quality should mean in terms of the relationship with ground truth concepts and the ability to make accurate predictions for a downstream task. To this end, they propose two main metrics for representations that are explicitly or implicitly encouraged to encode concepts. While the premise of this paper has been appreciated by the reviewers, some concerns about the details of the metrics proposed and experimental results which have been raised by the reviewers remain post rebuttal. Given this, we are unable to recommend the acceptance of the paper at this time. We hope the authors find the reviewer feedback useful.
This paper introduces Transformer QL, a new variant of transformer networks that can process long sequences more efficiently. This is an important research problem, which has been widely studied recently. Unfortunately, this paper does not compare to such previous works (eg. see "Efficient transformers: A survey"), the only considered baselines being Transformer XL and Compressive transformer. Moreover, the reviewers found the experimental section to be lacking, as the results are weak compared to existing work, and important ablation studies are missing. The authors did not provide a rebuttal. For these reasons, I recommend to reject the paper.
This paper proposes training binary values LSTMs for NLP using the Gumbel softmax reparameterization.  The motivation is that this will generalize better, and this is demonstrated in a couple of instances.  However, it s not clear how cherry picked the examples are, since the training loss wasn t reported for most experiments.  And, if the motivation is better generalization, it s not clear why we would use this particular setup.
Paper studies an important problem   producing contrastive explanations (why did the network predict class B not A?). Two major concerns raised by reviewers   the use of one learned "black box" method to explain another and lack of human studies to quantify results   make it very difficult to accept this manuscript in its current state. We encourage the authors to incorporate reviewer feedback to make this manuscript stronger for a future submission; this is an important research topic. 
The paper is well written and presents an extensive set of experiments. The architecture is a simple yet interesting attempt at learning explainable rumour detection models. Some reviewers worry about the novelty of the approach, and whether the explainability of the model is in fact properly evaluated. The authors responded to the reviews and provided detailed feedback. A major limitation of this work is that explanations are at the level of input words. This is common in interpretability (LIME, etc), but it is not clear that explanations/interpretations are best provided at this level and not, say, at the level of training instances or at a more abstract level. It is also not clear that this approach would scale to languages that are morphologically rich and/or harder to segment into words. Since modern approaches to this problem would likely include pretrained language models, it is an interesting problem to make such architectures interpretable. 
This paper investigates how well properties invariant to changes such as lightening and background learned in the major class can be transferred to the minor class. In this paper, the authors reveal that invariances do not transfer well to small classes, and suggest that resolving this phenomenon can help increase the performance on imbalanced datasets. From this point of view, the authors propose a generative model based augmentation technique.  Three reviewers suggested acceptance, and one reviewer judged borderline reject. It seems true that the method is not novel enough, but it is solid and well motivated. In particular, the finding of the paper is interesting and the design of the experiment is well done, so I think that it will have a great influence on research in this field in the future. As the negative reviewer mentioned, the lack of large scale experiments is a major weakness of this paper. I strongly encourage the final version to supplement the promises made to the reviewer, including adding iNaturalist experiments.
The reviewers have a strong consensus towards rejection here, and I agree with this consensus , although I think some of the reviewers  concerns are misplaced. For example, the paper does not appear to use a magnitude upper bound that would be vacuous together with a strong convexity assumption (although variance bounds + strong convexity do cover only a small fraction of strongly convex learning tasks, these assumptions aren t vacuous).  Some feedback I have that perhaps was not covered by the reviewers:  Pros:     Studying the setting where the number of bits varies dynamically is very interesting (although, as Reviewer 3 points out, not entirely novel). There is significant possibility for improvement from this method, and your theory seems to back this up.  Cons:     The experimental setup is weak, and is measuring the wrong thing. When we run SGD to train a model, what we really care about is when the training finishes: the total wall clock time to train on some system. For compression methods with fixed compression rates, it s fine to use the number of bits transmitted as a proxy, because (when the number of bits transmitted is uniform over time) this will be monotonic in the wall clock time. However, when the bits transmitted per iteration can change over time, this can have a difficult to predict effect on the wall clock time, because of the potential for overlap between communication and computation (where below a certain number of bits sent, the system is not communication bound). Wall clock time experiments comparing against other more modern compression methods would significantly improve this paper.
First as a procedural point, the paper got 7, 7, 5, 5. AnonReviewer3 gave it a 5, but seemed satisfied by the discussion and promised to raise their score. They did not do so, but I must interpret their last messages as indicating they now support the paper. AnonReviewer2, the other 5, had some concerns that other reviewers seem to have helped address during rebuttal. They did not update their score, but were happy to leave their certainty low and defer to other reviewers  recommendation. As such, although the average score looks low in the system, the paper is of an acceptable standard according to reviews.  The paper adapts a method from tabular RL to Deep RL, allowing (as the title aptly says), agents to learn What to do by simulating the past. Reviewers speaking in support of the paper found that the paper was clear and sound in its evaluation, providing interesting results and a useful and reusable method. It is my feeling that after discussion, the case for the paper has been clearly made, and in the absence of any strong objections from the reviewers, I am happy to go with the consensus and recommend acceptance.
This paper claims to demonstrate that CNNs, unlike human vision, do not have a bias towards reliance on shape for object recognition. Both AnonReviewer1 and AnonReviewer2 point to fundamental flaws in the paper s argument, which the rebuttal fails to resolve. (AnonReviewer1 s criticisms are unfortunately conflated with AnonReviewer1 s reluctance to view neuroscience or biological vision as an appropriate topic for ICLR; nonetheless AnonReviewer1 s technical criticism stands).  These observations are:  AnonReviewer2:  "Authors have carefully designed a set of experiments which shows CNNs will [overfit] to non shape features that they added to training images. However, this outcome is not surprising."  AnonReviewer1:  "The experiments don t seem to effectively demonstrate the main claim of the paper that categorization CNNs do not have inductive shape bias"  "The best way to demonstrate this would have been to subject a trained image categorization CNN to test data with object shapes in a way that the appearance information couldn’t be used to predict the object label. The paper doesn’t do this. None of the experiments logically imply that with an unaltered training regime, a trained network would not be predictive of the category label if shapes corresponding to that category are presented."  The AC agrees with both of these observations. CNN behavior is partially a product of the training regime. To examine the scientific question of whether CNNs have similar biases as human vision, the training regimes should be similar. Conversely, if human vision evolved in an environment in which shortcut recognition cues were available via indicator pixels, perhaps it would not have a shape bias.  This paper appears fundamentally flawed in its approach. The results are not informative about differences between human vision and CNNs, nor are they surprising to machine learning practitioners.
The paper proposes a method for learning state value functions from (s,s ,r) tuples, founded on the theoretical analysis in MDP setting. The extensive evaluation in several environments shows the benefit of the algorithm.  The consensus among the reviewers, and I concur, that the paper proposes an interesting and novel method. It is cleanly presented, and well founded. The evaluations across range of environments, including robot manipulation validate the method.  During the rebuttal, the authors provided additional evaluation, added a discussion on the latent MDPs, and made numerous clarification, addressing most / all reviewers  questions.
The paper presents a method for learning sequential decision making policies from a mix of demonstrations of varying quality. The reviewers agree, and I concur, that the method is relevant to the ICLR community. It is non trivial, the empirical evaluations and theoretical analysis are rigorous, resulting in a novel method that produces near optimal policies from more readily available demonstrations. The authors revised the manuscript to reflect the reviewers  comments.
This paper presents an interesting method dubbed quotient manifold modeling to handle the "multi manifold" structure of natural data and generalize to new manifolds that arise from novel discrete combinations. While some of the methods and ideas were appreciated by reviewers, there were a number of experimental and clarity concerns. The authors s did not submit a rebuttal, and the many unaddressed concerns (especially around experimental baselines) lead me to recommend rejecting this work.
This manuscript proposes feature selection inspired by knockoffs, where the generative models are implemented using modern deep generative techniques. The resulting procedure is evaluated in a variety of empirical settings and shown to improve performance.  The reviewers and AC agree that the problem studied is timely and interesting, as knockoffs combined with generative models have recently shown promise for inferential problems. However, the reviewers were unconvinced about the motivation of the work, and the strength of the empirical evaluation results. In the option of the AC, this work might be improved by focusing (both conceptually and empirically) on applications where inferential variable selection is most relevant e.g. causal settings, healthcare applications, and so on.
This paper proposes constraints to tackle the problems of dead neurons and dead points. The reviewers point out that the experiments are only done on small datasets and it is not clear if the experiments will scale further. I encourage the authors to carry out further experiments and submit to another venue.
This paper contributes a novel approach to evaluating the robustness of DNN based on structured sparsity to exploit the underlying structure of the image and introduces a method to solve it. The proposed approach is well evaluated and the authors answered the main concerns of the reviewers. 
The overall view of the reviewers is that the paper is not quite good enough as it stands. The reviewers also appreciate the contributions so taking the comments into account and resubmit elsewhere is encouraged. 
The paper proposes a tree search based policy optimization methods for continuous action state spaces. The paper does not have a theoretical guarantee, but has empirical results.  Reviewers brought up issues such as lack of using other policy optimizations methods (SAC, RERPI, etc.), sample inefficiency, and unclear difference with some other similar papers. Even though the authors have provided a rebuttal to address these issues, all the reviewers remain negative. So I can only recommend rejection at this stage.
This paper proposes a method to use a pretrained language model for language generation with arbitrary conditional input (images, text). The main idea, which is called pseudo self attention, is to incorporate the conditioning input as a pseudo history to a pretrained transformer. Experiments on class conditional generation, summarization, story generation, and image captioning show the benefit of the proposed approach.  While I think that the proposed approach makes sense, especially for generation from multiple modalities, it would be useful to see the following comparison in the case of conditional generation from one modality (i.e., text text such as in summarization and story generation). How does the proposed approach compare to a method that simply concatenates these input and output? In Figure 1(c), this would be having the encoder part be pretrained as well, as opposed to randomly initialized, which is possible if the input is also text. I believe this is what R2 is suggesting as well when they mentioned a GPT 2 style model, and I agree this is an important baseline.  This is a borderline paper. However, due to space constraint and the above issues, I recommend to reject the paper.
This paper presents a simple tweak to hyperband to allow it to be run asynchonously on a large cluster, and contains reasonably large scale experiments.  The paper is written clearly enough, and will be of interest to anyone running large scale ML experiments.  However, it falls below the bar by: 1) Not exploring the space of related ideas more. 2) Not providing novel insights. 3) Not attempting to compare against model based parallel approaches.
The paper proposed a novel assisted learning scenario which would likely be useful for organizational level learners (i.e. learners with sufficient computational resources but limited and imbalance data). The paper is generally well presented, but there are shared concerns amongst the reviewers in the significance of technical contributions: (1) Due to the asymptotic nature of the consistency results, the technical strength is not strongly supported with the existing theoretical analysis. (2) Although the problem setup is novel and seems interesting, the practical significance of the results is not well supported without a concrete real world application. (3) There are a few clarity issues raised in the reviews, which suggest that the paper could benefit from a major revision to address the above concerns.
This work identifies an interesting bias that can occur when applying occlusion based interpretability methods to debug image classifiers. For context, the motivation behind many of these methods is that by occluding various parts of the image, one can ask counterfactuals such as "what would the model have predicted if this object were not present in the image"? However, the authors note that when occluding pixels, classifiers are still functions of the occlusions themselves, so this process may introduce a bias as a result. This is most clearly demonstrated in Figure 2 where a convolutional architecture classifies various occluded images as "jigsaw" or "crossword puzzle", arguably due to the fact that scattered patch based occlusions resemble crossword puzzles. The authors then demonstrate that ViT models can be modified in a way to ask the above counter factual in a more principled manner namely by dropping image tokens within the transformer model, the resulting function doesn t take any occluded pixels as input. Reviewers all found the analysis quite insightful, and did not find any significant flaws in the experiments. During the rebuttal, the authors added numerous experiments to address concerns raised by reviewers regarding lack of datasets for which the method was run on. Unfortunately, only one of the reviewers acknowledged the rebuttal and did not raise their score citing doubts that the method may not work well on datasets with differing image statistics (e.g. medical imaging). After reading all of the reviews and rebuttal, the AC feels the authors have adequately addressed the most pressing reviewer concerns, and finds the presented analysis sufficient to warrant acceptance.
The authors provided a comprehensive rebuttal to the reviewers  feedback that addressed most of the concerns. AnonReviewer3 raised some major concerns that were partially resolved in a revision. The paper has received a split recommendation from the reviewers but within the review and discussion periods, there was no strong support towards accepting the paper. Although the paper has received some positive feedback, some of the reviewers  concerns were not fully addressed. I d recommend the authors to address all the comments and add clarifying notes to the paper to avoid such misunderstandings if they decide to resubmit the paper to another venue.  
The paper considers matrix and tensor factorization, and provides a bound on the excess risk which is an improved bound over the bounds for ordinary matrix factorization. The authors also show how to solve the model with standard gradient based optimization algorithms, and present results showing good accuracy. The method can be a bit slow but this depends a bit on the number of iterations, and in general it achieves better accuracy in a similar amount of time to other baseline algorithms.  The reviewers raised a few points, such as jdoi noting the tensor experiments were for small tensors and should include the method Costco as well; other reviewers mentioned more methods as well.  The authors seemed to address most of these concerns in the rebuttal, adding more experiments and more details on timing.  26KD mentioned the optimization procedure was unclear, but the revision includes pseudocode in the appendix that clarifies.  Overall, the paper has both a theoretical and algorithmic contribution, and would be of interest to many ICLR readers.
The paper investigates attacks against time series analysis methods such as GNN and DNN for anomaly and intrusion detection. Standard attacks such as FGSM and PGD are extended for the time series domain and evaluated on several datasets including automotive, aerospace and resource utilization datasets. While the authors claim to be the first to investigate such attacks, some related work was not considered in the paper, which was pointed out by reviewers. Also some other weaknesses of the proposed method, e.g., its focus on feature space perturbations were pointed out. Hence, while acknowledging the importance and the novelty of this paper s contributions, the reviewers agree that the paper must be better positioned in the context of the related work in order to be accepted.
This paper focuses on the problem of robust overfitting. The philosophy behind sounds quite interesting to me, namely, injecting  more learned smoothening during adversarial training. This philosophy leads to two simple yet effective methods: one leveraging knowledge distillation and self training to smooth the logits, and the other performing stochastic weight averaging to smooth the weights.  The clarity and novelty are above the bar of ICLR. While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. Thus, all of us have agreed to accept this paper for publication! Please carefully address all  comments in the final version.
The area chair agrees with reviewer 1 and 2 that this paper does not have sufficient machine learning novelty for ICLR. This is competent work and the problem is interesting, but ICLR is not the right venue since the main contributions are on defining the task. All the models that are then applied are standard.
This paper led to significant discussion, and the AC is generally on the fence. First of all, thanks to the reviewers for the significant time they invested in the discussion, and thanks for the authors for promptly and patiently answering our questions.   Overall, the reviewer recommendations are positive. However, the discussion showed that despite the positive recommendation, the reviewers struggled to distill the general contribution of the paper beyond performance on ALFRED. In discussion, the authors distinguished their contribution from existing work by focusing on using a set of low level policies at the root of the overall policy. This relies on the discrete set of behaviors that is defined within the ALFRED benchmark. It s not clear how it generalizes to the actual problem of instructing a robot to execute natural language instruction. In realistic scenarios, is it possible to define a set of behaviors in such a clean way, and at scale? And then train/manage a separate model for each behavior? The set of interaction policies in Figure 2 illustrates this challenge well. The answer to this scaling question is not clear. This corresponds to a concern raised repeatedly by the reviewers about the approach too specialized to ALFRED. The AC shares this concern.   (which are roughly equal to the SOTA at the time of submission, but show significantly more overfitting to seen environments)  On the positive side, this is solid work, with good results. The paper is well written, and the authors largely addressed the concerns raised as much as possible. The results are not SOTA though. The current SOTA was submitted on 09/19/2021, prior to the ICLR deadline   it s not included in the results table in this paper. (To clarify, the fact that it s not the current SOTA does not affect the final decision, as they are considered as contemporaneous.) With concerns regarding the specificity of the approach, this paper may interest researchers working on ALFRED, but not clear to what depth, despite the clearly significant work and effort the authors put into the paper.   (If the paper is accepted, the AC asks the authors to fix the standing errors with regard to previous work, as discussed below, and to include more recent results from the leaderboard)
This paper seeks to shed light on why seq2seq models favor generic replies. The problem is an important one, unfortunately the responses proposed in the paper are not satisfactory. Most reviewers note problems and general lack of rigorousness in the assumptions used to produce the theoretical part of the paper (e.g., strong assumption of independence of generated words). The experiments themselves are not convincing enough to warrant acceptance by themselves.
This paper got 2 clear acceptance and 2 borderline recommendation. The main concerns lie in the clarity of the experiment results and settings (AR3). The authors address these questions in their response. AR2 has two important questions. One is whether the simplified assumption holds in the considered very complicated settings (i.e., the labels are noisy and long tailed). The other one is the lack of comparison with SOTA method for long tailed classification. The authors did good job in their response. They provide additional experiment results to address these questions. Overall, the quality of this submission meet the bar of ICLR acceptance, though AC has concerns on the complicated settings and the marginal performance improvement over the existing long tailed works.
The reviewers, AC, and PCs participated in a very thorough discussion. AC ultimately felt that the work was unfinished, and in particular that details in the proofs still needed work before publication.  
The pros and cons of this paper cited by the reviewers can be summarized below:  Pros: * The paper is a first attempt to investigate an under studied area in neural MT (and potentially other applications of sequence to sequence models as well) * This area might have a large impact; existing models such as Google Translate fail badly on the inputs described here * Experiments are very carefully designed and thorough * Experiments on not only synthetic but also natural noise add significant reliability to the results * Paper is well written and easy to follow  Cons: * There may be better architectures for this problem than the ones proposed here * Even the natural noise is not entirely natural, e.g. artificially constrained to exist within words * Paper is not a perfect fit to ICLR (although ICLR is attempting to cast a wide net, so this alone is not a critical criticism of the paper)  This paper had uniformly positive reviews and has potential for large real world impact.
Authors propose a novel scheme to perform active learning on image segmentation. This structured task is highly time consuming for humans to perform and challenging to model theoretically as to potentially apply existing active learning methods. Reviewers have remaining concerns over computation and that the empirical evaluation is not overwhelming (e.g., more comparisons). Nevertheless, the paper appears to bring new ideas to the table for this important problem.    
Thanks for your submission to ICLR.  This paper considers a variational inference hierarchical model called Variational Predictive Routing.  Prior to discussion, several reviewers were on the fence about the paper, most notably having concerns about some of the experimental results as well as various clarity issues throughout the paper.  However, the authors did a really nice job addressing many of these concerns.  Ultimately, several of the reviewers updated their scores, leading to a clear consensus view that this paper is ready for publication.  We really appreciate your effort in providing additional details and results.  Please do keep in mind the concerns of the reviewers when preparing a final version of the manuscript.
The paper presents a deep learning approach for tasks such as symbolic integration and solving differential equations.   The reviewers were positive and the paper has had extensive discussion, which we hope has been positive for the authors.   We look forward to seeing the engagement with this work at the conference.
This submission proposes a few small changes to the (PreLN) Transformer architecture that enable training with higher learning rates (and therefore can result in faster convergence). The changes include the addition of two layer norm operations as well as a learnable head scaling operation in multi headed attention. The proposed operations add only a small computational overhead and should be simple to implement. Experiments are conducted on language modeling and masked language modeling, with improved results demonstrated at various scales and according to various evaluation procedures. The paper also includes a good amount of ablation study as well as some analysis. Reviews on the paper were mixed, and a great deal of changes were made to the paper during the rebuttal period. To summarize the concerns and recommendations, reviewers requested   better connection between the proposed changes and the purported issue (gradient scale mismatch between early/late layers)   better analysis of why gradient scale mismatch is a major issue and investigation of where it comes from   better comparison to existing techniques that allow for higher learning rate training of Transformers   additional experiments on different model types and ideally different codebases/implementations  I think overall this is a solid submission, since it proposes a simple change that is reasonably likely to be helpful (or at least not harmful). However, I think that there are enough concerns with the current draft and there were enough changes made during rebuttal that this paper should be resubmitted to a future conference. I would suggest the authors take the final updated form from this round, add additional motivation/analysis/experiments, and resubmit, and I suspect a positive outcome.
This paper proposed a  new type of models that are invariant to entities by exploring the symbolic property of entities. This problem is important in language modeling since it gives intrinsically more proper representation of sentences, which can better generalize to new entities.  However I still suggest to reject this paper for the following reasons  1. The description of model is not clear enough which can certainly use a serious round of revision. 2. The experiments on bAbi is not convincing enough since it is an overly simple and toyish data set with many ways to hack 3. Similar entity invariant idea has been explored long time ago by  (https://arxiv.org/pdf/1508.05508.pdf) which attempted to represent entities as “variables”  
The paper proposes a variant derivative free optimization algorithm, that belongs to the family of Evolution Strategies (ES) and zero order optimization algorithms, to train deep neural networks. The proposed Random Search Optimization (RSO) perturbs the weights via additive Gaussian noise and updates the weights only when the perturbations improve the training objective function. Unlike the existing ES and black box optimization algorithms that perturb all the weights at once, RSO perturbs and updates the weights in a coordinate descent fashion. RSO adds noise to only a subset of the weights sequentially, layer by layer, and neuron by neuron. The empirical experiments demonstrated RSO can achieve comparable performance when training small convolutional neural networks on MNIST and CIFAR 10.   The paper contains some interesting ideas. However, there are some major concerns in the current submission:  1) Novelty: there is a wealth of literature in optimization neural networks via derivative free methods. The proposed algorithm belongs to Evolution Strategies and other zero order methods, (Rechenberg & Eigen, (1973); Schmidhuber et al., (2007); Salimans et al., (2017). Unforunately, among all the rich prior works on related algorithms, only Salimans et al. (2017) is merely mentioned in the related works. Furthermore, the experiments only compared against SGD rather than any other zero order optimization algorithms.   Many ideas in Algorithm 1 was proposed in the prior ES literature:    Evaluate the weights using a pair of noise,  \deltaW and +\deltaW in Alg1 Line13 14 is known as antithetic sampling Geweke (1988), also known as mirrored sampling Brockhoff et al. (2010) in the ES literature.    Update the weights by considering whether the objective function has improved or not was proposed in Wierstra et al. (2014) that is known as fitness shaping.  Given the current submission, it is difficult to discern the contribution of the proposed method when compared to the prior works. In addition, the convergence analysis of the zero order optimization was studied in Duchi et. al. (2015) that includes the special coordinate descent version closely related to the proposed algorithm.  2) Experiments:     Although the experiments showcase the performance of sequential RSO, the x axis in Figure 4 only reported the iterations after updating the entire network. The true computational cost of the proposed RSO is the #forwardpass x #parameters, that is much more costly than the paper currently acknowledges. Also, RSO requires drawing 5000 random samples and perform forward passes on all 5000 samples for every single weight update. It will be a great addition to include the #multiplications and computation complexity of RSO and the baseline algorithms.     More importantly, the paper only compared RSO with SGD in all the experiments. It will significantly strengthen the current paper by including some of the existing ES algorithms.    In summary, the basic idea is interesting, but the current paper is not for publication and will need further development and non trivial modification.   
After reading the author’s response, all reviewers recommend accepting the paper. R2 and R3 strongly support the paper while R1 and R4 consider it borderline.  There is agreement that the idea of the work is interesting and novel. The experimental results look solid.   The authors provided an extensive response addressing most of the concerns of the reviewers. In light of this feedback, the reviewers provided some additional comments (which the authors could not address, as the discussion period was over). The AC considers that the authors should incorporate this feedback to the final version of the manuscript. Specifically,  Responding to R1 s first question regarding the noise distribution on the original image being significantly different from Gaussian. The authors provided detailed results, which is highly appreciated. As R1 points out, the authors had to introduce an additional VST for the method. These results should be added to the manuscript is important, to show the limitations of the approach.  R3 asks about the importance of the initialization of the weights of the encode and decoder. This is a natural question as this is a non convex problem. The authors clarify in the manuscript the initialization of x, but do not comment on the weights. It would be good to add a sentence in this regard (as done in the discussion).  R4 mentioned, and the AC agrees, that the authors should try to improve the clarity of the exposition.  The AC considers it important to add in the appendix more visual examples to quantitatively show the performance of the method.
All three reviewers are positive, and the authors have addressed essentially all the questions raised by the reviewers. The main insight of the paper is clear, and the empirical results are good, so a spotlight is deserved.
This manuscript describes a method that turns sentences into reward functions by recognizing objects, parsing sentences into a simple formalism, and then grounding the parse in the recognized objects to form a reward for an agent.  1. The title and much of the manuscript are written in a way that reviewers found confusing. It would seem from the title and most of the text that the method integrates language models, CLIP specifically, into RL in a novel way to provide zero shot rewards. But this is not the case. CLIP is used purely as an object detector. Yes, the method requires a good object detector and CLIP provides that, but any good object detector that can handle arbitrary phrases would have done.  2. The overall setup of the work: extract the state of the world and then parse sentences to formulate rewards by grounding parts of the parse into parts of the world state has been explored widely in robotics. Reviewers provided citations going back several years, but many others exist.  I would encourage the authors to rewrite the manuscript around their central contributions and downgrade their use of CLIP and language models in general to a minor technical footnote. Similarly refocusing related work on the robotics literature and demonstrating how this approach differs and improves on the state of the art there could result in a strong contribution.
The Authors propose a neural network based approach for the phase retrieval problem. Solving the phase retrieval problem is key for important application areas such as crystallography or radioastronomy.  After adding more baselines and other changes, 3 out of 4 reviewers recommended acceptance. Reviewer kQWk recommended rejection mostly based on the fact that the paper is quite narrow in scope.  Reviewer kQWk is right that the topic might not appeal to most of the ICLR community. It is worth noting that the main contribution of the paper is not about neural networks but rather about connecting phase retrieval with Blaschke products. As it stands, it seems that after making this connection, any non linear approximator could do well.  Having said that, this is an important application area and the progress is welcomed. Hopefully, it will draw inspire more research in this area.  Currently, the key issue of the paper is that it is very challenging to understand for people without background in the phase retrieval area or complex analysis. To make this paper more valuable for the ICLR community, I would strongly encourage the Authors to devote at least a page to explanation of what is the phase retrieval problem, and the intuition behind the solution. Perhaps [1] could serve as an inspiration.  [1] Phase retrieval in crystallography and optics, R. P. Millane
The paper presents an extension of MADDPG, adding communication between agents. The methods targets extremely noisy observations settings, so that agents need to decide if they communicate their private observations (or not). There is no intrinsic/explicit reward to guide the learning of the communication, only the extrinsic/implicit reward of the downstream task.  The paper is clear and easy to follow, in particular after the updated writing. I believe some of the reviewers  points were addressed by the rebuttal. Nonetheless, some of the weaknesses of the paper still hold: namely the complexity of the approach compounded with a very specific experimental evaluation. The more complex an approach is (and it may be justified by the complexity of the setting!), the more varied its supporting evidence should be.  In its current form, the paper would constitute a good workshop contribution (to discuss the approach), but I believe it needs more varied (and/or harder) experiments to be published at ICLR.
I thank the authors for their submission and very active participation in the author response period. World state tracking is an important problem that encompasses existing problems like coreference resolution. I agree with R2 and R3 that proposing a novel environment in which we can investigate to what extend Transformers can tackle world state tracking should be interesting to the community. The majority of the reviewers agree that this paper presents an interesting benchmark [R2,R3,R4] with good thorough experimental work [R1,R2,R4]. However, R1 is confused about the positioning of the work and R4 finds the work narrow. R2, despite positive review, agrees with this assessment. I agree with this assessment as well and, after discussion with the program chairs, came to the decision that this paper is not ready for publication in its current state. I strongly encourage the authors to incorporate R1 s and R4 s feedback, in particular with respect to positioning this environment in comparison to TextWorld, and resubmit to the next venue.
This paper proposes a new hard attention model for the image classification as a way to achieve explainability. Two of the reviewers do not find the output of the system interpretable, which is a fatal weakness for a paper on XAI. R1: The visualization in Fig.5 shows only that the region selected in each timestep indeed has the maximum EIG. But how to interpret the explainability from the glimpse sequence is still confusing. I can hardly perceive the sequence using my knowledge. R2: However the output of the system is not so appealing either in performance or explainability. R3: Post discussion note: For me, it s a bit hard to say the proposed methodology is novel. Authors needs to explain why the proposed model is different from pre existing methodologies regarding attention mechanism.   R4: Due to the above, the recommendation is Reject   but the authors are strongly encouraged to do experiments on more challenging data and compare to a newer baseline. 
The paper proposes an unconditional GAN that learns a set of structured keypoints as the intermediate representation. It was shown that these learned keypoints may be used to control the image synthesis output. The paper received a mixed rating before the rebuttal, with one reviewer rating the paper marginally above the bar and three reviewers rating it marginally below the bar. While a couple of reviewers commented that the keypoint idea was interesting, several concerns were raised, including the seemingly challenging tuning requirement and the usability of the proposed method. Several missing related works were also pointed out. The rebuttal addressed some of the raised concerns but not fully. While Reviewer Jmzu raised the score from marginally below the bar to marginally above the bar, Jmzu still expressed concerns about the quality of the paper. Reviewer kpkc kept marginally above the bar rating but was not impressed with the contribution. Consolidating the reviews and rebuttals, the meta reviewer found the raised concerns valid and would not recommend acceptance of the paper. The authors are encouraged to incorporate the feedback to strengthen the contribution.
This paper investigates composition and decomposition for adversarially training generative models that work on composed data. Components that are sampled from component generators are then fed into a composition function to generate composed samples, aiming to improve modularity, extensibility, and interpretability of GANs. The paper is written very clearly and is easy to follow. Experiments considered application to both images (MNIST) and text (yelp reviews). The original version of the paper lacks any qualitative analysis, even though experiments were described. Authors revised the paper to include some experimental results, however, they are still not sufficient. State of the art baselines, from previous work suggested by the reviewers should be included for comparison.
A good paper with significant contribution on XAI and the on  vs off  data manifold explainability. Reviewers have appreciated authors’ feedback and update of the paper (R1, R2, R4). I would like to personally thank the authors for a smooth, extensive and focused interaction w/ updates. 
The paper consider the problem of program induction from a small dataset of input output pairs; the small amount of available data results a large set of valid candidate programs. The authors propose to train an neural oracle by unsupervised learning on the given data, and synthesizing new pairs to augment the given data, therefore reducing the set of admissible programs. This is reminiscent of data augmentation schemes, eg elastic transforms for image data.  The reviewers appreciate the simplicity and effectiveness of this approach, as demonstrated on an android UI dataset. The authors successfully addressed most negative points raised by the reviewers in the rebuttal, except the lack of experimental validating on other datasets.  I recommend to accept this paper, based on reviews and my own reading. I think the manuscript could be further improved by more explicitly discussing  (early in the paper) the intuition why the authors think this approach is sensible: The additional information for more successfully infering the correct program has to come from somewhere; as no new information is eg given by a human oracle, it was injected by the choice of prior over neural oracles. It is essential that the paper discuss this. 
The paper introduces a convolutional like operator called optimized separable convolution, which scales well in the number of channels, C. The paper studies the classification performance of ResNet with the optimized separable convolution, and with other choices of the convolutional operators. The paper finds the introduced convolutional  operator to be more parameter efficient than competing operators, however only by a relatively small margin, and this also comes at the cost of computational performance.   The reviewers appreciated that the proposed convolutional operation is more parameter efficient and that any improvement in convolutional networks potentially benefits a large array of methods and tools. The reviewers  criticize that the operation only offers marginal gains at the cost of slower runtimes, and agree that the contribution is only marginally significant and therefore below the acceptance threshold.  I agree with the reasoning of the reviewers that the extra computational cost is not worth the marginal improvement, and therefore recommend to reject the paper. Also, the authors didn t respond to the comments of the reviewers and their reasonable questions.
Reviewers always find problems in papers like this.  AnonReviewer1 would have preferred to have seen a study of traditional architectures, rather than fully connected ones, which are now less frequently used. They thought the paper was too long, the figures too cluttered, and were not convinced by the discussion around linear v. elliptical trajectories.  I appreciate the need for a parametrizable architecture, although it may not be justified to translate these insights to other architectures, and then the fact that fully connected architectures are less common undermines the impact of the work. I don t find the length a problem, and I don t find the figures a problem.  After the back and forth, AnonReviewer3 believes that there are data compatibility issues associated with the studied transformations and that non linear transformations would have been more informative. I find the reviewers response to be convincing.  AnonReviewer2 is strongly in favor of acceptance, finding the work exhaustive, interesting, and of high quality. I m inclined to agree.  
The authors propose a new framework of population learning that optimizes a single conditional model to learn and represent multiple diverse policies in real world games. All reviewers agree the ideas are interesting and the empirical results are strong. The meta reviewer agrees and recommends acceptance.
In considering the reviews and the author response, I would summarize the evaluation of the paper as following: The main idea in the paper   to combine goal conditioning with successor features   is an interesting direction for research, but is somewhat incremental in light of the prior work in the area. Most of the reviewers generally agreed on this point. While a relatively incremental technical contribution could still result in a successful paper with a thorough empirical analysis and compelling results, the evaluation in the paper is unfortunately not very extensive: the provided tasks are very simple, and the difference from prior methods is not very large. All of the tasks are equivalent to either grid worlds or reaching, which are very simple. Without a deeper technical contribution or a more extensive empirical evaluation, I do not think the paper is ready for publication in ICLR.
The authors analyze the IWAE bound as an estimator of the marginal log likelihood and show how to reduce its bias by using the jackknife. They then evaluate the effect of using the resulting estimator (JVI) for training and evaluating VAEs on MNIST. This is an interesting and well written paper. It could be improved by including a convincing explanation of the relatively poor performance of the JVI trained, JVI evaluated models.
Motivated by the recently proposed EigenGame, this paper proposes an unbiased stochastic update to replace the biased one in the original EigenGame. The new algorithm is asymptotically equivalent to EigenGame, enjoys better parallelism on big data, and beats EigenGame in experiments. Some reviewers are originally concerned about the lack of finite sample convergence results. After the author s response and reviewer discussion, this paper does get sufficient support. Therefore, I recommend acceptance and encourage the authors to think about how to deliver a finite sample analysis in future work.
This paper studies knowledge distillation and explores why distillation gains are not uniform. Reviewers consistently find this paper an interesting read, but had common concerns on generalizability and limited improvements/contributions. In general, reviewers mostly gave a score that is below the acceptance threshold, or expressed concerns otherwise. Summing these up, we conclude this paper is of interest to the ICLR audience, but current form is not ready yet for acceptance.    Summary Of Reasons To Publish: interesting analysis of the causes of non uniform gains in distillation   Summary Of Suggested Revisions:   (1) the improvements are marginal and (2) the contribution of AdaMargin is limited, (3) generalizability to other KDs
The authors present a theoretical and practical study on low precision training of neural networks. They introduce the notion of variance retention ratio (VRR) that determines the accumulation bit width for precise tailoring of computation hardware.  Empirically, the authors show that their theoretical result extends to practical implementation in three standard benchmarks.  A criticism of the paper has been certain hyperparameters that a reviewer found to be chosen rather arbitrarily, but I think the reviewers do a reasonable job in rebutting it.   Overall, there is consensus that the paper presents an interesting framework and does both practical and empirical analysis, and it should be accepted.
This paper proposes a VAE model with arbitrary conditioning. It is a novel idea, and the model derivation and training approach are technically sound. Experiments are thoughtfully designed and include comparison with latest related works.  R1 and R3 suggested the original version of the paper was lack of comparison with relevant work and the authors provided new experiments in the revision. The rebuttal also addressed a few other concerns about the novelty and clarity raised by R3.  Based on the novel contribution in handling missing feature imputation with VAE, I would recommend to accept the paper. It is worth noticing that there is another submission to ICLR (https://openreview.net/forum?id ByxLl309Ym) that shares a similar idea of constructing the inference network with binary masking, although it is designed for a pre trained VAE model.  There are still two weaknesses pointed out by R3 that would help improve the paper by addressing them: 1. The paper does not handle different kinds of missingness beyond missing at random. 2. VAE model makes the trade off between computational complexity and accuracy. Point 1 would be a good direction for future research, and point 2 is a common problem for all VAE approaches. While the latter should not become a reason to reject the paper, I encourage the authors to take MCMC methods into account in the evaluation section. 
The paper introduces additional layers on top BERT type models for disentangling of semantic and positional information.  The paper demonstrates (small) performance gains in transfer learning compared to pure BERT baseline.  Both reviewers and authors have engaged in a constructive discussion of the merits of the proposed method. Although the reviewers appreciate the ideas and parts of the paper the consensus among the reviewers is that the evaluation of the method is not clearcut enough to warrant publication.  Rejection is therefore recommended. Given the good ideas presented in the paper and the promising results the authors are encouraged to take the feedback into account and submit to the next ML conference.   
The paper provides a simple prediction procedure to defend against (rectangular) patch attacks, and also a method to obtain some random estimates of the certified robustness of the method. The simplicity of the method is certainly appreciated. On the other hand, there are a number of issues preventing the acceptance of this paper. The main problem is that the paper deals with a randomized predictor, yet the certification guarantee developed for deterministic predictors is applied. This leads to several problems, starting from the target being undefined to unfair comparisons. While the authors made an attempt to address this in the rebuttal, more work is needed to properly settle this issue.
The paper proposes an interesting architecture that dues Graph Neural Networks (GNN) and Gradient Boosting Decision Tree. This new architecture works on graphs with heterogeneous tabular features and BGNNs work well on graphs where the nodes contain heterogeneous tabular data and is optimized end to end and seems to obtain great SOTA results. End to end learning is done by iteratively adding trees that fit the GNN gradient updates, allowing the GNN to backpropagate into the GBDT. All reviewers agreed that the idea is interesting, the paper is well written, and the results found in the paper are impressive. In addition, author response satisfactorily addressed most of the points raised by the reviewers, and most of them increased their original score. Therefore, I recommend acceptance.
This paper deals with unsupervised image to image translation and proposed a geometric constrains for better structural similarity between the source and the target. Experiments are done using multiple GAN frameworks and demonstrate reduction in distortions in the generated images.  The reviewers appreciated the contributions, but were overall not very enthusiastic about the paper, with two rejection recommendations. In particular, the criticism regarded   limited applicability; shape similarity does not always translate into a good visual result; scenes with multiple similar objects might be severely distorted   some results show a strange mixture of styles   missing implementation details   only small quantitative improvement   similarity to prior works on perceptual loss   lack of clarity about the use of mutual information for geometry preservation, and implementation details   unconvincing baselines  The authors provided an extensive rebuttal addressing some of the above comments. However, many of the doubts remained because of which we believe the paper cannot be accepted. 
This paper receives 3 initial rejection ratings. No rebuttal was submitted by the authors. There is no basis for overturning the reviewers  decisions. This paper should be rejected.
This paper considers the exploding gradient problem in RNNs. The proposed network SGORNN can be seen as an extension to the FastRNN model by adding orthogonal weight matrices.   I recommend rejection for this paper mainly for two reasons.   First, as mentioned in the review of Reviewer 815o and Reviewer W7nS, adding orthogonal constraints into FastRNN should not be considered as a significant technical contribution.   Second, more importantly, the experiments of the paper are not that convincing. All reviewers raise concerns about this issue. I also do not see the point of comparing the proposed model with a baseline LSTM model of much larger parameter size. I can’t think of a reason to do so. Also I think the small datasets will not give you a lot of meaningful insights in comparing the models – PTB for example, is a rather small dataset for language modeling and the results presented there are far from well. The numbers look really bad, reflecting the quality of how these experiments are done ( https://arxiv.org/pdf/1707.05589.pdf ).
Mixed precision application of CNNs is being explored for e.g. hardware implementations of networks trained at full precision.  Mixed precision at training time is less common.  This submission primarily concerns itself with the practical implementation details of training with mixed precision, and focuses primarily on representation of mixed precision floating point and algorithmic issues for learning.  In the end the support for the approach is primarily empirical, with the mixed precision approach giving a factor of two speedup with half the precision, while accuracies remain effectively statistically tied on the ImageNet 1k database.  Table 1 should avoid the use of bold as there is likely no statistical significance.  The reviewers appreciated the paper. The proposed approach is sensible, and appears correct.
The paper presents a novel approach to exploration in long horizon / sparse reward RL settings. The approach is based on the notion of abstract states, a space that is lower dimensional than the original state space, and in which transition dynamics can be learned and exploration is planned. A distributed algorithm is proposed for managing exploration in the abstract space (done by the manager), and learning to navigate between abstract states (workers). Empirical results show strong performance on hard exploration Atari games.  The paper addresses a key challenge in reinforcement learning   learning and planning in long horizon MDPs. It presents an original approach to this problem, and demonstrates that it can be leveraged to achieve strong empirical results.   At the same time, the reviewers and AC note several potential weaknesses, the focus here is on the subset that substantially affected the final acceptance decision. First, the paper deviates from the majority of current state of the art deep RL approaches by leveraging prior knowledge in the form of the RAM state. The cause for concern is not so much the use of the RAM information, but the comparison to other prior approaches using "comparable amounts of prior knowledge"   an argument that was considered misleading by the reviewers and AC. The reviewers make detailed suggestions on how to address these concerns in a future revision. Despite initially diverging assessments, the final consensus between the reviewers and AC was that the stated concerns would require a thorough revision of the paper and that it should not be accepted in its current stage.  On a separate note, a lot of the discussion between R1 and the authors centered on whether more comparisons / a larger number of seeds should be run. The authors argued that the requested comparisons would be too costly. A suggestion for a future revision of the paper would be to only run a large number (e.g., 10) of seeds for the first 150M steps of each experiment, and presenting these results separately from the long running experiments. This should be a cost efficient way to shed light on a particularly important range, and would help validate claims about sample efficiency.
The submission addresses the problem of continual learning with large numbers of tasks and variable task ordering and proposes a parameter decomposition approach such that part of the parameters are task adaptive and some are task shared. The validation is on omniglot and other benchmarks.  The reviews were mixed on this paper, but most reviewers were favorably impressed with the problem setup, the scalability of the method, and the results. The baselines were limited but acceptable. The recommendation is to accept this paper, but the authors are advised to address all the points in the reviews in their final revision.
This paper explores the connection between diversity of gradients and discriminativeness of representations. Based on the observations, authors propose Discriminative Representation Loss (DRL).  This paper resulted in a lot of discussions and specifically, R5 s detailed comments helped the authors improve their paper. Authors did a good job in making significant improvements to the paper based on the reviews, including better connecting the theory and experiments. However, after much discussion it was felt that experiments and analysis still needed improvement, leading to a decision to reject. The authors are encouraged to use the reviewers  post discussion updates to further improve and submit to a future venue.
The paper addresses the important problem of classification with unbalanced semantic classes. The key idea is a two stage process that first learns a representation under various distributions (experts), then assign a cascade of experts to hard samples, whose predictions are combined. The approach can be added on top of various backbone networks. Experiments are systematic and extensive, showing improvement on three standard benchmarks for this task.  All four reviewers recommended accept.  The paper extends a recent research direction of learning "balanced" representations, followed by distribution aware experts. This general approach could have wider impact on architectures designed for out of distribution and low shot learning.  The authors should update the final paper based on their answers and on reviewer feedback. We also encourage authors to make their code available. 
This paper tackles the problem of classifying a set of points given the knowledge that all points should have the same class. There seems to be a consensus among the reviews that under the assumptions made, the authors provide thorough experiments convincing that their method is useful. However, the paper has two weaknesses that are too strong to ignore. First, the clarity of exposition seems to be lacking, specifically a clear motivation for this new setup as well as its connection to the abstract problem being solved. Second, the assumptions made seem to be too strong, and the solution seems to rely on these strong assumptions too much.  
The paper addresses and interesting problem, but the reviewers found that the paper is not as strong as it could be: improving the range of evaluated data (significantly improve the convincingness of the experiments, and clearly adressing any alternatives, their limitations and as baselines).
In this paper, the authors proposed a large margin based domain adaptation method for cross domain sentiment analysis.  The idea of developing a large margin based method for domain adaptation is not new. Though the proposed method contains some new ideas,  the difference between the proposed method and the existing large margin based methods needs to be discussed and studied empirically.  In addition, the experimental results are not convincing: some related baselines are missing and experiments need to be conducted on more datasets.   Though the authors did provide long responses to each reviewer, after a lot of discussions, the reviewers still find that their concerns are not well addressed.   Therefore, this paper is not ready to be published in ICLR based on its current shape. 
The paper proposes an adversarial data augmentation technique searching for adversarial weight perturbations of a corruption network (e.g. a pretrained image to image model). The goal is to achieve common corruption robustness as well as a non trivial level of adversarial robustness. The authors claim state of the art performance on CIFAR10 C.  Most reviewers had initial concerns which the authors could clarify in most cases. Finally, all reviewers argue for acceptance.  Strengths:   no pre defined corruption model necessary   extensive experiments on CIFAR10 and ImageNet with SOTA results   all reviewers agree that this paper would be valuable as a future reference  Weaknesses:   the theoretical part is in my point of view rather misleading and should be completely rewritten as the authors lack here rigor concerning the setting they are working in (as also Reviewer 31sh criticizes). In particular the corruptions are not just a covariate shift (p(y|x) is invariant, only p(x) changes) as the corruptions mix the conditional distributions of different points. Thus under the given assumptions (which should be summarized at one point rather than being scattered over the text) the Bayes optimal classifer is invariant but not the Bayes optimal predictive probability distribution (which is clearly important for assessing uncertainty of the prediction).  However, when training with the cross entropy loss we are estimating the predictive probability distribution and thus the given statement about convergence of the risks makes no sense for me and the optimal parameters of the classifier need not be equal even if their classifications agree everywhere. Thus the required changes to fix this theoretical part go significantly beyond what the authors suggest in their rebuttal.    the improvements over AugMix+DeepAugment (7.83 mCE vs 7.99 mCE) are negligible and most likely not statistically significant While AdA has significantly higher adversarial robustness than AugMix+DeepAugment, it remains unclear if using AugMix+DeepAugment together with adversarial training for l_2 as done in  [3] Kireev, Klim, Maksym Andriushchenko, and Nicolas Flammarion. "On the effectiveness of adversarial training against common corruptions." arXiv preprint arXiv:2103.02325 (2021). could have led to a similar result   The paper provides an interesting approach to achieve robustness against common corruptions and all reviewers recommend acceptance. The theoretical part as written currently is misleading as discussed above   the authors have to make it completely rigorous (including proofs, formal statements/defininitions etc) or get rid of it.  Minor weak points:   According to the leaderboard of RobustBench the SOTA for CIFAR10 C is  by now taken by the NeurIPS 2021 paper Diffenderfer et al,  A Winning Hand: Compressing Deep Networks Can Improve Out Of Distribution Robustness which achieves 96.56% standard accuracy and 92.78% mCE. This is 1.64% and 0.61% better than in the present paper and needs to be discussed as prior work.   for the adversarial robustness evaluation regarding "AutoAttack & MultiTargeted" you refer to Gowal et al (2020) but the robustness evaluation has to be properly discussed in this paper
While there was some support for the ideas presented, the majority of the reviewers did not think the submission is ready for publication at ICLR. Significant concerns were raised about clarity of the exposition.
The authors provide a framework for unsupervised clarification based on minimizing a between cluster discriminative similarity. It is more flexible than existing methods whose kernel similarity implicitly assumes uniform weights, and the authors connect to ideas such as max margin and weighted kernel approaches. This yields a clustering algorithm naturally that alternates between updating class labels and similarity weights. Moreover the reviewers (and I) appreciate the analysis of generalization error through Rademacher complexity arguments and detailed author responses. I might add while the paper draws connections to weighted kernel methods and have since added references to sparse subspace clustering etc, there is recent interest in using similar arguments to derive error bounds and uniform concentration results for center based methods that might be included in the survey of related work, for instance recent work from Swagatam Das and collaborators. The authors have importantly added details on the optimization using SMO, and the revision should include these details in a clear exposition together with the computational complexity discussion mentioned in their response.
The paper proposed a method for adversarial robustness by considering information from the edge map of the images. Two reviewers point out the similarities of the paper with previous work ([1]) and it is unclear whether the benefits come from binarization of the input or from shape information. As such, the paper is not suggested for publication at this time
After carefully reading the reviews and rebuttal, I believe this work is of sufficient quality for acceptance. Understanding continual learning from a theoretical stand point is a very important topic. I find that one of the main issue raised by reviewers was about the exact meaning of Continual learning, and whether what the authors studied was more akin to sequential learning. While I don t mind the term sequential learning, and is quite descriptive of the work as well, I disagree that the considered setup is not continual learning.
I thank the authors both for going the extra mile in doing further experiments for their response, and making the efforts to synthesize the main comments and concerns of the reviewers.  Overall, I m pretty sympathetic to the idea that syntactic and semantic representations should be very helpful to learning sentence embeddings. They provide a form of scaffolding. But a reviewer notes and I think anyone will admit in 2020 that contextual language models like BERT also provide much of this scaffolding, and it falls to the paper author to provide convincing evidence that using external parsers is valuable and necessary in this quest. In this, the current paper seems to fall somewhat short.  Pros:    Clearly and honestly written paper    Good exploration of value of constituency & dependency parse representations    Exploits recent work in contrastive learning  Cons:    Insufficient novelty    Experimental comparisons not well controlled – too much apples and oranges.     No comparisons of inference speed tradeoffs    Value of exploiting explicit syntax is too much assumed rather than explored    It s not established that use of explicit syntax really delivers versus alternatives such as contextual language models  Several of the reviewers felt that this paper was a fairly limited extension of L & L 2018, without any clearly novel contribution. The issue of comparability in results is complicated. There is a reason to move to a new standard corpus, rather than privileged people passing around archived copies of the old BooksCorpus, and I think your additional experiments show the results are "near enough" but there would still be much more archival value in a new paper having a set of comparable results on a new corpus. The big question of whether to do this or use BERT is better addressed in your additional experiments presenting a random projection of BERT to a comparable higher dimensional space. But unfortunately these results further weaken the clarity of the case for needing to head in the direction of this paper rather than just using a large pre trained contextual LM.
All the reviewers are agree on the significance of the topic of understanding expressivity of deep networks. This paper makes good progress in analyzing the ability of deep networks to fit multivariate polynomials. They show exponential depth advantage for general sparse polynomials.   I am very surprised that the paper misses the original contribution of Andrew Barron. He analyzes the size of the shallow neural networks needed to fit a wide class of functions including polynomials. The deep learning community likes to think that everything has been invented in the current decade.  @article{barron1994approximation,   title {Approximation and estimation bounds for artificial neural networks},   author {Barron, Andrew R},   journal {Machine Learning},   volume {14},   number {1},   pages {115 133},   year {1994},   publisher {Springer} }
All reviewers wrote strong and long reviews with good feedback but do not believe the work is currently ready for publication. I encourage the authors to update and resubmit. 
The paper addresses a few very important points on sequential latent variable models, and introduce a different view on meta RL.  Even  though the methods that this paper poses are incremental, it is such a hot debated topic that I would prefer to see this published now.
The reviewers recognized that the proposed method is interesting and seems to be useful in some cases, and the authors provided sufficient empirical results to support their claim. In addition, some comments have already been clarified. However, some reviewers still concerned that the proposed defence method will be defeated under some conditions, and still have the major concern regarding the issue of adopting some attack strategies to find adversarial examples near the safe spots, even though the authors clarified some critical points of the proposed method.  These drawbacks led to the decision to not accept. However, this paper has some merit and can be made into a stronger contribution in the future. 
The authors introduce a notion of stability to pruning and argue through empirical evaluation that pruning leads to improved generalization when it introduces instability. The reviewers were largely unconvinced, though for very different reasons. The idea that "Bayesian ideas" explain what s going on seems obviously wrong to me. The third reviewer seems to think there s a tautology lurking here and that doesn t seem to be true to me either. It is disappointing that the reviewers did not re engage with the authors after the authors produced extensive rebuttals. Unfortunately, this is a widespread pattern this year.   Even though I m inclined to ignore aspects of these reviews, I feel that there needs to be a broader empirical study to confirm these findings. In the next iteration of the paper, I believe it may also be important to relate these ideas to [1]. It would be interesting to compare also on the networks studied in [1], which are more diverse.   [1] The Lottery Ticket Hypothesis at Scale (Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin) https://arxiv.org/abs/1903.01611
The paper studies the role of entropy in maximum entropy RL, particularly in soft actor critic, and proposes an action normalization scheme that leads to a new algorithm, called Streamlined Off Policy (SOP), that does not maximize entropy, but retains or exceeds the performance of SAC. Independently from SOP, the paper also introduces Emphasizing Recent Experience (ERE) that samples minibatches from the replay buffer by prioritizing the most recent samples. After rounds of discussion and a revised version with added experiments, the reviewers viewed ERE as the main contribution, while had doubts regarding the claimed benefits of SOP. However, the paper is currently structured around SOP, and the effectiveness of ERE, which can be applied to any off policy algorithm, is not properly studied. Therefore, I recommend rejection, but encourage the authors to revisit the work with an emphasis on ERE.
This paper presents a variant of MAML or Reptile, where the meta update along the long trajectory of the inner loop optimization is bypassed to reduce the computational overhead appeared in MAML. The main idea is to use the look ahed optimizer with careful tuning of relevant hyperparameters, which is done by a teacher student scheme. Lazy MAML/Reptile are presented and experiments  demonstrated their validity. While the paper contains interesting ideas, most of reviewers have a few concerns which are not even resolved even after the author responses. First of all, ResNet analogy with respect to teacher update was claimed but it was never clearly shown in the paper. The method needs careful tuning of hyperparameters in the inner loop, but  the study about the computation requirements is not convincing yet.  Long inner loops are computationally feasible for both fomaml and reptile so its not clear in which way the proposed method is improving lengthy exploration in the inner loop other than the performance being better in the experiments. Improving the paper, taking these comments into account, will lead to a good work in near future. 
In order to learn good exploratory behaviors in settings where agents encounter diverse environments, the authors propose an approach which involves learning from episodes that exhibit good episode level exploratory behaviors.  The innovation is in the scoring and learning from these episode level behaviors rather than trying to come up with shorter timescale proxies of exploration.  In making this concrete, the authors propose to score trajectories based effectively on state coverage within an episode (i.e. good exploration corresponds to good state coverage) as well as by scoring episodes relative to one another and giving preference to episodes that explore less often encountered states.  To learn, the core algorithm interleaves standard RL updates with behavioral cloning updates using the best episodes of data, thereby training the policy to both solve the task and explore well at the episode level.  A weakness is that the paper uses low level state in grid worlds and there is some ambiguity in applying this to settings with continuous states.  The authors discuss general strategies for dealing with these limitations as potential future work.  The reviewers were positive about the clarity of the text and felt the core idea that was proposed was simple and effective.  The authors put in solid effort to address reviewer concerns.  The most salient remaining concern, which I share, is that there will be challenges in scaling this approach to more complex environments with continuous state/observation spaces.  Overall, this paper had a consensus "accept" rating (7,7,7,6), and I endorse this as my decision.
The authors provide a homotopy framework for SGD in order to exploit structures that arise by construction, such as PL. I very much liked the delineated homotopy analysis which is general (i.e., as opposed to simply adding a quadratic, the authors consider a homotopy mapping). While the algorithm should not be considered new, it is still a good proposal to consider in the SGD applications setting. Unfortunately, I cannot recommend acceptance because of several issues that the reviewers raised in detail: Strength of the assumptions, unclear performance improvement in practice, applicability of the locally PL condition, among others. 
This work proposes a concept called Populated Region Set (PRS) as a measure of robustness of deep neural networks (DNNs). The paper provides a suit of empirical results to demonstrate the strong correlation between the PRS ratio and adversarial robustness of DNNs. The authors made great efforts on addressing reviewers  concern, which is greatly appreciated. However, the theory of the work is a bit thin, and it leaves a number of outstanding issues unaddressed. For example, it is not clear the practical advantage of calculating PRS over the direct measure of robust accuracy. What new and better computational procedure can be constructed based on PRS? We encourage the authors keep improving their work for future submission.
The paper provides a framework for recourse (i.e. counterfactual explanations) that is robust to model shifts. The setup for the proposed method is a min max optimization problem, where the max is over a neighborhood around the distribution over model parameters. The model parameters are drawn from a mixture of K distributions, so that the neighborhood is specified by the Gelbrich distance on each component. The authors propose a finite dimensional version of the robustified optimization problem, which can be optimized using projected gradient descent. They evaluate their approach on the German credit dataset, the Small Business Administration dataset, and the Student performance dataset, each of which demonstrates a different type of data distribution shift.  Strengths:    Most existing work on recourse actions do not consider model change, so the problem addressed by the paper is relatively new   The experiment results demonstrate the superiority of the proposed method over baselines.  Weaknesses:    The solution provided is somewhat limited as it relies heavily on the structural properties of the mixture distribution and Gelbrich distance to reformulate the optimization problem.  Most of the reviewers voted initially for rejection. The paper is borderline, tending to rejection after the rebuttal. The authors have also considerably updated the paper with new results after the initial reviews. It seems therefore that the paper may benefit from another round of reviewing and, because of this, I recommend rejection and the authors to use the reviewers  comments to improve the paper before resubmitting to another venue for another round of reviewing.
This paper analyzes self training for sequence to sequence models and proposes a noisy version of self training. An empirical study shows the proposed noisy version improves results for machine translation and summarization tasks.  All reviewers appreciate the interesting contributions of the research, as well as clear writing. They also offer several comments for the revision of the paper.   We look forward to seeing this paper presented at the conference!
While I m sure there are many merits to the underlying work here, the consensus of the reviews is to recommend a rejection as an ICLR paper. That recommendation is based on issues with significance as well as on clarity issues, noted by reviewers even after the revisions.  One pattern I noticed was that it seemed unclear whether the paper was to be regarded primarily as a software paper or as a paper on preprocessing. Most initial reviews evaluated it primarily as a software paper, but some comments from the authors in the discussion period seemed to frame it instead as a paper about research on preprocessing (independent of software). See my other comment for more detail on this question.  Regardless of the intended framing, on significance as an ICLR submission, reviewers did not support its acceptance by either standard: * R1 post response: "Regarding how to frame the paper (either about feature pre processing or the software library), my (favourable) interpretation is to frame it as about the software library. As a paper about feature pre processing it would have even less merit." * R3 post response: "the work has more upside in the software contribution than the feature pre processing research" * R4 post respones: "After reading the revised version and the author response, I am still not convinced that the paper makes a substantial contribution either on the fundamental research angle or the software library angle"  One specific issue was that the experiments did not adequately support the main claims: * R2 post response: "I feel that the experiments in Section 8 are still too limited to demonstrate the value of the techniques and software" * R4 post response: "While the main contributions of the paper are still a little ambiguous, the experiments don t seem to support the claims (ease of use of the library). The results seem to suggest that one of the contributions is the improved accuracy of results, but then the experimentation is far too limited to draw such a conclusion."  On clarity: * R1 post response: "The new section 5 is still very hard to understand and I still couldn t make sense of the family tree primitive mechanism" * R3 post response: "I agree with AnonReviewer1 about the difficulty of understanding the family tree primitives, both before and after the revision" * R2 post response: "I am still unclear on the family tree primitives [...] it s not clear to me what problem the family tree primitives solve"   The authors showed a lot of enthusiasm and good spirits in working to improve the submission. I hope the feedback provided here is useful.  However, based on the consensus of the reviews, I recommend rejection of this submission.
This paper proves that gradient descent with random initialization converges to global minima for a squared loss penalty over a two layer ReLU network and arbitrarily labeled data. The paper has several weakness such as, 1) assuming top layer is fixed, 2) large number of hidden units  m , 3) analysis is for squared loss. Despite these weaknesses the paper makes a novel contribution to a relatively challenging problem, and is able to show convergence results without strong assumptions on the input data or the model. Reviewers find the results mostly interesting and have some concerns about the \lambda_0 requirement. I believe the authors have sufficiently addressed this issue in their response and  I suggest acceptance. 
This paper presents a sampling based approach for generating compact CNNs by pruning redundant filters. One advantage of the proposed method is a bound for the final pruning error.  One of the major concerns during review is the experiment design. The original paper lacks the results on real work dataset like ImageNet. Furthermore, the presentation is a little misleading. The authors addressed most of these problems in the revision.  Model compression and purring is a very important field for real world application, hence I choose to accept the paper. 
The paper considers whether Neural ODEs have a valid interpretation as an ODE, showing that such an interpretation is not correct unless the discretization is chosen properly.  This is important, given interest in Neural ODEs as models as well as they way they will be used, both for problems involving physical/temporal data as well as more generally.  The paper proposes an algorithm for adapting integration step size during training to partially address this issue, and empirical results are shown.  There was a detailed discussion between reviewers and authors which led to improvements.  The authors should also discuss the relationship of their work with https://arxiv.org/abs/2008.02389, which makes a similar point, in the final version.
While the paper contains interesting ideas, the reviewers suggest improving the clarity and experimental study of the paper. The work holds promises but is not ready for publication at ICLR.
The paper studies rate at which SGD escapes local minima and provides a potential justification for the "flat minima" observation.  Reviewers agree that the paper studies an interesting problem and provides a nice result. But it seems like that paper in it s current shape is not ready for publication at ICLR. Issue is that the paper s writing is not up to bar, and requires a fair bit of work. In particular, the paper doesn t define the key quantities formally, doesn t provide all the assumptions in one place and justify why they might be reasonable. Finally, it would be great if the final result about escape rate is provided clearly with a self contained theorem/lemma that define/describe most of the key quantities in the rate.
This paper studies a data driven similarity metric for physical simulation data, based on entropy rate of a physical system. The authors consider a one parameter family of spatial fields obtained by varying certain parameter, and use those in a self supervised setup.  Reviewers were split in this submission. While some reviewers highlighted the novelty in the problem setup and the idea of considering one parameter families, they also expressed concern about the lack of proper justification of the entropy analogy, as well as doubts on the empirical evaluation. Ultimately, and taking all these considerations into account, the AC believes this work would greatly benefit from another review cycle, by addressing the concerns expressed here. Therefore, the AC recommends rejection at this time.
Two reviewers are negative on this paper while the other one is slightly positive. Overall, this paper does not make the bar of ICLR. A reject is recommended.
This paper proposes a latent variable approach to the neural module networks of Andreas et al, whereby the program determining the structure of a module network is a structured discrete latent variable. The authors explore inference mechanisms over such programs and evaluate them on SHAPES.  This paper may seem acceptable on the basis of its scores, but R1 (in particular) and R3 did a shambolic job of reviewing: their reviews are extremely short, and offer no substance to justify their scores. R2 has admirably engaged in discussion and upped their score to 6, but continue to find the paper fairly borderline, as do I. Weighing the reviews by the confidence I have in the reviewers based on their engagement, I would have to concur with R2 that this paper is very borderline. I like the core idea, but agree that the presentation of the inference techniques for V NMN is complex and its presentation could stand to be significantly improved. I appreciate that the authors have made some updates on the basis of R2 s feedback, but unfortunately due to the competitive nature of this year s ICLR and the number of acceptable paper, I cannot fully recommend acceptance at this time.  As a complete side note, it is surprising not to see the Kingma & Welling (2013) VAE paper cited here, given the topic.
This paper proposes an efficient approach for computing equivariant spherical CNNs, significantly reducing the memory and computation costs. Experiments validate the effectiveness of the proposed approach.  Pros: 1. Speeding up equivariant spherical CNNs is a valuable topic in deep learning.  2. The proposed approach is effective, in all parameter size, memory footprint and computation time. 3. The theory underpinning the speedup method is sound.  Cons: 1. The readability should be improved. Two of the reviewers complained that the paper is hard to read and only Reviewer #2 reflected that it is "easy" to read (but only under the condition that the readers are familiar with the relevant mathematics), and this situation is improved after rebuttal. Nonetheless, this should be further done. 2. The experiments are a bit limited. This may partially be due to limited benchmark datasets for spherical data, but for the existing datasets used for comparison, Esteves et al. (2020) is not compared on all of them. Esteves et al. (2020) is only reported on spherical MNIST, which has very close performance to the proposed one. This worries the AC, who is eager to see whether on QM7 and SHREC’17 the results would be similar.   After rebuttal, three of the reviewers raised their scores. So the AC recommended acceptance.
The paper pursues an interesting approach, but requires additional maturation.  The experienced reviewers raise several concerns about the current version of the paper.  The significance of the contribution was questioned.  The paper missed key opportunities to evaluate and justify critical aspects of the proposed approach, via targeted ablation and baseline studies.  The quality and clarity of the technical exposition was also criticized.  The comments submitted by the reviewers should help the authors strengthen the paper. 
This paper addresses the question of how to regularize when starting from a pre trained convolutional network in the context of transfer learning.  The authors propose to regularize toward the parameters of the pre trained model and study multiple regularizers of this type.  The experiments are thorough and convincing enough.  This regularizer has been used quite a bit for shallow models (e.g. SVMs as the authors mention, but also e.g. more general MaxEnt models).  There is at least some work on regularization toward a pre trained model also in the context of domain adaptation with deep neural networks (e.g. for speaker adaptation in speech recognition).  The only remaining novelty is the transfer learning context.  This is not a sufficiently different setting to merit a new paper on the topic.
All reviewers agree that this research is novel and well carried out, so this is a clear accept. Please ensure that the final version reflect the reviewer comments and the new information provided during the rebuttal
This paper does not meet the acceptance bar this year, and thus I must recommend it for rejection.
This paper proposes a learning based method for shape registration that conditions on regions of the shape rather than learning from the entire point cloud in one shot.  The reviewers point out several questions about the method, thanks to expository issues as well as missing comparisons/ablation studies.  As the authors have chosen not to submit a rebuttal, I will refer them to the original reviews for details here for additional points of improvement.
This paper proposes to address the class imbalance problem by defining an over sampling strategy based on oversampling. It brings potentially interesting ideas. The reviewers agree on the fact that the experiments are limited, some methodological aspects require some clarifications and the writing needs to be improved.  The authors did not provide any rebuttal. Hence I recommend rejection. 
This paper presents a model to identify entity mentions that are synonymous.  This could have utility in practical scenarios that handle entities.  The main criticism of the paper is regarding the baselines used.  Most of the baselines that are compared against are extremely simple.  There is a significant body of literature that models paraphrase and entailment and many of those baselines are missing (decomposable attention, DIIN, other cross attention mechanisms).  Adding those experiments would make the experimental setup stronger.  There is a bit of a disagreement between reviewers, but I agree with the two reviewers who point out the weakness of the experimental setup, and fixing those issues could improve the paper significantly.
In this paper, the authors proposed a method for causal inference under limited overlap   an important and understudied complication.  The authors propose to recover a prognostic score using a variational autoencoder, and thereby map a higher dimensional set of covariates with limited overlap to a lower dimensional set where overlap holds, and such that ignorability is maintained.  The paper was reviewed quite favorably by reviewers, and the authors updated the manuscript to address specific issues raised by reviewers.
This paper studies the question of memorization within overparametrised neural networks. Specifically, the authors conjecture that memorization is linked to the downsampling operators present in many convolutional autoencoders.   All reviewers agreed that this is an interesting question that deserves further analysis. However, they also agreed that in its current form, the paper lacks mathematical and experimental rigor. In particular, the paper does not follow the basic mathematical standards of proving any stated proposition/theorem, instead mixing empirical with mathematical proofs. The AC fully agrees with the points raised by reviewers, and therefore recommends rejection at this point, encouraging the authors to address these important points before resubmitting their work. 
This paper studies the low rank properties of DAG models, and illustrates through proof of concept how low rank ness can be exploited in structure learning of DAGs. After a lengthy discussion amongst the reviewers, it became clear that although there are some interesting ideas here, there is not enough enthusiasm for this work in its current form. The results in Section 4 connecting rank to structural properties are interesting, but the reviewers were concerned by the lack of precise statements connecting these results to known ensembles such as scale free graphs (even though the authors discuss some heuristic connections). In the end, despite considerable enthusiasm regarding these ideas and the importance of the problem studied, there remained too many concerns that require a major revision before acceptance.
The reviewers all agree that the work is interesting, but none have stood out and championed the paper as exceptional. The reviewers note that the paper is well written, contributes a methodological innovation, and provides compelling experiments. However, given the reviewers  positive but unenthusiastic scores, and after discussion with PCs, this paper does not meet the bar for acceptance into ICLR.
This paper proposes two methods to speed up the evaluation of neural ODEs: regularizing the ODE to be easier to integrate, and adaptively choosing which integrator to use.  These two ideas are fundamentally sensible, but the execution of the current paper is lacking.  In addition to writing and clarity issues, the main problem is not comparing to Finlay et al.  The Kelly et al paper could potentially be considered concurrent work.  I also suggest broadening the scope of the DISE method to ODE / SDE /PDE solvers in general, in situations where many similar differential equations need to be solved, amortizing the solver selection will be worthwhile even if there are no neural nets in the differential equation.  I also encourage the authors to do experiments that explore the tradeoffs of different approaches, rather than aiming just for bold lines in tables.
The paper revisits representation learning for extreme settings (large number of class categories) in a federated learning setup. The authors show how each client can sample a set of negative classes and optimize only the corresponding model parameters with respect to a sampled softmax objective that approximates the global full softmax objective. The authors investigate the interest of the approach for image classification and image retrieval.   The reviewers appreciated the interest of the approach to reduce communication and the experimental evaluation on several datasets. The reviewers also expressed concerns about privacy, a central concern in federated learning. One reviewer noted for instance that ‘since every sampled set of each client has to include the classes that the client has, the central server can infer the classes the client has’. The reviewers would also have liked to see a more comprehensive evaluation, in the absence of the theoretical guarantees. Finally, the reviewers expressed regarding accuracy/efficiency trade offs, one reviewer commenting that “the proposed method degrades the accuracy”.   The authors submitted responses to the reviewers  comments. The authors discussed the challenges related to privacy. The authors also commented on other gradient sparsification communication reducing competing approaches (FedAwS) and the choice of datasets. After reading the response, updating the reviews, and discussion, the reviewers found that ‘the current good results are only obtained on smaller scale datasets with fewer classes [while] in machine learning, the phenomenon could be quite different at different scales’ and that ‘it is not clear if the proposed method can outperform TernGrad at the same amount of transferred data [and] TernGrad also has a better convergence proof compared to the proposed method’.   We encourage the paper to pursue their approach further taking into account the reviewers  comments, encouragements, and suggestions. Recent progress in privacy protection theoretical frameworks in FL (secure multi party computation, etc.), see the recent survey by Kairouz et al. in FnT in ML, should help the authors develop guarantees for their approach. Moreover the reviewers suggested a clear path towards further improvements of the experimental evaluation.   The revision of the paper will generate a stronger submission to a future venue.  Reject.
I think there is good research behind this paper, but the presentation issues make it difficult to argue for acceptance.   On the positive side, the paper has made a clear advance in terms of the ability to do full SAT based verification of neural networks. However, there are also important issues with the paper that prevent it from being accepted:  * The paper argues for the value of the new approach for *both* verifiability and interpretability, where interpretability is measured in terms of the ability to make targeted adjustments to the network to change its behavior. These are very different goals, but they are conflated in different parts of the paper, leading to confusion, for example, from reviewer RhEH.  * The paper only compares against SAT/SMT based verification, but completely ignores other approaches to verification that are arguably more effective for many problems. In particular, there is an emerging literature on Abstract Interpretation based verification that is significantly more scalable than SAT based verification and which this paper ignores. * The paper s claims sometimes get ahead of the presented evidence, as pointed out by reviewer garj.   So overall, I think this paper needs another iteration before it is ready for acceptance.
Four knowledgeable referees reviewed this paper; one reviewer (weakly) supports accept and other three indicate reject. Even with the rebuttal, all reviewers (including positive reviewer) have concerns on unconvincing experimental results (due to missing baselines for instance). I basically agree on negative reviews that this submission fails to have enough quality considering the high standard of ICLR.
The paper addresses the problem of offline meta reinforcement learning. The authors build on the FOCAL algorithm, adding intra task attention and inter task contrastive representation learning objectives. The resulting FOCAL++ algorithm outperforms several strong baseline, including FOCAL and a theoretical analysis attempting to show that FOCAL++ provably improves on FOCAL is included.  Reviewers agreed that the novelty of the proposed approach is limited since attention and contrastive representation learning have been used in the closely related (online) meta RL setting. At the same time reviewers agreed that the results in the paper and the rebuttal show that FOCAL++ improves on a strong set of baselines.  The main shared concern was regarding the significance and validity of the theoretical analysis. After considering the rebuttal reviewers voting for both acceptance and rejection were in agreement that there are issues with the theoretical analysis/justification. While we agree with the authors that the algorithmic and experimental part of the paper is strong, we have to base our decision on the state of the whole paper. In the end we decided not to accept the paper because 1) the paper put a significant focus on a theoretical argument the reviewers found problematic and 2) the authors did not modify the paper to sufficiently address these concerns during the available window.
Overall, this paper receives positive reviews. The reviewers find the technical novelty and contributions are significant enough for acceptance at this conference. The authors  rebuttal helps address some issues. The area chair agrees with the reviewers and recommend it be accepted at this conference.
This paper a framework of learning with noisy labels named PARS that combines three types of approaches, i.e., sample selection, noise robust loss, and label correction.  The framework leverages both original noisy labels and estimated pseudo labels of all samples for improving the training performance, and the empirical studies demonstrated competitive results on CIFAR datasets especially in high noise and low resource settings.   Reviewers raised some major concerns about the weaknesses. For example, empirical gain in small noise regime are small or negligible, and no empirical gain against SOTA in large dataset with real world noise (Clothing1M).  While large gains in large noise regime (more than 80%), such setting may not be very realistic and there also  lack of in depth analysis on the sources of the gain (e.g., it is unknown if the gain is mainly because of using a better SSL or other factors since LNL becomes more similar to SSL when noise is very high). For technical novelty perspective, while the proposed approach is new, the overall novelty may not be very significant as this paper mainly combines existing techniques, e.g., negative learning and FixMatch (a semi supervised learning method) in the proposed learning approach.   Authors have made great efforts for addressing the reviewers’ concerns partly, but some major concerns on the technical novelty and empirical studies remain.  Therefore, the paper is not recommended for acceptance in its current form. I hope authors found the review comments and discussions useful and constructive, and like to see it accepted in the near future after these issues are fully addressed.
This paper proposes a graph neural network architecture to learn representations for directed acyclic graphs. Specifically, the proposed method performs the aggregation of the representations from neighboring nodes in the topological order defined by the DAG, with a novel topological batching scheme, which allows to process the message passing operations in parallel. The authors propose theoretical analysis of the proposed methods, to show that it is invariant to node indexing and learns an injective mapping to discriminate between two different graphs. The proposed method is further experimentally validated on multiple tasks involving DAGs, and the results show that it outperforms existing GNNs, including existing methods that can capture DAGs such as D VAE (encoder).   The reviewers were unanimously positive about the paper. All reviewers find the performance improvements and time efficiency obtained with the proposed method to be satisfactory or promising, and one of the reviewers (R4) mentions that the tackled problem is important and the paper is well written. However, there were concerns regarding insufficient explanations, missing ablation studies, and missing details of some parts of the proposed method. Yet, most of the issues have been satisfactorily addressed during the interactive discussion period. I agree with the reviewers that the paper is tackling an important problem, find the paper well written, and the proposed DAGNN as practically useful. Thus I recommend an acceptance.   However, the contributions of the proposed work over D VAE, which also deals with DAGs, should be better described, as also noted by R2. The DAGNN uses attention, and can stack multiple layers as it is a more general GNN framework while D VAE is a generative model, but these seem like incremental differences over D VAE, and it is not clear which contributes to DAGNN’s superior performance over D VAE. Topological batching is a clear advantage of DAGNN over D VAE, but the experimental results showing the advantage of it over D VAE’s sequential training was missing in the original paper (while it was added later to the appendix). I suggest the authors to introduce D VAE in the introduction, acknowledge that it also tackles DAGs, and clearly describe how the proposed method differs from D VAE encoder in a separate section. Also, there needs to be an analysis on why the proposed DAGNN outperforms D VAE, as well as time efficiency comparison with the original D VAE in the main text.  
The paper provides a method for with tuning continuous hyperparameters (HPs). It is closely related to a previous work (Lorraine, 2019) that was limited to certain HPs, and in particular could not be applied to HPs controlling the learning such as learning rate, momentum, and are known to be influential to the convergence and overall performance (for non convex objectives).  The reviews indicate a uniform opinion that the paper tackles an important problem, that its methods provide a non trivial improvement over previous techniques and in particular those of (Lorrain, 2019), and that the provided experiments are extensive and convincing. The initial reviews had several concerns about technical details in the paper such as the analysis or how the meta hyperparameters are tuned. However, in the discussions the authors provided adequate responses, resolving these concerns. I believe that with minor edits that are possible to get done by the camera ready deadline the authors can incorporate their responses into the paper making it a welcome addition to ICLR.
The consensus of reviewers is that this paper is not acceptable in present form, and the AC concurs.
This paper offers a theoretical and empirical analysis of the expressivity of RNNs, in particular in comparison to TT decomposition. The reviewers argued the results was interesting and important, although there were issues with clarity of some of the explanations. More critical reviewers argued the comparison basis with CP networks was not "fair" in that their shallowness restricted their expressivity w.r.t. TT. The experiments could be strengthened by making the explanations surrounding the set up clearer. This paper is borderline acceptable, and would have benefited from a more active discussion between the reviewers and the author. From reading the reviews and the author responses, I am leaning towards recommending acceptance to the main conference rather than the workshop track, as it is important to have theoretical work of this nature discussed at ICLR.
Summary: The authors observe that a range of Laplacian type operators used in graph neural networks can be embedded in a parametric family, so that the precise form of the Laplacian used can be determined by the learning process. Empirical evaluation and some (limited) theoretical analysis are provided.   Discussion: The authors have provided detailed replies and also additional experiments. That has addressed major concerns, and most reviewers now agree the paper is good. One reviewer is more skeptical, mostly regarding presentation. I agree with some of the points raised in this regard, but see them as less of an issue   I would consider the presentation improvable, but acceptable.  One weakness I should mention is that the two theorems provided are frankly trivial. I appreciate this is  only  a conference submission, but I would nonetheless call the fact that symmetric matrices have real eigenvalues (theorem 1) an observation, not a result.  That similarly holds for any direct consequence of Gershgorin s theorem (theorem 2). The entire page used to state this could perhaps be put to better use for additional empirical results.   Recommendation: The program committee (the AC and program chairs) were hesitating about this paper but decided to recommend acceptance. The idea is neat and simple, presentation and empirical evaluation are fine, if improvable (we strongly recommend the authors to invest time). What is phrased as theory is trivial, but also admittedly not the main focus of the paper.    
This paper generally presents a nice idea, and some of the modifications to searn/lols that the authors had to make to work with neural networks are possibly useful to others. Some weaknesses exist in the evaluation that everyone seems to agree on, but disagree about importance (in particular, comparison to things like BLS and Mixer on problems other than MT).  A few side comments (not really part of meta review, but included here anyway):   Treating rollin/out as a hyperparameter is not unique to this paper; this was also done by Chang et al., NIPS 2016, "A credit assignment compiler..."   One big question that goes unanswered in this paper is "why does learned rollin (or mixed rollin) not work in the MT setting." If the authors could add anything to explain this, it would be very helpful!   Goldberg & Nivre didn t really introduce the _idea_ of dynamic oracles, they simply gave it that name (e.g., in the original Searn paper, and in most of the imitation learning literature, what G&n call a "dynamic oracle" everyone else just calls an "oracle" or "expert")
An interesting paper, generally well written. Though it would be nice to see that the methods and observations generalize to other datasets, it is probably too much to ask as datasets with required properties do not seem to exist.  There is a clear consensus to accept the paper.  + an interesting extension of previous work on emergent communications (e.g., referential games) + well written paper  
Dear authors,  The reviewers agreed that the theoretical part lacked novelty and that the paper should focus on its experimental part which at the moment is not strong enough to warrant publication.  Regarding the theoretical part, here are the main concerns:   Even though it is used in previous works, the continuous time approximation of stochastic gradient overlooks its practical behaviour, especially since a good rule of thumb is to use as large as stepsize as possible (without reaching divergence), as for instance mentioned in The Marginal Value of Adaptive Gradient Methods in Machine Learning by Wilson et al.   The isotropic approximation is very strong and I don t know settings where this would hold. Since it seems central to your statements, I wonder what can be deduced from the obtained results.   I do not think the Gaussian assumption is unreasonable and I am fine with it. Though there are clearly cases where this will not be true, it will probably be OK most of the time.  I encourage the authors to focus on the experimental part in a resubmission.
This paper addresses the problem of many to many cross domain mapping tasks with a double variational auto encoder architecture, making use of the normalizing flow based priors.  Reviewers and AC unanimously agree that it is a well written paper with a solid approach to a complicated real problem supported by good experimental results. There are still some concerns with confusing notations, and with human study to further validate their approach, which should be addressed in a future version.  I recommend acceptance.
The reviewers initially struggled to position this contribution in terms of usefulness. During the discussion phase, it became (more) clear that the proposed method is best used to reduce the communication overhead of ZeRO3. While the integration of this work and ZeRO hasn t been attempted yet, the authors claim that this work "clears the theoretical barrier". From that point of view, the reviewers were not satisfied with the guarantees of the method, arguing that the resulting algorithm is slower than standard EF and could suffer in terms of runtime (when one factors the cost of compression) even when compared to standard uncompressed SGD. Overall, the discussion greatly improved the paper, although directly integrating ConEF with ZeRO could be even more convincing.
The paper analyzes the behavior of SGD using diffusion theory. They focus on the problem of escaping from a minimum (Kramers escape problem) and derive the escape time of continuous time SGD and Langevin dynamics. The analysis is done under various assumptions which although might not always hold in practice do not seem completely unreasonable and have been used in prior work. Overall, this is a valuable contribution which is connected to some active research questions regarding the flatness of minima found by SGD (with potential connections to generalization). I would advise the authors to improve the quality of the writing and address other problems raised by the reviewers. I think this would help the paper maximize its impact.
This work explores weight pruning for BERT in three broad regimes of transfer learning: low, medium and high.  Overall, the paper is well written and explained and the goal of efficient training and inference is meaningful. Reviewers have major concerns about this work is its technical innovation and value to the community: a reuse of pruning to BERT is not new in technical perspective, the marginal improvement in pruning ratio compared to other compression method for BERT, and the introduced sparsity that hinders efficient computation for modern hardware such as GPU. The rebuttal failed to answer a majority of these important concerns.  Hence I recommend rejection.
The paper proposes combining paired attention with co attention. The reviewers have remarked that the paper is will written and that the experiments provide some new insights into this combination. Initially, some additional experiments were proposed, which were addressed by the authors in the rebuttal and the new version of the paper. However, ICLR is becoming a very competitive conference where novelty is an important criteria for acceptance, and unfortunately the paper was considered to lack the novelty to be presented at ICLR.
This paper proposes a non parametric method for metric learning and classification. One of the reviewers points out that it can be viewed as an extension of NCA. There is in fact a non linear version of NCA that was subsequently published, see [1]. In this sense, the approach here appears to be a version of nonlinear NCA with learnable per example weights, approximate nearest neighbour search, and the allowance of stale exemplars. In this view, there is concern from the reviewers that there may not be sufficient novelty for acceptance.  The reviewers have concerns with scalability. It would be helpful to include clarification or even some empirical results on how this scales compared to softmax. It is particularly relevant for larger datasets like Imagenet, where it may be impossible to store all exemplars in memory.  It is also recommended to relate this approach to metric learning approaches in few shot learning. Particularly to address the claim that this is the first approach to combine metric learning and classification.  [1]: Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure. Ruslan Salakhutdinov and Geoffrey Hinton.  AISTATS 2007 
The paper proposes a new method to improve exploration in sparse reward problems, by having two agents competing with each other to generate shaping reward that relies on how novel a newly visited state is.  The idea is nice and simple, and the results are promising.  The authors implemented more baselines suggested in initial reviews, which was also helpful.  On the other hand, the approach appears somewhat ad hoc.  It is not always clear why (and when) the method works, although some intuitions are given.  One reviewer gave a nice suggestion of obtaining further insights by running experiments in less complex environments.  Overall, this work is an interesting contribution.
This paper introduces a new model, called Memformer, that combines the strength of transformer networks and recurrent neural networks. While the reviewers found the idea interesting, they also raised issues regarding the experimental section. In particular, they found the results unconvincing, because of weak baselines, non standard experimental settings (eg. using reporting perplexity results on BPE tokens), or evaluating on only one dataset. These concerns were not well addressed by the rebuttal. For these reasons, I recommend to reject the paper.
This paper presents a quantization scheme with the advantage of high computational efﬁciency. The experimental results show that the proposed scheme outperforms SOTA methods and is competitive with the full precision models. The reviewers initially raised some concerns including baseline ResNet performance,  detailed comparison of the quantization size, and comparison with ResNet50. Authors addressed these concerns in the rebuttal and revised the draft to accommodate the requested items. The reviewers appreciated the revision and find it highly improved. Their overall recommendation is toward accept, which I also support.
This paper deals with solving the problem of scheduling machines in a semiconductor factory using an RL approach. As the different actions take a different amount of time to complete, the authors propose to use a predictron architecture to estimate the targets in DQN. The experimental results show that the proposed method outperforms the considered baselines on two scheduling problems.  After reading the authors  feedback and discussing their concerns, the reviewers agree that this paper is still not ready for publication. In particular, the main issues are about the novelty/similarity with respect to related works, the lack of theoretical insights and formal definitions, the effectiveness of the presented benchmarking, lack of analysis of some unexpected results.  I encourage the authors to take into consideration the concerns raised by the reviewers when they will work on the updated version of their paper.
 The paper proposes to generate human like question for a image by using additional information (hints) such as the textual answer and visual regions of interest (ROIs).  The visual regions are used to guide the question generation so that the model can generate relevant and informative question.  The question generation problem is formulated as a graph to sequence problem, starting from an object graph, and using GCNs with attention to align text and visual regions and to generate an appropriate question.  Review Summary: The submission initially mixed reviews (scores from 3 to 6).  While reviewers find the problem interesting and work to be mostly solid, reviewers felt that the novelty of the work was limited (R1, R3) and that some of the presentation was unclear (R1, R3) with some missing details (R2).  R2 was not sure if the VQG was a useful task, and R4 felt that the initial submission was missing a key experiment on whether the generated questions can be useful as data augmentation for VQA. The reviewers were impressed by the extra experiments performed by the authors in the rebuttal and the revised draft, and indicated that most of their concerns were addressed, In particular,  the generated questions were shown to be useful as data augmentation.  Many reviewers increased their scores, ending with 3 scores of 6 (R1,R2,R4) and 1 score of 5 (R3).    Pros:   The use of generated questions as data augmentation for VQA is an interesting direction   Strong empirical results with thorough experiments (and user study)  Cons:   The technical novelty of the work is still rather limited (R1)   The paper was difficult to follow (R1,R3) with a lot of moving parts, making it potentially difficult reproduce (R1)   The authors indicated that they will open source the code.   The grammar and wording of the writing is poor even after revision and should be improved  Example of poor writing (a full proofreading pass is recommended):   Section 2.1: "Mora et al. (2016) firstly makes an attempt to adapt"  > "Mora et al. (2016) adapted", "abstract and general results"  > "imprecise and generic questions"   Section 3.2: "The most important point of our first issue located in how to effectively find..." (it s unclear what this means)   Section 4.1: "standfordcoreNLP"  > "Stanford CoreNLP"   Section 4.5: "shown in 3"  > "shown in Figure 3"  Recommendation: The AC agrees that the work is addresses an interesting area of generating questions as data augmentation for VQA.  Despite the improved reviewer scores, the AC agrees with the initial assessment that the work has limited technical novelty.  The AC also found the writing of the paper to be poor and difficult to follow at places even after revision.  Due the limited novelty, the issues with exposition, and the many changes to paper, the AC believe that the work would benefit from another round of review and should not be accepted at ICLR in its current form.  Given the positive response, the authors are encourage to improve their work and writing and resubmit to an appropriate venue (the AC believes the work would be more appreciated in a vision or language venue).
This paper presents an uncertainty quantification method that is conceptually interesting and practical. All reviewers are in consensus regarding the quality and significance of this manuscript. 
This paper gives a method of performing experimental design (one round active learning) in overparameterized regression. Although the comparison with the coreset method baseline is a nice addition, the reviewers still have concerns in the following aspects:   The novelty compared to the classical v optimality design is limited   The hyperparameter t is hard to choose in practice. This is important because each different hyperparameter setting would induce a different set of examples for label queries.    Computational complexity   It is unclear exactly how the proposed method (or other experimental design method) can mitigate double descent   We encourage the authors to take these into account in the revision.  
Overview: This paper introduces a method to distill a large dataset into a smaller one that allows for faster training. The main application of this technique being studied is neural architecture search, which can be sped up by quickly evaluating architectures on the generated data rather than slowly evaluating them on the original data.  Summary of discussion: During the discussion period, the authors appear to have updated the paper quite a bit, leading to the reviewers feeling more positive about it now than in the beginning. In particular, in the beginning, it appears to have been unclear that the distillation is merely used as a speedup trick, not to generate additional information out of thin air. The reviewers  scores left the paper below the decision boundary, but closely enough so that I read it myself.   My own judgement: I like the idea, which I find very novel. However, I have to push back on the authors  claims about their good performance in NAS. This has several reasons:  1. In contrast to what is claimed by the authors, the comparison to graph hypernetworks (Zhang et al) is not fair, since the authors used a different protocol: Zhang et al sampled 800 networks and reported the performance (mean +/  std) of the 10 judged to be best by the hypernetwork. In contrast, the authors of the current paper sampled 1000 networks and reported the performance of the single one judged to be best. They repeated this procedure 5 times to get mean +/  std. The best architecture of 1000 is of course more likely to be strong than the average of the top 10 of 800.  2. The comparison to random search with weight sharing (here: 3.92% error) does not appear fair. The cited paper in Table 1 is *not* the paper introducing random search + weight sharing, but the neural architecture optimization paper. The original one reported an error of 2.85% +/  0.08% with 4.3M params. That paper also has the full source code available, so the authors could have performed a true apples to apples comparison.   3. The authors  method requires an additional (one time) cost for actually creating the  fake  training data, so their runtimes should be increased by the 8h required for that.  4. The fact that the authors achieve 2.42% error doesn t mean much; that result is just based on scaling the network up to 100M params. (The network obtained by random search also achieves 2.51%.)  As it stands, I cannot judge whether the authors  approach yields strong performance for NAS. In order to allow that conclusion, the authors would have to compare to another method based on the same underlying code base and experimental protocol. Also, the authors do not make code available at this time. Their method has a lot of bells and whistles, and I do not expect that I could reproduce it. They promise code, but it is unclear what this would include: the generated training data, code for training the networks, code for the meta approach, etc? This would have been much easier to judge had the authors made the code available in anonymized fashion during the review.  Because of these reasons, in terms of making progress on NAS, the paper does not quite clear the bar for me. The authors also evaluated their method in several other scenarios, including reinforcement learning. These results appear to be very promising, but largely preliminary due to lack of time in the rebuttal phase.    Recommendation: The paper is very novel and the results appear very promising, but they are also somewhat preliminary. The reviewers  scores leave the paper just below the acceptance threshold and my own borderline judgement is not positive enough to overrule this. I believe that some more time, and one more iteration of reorganization and review, would allow this paper to ripen into a very strong paper. For a resubmission to the next venue, I would recommend to either perform an apples to apples comparison for NAS or reorganize and just use NAS as one of several equally weighted possible applications. In the current form, I believe the paper is not using its full potential.
This paper aims to estimate the parameters of a projectile physical equation from a small number of trajectory observations in two computer games. The authors demonstrate that their method works, and that the learnt model generalises from one game to another. However, the reviewers had concerns about the simplicity of the tasks, the longer term value of the proposed method to the research community, and the writing of the paper. During the discussion period, the authors were able to address some of these questions, however many other points were left unanswered, and the authors did not modify the paper to reflect the reviewers’ feedback. Hence, in the current state this paper appears more suitable for a workshop rather than a conference, and I recommend rejection.
This paper presents a new reinforcement learning algorithm for POMDPs that specifically deals with the credit assignment problem. The proposed algorithm consists in using at each time step t of a training trajectory the subsequent future trajectory that starts at time t+1 as additional inputs to the policy and value networks. Instead of using the trajectories directly, two RNNs are used to encode the trajectories into latent two variables that are then given as inputs to the policy and value networks. A key novel contribution of this work is the use of "Z forcing" to help the RNNs learn the relevant information. Since future trajectories are not available during testing, a "prior" network is trained to predict the latent variable given a state. During testing, the latent variable is sampled from the network. Empirical experiments on simple simulated environments show that the proposed algorithm outperforms several baselines.  Key issues raised by the reviewers include the complexity of the proposed algorithm, the fact that several interesting results are in the appendix rather than the main paper, and the weakness of certain baselines. The authors responses helped clarify these issues, and additional experiments (such as a comparison to a DQN with n step value updates) were performed and added to the paper. The reviews are updated accordingly.  In summary, the paper contains several novel ideas in the context of learning in partially observable environments. It is not entirely clear similar effects of the proposed algorithm can be obtained by using simpler tricks, but the evidence provided by the authors supports the claim that the algorithm outperforms several SOTA techniques in the context of POMDPS.
The paper proposes a novel network architecture for sequential learning, called trellis networks, which generalizes truncated RNNs and also links them to temporal convnets. The advantages of both types of nets are used to design trellis networks which appear to outperform state of art on several datasets.  The paper is well written and the results are convincing.
This paper presents a method to learn a pruned convolutional network during conventional training.  Pruning the network has advantages (in deployment) of reducing the final model size and reducing the required FLOPS for compute.  The method adds a pruning mask on each layer with an additional sparsity loss on the mask variables. The method avoids the cost of a train prune retrain optimization process that has been used in several earlier papers.  The method is evaluated on CIFAR 10 and ImageNet with three standard convolutional network architectures.  The results show comparable performance to the original networks with the learned sparse networks.   The reviewers made many substantial comments on the paper and most of these were addressed in the author response and subsequent discussion.  For example, Reviewer1 mentioned two other papers that promote sparsity implicitly during training (Q3), and the authors acknowledged the omission and described how those methods had less flexibility on a target metric (FLOPS) that is not parameter size.  Many of the author responses described changes to an updated paper that would clarify the claims and results (R1: Q2 7, R2:Q3).  However, the reviewers raised many concerns for the original paper and they did not see an updated paper that contains the proposed revisions.  Given the numerous concerns with the original submission, the reviewers wanted to see the revised paper to assess whether their concerns had been addressed adequately. Additionally, the paper does not have a comparison experiment with state of the art results, and the current results were not sufficiently convincing for the reviewers.  Reviewer1 and author response to questions 13 15 suggest that the experimental results with ResNet 34 are inadequate to show the benefits of the approach, but results for the proposed method with the larger ResNet 50 (which could show benefits) are not yet ready.   The current paper is not ready for publication. 
The paper proposes a LSTM based meta learning approach that learns how to update each neuron in another model for best few shot learning performance.  The reviewers agreed that this is a worthwhile problem and the approach has merits, but that it is hard to judge the significance of the work, given limited or unclear novelty compared to the work of Ravi & Larochelle (2017) and a lack of fair baseline comparisons.  I recommend rejecting the paper for now, but encourage the authors to take the reviewers  feedback into account and submit to another venue.
The reviewers consider the authors  approach to pruning of convolutional networks reasonable; but neither sufficiently novel nor sufficiently well explored for inclusion in the conference.  In particular, the reviewers would like to see a more explicit discussion of the effect on training time of the authors  method, and more discussion and comparison against previous probabilistic pruning methods.
This meta review is written after considering the reviews, the authors’ responses, the discussion, and the paper itself.  The paper proposes a training scheme for autoencoders, involving data augmentation and interpolation, that results in autoencoders for which interpolations in the latent space lead to meaningful interpolations in the image space. The paper notices that this property carries over reasonably well to datasets different from the training one.  The reviewers point out that the idea is interesting (R1, R2, R4) and simple (R2), but the experiments are substandard (R2, R4) and presentation is at times suboptimal (R1). Overall consensus is towards rejection. Authors addressed some of the concerns in their responses, but failed to convince the reviewers to change their evaluations.  I agree with the reviewers and recommend rejection at this point. The idea is indeed interesting and could be publishable if presented and evaluated well, but in the current manuscript the presentation is at times unclear or somewhat misleading (e.g. presenting the method as a general image generation method, not an interpolation method) and the experiments are reasonable, but not quite convincing, mainly because the architectures and the baselines are outdated (as also pointed out by R1 and R4). I encourage the authors to further improve the paper and resubmit to a different venue.  
This paper got uniformly strongly negative reviews.  The issue of estimating or bounding generalization accuracy from performance on the training set has a huge history and literature.  After considerable discussion the reviewers uniformly find this paper lacking in making a contribution to that literature.
Reviewers all agree that this is a strong submission. I also believe it is interesting that only by changing the geometry of embeddings, they can use the space more efficiently without increasing the number of parameters.
This paper presents a method for building representations of logical formulae not by propagating information upwards from leaves to root and making decisions (e.g. as to whether one formula entails another) based on the root representation, but rather by propagating information down from root to leaves.  It is a somewhat curious approach, and it is interesting to see that it works so well, especially on the "massive" train/test split of Evans et al. (2018). This paper certainly piques my interest, and I was disappointed to see a complete absence of discussion from reviewers during the rebuttal period despite author responses. The reviewer scores are all middle of the road scores lightly leaning towards accepting, so the paper is rather borderline. It would have been most helpful to hear what the reviewers thought of the rebuttal and revisions made to the paper.  Having read through the paper myself, and through the reviews and rebuttal, I am hesitantly casting an extra vote in favour of acceptance: the sort of work discussed in this paper is important and under represented in the conference, and the results are convincing. I however, share the concerns outlined by the reviewers in their first (and only) set of comments, and invite the authors to take particular heed of the points made by AnonReviewer3, although all make excellent points. There needs to be some further analysis and explanation of these results. If not in this paper, then at least in follow up work. For now, I will recommend with medium confidence that the paper be accepted.
This paper proposes a stepped sampler for LSTM based video detection. However, reviewers raised a series of issues of this paper, including the weakness in novelty, experiment evaluations, and generalizability of the method. Considering the limited contribution of this paper, and limited experiment evaluations, the AC agrees with the reviewers and recommends reject for this paper.
The paper presents a simple one shot approach on searching the number of channels for deep convolutional neural networks. It trains a single slimmable network and then iteratively slim and evaluate the model to ensure a minimal accuracy drop. The method is simple and the results are promising.   The main concern for this paper is the limited novelty. This work is based on slimmable network and the iterative slimming process is new, but in some sense similar to DropPath. The rebuttal that PathNet "has not demonstrated results on searching number of channels, and we are among the first few one shot approaches on architectural search for number of channels" seem weak.
The reviewers raised a number of major concerns including the limited novelty of the proposed, inadequate motivation of the design choices and, most importantly, insufficient and unconvincing experimental evaluation presented. The authors’ rebuttal addressed some of the reviewers’ questions but failed to alleviate all reviewers’ concerns. Hence, I cannot suggest this paper for presentation at ICLR.
Clarity: Well written paper with a clear contribution statement; related work is up to date; concise algorithm description and corresponding theoretical guarantees. However, the presentation could be still improved.  Significance: The polynomial running time guarantee makes the practicality of the proposed algorithm marginal. Experimental results do not back up strongly the significance of the algorithm.  Main pros:   Solid theoretical work on the distributed CSSP problem.  Main cons:   The reviewers point out the significance of the experimental results, beyond the theoretical contribution. For the ICLR audience, real, large scale experiments, that really dictate that the proposed solutions is (if not the only) one of the few solutions to follow, are necessary. The reviewers highlight that the theoretical results need to be applied to really large scale scenarios (e.g., the problems considered in the paper can definitely be handled by a single computer, and no distributed implementation is required).    Τhe polynomial time complexity of the method makes the proposed protocol hard to be useful in real applications.   How does the distributed protocol compares with a centralized one? This is not fully addressed in the rebuttal. 
The reviewers noted that this is an important, interesting but difficult topic. They appreciated that the authors clarified their assumptions in the theorem statements. Nevertheless, they recommend the authors to detail in depth when the method work better than the method where only the covariates are adjusted. They still think that the paper would require major modifications to be considered for publication hence the decision is rejection the paper.  
Thank you for submitting you paper to ICLR. Two of the reviewers are concerned that the paper’s contributions are not significant enough —either in terms of the theoretical or experimental contribution   to warrant publication. The authors have improved the experimental aspect to include a more comprehensive comparison, but this has not moved the reviewers. 
Description of paper content:  The paper describes a technique to learn option policies using behavioral cloning and then recombine them using a high level controller trained by RL. The underlying options are frozen. The method is tested in two published environments: a discrete grid world environment and a continuous action space robot. It is compared to three baselines.  Summary of paper discussion:  All reviewers moved to reject based on a lack of novelty and a lack of significant empirical results. No rebuttals were provided.
Thank you for submitting you paper to ICLR. This paper provides an informative analysis of the approximation contributions from the various assumptions made in variational auto encoders. The revision has demonstrated the robustness of the paper’s conclusions, however these conclusions are arguably unsurprising. Although the work provides a thorough and interesting piece of detective work, the significance of the findings is not quite great enough to warrant publication.  Reviewer 1 was searching for a reference for work in similar vein to section 5.4: The second problem identified in the reference below shows examples where using an approximating distribution of a particular form biases the model parameter estimates to settings that mean the true posterior is closer to that form.  R. E. Turner and M. Sahani. (2011) Two problems with variational Expectation Maximisation for time series models. Inference and Learning in Dynamic Models. Eds. D. Barber, T. Cemgil and S. Chiappa, Cambridge University Press, 104–123, 2011.
The paper proposed a novel RL based solution to the optimal partial of DNNs which is interesting to readers.  However, the paper is not well presented and hard to follow. The lack of comparisons agains existing solutions and inconsistencies in the writing as pointed out by the reviewers largely weakens the submission.  There s also no updates to the paper based on reviewers  comments.   The main reason for the decision is lack of clarity and significance justifications.
**Overview** This paper provides a way to combine SVRG and greedy GQ to improve the algorithm performance. In particular, the finite iteration complexity is improved from $\epsilon^{ 3}$ to $\epsilon^{ 2}$.  **Pros** The paper is well written. Reviewers believe this is a solid theoretical work on advancing value based algorithms for off policy optimal control. It has sufficient theoretical advancement and experiments demonstrations of the methods.   **Cons** Some reviewers are concerned that SVRG is not SOTA. SVRG is not used in practice. The techniques appear to be similar to some existing works.   **Recommendation** The meta reviewer believes that the paper has solid theoretical contributions. SVRG is a component in the new algorithm to improve the complexity. It does not need to be "useful" or "SOTA". The paper is also well written. Hence the recommendation is accept.
I recommend a rejection of this paper.  My overall impression is that this is genuinely an interesting topic and this a good basis for a solid paper, however, as pointed by several reviewers, there are multiple unanswered questions due to a very large scope of this work. It might be that a format of a conference paper is not the most appropriate for this work. The authors should consider instead submitting to some of the leading journals on medical image analysis, e.g. IEEE Transactions on Medical Imaging or Medical Image Analysis. I expect this work, as it is mostly empirical, would be appreciated there and could in fact make a much bigger impact if published there.
This paper studies synthetic data generation for graphs under the constraint of edge differential privacy. There were a number of concerns/topics of discussions, which we consider separately: 1. Theoretical contributions. There are not that many theoretical contributions in this paper. I think this is OK, if the other components are compelling enough. On the theory, the authors mention that accounting for the constants is important in the analysis of DPSGD. On the contrary, I would say that these constants are not very important: if one requires specific constants, numerical procedures can determine values, otherwise for the sake of theory, no one generally needs these constant factors.  2. Empirical/experimental contributions. This was the primary axis for evaluation for this paper. None of the authors were especially compelled by the results. The methods are essentially combinations of known tools from the literature, and it is not clear why these are the right ones to solve this problem in particular. If the results were very exciting, that might be sufficient to warrant acceptance, but it is still not clear how significant the cost of privacy is in this setting. The experiments are not thorough enough to give serious insight here. It is a significant oversight to not provide results on DPGGAN without the privacy constraint, as this is the best performing model with privacy. The omission of something as important as this (and lack of inclusion in the response, with only a promise to include later) is indication that the experiments are not sufficiently mature to warrant publication at this time. The decision of rejection is primarily based on concerns related to the empirical and experimental contributions.  3. Privacy versus link reconstruction. Reviewer 4 had concerns about the notion of privacy, claiming that it does not correspond to the probability of a link being irrecoverable. This is differential privacy "working as intended", which is not intended to make each link be irrecoverable: it is simply to make sure the answer would be similar whether or not the edge were actually present, so it may be possible to predict the presence of an edge even if we are differentially private with respect to it (e.g., the presence of many other short paths between two nodes are likely to imply presence of an edge). Some discussion of this apparent contradiction might be warranted, as this might mislead reader who are specifically trying to prevent edge recovery. It might also be worthwhile to have discussion of node DP in the final paper. The authors comment "we focus on edge privacy because it is essential for the protection of object interactions unique for network data compared with other types of data"   the stronger notion of node differential privacy might also be applicable here. It would indeed be interesting to know whether it can preserve the relevant statistics (some of which seem more "global" and thus preservable via node DP).  
This paper is very pleasant to read. The reviewers also like the key idea discussed and find the targeted application interesting and practical. However, after reading the indeed interesting motivation, all four reviewers expected to see more from the evaluation section, including more challenging and realistic set ups and clearer gains over standard methods. The reviewers also discuss how both the navigation problem as well as the GP constraint problem have been tackled in the past, often in combination (e.g. reference [1] by R1). Therefore, it would be needed to see additional experimental evaluation in line with those previous works.
This paper describes a new language model that captures both the position of words, and their order relationships.  This redefines word embeddings (previously thought of as fixed and independent vectors) to be functions of position.  This idea is implemented in several models (CNN, RNN and Transformer NNs) to show improvements on multiple tasks and datasets.  One reviewer asked for additional experiments, which the authors provided, and which still supported their methodology.   In the end, the reviewers agreed this paper should be accepted.
This paper presents a sampling inference method for learning in multi modal demonstration scenarios. Reference to imitation learning causes some confusion with the IRL domain, where this terminology is usually encountered. Providing a real application to robot reaching, while a relatively simple task in robotics, increases the difficulty and complexity of the demonstration. That makes it impressive, but also difficult to unpick the contributions and reproduce even the first demonstration. It s understandable at a meeting on learning representations that the reviewers wanted to understand why existing methods for learning multi modal distributions would not work, and get a better understanding of the tradeoffs and limitations of the proposed method. The CVAE comparison added to the appendix during the rebuttal period just pushed this paper over the bar. The demonstration is simplified, so much easier to reproduce, making it more feasible others will attempt to reproduce the claims made here. 
This submission proposes an interesting new approach on how to evaluate what features are the most useful during training. The paper is interesting and the proposed approach has the potential to be deployed in many applications, however the work as currently presented is demonstrated in a very narrow domain (stability prediction), as noted by all reviewers. Authors are encouraged to provide stronger experimental validation over more domains to show that their approach can truly improve over existing multitask frameworks.
Authors proposed a multi modal unsupervised algorithm to uncover the electricity usage of different appliances in a home. The detection of appliance was done by using both combined electricity consumption data and user location data from sensors. The unit of detection was set to be a 25 second window centered around any electricity usage spike. Authors used a encoder/decode set up to model two different factors of usage: type of appliance and variety within the same appliance. This part of the model was trained by predicting actual consumption. Then only the type of appliance was used to predict the location of people in the house, which was also factored into appliance related and unrelated factors. Locations are represented as images to avoid complicated modeling of multiple people.  The reviewers were satisfied with the discussion after the authors, and therefore believe this work is of general interest to the ICLR community.
The paper studies the variance reduced TD algorithm by Konda and Prashanth (2015). The original paper provided a convergence analysis that had some technical issues. This paper provides a new convergence analysis, and shows the advantage of VRTD to vanilla TD in terms of reducing the bias and variance. Several of the five reviewers are expert in this area and all of them are positive about it. Therefore, I recommend acceptance of this work.
This paper studies the embedding compression problem related to GNNs and graph representation. A two stage method is proposed to generate the compressed embeddings: firstly, it encodes each node into its composite code with hashing; secondly, it uses a MLP module to decode the embedding for the node. Experiments are performed to evaluate the compression effect with both pretrained graph embeddings and node classification tasks with GraphSage.  The paper considers hashing/compressing the rows/columns of adjacency matrices and using the compressed rows of the adjacency matrices as node features. The adjacency matrices are  intrinsically redundant. Therefore, it is unclear whether the achieved compression rate is significant, especially when applied to settings with known node features.   Some reviewers pointed out existing methods on learning to hash methods, which train a GNN based encoder to compress the hidden representation/embedding, are relevant. Although the authors claim that in their scenario, the goal is to efficiently compress the input feature/embedding without any embedding/encoder pre training step, it is unclear how the proposed method compares with the learning to hash methods when considering the adjacency matrices as the auxiliary information.   The dependence on the number of nodes is also a concern in terms of scalability, as we know the bottleneck of scalability in GNNs is the number of nodes.   The authors use the adjacency matrix of the input graph as the auxiliary information in the paper, which only considers local structure information. The reviewers are curious whether this approach would work for tasks in which global graph structure information is required.   On a minor note, the reviewers also think that the paper would be stronger if the authors provide more principled guidance on how to select the code cardinality c and the code length m.
Pros:   The paper proposes to use a hierarchical structure to address reconstruction issues with ALI model.   Obtaining multiple latent representations that individually achieve a different level of reconstructions is interesting.     Paper is well written and the authors made a reasonable attempt to improve the paper during the rebuttal period.   Cons:   Reviewers agree that the approach lacks novelty as similar hierarchical approaches have been proposed before.    The main goal of the paper to achieve better reconstruction in comparison to ALI without changing the latter s objective seems narrow. More analysis is needed to demonstrate that the approach out performs other approaches that directly tackle this problem in ALI.   The paper does not provide strong arguments as to why hierarchy works (limited to 2 levels in the empirical analysis presented in the paper).    Semi supervised learning as a down stream task is impressive but limited to MNSIT.  
This paper provides a theoretical analysis of the Turing completeness of popular neural network architectures, specifically Neural Transformers and the Neural GPU. The reviewers agreed that this paper provides a meaningful theoretical contribution and should be accepted to the conference. Work of a theoretical nature is, amongst other types of work, called for by the ICLR CFP, but is not a very popular category for submissions, nor is it an easy one. As such, I am happy to follow the reviewers  recommendation and support this paper.
The novelty of the claims of the paper has been challenged by one of the reviewers, and in addition, one reviewer also raised concerns about the validity of the proof of the main result (Theorem 1). I looked into the proof myself, and I agree with these concerns. The authors did not use the rebuttal phase to clarify these issues.
This paper tackles a very timely problem.  Scores of 5,6,6,8 put it in the borderline region, but in the private discussion the more negative reviewer noted that they would also be OK with the paper being accepted. I therefore recommend acceptance.  Going through the paper I missed any mention of available source code. I strongly recommend that the authors make code available; this would greatly increase the paper s impact.
All the reviewers and AC agrees that the main strength of the paper that it studies a rather important question of the validity of using linear interpolation in evaluating GANs. The paper gives concrete examples and theoretical and empirical analysis that shows linear interpolation is not a great idea. The potential weakness is that the paper doesn t provide a very convincing new evaluation to replace the linear interpolation. However, given that it s largely unclear what are the right evaluations for GANs, the AC thinks the "negative result" about linear interpolation already deserves an ICLR paper. 
This paper proposed an interesting approach to weight sharing among CNN layers via shared weight templates to save parameters. It s well written with convincing results. Reviewers have a consensus on accept.
The paper addresses a variant of multi agent reinforcement learning that aligns well with real world applications   it considers the case where agents may have individual, diverging preferences. The proposed approach trains a "manager" agent which coordinates the self interested worker agents by assigning them appropriate tasks and rewarding successful task completion (through contract generation). The approach is empirically validated on two grid world domains: resource collection and crafting. The reviewers point out that this formulation is closely related to the principle agent problem known in the economics literature, and see a key contribution of the paper in bringing this type of problem into the deep RL space.  The reviewers noted several potential weaknesses: They asked to clarify the relation to prior work, especially on the principle agents work done in other areas, as well as connections to real world applications. In this context, they also noted that the significance of the contribution was unclear. Several modeling choices were questioned, including the choice of using rule based agents for the empirical results presented in the main paper, and the need for using deep learning for contract generation. They asked the authors to provide additional details regarding scalability and sample complexity of the approach.  The authors carefully addressed the reviewer concerns, and the reviewers have indicated that they are satisfied with the response and updates to the paper. The consensus is to accept the paper.  The AC is particularly pleased to see that the authors plan to open source their code so that experiments can be replicated, and encourages them to do so in a timely manner. The AC also notes that the figures in the paper are very small, and often not readable in print   please increase figure and font sizes in the camera ready version to ensure the paper is legible when printed.
This paper proposed a conditional graph generative model closely following the unconditional generative model NetGAN and extending it by adding conditioning on extra information available for graph generation (“shadow” node attributes as the authors call it).  Overall this is an extension over NetGAN and gives this class of models the ability to utilize extra information when generating graphs.  However, all reviewers lean toward the reject side and the concerns raised by the reviewers range from limited novelty to the could be improved writing.  I want to highlight two issues.  The first is, is the GAN formulation really necessary?  Could you directly learn using a likelihood objective, e.g. treat the sampled random walks as sequence data, and learn an autoregressive sequence generative model on these data (as a language model for example)?  You could then also generate sample walks from your model.  The benefit is training would be much simpler and more stable.  I believe this could also be related to R2’s concern that the model might be learning the distribution from s alone, as that is entirely possible and potentially easier (not with the GAN formulation proposed here, though).  The second issue is that I encourage the authors to dig deeper into explainability.  The notion of explainability is obviously important but it is not very well defined.  Does controllability equal explainability?  Is a visualization sufficient to demonstrate explainability (and further, is this sufficient to show this method offers more explainability than alternatives)?  Also intuitively random walk models are not very explainable due to the large amount of randomness in the generation process, compared to e.g. autoregressive graph generative models that generate graphs part by part (related to R1’s concern).  If explainability is the main motivation, maybe other alternatives can be more competitive.  Overall I would recommend a reject at this time but encourage the authors to improve the paper further.
Unfortunately, reviewers unanimously agreed that this paper does not meet the ICLR acceptance standards, citing generally unpolished experiments. I would recommend substantially expanding the experimental results in the future.
All reviewers agree that this paper is worth publishing. It investigates a novel idea on how to adaptively prioritise experiences from replay based on relative (within batch) importance. The empirical investigation is thorough, and while the performance improvements are not stunning, the benefit is surprisingly consistent across many environments.
As of now, automatic data augmentation methods have mostly been proposed for supervised learning tasks, especially classification. This paper introduces automatic data augmentation to deep (image based) reinforcement learning agents, aiming to make the agents generalize better to new environments. A new algorithm called data regularized actor critic (DrAC) is proposed, with three variants that correspond to different methods for automatically finding a useful augmentation: UCB DrAC, RL2 DrAC, and Meta DrAC. Promising results are reported on OpenAI’s Procgen generalization benchmark which consists of 16 procedurally generated environments (games) with visual observations. Further experiments have been added in the revised version.  **Strengths:**   * This work is among the first attempts that propose an automatic data augmentation scheme for reinforcement learning.   * The paper articulates well the problem of data augmentation for reinforcement learning.   * The experiment results are generally promising.  **Weaknesses:**   * Although the experiment results reported seem promising, there are missing pieces in order to help the readers gain a deeper understanding to justify more thoroughly why the proposed regularization based scheme works.   * Theoretical justification is lacking.  This is a borderline paper. While it presents some interesting ideas supported empirically by experiment results, the paper in its current form is premature for acceptance since a more thorough, scientific treatment is lacking before drawing conclusions. Moreover, considering that there are many competitive submissions to ICLR, I do not recommend that the paper be accepted. Nevertheless, the authors are encouraged to address the concerns raised to fill the gaps when revising their paper for resubmission in the future. 
This paper incorporates phrases within the transformer architecture.  The underlying idea is interesting, but the reviewers have raised serious concerns with both clarity and the trustworthiness of the experimental evaluation, and thus I cannot recommend acceptance at this time.
Summary: The given work studies one shot object detection, and demonstrates an array of experiments that show that by increasing the number of categories in training data, the model can get better at one shot detection.   Pros:   Well written   Presents many experiments that are interesting   Improves SOTA one shot performance on COCO by using more data.  Cons:   Authors did not test their claims on a variety of model architectures   Similar conclusions have been made in prior art.   Conclusions are intuitive. As more categories are added, the likelihood of reducing semantic dissimilarity to a new novel category is quite high.   Overall contribution is currently limited.  Reviewers are unanimous in their decision. Authors did not alleviate concerns of reviewer. AC recommends authors take all feedback into consideration and submit to another venue or workshop.
The reviewers all agreed on accepting this paper, stating that it makes a compelling point about the usefulness of saliency methods to diagnose generalization.  The reviewers found that the experiments were a strong point and applauded the thorough hyperparameter tuning and re runs for statistical significance.  One reviewer commented that the paper was too dense with information, so much so as to make it difficult to digest.  However, overall this seems like an interesting paper that is relevant to the community and will hopefully foster some good discussion about the shortcomings and future directions of saliency methods.
The authors addressed the issues raised by the reviewers, so I suggest the acceptance of this paper.
This paper conducted a number of empirical studies to find whether units in object classification CNN can be used as object detectors. The claimed conclusion is that there are no units that are sufficient powerful to be considered as object detectors. Three reviewers have split reviews. While reviewer #1 is positive about this work, the review is quite brief. In contrast, Reviewer #2 and #3 both rate weak reject, with similar major concerns. That is, the conclusion seems non conclusive and not surprising as well. What would be the contribution of this type of conclusion to the ICLR community? In particular, Reviewer #2 provided detailed and well elaborated comments. The authors made efforts to response to all reviewers’ comments. However, the major concerns remain, and the rating were not changed. The ACs concur the major concerns and agree that the paper can not be accepted at its current state.
The paper is proposed a rejection based on majority reviews.
This paper tackles a very important problem of detecting depression on Twitter. As the reviewers expressed in their reviews. this paper will be of interest for the community of researchers applying ML models to mental health domain. It is unfortunate that the authors did not respond to the reviewers  concerns and questions. I strongly encourage the authors to improve the paper based on the authors comments and questions and resubmit to a future venue.
The paper proposes a method to learn and explain simultaneously. The explanations are generated as part of the learning and in some sense come for free. It also goes the other way in that the explanations also help performance in simpler settings. Reviewers found the paper easy to follow and the idea has some value, however, the related work is sparse and consequently comparison to existing state of the art explanation methods is also sparse. These are nontrivial concerns which should have been addressed in the main article not hidden away in the supplement.
This paper proposes to use RL to learn how to prune attention heads in BERT to achieve regularization for tasks with small dataset size. Specifically, the authors use DQN to learn a policy to prune heads layer by layer.    This paper receives 4 reject recommendations with an average score of 4.5. Though the idea in this paper is interesting, the experiments in the current draft are far from convincing. The reviewers have raised many concerns regarding the paper. (i) Experiments are weak. Only 4 GLUE tasks are considered; it is necessary to also test the proposed methods on other GLUE tasks. (ii) The comparison with other regularization techniques is lacking. (iii) The training overhead of this method needs more careful discussion, as it involves repeated finetuning after each layer is pruned, therefore could be very time consuming. (iv) More comprehensive related work discussion is needed.  The rebuttal unfortunately did not address the reviewers  main concerns. Therefore, the AC regrets that the paper cannot be recommended for acceptance at this time. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
This was a borderline paper, as reviewers generally agreed that the method was a new method that was appropriately explained and motivated and had reasonable experimental results. The main drawbacks were that the significance of the method was unclear. In particular, the method might be too inflexible due to being based on a hard coded rule, and it is not clear why this is the right approach relative to e.g. GANs with a modified training objective). Reviewers also had difficulty assessing the significance of the results on biological datasets. While such results certainly add to the paper, the paper would be stronger if the argument for significance could be assessed from more standard datasets.  A note on the review process: the reviewers initially scored the paper 6/6/6, but the review text for some of the reviews was more negative than a typical 6 score. To confirm this, I asked if any reviewers wanted to push for acceptance. None of the reviewers did (generally due to feeling the significance of the results was limited) and two of the reviewers decided to lower their scores to account for this.
Meta score: 5  The paper explores an interesting idea, addressing a known bias in truncated BPTT by sampling across different truncated history lengths.  Limited theoretical analysis is presented along with PTB language modelling experimentation.   The experimental part could be stronger (e.g. trying to improve over the baseline) and perhaps more than just PTB.  Pros:    interesting idea Cons:    limited analysis    limited experimentation  
This paper proposes a hypothesis about the kinds of visual information for which popular neural networks are most selective.  It then proposes a series of empirical experiments on synthetically modified training sets to test this and related hypotheses.  The main conclusions of the paper are contained in the title, and the presentation was consistently rated as very clear.  As such, it is both interesting to a relatively wide audience and accessible.  Although the paper is comparatively limited in theoretical or algorithmic contribution, the empirical results and experimental design are of sufficient quality to inform design choices of future neural networks, and to better understand the reasons for their current behavior.  The reviewers were unanimous in their appreciation of the contributions, and all recommended that the paper be accepted.  
This paper analyzes BatchNorm through a series of experiments and shows that BatchNorm improves training and generalization by preventing excessive growth of the activations of the previous to the last layer. The paper received mixed reviews. Two reviewers find that the paper brings clear and important new contributions to the understanding of BatchNorm, and appreciate the experimental evaluation, though some suggestions for improving the experiments were also provided. The other reviewer appreciated the contribution of regularizing against explosive growth in the previous to the last layer, but did not find the results to be very convincing as they can limit the learning rate. The authors responded that, contrary to the reviewer s claim, the learning rate is not limited. Overall, while there are some aspects that need improvement, and the authors should address those in the final version, this is a solid paper that brings an interesting contribution to ICLR.
This is a nice paper using contrastive learning for code representation. The idea is to generate variations on unlabeled source code (using domain knowledge) by creating equivalent version of code. Improvements over baselines on two multiple tasks are shown. While some of the reviewers liked the (and R4 should have responded), none of the reviewers found the paper exciting enough to strongly recommend its acceptance. 
The paper builds upon previous work on neural temporal point processes. It mainly proposes the replacement of the LSTMs with Transformers as transformers are widely considered as a more powerful sequence modeling tool and the three advantages listed in the end of section 1 in this paper.  However, on the modeling side, it is not straight forward how to apply the transformer (the attention architecture) on to the continuous time sequence problem using NDTT. I think because I read a revised version of this paper, it is actually more understandable to me as compared to the reviewers who read the first draft of the paper. I think A NDTT is a natural and principled way of introducing the attention mechanism into the continuous time neural symbolic framework, although I agree it unfortunately does not leading to a significant improvement in every experimental setting.  To summarize the discussions, I think the authors did a good job in resolving the concerns on the related work and made the paper easier to understand. I appreciate these efforts from the authors even though I also understand there are concerns left still from the reviewers.  In summary, I am leaning to accept this paper because I think it is an interesting contribution. However, I do agree with the reviewers that the writing of the paper needs to be improved and the experiment section is relatively weak in this paper.
The paper presents a comprehensive analysis of lottery tickets hypothesis (LTH) on automatic speech recognition. The authors verified the existence of highly sparse “winning tickets” in ASR task, and analyzed its robustness to noise, transferable to other datasets, and supported with structured sparsity.  As agreed with the reviewers, the paper is well written, the justification is thorough, and the finding that LTH does perform well on ASR is interesting. Though the novelty is marginal as it s a direct application of the LTH, this is the first investigation of LTH and brings new insights to the community.   The decision is mainly based on the thorough justification of the LTH to ASR and new insights it brings to the community.
This is a clear reject. None of the reviewers supports publication of this work. The concerns of the reviewers are largely valid.
This paper proposes and evaluates using graph convolutional networks for semi supervised learning of probability distributions (histograms). The paper was reviewed by three experts, all of whom gave a Weak Reject rating. The reviewers acknowledged the strengths of the paper, but also had several important concerns including quality of writing and significance of the contribution, in addition to several more specific technical questions. The authors submitted a response that addressed these concerns to some extent. However, in post rebuttal discussions, the reviewers chose not to change their ratings, feeling that quality of writing still needed to be improved and that overall a significant revision and another round of peer review would be needed. In light of these reviews, we are not able to recommend accepting the paper, but hope the authors will find the suggestions of the reviewers helpful in preparing a revision for another venue. 
The range of the initial reviews was fairly high with overall scores ranging from 4 to 7.  The authors provided a good response that answered most of the reviewers  comments and questions. One of the reviewers even increased their score following the authors  response.   The focus of some of our discussions and what ultimately led to my suggestion was the related work of MIR [1]. The methodological differences between (the three versions of) MIR in [1] and GMED [this paper] appear less significant than what the current submission suggests. While there is some disagreement between the authors and Reviewer1 about the exact differences, I find that the current manuscript does not acknowledge the close relationship between these two contributions. Further, from the experimental standpoint and without further justifications the gains from GMED+MIR could be attributed to using more replay (from combining GMED and MIR).   In their response, the authors disputed the view of Reviewer1. I believe the source of the confusion between the author and the reviewer might be captured in this sentence from the author response to Reviewer1: The approach does not learn a generator that can “generate examples that are more forgettable for the classifier”; instead, feeding more forgettable examples in GEN MIR aims at reducing the forgetting of the generator.  Looking at Equations 2, 3 and Algorithm 1 from [1], in GEN MIR while two different procedures are used to obtain forgettable examples for the generator (B_G in Alg. 1) and the classifier (B_C in Alg. 1), the generator is used in both cases. In other words, the generator is used to generate examples for both itself and for the classifier. So, I think it s fair to say that the generator does indeed generate examples that are more forgettable for the classifier (Eq. 2).    I strongly encourage the authors to prepare another version of their work where the differences between MIR [1] and their contribution are clearly highlighted and the results show the advantages of GMED (including memory editing in data space).    [1] Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Laurent Charlin, and Tinne Tuytelaars. Online continual learning with maximally interfered retrieval. In NeurIPS 2019. https://arxiv.org/abs/1908.04742
This paper explores an approach to testing the information bottleneck hypothesis of deep learning, specifically the idea that layers in a deep model successively discard information about the input which is irrelevant to the task being performed by the model, in full scale ResNet models that are too large to admit the more standard binning based estimators used in other work. Instead, to lower bound I(x;h), the authors propose using the log likelihood of a generative model (PixelCNN++). They also attempt visualize what sort of information is lost and what is retained by examining PixelCNN++ reconstructions from the hidden representation at different positions in a ResNet trained to perform image classification on the CINIC 10 task. To lower bound I(y;h), they perform classification. In the experiments, the evolution of the bounds on I(x;h) and I(y;h) are tracked as a function of training epoch, and visualizations (reconstructions of the input) are shown to support the argument that color invariance and diversity of samples increases during the compression phase of training. These tests are done on models trained to perform either image classification or autoencoding. This paper enjoyed a good discussion between the reviewers and the authors. The reviewers liked the quantitative analysis of "usable information" using PixelCNN++, though R2 wanted additional experiments to better quantify the limitations of the PixelCNN++ model to provide the reader with a better understanding of plots in Fig. 3, as well as more points sampled during training. Both R2 and R3 had reservations about the qualitative analysis based on the visualizations, which constitute the bulk of the paper. Unfortunately, the PixelCNN++ training is computationally intensive enough that these requests could not be fulfilled during the ICLR discussion phase. While the AC recommends that this submission be rejected from ICLR, this is a promising line of research. The authors should address the constructive suggestions of R2 and R3 and submit this work elsewhere.
This paper proposes a technique of communicating predicted local states between agents in multi agent reinforcement learning to deal with the delay in communication.  While the paper addresses an important practical problem, the reviewers have concerns about the insufficiency of novelty and experimental validation.
Meta Review of Robust Robotic Control from Pixels using Contrastive Recurrent State Space Models  This work investigates a recurrent latent space planning model for robotic control from pixels, but unlike some previous work such as Dreamer and RNN+VAE based World Models, they use a simpler contrastive loss for next observation prediction. They presented results on the DM control suite (from pixels) with distracting background settings. All reviewers (including myself) agree that this is a well written paper, with clear explanation of their approach. The main weaknesses of the approach are on the experimental side (see review responses to author’s rebuttal by skrV and cjX3). Another recommendation from me is to strengthen the related work section to clearly position the work to previous work   there is clear novelty in this work, but this should be done to avoid confusion. The positive sign is that in the discussion phase, even the very critical cjX3, had increased their score and acknowledged the novelty from previous related work. In the current state, I cannot recommend acceptance, but I’m confident that with more compelling experiments recommended by the reviewers, and better positioning of the paper to previous work, I believe that this paper will surely be accepted at a future ML conference or journal. I’m looking forward to seeing a revised version of this paper for publication in the future.
This work compares and contrasts the learning rate dynamics of GD and SGD and shows that under practical learning rate settings, SGD is biased to approach the minimum along the direction of steepest descent, leading to better performance. Reviewers agree that the theoretical results are significant. The authors satisfactorily responded to reviewers’ questions and improved the paper’s clarity during the discussion phase.
The paper is rejected based on unanimous reviews.
This paper clearly has great ideas and reviewers appreciated that. However, the lack of experiments that can be validated by the community (only 1 experiment on the proprietary dataset) is an issue. We don t know if the reported accuracy is a respectable one (in the public domain).   Having a proprietary dataset is a plus, but no public benchmark raises concerns about reproducibility.  We recommend the authors to add some tasks and benchmarks for the community to check for themselves that the numbers reported are non trivial.  
The reviewers thought the paper provides an interesting line of research.
The paper tries to analyze the relationship between regularization, adversarial robustness, and transferability.  Pros:   An interesting problem was tackled.  Cons:   The main claim (Prop.3.1) is almost trivial.  Prop. 3.1 shows that "relative" transferability is smaller for stronger regulariation, which is just a slight generalization of the triangler inequality ||YT   YS|| <  ||YT   Y|| + ||YS   Y|| for any Y in Fig.2.     Experiments show negative correaltion between the relative transferability and accuracy, which is trivial.  Large regularization degrades the accuracy which increases the "relative transferability".  "Absolute" transferability in Appendix doesn t show clear negative correlations.   Salmann et al. claimed that adversarially "trained" models transfer better, and did not claim that there are positive correlations between the transferability and robustness for general classifiers without adversarial training.  So the finding in this paper is not surprising nor against Salmann et al.  To prove that adversarial robustness is just a subproduct of regularization, the authors should show that the "absolute" transferability by adversarially trained classifier can be achieved by other regularization.  Defining relative transferability is fine if it is just a decomposition to conduct an analysis of the absolute transferability.  But no conclusion on the performance should be made from its analysis, because a trivial correlation will appear, i.e., (A B) and B should be negatively correlated unless A strongly correlates to B.  Also, this is highly misleading so that some reviewers seem to have misunderstood that the authors would have claimed that negative correlations between regularization and absolute transferability were observed in the original submission.  Overall, the paper requires major revision.
An empirical study of weight sharing for neural networks is interesting, but all of the reviewers found the experiments insufficient without enough baseline comparisons.
As discussed by several reviewers the paper is an application of classical ML approaches for a very relevant problem of calibration of radio interferometers. The application is interesting but lacks novelty in terms of ML methodology and the experiments do not provide a meaningful comparison between the state of the art and the proposed approach or justification for the choice of predictors and their parameters. This paper in clearly not a good fit for a general ML conference.
The paper investigates graph convolutional filters, and proposes an adaptation of the Fisher score to assess the quality of a convolutional filter. Formally, the defined Graph Filter Discriminant Score assesses how the filter improves the Fisher score attached to a pair of classes (considering the nodes in each class, and their embedding through the filter and the graph structure, as propositional samples), taking into account the class imbalance.  An analysis is conducted on synthetic graphs to assess how the hyper parameters (order, normalization strategy) of the filter rule the GFD score depending on the graph and class features. As could have been expected there no single killer filter.  A finite set of filters, called base filters, being defined by varying the above hyper parameters, the search space is that of a linear combination of the base filters in each layer. Three losses are considered: with and without graph filter discriminant score, and alternatively optimizing the cross entropy loss and the GFD; this last option is the best one in the experiments.  As noted by the reviewers and other public comments, the idea of incorporating LDA ideas into GNN is nice and elegant. The reservations of the reviewers are mostly related to the experimental validation: of course getting the best score on each dataset is not expected; but the set of considered problems is too limited and their diversity is limited too (as demonstrated by the very nice Fig. 5).  The area chair thus encourages the authors to pursue this very promising line of research and hopes to see a revised version backed up with more experimental evidence. 
This paper proposes an adaptive tree search algorithm for NMT models with non decomposable metrics and shows its efficacy against strong baselines. This is an interesting contribution towards overcoming the performance caps introduced by the uncontrolled for biases of beam search, and it speaks to a growing community interested in decoding beyond greedy surprisal minimisation.  The initial reviews brought to light a number of concerns that in my view are well addressed in the rebuttal and in the current version of the manuscript. One of the key issues was a confusion caused by the use of the term  non autoregressive  to refer to the intractability of the metric / objective function of certain models. This use clashed with the more standard use in MT, which refers to a tractable factorisation of a joint probability by means of strong conditional independence assumptions.   The confusion is easy to address and in no way compromises the thoroughness of the empirical section. The authors are aware of the confusion and how to resolve it, and they have acknowledged the need to pick a less ambiguous term.   I d like to recommend this for acceptance, but I urge that the authors do not ignore the confusion caused by  auto/non auto regressive  and the missing literature that came up in the discussion with reviewer i2pz (I understand the discussion happened too late for the manuscript to be updated, but I trust this can be done for the final version).
Three reviewers recommended accept or weak accept. There are some concerns on the novelty of this approach since this work mainly validates that lottery tickets can be found on GANs, which seems like applying an existing idea to a new problem. Nevertheless, there are two reasons that such an effort is interesting: first, the implementation of this idea may be harder than it seems; second, it was not known a priori whether lottery tickets would exist for GANs due to the significantly different optimization problem (game instead of minimization). I think this work will be of interest to the community, and recommend acceptance. 
The paper proposed a two stage method to select instances from a set, involving candidate selection (learning a function  to determine a Bernoulli probability for each input) and AutoRegressive subset selection (learning a function  to generate probabilities for sampling elements from a reduced set); both stages use the Concrete distribution to ensure differentiability. The experiments show the performance of the proposed method on several use cases, including reconstruction of an image from a subset of its pixels, selecting sparse features for a classification task, and dataset distillation for few shot classification. I read the paper and I agree with the reviewers that in its current format the paper is hard to follow. I strongly encourage the authors to add more discussion and intuition on the proposed method and extend the experiments with more baseline comparison and ablation studies in the revised version. 
The paper proposes to use conditional GANs to generate protein sequence with respect to GO molecular functions. The idea is nice. But the reviewers find that there are many things that are not clear. For example, some sentences, phrase, the model and experiments pointed by the reviewers that should be  rigorous described. The technical contribution is also limited. The author are encouraged to revise the paper according to the comments.
This work tries to extend mixup to graph structured data, where graphs can differ in the number of nodes, and the space is not Euclidean.  This is achieved by G Mixup, which interpolates the generator (graphon) of different classes of graphs through the latent Euclidean space.  Experimental results show some promise.  Several concerns have been raised by the reviewers, and although the rebuttal helped, some concerns remain.  For example, how to confirm that the graphon can be accurate estimated.  Several weakness in experiment is also raised, and a revision is needed before the paper can be published.
This paper turned out to be quite difficult to call.  My take on the pros/cons is:  1. The research topic, how and why humans can massively outperform DQN, is unanimously viewed as highly interesting by all participants.  2. The authors present an original human subject study, aiming to reveal whether human outperformance is due to human knowledge priors.  The study is well conceived and well executed.  I consider the study to be a contribution by itself.  3. The study provides prima facie evidence that human priors play a role in human performance, by changing the visual display so that the priors cannot be used.  4. However, the study is not definitive, as astutely argued by AnonReviewer2.  Experiments using RL agents (with presumably no human priors) yield behavior that is similar to human behavior.  So it is possible that some factor other than human prior may account for the behavior seen in the human experiments.  5. It would indeed be better, as argued by AnonReviewer2, to use some information theoretic measure to distinguish the normal game from the modified games.  6. The paper has been substantially improved and cleaned up from the original version.  7. AnonReviewer1 provided some thoughtful detailed discussion of how the authors may be overstating the conclusions that one can draw from the paper.  Bottom line: Given the procs and cons of the paper, the committee recommends this for workshop. 
This article studies the effects of BN on robustness. The article presents a series of experiments on various datasets with noise, PGD adversarial attacks, and various corruption benchmarks, that show a drop in robustness when using BN. It is suggested that a main cause of vulnerability is the tiling angle of the decision boundary, which is illustrated in a toy example.  The reviewers found the contribution interesting and that the effect will impact many DNNs. However, they the did not find the arguments for the tiling explanation convincing enough, and suggested more theory and experimental illustration of this explanation would be important. In the rebuttal the authors maintain that the main contribution is to link BN and adversarial vulnerability and consider their explanation reasonable. In the initial discussion the reviewers also mentioned that the experiments were not convincing enough and that the phenomenon could be an effect of gradient masking, and that more experiments with other attack strategies would be important to clarify this. In response, the revision included various experiments, including some with various initial learning schedules. The revision clarified some of these issues. However, the reviewers still found that the reason behind the effect requires more explanations. In summary, this article makes an important observation that is already generating a vivid discussion and will likely have an impact, but the reviewers were not convinced by the explanations provided for these observations.  
### Summary  This work investigates effective sparsity: an assessment of the sparsity of pruned networks that accounts for the fact that unpruned neurons can still be completely disconnected through pruning.  Hence, the effective sparsity of a network may be much lower than otherwise reported.  ### Discussion  #### Strengths    The paper studies an important metric that deserves additional attention in the community, where a change in metric may guide either the theory or practice of pruning.    The paper evaluates direct versus empirical sparsity for a healthy number of pruning techniques.   #### Weaknesess     While this paper appears to be the most direct study of effective sparsity at the moment, it is not the first. Appendix M of [1] defines effective sparsity and shows that direct and effective sparsity are similar for contemporary pruning at initialization techniques. However, that work does not evaluate random pruning. This work here will need to revise its novelty claims to account for these results as its characterization that [1] only considers direct sparsity is incorrect.     "Computing effective sparsity:" the procedure in question is similar to that of Appendix M in [1], thus its relationship should be detailed.     With the primary observations residing in the regime of extremely sparse neural networks, the elements of the response (and in the last paragraph of the paper) that claim this regime is productive for ensembling should make a more prominent appearance in the introduction of the work.  [1] Pruning Neural Networks at Initialization: Why Are We Missing the Mark?  Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, Michael Carbin. ICLR, 21   ### Recommendation  I recommend Reject. Generally, the paper is well written and the empirical characterization of direct versus effective sparsity is thorough (except for ResNet 50 results). However, the results and the language around these results need significant rescoping to account for novelty and the relation of the work to an area in which it is anticipated that these results will change theory, practice, or thinking (e.g., ensembling).  Though I cannot speak for future reviewers, IMO, an extension of the results here to ResNet 50+ImageNet should suffice to establish the extent of the discrepancy between direct and effective sparsity. However, to satisfy additional demands from reviewers for more practical relevance, I suggest an evaluation that demonstrates a consequential difference in behavior for a task that maps more closely to the anticipated area of impact (e.g., ensembling)
This paper approaches personalized federated learning from the perspective of meta learning and use the mutual information framework developed in a recent work to regularize local model training. All the reviewers consider the writing very poor and hard to understand, and the contributions not sufficient for acceptance.
While the reviewers appreciated the theoretical analysis performed in this work, some concerns were raised during discussion, such as how relevant the obtained results are wrt current contrastive learning practices, and whether the comparison against a simple auto encoder (basically PCA) is fair or insightful. The authors  response did not satisfactorily address these concerns. Overall, the current work appears to be preliminary, and important questions were left out: (a) how realistic the assumptions are (linear, spike covariance, Bernoulli random augmentation)? (b) performing better than PCA in a specifically designed setting may not be as impressive as it appears; what can we say about the optimality of CL against any algorithm? (c). validation on existing benchmark and CL algorithms would be welcome. The authors are encouraged to revise the current draft by incorporating the reviewers  comments and submit again. In its current form, we believe this work is not ready yet.
This paper investigates a semi supervised label refining approach to searching for similar voices for voice dubbing.  The apporach is based on generating refined labels using a clustering algorithm on the initial labels.  Therefore, better voice characteristics can be extracted and used to select a new voice in the target language that closely matches the voice characteristics of the source language.  Experiments are carried out on MassEffect as the main dataset and Skyrim as the second dataset and results show that the proposed approach slightly outperforms state of the art.  While the topic under investigation is interesting and has its value to the applications such as voice casting,  there are strong concerns raised by the reviewers.  Reviewers find the paper difficult to follow.  Some important pieces of information are either missing or only vaguely explained (e.g. non expert initial labels,  clear interpretation of p vectors, etc.), which greatly hinders a deep understanding of the work.  Some technical details such as network architecture and its training should be elaborated.  This paper needs some good improvement in order to get accepted.  No rebuttal is provided by the authors so all these concerns still stand.
This paper is concerned with the problem of distribution shift, and develops techniques for detecting when the risk of a deployed model performs significantly worse on a testing distribution than on the training distribution.  The reviews for this paper were extremely consistent: after the discussion period, all five reviewers unanimously recommended acceptance, and several praised the authors for significantly improving their paper in response to reviewer criticism. Outstanding issues are (i) motivating the importance of the setting, and (ii) comparing with prior work. None of the reviewers seemed to think that these issues should be barriers to acceptance, but please seriously consider them, and all reviewer concerns.
This paper received scores of 6,6,6 after the reviewers succeeded in making two authors raise their scores from 5 to 6. However, even after this, none of the reviewers actively argued for the paper. The only positive point raised in the private discussion was that the results are strong. (However, there is still the question of how much of this was due to the different design space used.) Negative points raised in the private discussion included that    despite the authors clarification on the differences to Zen NAS, the difference is perceived not to be large.   there is no theoretical foundation behind the selection of a critical parameter, and this directly limits the applicability of ZenDet in searching for FPN connections.    as a paper focused on detection NAS, a limitation to only search for the backbone may not be novel enough for publication at ICLR.   Overall, I agree with this criticism and weakly recommend rejection.
This work proposes a "teaching to teach" (T2T) method to incorporate structured prior knowledge into the teaching model for machine learning tasks. This is an interesting and timely topic. Unfortunately, among many other issues, this paper is fairly poorly writing and can benefit from a significant rewriting. The authors did not provide a rebuttal and hence we recommend a rejection.    
The authors propose an adversarial training method to increase network robustness to parameter variations. The proposed approach performs adversarial attacks on network parameters during training. They demonstrate that their method flattens the loss landscape of the network. Experiments were performed on F MNIST, ECG data, and speech command detection datasets using a conventional CNN and a recurrent spiking neural networks (SNNs).  The manuscript is well written and the method is interesting.  One reviewer was somewhat concerned about the novelty of the work, but acknowledged that the application to recurrent SNNs was new. The main initial criticism was the question of scalability of the method, as it was tested only on networks with a relatively small number of parameters.  In the revision, the authors addressed these issues. Their method was compared to related approaches, and experiments on CIFAR 10 with a ResNet32 were performed. The reviewers acknowledged these larger size experiments, but were not fully convinced as much larger models are typically used today.  Nevertheless, the reviewers acknowledged the improvements and ratings were increased, so all are voting for acceptance.
This paper aims at learning disentangled representation at different level without the supervision signal of group information. To achieve this, the proposed UG VAE model uses both global variable $\beta$ to represent common information shared across all data, as well as a mixture of Gaussian prior for the local latent variable $p(z)   \int p(z|d)p(d)d$ where $d$ represents the assignment of the group for a particular datapoint. Experiments considered evaluation on unsupervised global factor learning, domain alignment and a downstream application task on batch classification.  Reviewers agreed that the proposed model seems interesting and novel, however some reviewers raised clarity concerns on how to interpret the learned representation by UG VAE. Revision has addressed this clarity issue to some extent, although some doubts from some reviewers still exists. Also reviewers raised concerns on less competitive experimental results, and the authors have updated the manuscript with improved results.   To me the main issues of the experimental section are (1) no quantitative result is provided regarding global factor learning and domain alignment, and (2) there is no other benchmark being studied in the experimental section. In my view, at least some other VAE representation learning baselines can be included in the batch classification section in order to demonstrate the real benefit of learning global factor based representations in downstream tasks. 
This paper proposes an RL based structure search method for causal discovery. The reviewers and AC think that the idea of applying reinforcement learning to causal structure discovery is novel and intriguing. While there were initially some concerns regarding presentation of the results, these have been taken care of during the discussion period. The reviewers agree that this is a very good submission, which merits acceptance to ICLR 2020.
Protein molecule structure analysis is an important problem in biology that has recently become of increasing interest in the ML field. The paper proposes a new architecture using a new type of convolution and pooling both on Euclidean as well as intrinsic representations of the proteins, and applies it to several standard tasks in the field.   Overall the reviews were strong, with the reviewers commending the authors for an important result on the intersection of biology and ML. The reviewers raised the points of:   weak baselines (The authors responded with adding suggested comparison, which were not completely satisfactory)   focus mostly on recent protein literature   the reliance of the method on the 3D structure. The AC however does not find this as a weakness, as there are multiple problems that rely on 3D structure, which with recent methods can be predicted computationally rather than experimentally.   We believe this to be an important paper and thus our recommendation is Accept. As the AC happens to have expertise in both 3D geometric ML and structural biology, he/she would strongly encourage the authors to better do their homework as there have been multiple recent works on convolutional operators on point clouds, as well as intrinsic representation based ML methods for proteins. 
This paper studies the *last iterate* convergence of the projected Heavy ball method (and an adaptive variant) for convex problems, and propose a specific coefficient schedule. All reviewers thought that looking at the last iterate convergence of the HB method was interesting and that the proofs, while simple, were interestingly novel. Several concerns were raised by the quality of the writing. Several were addressed in a revision and the rebuttal. While R1 did not update their score, the AC thinks that the rebuttal has addressed appropriately their initial concerns. The AC recommends the paper for acceptance, *but* it is important that the authors make an appropriate careful pass over their paper for the camera ready version.  ### comments about the write up    The paper still contains many typos (e.g. missing $1/t$ term in the average after equation (2); many misspelled words, etc.), please carefully proofread your paper again.   The AC agrees with R1 that the quality of presentation still needs improvement. $\beta_{1t}$ is still used in the introduction without being defined   please define it properly first e.g.    The word "optimal" and "optimality" is usually misused in the manuscript. To refer to the convergence rate of an optimization algorithm, the standard terminology is to talk about the "suboptimality" or the "error" (e.g. see the terminology used by the cited [Harvey et al. 2019, Jain et al. 2019] papers). For example, one would say that the error or suboptimality of SGD has a $O(1/\sqrt{t})$ convergence rate. Saying "optimality of" or "optimal individual convergence rate" is quite confusing, and should be corrected. The adjective "optimal" (when talking about a convergence rate) should be restricted to when a matching lower bound exists.   Finally, the text introducing the experimental section should be fixed to clarify the actual results and motivation. Specifically, the "validate the correctness of our convergence analysis" only applies in the convex setting. I recommend that a high level description of the convex experiment and the main message of the results is moved from the appendix to the main paper there (there is space). And then, the deep learning experiments can be introduced as just investigating the practical performance of the suggested coefficient schedule for HB.
The paper proposes three discretization schemes for two first order optimization flows, and proves the "convergence" to the minimizers of the problem that the optimization flows approach. The methods are tested on the DNN training problem and show comparable performance.  Pros: 1. The problem being studied, the discretization of optimization flows, is of interest to the community. 2. "Convergence" guarantee is provided.  Cons: 1. The theoretical analysis is somewhat preliminary, as the authors have admitted. There is a prescribed \epsilon in the approximation error (23) that prevents the right hand side of (23) from approaching zero. The parameter \eta, depending on the chosen accuracy \epsilon, should be provided so that a user can implement the discretization schemes if s/he is interested. Moreover, by specifying \eta, it may be possible to compare the numbers of iterations to approach an \epsilon solution between the proposed discretization schemes and other optimization methods for solving the original optimization problem. By doing this, the motivation issue from Reviewer #1 (and the AC) can be resolved. Purely discretizing an optimization flow is of less interest to the machine learning community. 2. Although the comparison on academic problem is obviously advantageous, the comparison on DNN training is only comparable or marginally better.   The author responses resolved part of the challenges from the reviewers, but the key issues remained (as communicated in confidential comments). Since the final average score is below threshold, the AC decided to reject the paper.
This is a very clearly written, well composed paper that does a good job of placing the proposed contribution in the scope of hyperparameter optimization techniques.  This paper certainly appears to have been improved over the version submitted to the previous ICLR.  In particular, the writing is much clearer and easy to follow and the methodology and experiments have been improved.  The ideas are well motivated and it s exciting to see that sampling from a k DPP can give better low discrepancy sequences than e.g. Sobol.  However,  the reviewers still seem to have two major concerns, namely novelty of the approach (DPPs have been used for Bayesian optimization before) and the empirical evaluation.    Empirical evaluation:  As Reviewer1 notes, there are much more recent approaches for Bayesian optimization that have improved significantly over the TPE method, also for conditional parameters.  There are also more recent approaches proposing variants of random search such as hyperband.    Novelty:  There is some work on using determinantal point processes for Bayesian optimization and related work in optimal experimental design.  Optimal design has a significant amount of literature dedicated to designing a set of experiments according to the determinant of their covariance matrix   i.e. D Optimal Design.  This work may add some interesting contributions to that literature, including fast sampling from k DPPs, etc.  It would be useful, however, to add some discussion of that literature in the paper.  Jegelka and Sra s tutorial at NeurIPS on negative dependence had a nice overview of some of this literature.  Unfortunately, two of the three reviewers thought the paper was just below the borderline and none of the reviewers were willing to champion it.  There are very promising and interesting ideas in the paper, however, that have a lot of potential.  In the opinion of the AC, one of the most powerful aspects of DPPs over e.g. low discrepancy sequences, random search, etc.  is the ability to learn a distance over a space under which samples will be diverse.  This can make a search *much* more efficient since (as the authors note when discussing random search vs. grid search) the DPP can sample more densely in areas and dimensions that have higher sensitivity.  It would be exciting to learn kernels specifically for hyperparameter optimization problems (e.g. a kernel specifically for learning rates that can capture e.g. logarithmic scaling).  Taking the objective into account through the quality score, as proposed for future work, also seems very sensible and could significantly improve results as well.  
The paper presents an approach to learning interpretable word embeddings. The reviewers put this in the lower half of the submissions. One reason seems to be the size of the training corpora used in the experiments, as well as the limited number of experiments; another that the claim of interpretability seems over stated. There s also a lack of comparison to related work. I also think it would be interesting to move beyond the standard benchmarks   and either use word embeddings downstream or learn word embeddings for multiple languages [you should do this, regardless] and use Procrustes analysis or the like to learn a mapping: A good embedding algorithm should induce more linearly alignable embedding spaces.   NB: While the authors cite other work by these authors, [0] seems relevant, too. Other related work: [1 4].   [0] https://www.aclweb.org/anthology/Q15 1016.pdf [1] https://www.aclweb.org/anthology/Q16 1020.pdf [2] https://www.aclweb.org/anthology/W19 4329.pdf [3] https://www.aclweb.org/anthology/D17 1198/ [4] https://www.aclweb.org/anthology/D15 1183.pdf
The reviewers raise a number of concerns including limited methodological novelty, limited experimental evaluation (comparisons), and poor readability. Although the authors did address some of the concerns, the paper as is needs a lot of polishing and rewriting. Hence, I cannot recommend this work for presentation at ICLR.
The paper addresses regression in a weakly supervised setting where the correct labels are only available for examples whose prediction lie above some threshold. The paper proposes a method using a gradient that is unbiased and consistent.  Pros:   Problem setting is new and this paper is one of the first works exploring it.   The procedure comes with some unbiasedness and consistency guarantees.    Experimental results on a wide variety of datasets and domains.  Cons:   Novelty and technical contribution is limited.   Motivation of the problem setting was found to be unclear.   Some gaps in the experimental section (i.e. needing the use of synthetic data or synthetic modifications of the real data).  Overall, the reviewers felt that as presented, the paper did not convincingly motivate the proposed upper one sided regression problem as important or relevant in practice, which was a key reason for rejection. The paper may contain some nice ideas and I recommend taking the reviewer feedback to improve the presentation. 
This paper proposes a graph soft counter (GSC) model which is very simple and lightweight  compared to the conventional graph neural network for solving QA tasks that benefit from knowledge graphs. Compared to the conventional KG GNN combination, the proposed method is much simpler but produces better results for QA tasks. The paper originally dealt only with multiple choice QA tasks, but during the rebuttal process, the authors added more complex QA tasks which the reviewers appreciated. Additionally, there was (and still remains) some concern over the exact reasons and mechanisms behind this "too good to be true" result, and the authors addressed this with additional ablation studies, to be included in the appendix. With the publicly released code, others will be able to try GSC and its too good to be true performance and figure out how it actually works.
In general, the reviewers and myself find this work of some interest, though potentially somewhat incremental in terms of technical novelty compared to the work for Makhzani et al. Another bothersome aspect is the question of evaluation and understanding how well the model actually does; I am not convinced that the interpolation experiments are actually giving us a lot of insights. One interesting ablation experiment (suggested privately by one of the reviewers) would be to try AAE with Wasserstein and without a learned generator   this would disambiguate which aspects of the proposed method bring most of the benefit. As it stands, the submission is just shy of the acceptance bar, but due to its interesting results in the natural language domain, I do recommend it being presented at the workshop track.
The paper proposes an alternative to BPTT for training recurrent neural networks based on an explicit state variable, which is trained to improve both the prediction accuracy and the prediction of the next state. One of the benefits of the methods is that it can be used for online training, where BPTT cannot be used in its exact form. Theoretical analysis is developed to show that the algorithm converges to a fixed point. Overall, the reviewers appreciate the clarity of the paper, and find the theory and the experimental evaluation to be reasonably well balanced. After a round of discussion, the authors improved the paper according to the reviews. The final assessments are overall positive, and I’m therefore recommending accepting this paper.
This paper presents mixout, a regularization method that stochastically mixes parameters of a pretrained language model and a target language model. Experiments on GLUE show that the proposed technique improves the stability and accuracy of finetuning a pretrained BERT on several downstream tasks.  The paper is well written and the proposed idea is applicable in many settings. The authors have addressed reviewers concerns  during the rebuttal period and all reviewers are now in agreement that this paper should be accepted.  I think this paper would be a good addition to ICLR and recommend to accept it. 
This paper carefully shows how all the stochastic elements in neural network training could be removed (by using full batch, and a dataset with fixed augmentation) and still maintain good performance, by adjusting hyper parameters and adding explicit regularization.  All reviewers were eventually positive and recommended acceptance, except one reviewer, who was initially not aware of the recent theoretical interest in this question, and was therefore less surprised.  There are three remaining issues with the current version:  1) Operations in cuDNN are, by default, non deterministic, but can be made deterministic. Though I believe this will not affect the final results, without it, the title and conclusions of this paper are technically unjustified. The authors have agreed to add this to the camera ready version of the paper. 2) Deterministic training was only shown on CIFAR10. I understand ImageNet would be too heavy for this task, but there are many other small and medium sized datasets, and I think showing that this on several such datasets would strengthen the message of this paper, and convince more readers.  3) The question of how to achieve good performance with deterministic training is still mostly unknown, as it seems to require significant hyperparameter search (with unknown sensitivity), and no conclusion was reached regarding the how to properly adjust them. However, I agree that a good answer to this question might not exist.  Lastly, I recommend the authors to mention in the main paper the new baseline experiment, where the explicit regularization is added to SGD. I saw it in the appendix, but I didn t see it mentioned in the main paper (maybe I missed it). I think without it, many readers will not be fully convinced (as several reviewers requested it).
After carefully reading the reviews and the rebuttal, unfortunately I feel this work is not yet ready for acceptance.   I want to acknowledge the effort put in the rebuttal for this work and I think all the changes greatly increased the value of the work. However, I feel that the work could greatly benefit from running on a different domain where the gain is more considerable. My worry is that the complexity of the method compared to the relatively small improvement (at least as perceived from the current results) will reduce considerably the attention the work will receive from the community (unfairly so).  Or some of the analysis and ablation done (e.g. the flow visualization) which are now in the appendix could be brought in the main manuscript to be able to drive the message home. An understanding of the impact on the accuracy of the flow model on the overall performance (which as pointed out by reviewer aHc1 is a really hard task in a more natural context). Or maybe a 3D visually complex environment is exactly where this method will shine as flows are more complex and hence more informative.   Overall I think this is solid work, but I feel it does not manage to convince the reader of the significance of the proposed approach. And hence, if published in this form, I feel it will do a disservice to the work, as it will not receive the attention it merits from the community.
The paper presents a technique for feature map compression at inference time. As noted by reviewers, the main concern is that the method is applied to one NN architecture (SqueezeNet), which severely limits its impact and applicability to better performing state of the art models.
The paper identifies the phenomenon of oversquashing in GNNs and relate it to bottleneck. While this phenomenon has been previously observed, the analysis is new and insightful. The authors conclude that standard message passing may be inefficient in cases where the graphs exhibit an exponentially growing number of neighbors and long range dependencies, and propose a solution in the form of a fully adjacent layer. While the paper does not offer much methodologically, it is the observation of bottleneck that is of importance.   We therefore believe that the criticism raised by some reviewers of the observation not being novel and the solution "too simple" rather unsubstantiated. The authors have well addressed these issues in their rebuttal. The AC recommends accepting the paper. 
This paper proposes to use CNN S prior to deal with the tasks in audio processing. The motivation is weak and the presentation is not clear.  The technical contribution is trivial.
The reviewers and ACs acknowledge that the paper has a solid theoretical contribution  because it give a convergence (to critical points) of the ADAM and RMSprop algorithms, and also shows that NAG can be tuned to match or outperform SGD in test errors. However, reviewers and the AC also note that potential improvements for the paper a) the exposition/notations can be improved; b) better comparison to the prior work could be made; c) the theoretical and empirical parts of the paper are somewhat disconnected; d) the proof has an error (that is fixed by the authors with additional assumptions.) Therefore, the paper is not quite ready for publications right now but the AC encourages the authors to submit revisions to other top ML venues.  
The authors propose a scheme to learn a mapping between the discrete space of network architectures into a continuous embedding, and from the continuous embedding back into the space of network architectures. During the training phase, the models regress the number of parameters, and expected accuracy given the continuous embedding. Once trained, the model can be used for compression by first embedding the network structure and then performing gradient descent to maximize accuracy by minimizing the number of parameters. The optimized representation can then be mapped back into the discrete architecture space. Overall, the main idea of this work is very interesting, and the experiments show that the method has some promise. However, as was noted by the reviewers, the paper could be significantly strengthened by performing additional experiments and analyses. As such, the AC agrees with the reviewers that the paper in its present form is not suitable for acceptance, but the authors are encouraged to revise and resubmit this work to a future venue. 
The paper presents an approach for learning inter object relations. The relationships are represented in terms of scene graphs, and are processed with graph convolutional networks. All of the reviewers find the problem interesting and meaningful, which is the main strength of the paper. The approach assumes good object segmentations and state changes are provided to the agent, which the AC believes is a very dangerous assumption. Object segmentation or the change detection from raw data is still an active research area and lack of end to end training capability of the proposed approach is a limiting aspect. Still, the authors were able to convince all the reviewers that the problem formulation and the proposed approach is valid. Experimental results were provided to support the argument.  Respecting the opinions from the reviewers, the AC recommends accepting the paper, although the AC himself/herself is very reluctant to give an accept rating.
The paper proposes an RL based approach for decision based attack. All the reviewers like the paper after the rebuttal phase, and we would like to encourage the authors to incorporate the new experiments in the camera ready version. Furthermore, some recent decision based attacks should also be included in the comparisons, such as  Li et al., QEBA: Query Efficient Boundary Based Blackbox Attack. (CVPR 2020)   Cheng et al., Sign OPT: A Query Efficient Hard label Adversarial Attack. (ICLR 2020)
This paper presents a new approach for posing control as inference that leverages Sequential Monte Carlo and Bayesian smoothing. There is significant interest from the reviewers into this method, and also an active discussion about this paper, particularly with respect to the optimism bias issue. The paper is borderline and the authors are encouraged to address the desired clarifications and changes from the reviewers.  
Most reviewers agree that the paper makes valuable contribution in analyzing single timescale actor critic algorithms. There were some doubts on the theoretical advantage over two timescale algorithms and the realizability assumptions, but the authors made satisfactory clarifications.   Therefore, acceptance is recommended, though I strongly suggest the authors to explicitly state key assumptions required to ensure global optimality in the abstract and introduction to avoid confusion.  
This paper sits at the borderline: the reviewers agree it is a well written and interesting paper, but have concerns about efficiency as well as a comparison with the neural process (the authors did include a revision with this comparison, though the numbers they report are worse than in the original neural processes paper on the same experiment). Ultimately, this paper probably requires another round of reviews before it is ready for publication.
The paper develops a new method for pruning generators of GANs. It has received a mixed set of reviews. Basically, the reviewers agree that the problem is interesting and appreciate that the authors have tried some baseline approaches and verified/demonstrated that they do not work.   Where the reviewers diverge is on whether the authors have been successful with the new method. In the opinion of the first reviewer, there is little value in achieving low levels (e.g. 50%) of fine grained sparsity, while the authors have not managed to achieve good performance with filter level sparsity (as evidenced by Figure 7, Table 3 as well as figures in the appendices). The authors admit that the sparsity levels achieved with their approach cannot be turned into speed improvement without future work.  Furthermore, as pointed out by the first reviewer, the comparison with prior art, in particular with LIT method, which has been reported to successfully compress the same GAN, is missing and the results of LIT have been misrepresented. While the authors argue that their pruning is an "orthogonal technique", and can be applied on top of LIT, this is not verified in any way. In practice, combination of different compression techniques is known to be non trivial, since they aim to explain the same types of redundancies.  Overall, while this paper comes close, the problems highlighted by the first reviewer have not been resolved convincingly enough for acceptance.
This paper introduced a Neural Rendering Model, whose inference calculation corresponded to those in a CNN. It derived losses for both supervised and unsupervised learning settings. Furthermore, the paper introduced Max Min network derived from the proposed loss, and showed strong performance on semi supervised learning tasks.  All reviewers agreed this paper introduces a highly interesting research direction and could be very useful for probabilistic inference. However, all reviewers found this paper hard to follow. It was written in an overly condensed way and tried to explain several concepts within the page limit such as NRM, rendering path, max min network. In the end, it was not able to explain key concepts sufficiently.  I suggest the authors take a major revision on the paper writing and give a better explanation about main components of the proposed method. The reviewer also suggested splitting the paper into two conference submissions in order to explain the main ideas sufficiently under a conference page limit.
This paper studies the notion of certified cost sensitive robustness against adversarial examples, by building from the recent [Wong & Koller 18]. Its main contribution is to adapt the robust classification objective to a  cost sensitive  objective, that weights labelling errors according to their potential damage.  This paper received mixed reviews, with a clear champion and two skeptical reviewers. On the one hand, they all highlighted the clarity of the presentation and the relevance of the topic as strengths; on the other hand, they noted the relatively little novelty of the paper relative [W & K 18]. Reviewers also acknowledged the diligence of authors during the response phase. The AC mostly agrees with these assessments, and taking them all into consideration, he/she concludes that the potential practical benefits of cost sensitive certified robustness outweight the limited scientific novelty. Therefore, he recommends acceptance as a poster. 
This paper proposes a method to transfer a pretrained language model in one language (English) to a new language. The method first learns word embeddings for the new language while keeping the the body of the English model fixed, and further refines it in a fine tuning procedure as a bilingual model. Experiments on XNLI and dependency parsing demonstrate the benefit of the proposed approach.  R3 pointed out that the paper is missing an important baseline, which is a bilingual BERT model. The authors acknowledged this in their rebuttal and ran a preliminary experiment to obtain a first set of results. However, since the main claim of the paper depends on this new experiment, which was not finished by the end of the rebuttal period, it is difficult to accept the paper in its current state. In an internal discussion, R1 also agreed that this baseline is critical to support the paper.  As a result, I recommend to reject this paper for ICLR. I encourage the authors to update their paper with the new experiment for submission to future conferences (given consistent results).
The paper proposes a new way to learn a disentangled representation by embedding the latent representation z into an explicit learnt orthogonal basis M. While the paper proposes an interesting new approach to disentangling, the reviewers agreed that it would benefit from further work in order to be accepted. In particular, after an extensive discussion it was still not clear whether the assumptions of Theorem 1 applied to VAEs, and whether Theorem 1 was necessary at all. In terms of experimental results, the discussions revealed that the method used supervision during training, while the baselines in the paper are all unsupervised. The authors are encouraged to add supervised baselines in the next iteration of the manuscript. For these reasons I recommend rejection.
All reviewers appreciated the main idea in the paper for solving the nonconvex nonconcave minimax problems, which is deemed an extremely hard open problem. However, as R1 also pointed out, neither the theoretical nor the experimental results seem particularly strong, given that many variations of GDA and theoretical understanding of different notions of optimality have been recently developed. The paper fails to draw proper comparisons to these existing work.  Unfortunately, the paper is slightly below borderline and cannot be accepted this time.  
I enjoyed reading the paper myself and I appreciate the unifying framework connecting RAML and SPG. While I do not put a lot of weight on the experiments, I agree with the reviewers that the experimental results are not very strong, and I am not convinced that the theoretical contribution meets the bar at ICLR.  In the interpolation algorithm, there seems to be an additional annealing parameter and two tuning parameters. It is important to describe how the parameters are tuned. Given the additional hyper parameters, one may consider giving all of the algorithms the same budget of hyper parameter tuning. I also agree with reviewers that the policy gradient baseline seems to underperform typical results. One possible way to strengthen the experiments is to try to replicate the results of SPG or RAML and discuss the behavior of each algorithm as a function of hyper parameters. 
The authors focus on large scale out of distribution (OOD) detection for which they propose three benchmarks with multiclass and multi label  high resolution images. In these settings, they find that a simple extension, using maximum logits (MaxLogit), of a common baseline  maximum softmax probability (MSP), is surprisingly competitive to prior methods.  Five knowledgeable reviewers found the idea of having these novel benchmarks potentially interesting, but highlighted some issues that needs to be taken into account before the paper can be publishable.   First, reviewers highlighted how the presentation can be better organized (more structure on and stronger overall motivation for the three different contributions) as to present the three ideas in a more cohesive way and more formal in introducing methods (e.g. the MaxLogic and MSP) as to clearly highlight the technical contributions and the differences with other models.   Second, more baselines need to be introduced and therefore experiments extended. For example, the comparison with another detector, LogSumExpLogit, a relaxation of MaxLogit already used for (small scale) OOD in the context of generative models. Authors provided in the rebuttal some preliminary experimental results but promised more (for a camera ready) that could not be evaluated by the reviewers.  Third, the scope of the proposed benchmarks raised some concerns by some reviewers. If not in the motivation behind the task of treating whole objects as anomalies, additional care shall be put into the provided annotations. As one reviewer highlighted a certain percentage of images are mis annotated. While this percentage is somehow low (3.9%) and should not change the empirical conclusions drawn in the paper, it highlights that the core contributions of the paper might have been rushed.
There is clear consensus on this submission. Reviewers cite a lack of comparison with recent state of the art methods and experiments on more realistic datasets. Though the reviewers find aspects of the approach interesting, the decision is to reject. 
All reviewers are very consistent with their evaluation of the paper. The discussion phase did not change their initial evaluation. Therefore, I also recommend to reject the paper.
This paper presents a method whereby a model learns to describe 3D shapes as programs which generate said shapes. Beyond introducing some new techniques in neural program synthesis through the use of loops, this method also produces disentangled representations of the shapes by deconstructing them into the program that produced them, thereby introducing an interesting and useful level of abstraction that could be exploited by models, agents, and other learning algorithms.  Despite some slightly aggressive anonymous comments by a third party, the reviewers agree that this paper is solid and publishable, and I have no qualms in recommending it from inclusion in the proceedings.
This paper investigates the stability plasticity dilemma in the class incremental learning context. It investigates which model components are eligible to be “reused, added, fixed, or updated” to achieve a good balance. Initially the paper had one supporter (xDnv) who liked the motivation and extensiveness of the ablation. NFc9 and UadE also echoed some similar points about motivation and liked that the work was easy to follow. Reviewers expressed concerns such as incrementality w.r.t. spaceNet, too many hyper parameters, unclear performance benefit, lack of comparison to SOTA, fixed sequence of classes specified by the authors, not clear how much forgetting is happening in each method (echoed by multiple reviewers), and limited datasets used for evaluations. The authors responded to the critical reviews and provided a revised version of the paper with additional comparisons to rehearsal free methods and with more datasets (MNIST/FashionMNIST).   Following the author response, NFc9 stated that they thought the paper was in better shape with the revisions and upgraded their score claiming it was “closer to acceptance”. Yet, they still had concerns with the practical implications of having too many hyper parameters. UadE engaged further with the authors but claimed that they avoided the reviewer’s direct concerns. UadE maintained their concerns with the manual ordering of classes and older baselines. I agree that there are several rehearsal or pseudo rehearsal methods to which they could have compared. Reviewer o6Js did not engage further. Overall this is a borderline paper. I appreciate the authors engaging in the discussion period, though my assessment is that the key issues still remain. This paper could use further development so my recommendation is reject.
This paper studies the relationship between attention networks such as Transformers and convolutional networks. The paper shows that a special case of attention can be cast as convolution. However this link depends on using relative positional embeddings and generalization to other encodings are not given in the paper. The reviewers found the results correct, but we caution that the writing should better reflect the caveats of the approach.
There is consensus that the submission is not yet ready for publication. The reviews contain multiple comments and suggestions and I hope they can be useful for the authors.
This paper proposes a method for finding neural architecture which, through the use of selective branching, can avoid processing portions of the network on a per data point basis.   While the reviewers felt that the idea proposed was technically interesting and well presented, they had substantial concerns about the evaluation that persisted post rebuttal, and lead to a consensus rejection recommendation.
This paper presents a novel VAE based model for multivariate spatial point process which can realize efficient inference by amortization and handle missing points via smooth intensity estimation. Authors also provide interesting theoretical analysis to connect their method to a popular VAE based collaborative filtering method. Overall, all reviewers appreciate the methodological and theoretical contributions of the paper. During the reviewer discussion, one reviewer decided to update to the score to Weak Acceptance. There are still some concerns regarding experimental validation, I think the paper provides enough theoretical contribution to the community and would like to recommend acceptance. 
Strength * The paper is relatively clearly written. * A new method is proposed.  Weakness * The evaluation is weak.  The experimental setup is not clear enough.  More quantitative evaluation is necessary. There are strong and new baselines that need to be compared with. * Relation with existing work needs to be more clearly described. * The novelty of the work is limited.  It is a combination of existing methods. * Justification of the proposed method needs to be provided. * The writing of the paper can be improved.
This paper proposes a method to learn representations to infer simple local models that can be used for policy improvement. All the reviewers agree that the paper has interesting ideas, but they found the main contribution to be a bit weak and the experiments to be insufficient.  Post rebuttal, the reviewers discussed extensively with each other and agreed that, given more work is done on a clear presentation and improving the experiments, this paper can be accepted. In its current form however, the paper is not ready to be accepted. I have recommended to reject this paper, but I will encourage the authors to resubmit after improving the work. 
The article studies a student teacher setting with over realised student ReLU networks, with results on the types of solutions and dynamics. The reviewers found the line of work interesting, but they also raised concerns about the novelty of the presented results, the description of previous works, settings and claims, and experiments. The revision clarified some of the definitions, the nature of the observations, experiments, and related works, including a change of the title. However, the reviewers still were not convinced, in particular with the interpretation of the results, and keep their original ratings. With many points that were raised in the original reviews, the article would benefit from a more thorough revision. 
The authors introduce a GNN based method for classifying irregular multivariate timeseries. They represent the dependencies among sensors using a graph structure and deploy message passing to   model the effect of a sensor on another)s . The approach jointly learns embeddings and the dependency graph.   The manuscript gathered a clear accept (8) and two marginal below the threshold scores (5).  I want to accept this work and I explain why.   The reviews and the ongoing discussion during the rebuttal showed that the work is interesting  with its main strength being the novel exploration of GNNs application on irregularly samples multivariate  time series.  There were many concerns raised by a reviewer regarding important  theoretical and methodological issues in the paper.  During the rebuttal phase, the authors clarified and resolved the majority of the concerns and there was an ongoing discussion among the two sides, authors and reviewer (which I have to admit was a pleasure to watch researchers communicating).  The authors took into account the feedback and revised the manuscript accordingly. Having read the edits myself, I believe the submission is substantially improved and addressed the concerns sufficiently.   I expect that this work will stimulate further research in the community and I would like to accept this.
This paper is novel, but relatively incremental and relatively niche; the reviewers (despite discussion) are still unsure why this approach is needed.
While the reviewers appreciated the clarity of the work, there is a concern about the meaning of the proposed result and method. It is known that adding knowledge about an additional variable, in this case the environment, leads to a lower variance estimate. What is not known is the practical impact of using this new baseline or perhaps some other intuition stemming from that use of the baseline (for instance the origin of the variance). However, the results shown are not that compelling, a point which was raised by the reviewers, making the work below the bar for publication.
The paper proposed a new synthetically generated video dataset (CATER) for benchmarking temporal reasoning. The dataset is based on the CLEVR dataset and provides videos make up of primitive actions ("rotate", "pick place", "slide", "contain") that can be combined to form for complex actions. The paper also benchmarks a variety of methods on three proposed tasks (atomic action classification, composite action classification, and  snitch  localization) and demonstrates that while it is possible to get high performance on atomic action classification, the other two task are still challenging and requires temporal modeling.    Overall, all reviewers found the paper to be well written and easy to follow, with care given to the dataset construction, as well as the task definitions and experiment setup and analysis.  The paper received strong scores from all reviewers (3 accepts).  Based on the reviewer comments, the authors further improved the paper by adding additional relevant datasets for comparison and providing missing details pointed out by the reviewers.  After the rebuttal, the reviewers remained positive.
The authors propose a new way of addressing the ML as a service problem through using garbled circuits. As the reviewers point out the novelty is limited and comparison to existing work is not complete. The authors have also not responded to the reviews.
This paper proposes a method to train generative adversarial nets for text generation. The paper proposes to address the challenge of discrete sequences using straight through and gradient centering. The reviewers found that the results on COCO Image Captions and EMNLP 2017 News were interesting. However, this paper is borderline because it does not sufficiently motivate one of its key contributions: the gradient centering. The paper establishes that it provides an improvement in ablation, but more in depth analysis would significantly improve the paper. I strongly encourage the authors to resubmit the paper once this has been addressed.
The current version of the paper receives a unanimous rejection from reviewers, as the final proposal. 
The paper introduces an approach for semi supervised learning based on local label propagation. While reviewers appreciate learning a consistent embedding space for prediction and label propagation, a few pointed out that this paper does not make it clear how different it is from preview work (Wu et al, Iscen et al., Zhuang et al.), in addition to complexity calculation, or pseudo label accuracy. These are important points that weren’t included to the degree that reviewers/readers can understand, and reviewers seem to not change their minds after authors wrote back. This suggests the paper can use additional cycles of polishing/editing to make these points clear. We highly recommend authors to carefully reflect on reviewers both pros and cons of the paper to improve the paper for your future submission.  
This paper uses deep kernel learning to develop a compelling framework for hyperparameter optimization in a few shot setting, with empirically strong results. Please carefully account for all reviewer comments in the final version. 
This paper presents a graph neural network (GNN) architecture that adopts locally permutation equivariant constructs, which has better scalability compared to globally permutation equivariant GNNs, and the paper claims this change also does not lose expressivity of the network.  All reviewers unanimously recommended rejection, and the main issues are the clarity and writing, to the point where it becomes hard for a reader to follow the precise implementation of the proposed approach and how that compares to prior work.  Therefore in its current form this paper is not yet ready for publication at ICLR.  When the authors work toward the next revision I’d suggest clarifying a little more about the precise algorithmic implementation of the proposed ideas, with a bit of additional intuition from a higher level, rather than staying at the current level of technicality.
This paper proposes a method to learn data augmentation policies using an adversarial loss. In contrast to AutoAugment where an augmentation policy generator is trained by RL (computationally expensive), the authors propose to train a policy generator and the target classifier simultaneously. This is done in an adversarial fashion by computing augmentation policies which increase the loss of the classifier. The authors show that this approach leads to roughly an order of magnitude improvement in computational cost over AutoAugment, while improving the test performance. The reviewers agree that the presentation is clear and that the proposed method is sound, and that there is a significant practical benefit of using such a technique. As most of the concerns were addressed in the discussion phase, I will recommend acceptance of this paper. We ask the authors to update the manuscript to address the remaining (minor) concerns. 
The paper shows that hat if the goal is to find invariant mechanisms in the data, these can be identified by finding explanations (e.g. model parameters) that are hard to vary across examples. To find those "explanations" it then proposes to combine gradients across examples in a "logical AND" fashion, i.e., pooling gradients sing a geometric mean with a logical AND masking. All reviewers agree that the direction is very interesting. While indeed mentioning sum and products of experts might be good, the overall idea is still very much interesting, also to the ICRL community, since it paves the way to apply this to larger set of machine learning methods, as actually shown in the experimental evaluation. Still, the authors should make the link to causality more obvious from the very beginning. This should also involve clarifying that "explanations" here do not refer to "explanations" as used in Explainable AI. Overall, this is an interesting and simple (in a positive sense) contribution to the question of getting at least "more" causal models. 
This paper provides generalization bounds for meta learning based on a notion of task relatedness. The result is natural and interesting intuitively, when tasks are similar, then meta learning algorithms should be able to utilize all data points across all tasks. The theoretical contribution is novel, and the results also provide more practical insight into the performances of some models.
The paper studies a hierarchical or multi level version of local SGD, extending earlier work by (Wang & Joshi, 2018), (Lin et al, 2018) and  (Jiang et al. 2019) among others. It gives novel convergence rates in relevant settings, such as by allowing different workers to take different numbers of local steps within a given time interval. The current analysis is restricted to the IID data case, but still insightful, and might serve as a useful building block for follow up research in the future. Smaller concerns remained that the presented multi level results cannot exactly recover local SGD as a special case. Nevertheless the consensus remained that the overall contributions and relevance of the paper remain above the bar. In the discussion phase, several concerns were clarified and additional deep learning experiments have been added to the paper, which is appreciated.
This is a borderline paper. Initially, it received  weaker reviews (than the current ones) and after rebuttal, one reviewer slightly increased the review rating.  Among 4 reviews, there is one clearly positive one.  Among negative reviews, there are repeated concerns about evaluation. While recognizing the difficulty of finding (large scale) datasets for sequential  regression tasks,  reviewers suggest running the methods on benchmark datasets. One reviewer also points out the need to compare to efficient ensemble methods for uncertainty quantification.  The AC also read  the manuscript. The AC appreciates the strong performance of the proposed method. However, given the limited number of datasets this method is evaluated, and the lacking of analysis of understanding why the method is successful or could fail, the AC recommends Reject.  The author(s) could improve the manuscript by following the suggestion of the reviewers, as well as a more detailed analysis of how robust the proposed method is: there are many parameters tuned in the empirical results and it is not clear why sensitive those parameters are. 
The paper proposes a new method for training generative models by minimizing general f divergences. The main technical idea is to optimize f divergence between joint distributions which is rightly observed to be the upper bound of the f divergence between the marginal distributions and address the disjoint support problem by convolving the data with a noise distribution.  The basic ideas in this work are not completely novel but are put together in a new way.   However, the key weakness of this work, as all the reviewer noticed, is that the empirical results are too week to support the usefulness of the proposed approach. The only quantitive results are in table 2, which is only a simple Gaussian example. It essential to have more substantial empirical results for supporting the new algorithm.  
This paper proposes a method for detecting two types of distributional shifts: covariate shifts in the input space  (due to input corruption) and semantic shifts (due to test data falling outside the support set of ID classes). The idea is based on the decomposition of KL divergence between softmax prediction and a uniform vector. Furthermore, the authors propose Geometric ODIN to improve OOD detection and calibration, outperforming strong baseline on CIFAR10, CIFAR 100, and SVHN datasets. The paper aims to solve a very important problem in ML and the approach is thought provoking.  However, there were several questions and confusions raised by the reviewers, such as the applicability of the model, justification of use of feature norm, discussion on sensitivity vs robustness, framing of the novelty, clear definition of OOD detection, definition of parameters, etc. (please see reviews for a comprehensive list). I invite authors to incorporate these points in the next version of the paper which will significantly improve the paper.
The authors propose UPSIDE, a method for improving state coverage in unsupervised skill learning. "Direct and diffuse" policies concatenate phases of directed behaviour followed by dithering to improve neighbourhood coverage in the region of state space visited, trained with a discriminability objective to ensure diversity in region coverage, and composing these skills into a tree structure to further improve coverage.  Reviewers generally found the submission highly novel and well written, were encouraged by the experimental results, and were unanimous on the importance of the problem addressed. Reviewers d1m1 and wdBX praised the visualizations as adding considerable insight. d1m1 raised the concern that a random walk in the second stage would not work in some environments (which the authors pointed out can be overcome by the tree method) and suggested DADS was a better baseline than DIAYN (also mentioned by zwRd, with which the authors agreed, and proceeded to implement). zwRd expressed concerns about gaps in the finetuning results (addressed by the authors in follow up) as well as details of the DIAYN setup (also addressed to zwRd s satisfaction). H3Jm s minor concerns around the text and equations, figures and DIAYN experimental results were fully addressed in discussion. wdBX was the reviewer with the most substantive concerns around the apparent complexity, ad hoc design, scalability and lack of theoretical grounding. After discussion (which resulted in the authors drafting a proof in the comments, an ICLR first for this AC) wdBX believed that the paper would be improved by additions proposed, but remained "borderline" in their evaluation.  The work presents a compelling, if somewhat complicated (and therefore perhaps unsatisfying, to some), method improving state coverage in unsupervised skill learning. Research in contemporary machine learning often takes the form of one piece of work pushing the boundaries significantly with a complex algorithm with further work dissecting and refining the ideas therein, reducing them to their essence. Viewed through this lens, the paper in question presents several ideas that all form parts of the overall method which appear likely to serve to inspire and motivate future work. With all reviewers leaning accept either strongly or weakly, there is little doubt in the AC s mind that this paper should be accepted, and widely read by researchers interested in unsupervised skill acquisition.
This paper considers the recent line of work on algorithms with predictions.  They give new results on the online facility location problem. Overall, the reviewers felt the topic was of interest to the community.  There were some concerns about the error metric used and the overall framework. However, the majority of reviewers still felt the paper was interesting and I think the paper can be accepted.
The paper presents a model for question answering where blocks of text can be skipped and only relevant blocks are further processed for extracting the answer span.    The reviewers mostly praised the general idea.    R3 raised concerns on generalizability of the presented approach.   R4 raised several issues regarding presentation and clarity.   R2 and R4 have concerns regarding execution and find some of the results unconvincing.   While I don t necessarily share R2s concern on small improvements (improvements are still statistically significance), and despite the approach being very interesting, there are several issues that reviewers pointed out and wasn t resolved after discussions.  
The paper investigates the interesting question whether increasing the width or the number of parameters is  responsible for improved test accuracy. The paper is very well written and the question is novel and innovative. From a methodological point of view, the experiments are well conducted, too. The theoretical part of the paper is somewhat detached from the experimental part and constitutes more of a heuristic conjecture. In addition, more experiments on a variety of other data sets would have been great. Ideally, the theoretical section would thus be replaced by such additional experiments, but this is of course not  an option for a conference reviewing system. Given the innovative question and well conducted experiments I think that the pros outweighs the cons, and for this reason I recommend to accept the paper. Reviewer concerns have been well addressed by the authors in their rebuttal and updated version of the paper.
This paper received borderline reviews. Initially, all reviewers raised a number of concerns (clarity, small improvements, etc). Even after some back and forth discussion, concerns remain, and it s clear that while the idea has potential, another round of reviewing is needed before a decision can be reached. This would be a major revision in a journal. Unfortunately, that is not possible in a conference setting and we must recommend rejection. We recommend the authors to use the feedback to make the manuscript stronger and submit to a future venue. 
The paper considers an important topic of the warmup in deep learning, and investigates the problem of the adaptive learning rate. While the paper is somewhat borderline, the reviewers agree that it might be useful to present it to the  ICLR community.
This work extends previous work (Castellini et al) with parameter sharing and  low rank approximations, for pairwise communication between agents.  However the work as presented here is still considered too incremental, in particular when compared to Castellini et al. The advances such as parameter sharing and low rank approximation are good but not enough of a contribution. Authors  efforts to address this concern did not change reviewers  judgment. Therefore, we recommend rejection.
The paper introduces the problem of continual knowledge (language) learning. The authors point out the interesting duality between continual learning and knowledge learning where: in knowledge learning one must avoid forgetting time invariant knowledge (avoid forgetting in CL), be able to acquire new knowledge (learn new tasks in CL), and replace outdated knowledge (a form of forgetting and re learning or adaptation). In their paper, the authors develop an initial benchmark for the task along with a set of baselines and provide empirical studies.  The initial reviews were quite mixed. The reviewers seem to agree this work studies an interesting and fairly novel direction for continual learning of language. However, the reviewers did not agree on whether this initial stab at the problem was "enough." In particular, reviewer U9Hk argues that the formulation is "oversimplified" and the current experiments are limiting.  After the discussion, the reviewers remained split with one high score (8), two borderline accepts (3), and one reject. So three reviewers believe that this manuscript is already a good contribution. The fourth reviewer disagrees, but the authors provided clear and convincing responses to many of their comments (and point to results already available in the appendix).  Overall, this is a clear and reasonable first step considering this paper proposes a new CL problem. The reviewers and I believe that this is interesting and rigorous enough to be impactful and to warrant follow up works. As a result, I m happy to recommend acceptance. I imagine that if the community demonstrates interest in this line of work, there will be work both on methodologies to improve the proposed baselines, but also work proposing extensions to the problem in line with some of the comments of reviewer U9Hk.  In preparing their camera ready version I strongly encourage the authors to take into account the suggestions of the reviewers and your replies. In particular, your discussion regarding encoder decoder and decoder only LMs and the associated results would be good to discuss in the main text (even if the full results are in the appendix).
This paper proposes a new regularizer, based on entropy maximization of samples near the decision boundary, to improve the calibration of neural networks while maintaining their accuracy.   The method seems simple, sufficiently novel, and has promising results. However, based on the review process (described below), I feel the paper needs to significantly improve its evaluation and presentation before it can be accepted.    The review process summary:  * Two reviews were eventually weakly positive about the paper: without major concerns, but not enthusiastic.  * One review (L8Yz) was not sufficiently informative.   * One review (ESue) raised many points. I disagreed with most of these points, following the authors  discussion. However, a few points seemed valid, such as the not so impressive performance for OOD detection, which the authors did not address.  * I therefore asked for an additional review (iva2). The review concluded the paper is interesting and potentially useful, but requires another round of revision before it can be accepted, mainly because of its clarity and missing comparisons. I agree with these conclusions.
The paper received mixed reviews of WR (R1), WR (R2) and WA (R3). AC has carefully read all the reviews/rebuttal/comments and examined the paper. AC agrees with R1 and R2 s concerns, specifically around overclaiming around reasoning. Also AC was unnerved, as was R2 and R3, by the notion of continuing to train on the test set (and found the rebuttal unconvincing on this point). Overall, the AC feels this paper cannot be accepted. The authors should remove the unsupported/overly bold claims in their paper and incorporate the constructive suggestions from the reviewers in a revised version of the paper.
All reviewers agree that the proposed idea looks interesting but the paper is seriously lacking in the definition of its scope: there is no quantitative result, experiments are quite limited, and there is not enough discussion of the limitations. With more work this could become a very interesting paper.
This paper presents a novel neural network architecture to predict interacting residues among two interacting proteins, and evaluates its performance on benchmarks. While the reviews were initially mixed, there has been a productive discussion and significant improvements in the paper during the discussion, including in particular much needed clarifications about the proposed methods, and more experimental results with an ablation study to better assess the benefits of various design choices. While no reviewer is willing to champion this paper as a "strong accept", due to the relatively modest novelty compared to existing methods, there is a consensus towards "weak accept" given the final quality of the work presented and potential usefulness of the method for the problem tackled.
This paper proposes the use of holographic reduced representations in language modeling, which allows for a cleaner decomposition of various linguistic traits in the representation. Results show improvements over baseline language models, and analysis shows that the representations are indeed decomposing as expected.  The main reviewer concern was the lack of strength of the baseline, although the authors stress that they were using the default baseline from TensorFlow, which seems like it will be reasonable to me. Another concern is that there is other work on using HRR to disentangle syntax and semantics in representations for language (e.g. "Distributed Tree Kernels" ICML 2012, but also others), that has not been considered.   Based on this, this seems like a very borderline case. Given that no reviewer is pushing strongly for the paper I m leaning towards not recommending acceptance, but I could very easily see the paper being accepted as well.
This work proposes a method for generating candidate molecules using a novel fragment based MCMC proposal mechanism.  Pros: * Well written paper * Novel idea for an important application * Very good empirical performance compared to the state of the art in multi objective molecule generation * Careful ablation studies  Cons: * Some details were missing (runtime, experimental details) and have been added to the revised version.  The authors engaged in an extensive discussion with the reviewers and modified their paper to address the reviewer concerns.  After discussions three reviewers recommend accepting the work and consider it a novel and useful contribution to the field.  One reviewer (Reviewer 3) is not satisfied by the authors comments and has concerns about the work regarding: asymptotic correctness of the sampling; fairness of the experimental comparison; and computational complexity.  The authors provide detailed justifications for their choices.  After looking at the discussion there are two factors: 1. technical arguments regarding the correctness of the sampling method; the authors justify the correctness by known results for adaptive MCMC methods, and the argument is sound, and the area chair fully accepts the authors  arguments as correct and applicable. 2. extend of the experimental evaluation and suitable baseline methods; this is partially subjective.  The authors provide extensive experiments in their work and justify exclusion of certain methods in that they do not easily apply to the multi objective setting.  In addition, Reviewer 3 demands a comparison of generated molecules per time, which is plausibly useful, however, none of the prior works have used such a metric in a consistent manner and it is clearly challenging to do so fairly as such metric would depend on specifics of the implementation and computer.  The authors have updated their paper and added runtime information for their method.  The area chair fully accepts the authors  arguments and justification for the current experimental scope.  In summary the area chair considers the remaining concerns by Reviewer 3 as invalid; in particular, the authors have made extensive efforts to engage and educate the reviewer.
(1) the authors emphasize the theoretical contribution and claims the bound are tighter. However, they did not directly compare with any certified robust methods, or previous bounds to support the argument.  HM, not sure, need to check this  (2) The empirical results look suboptimal. The authors did not convince me why they sampled 1000 images for test for a small CIFAR 10 dataset. The proposed method is 10% less robust comparing to Madry s in table 1.   Seems ok, understand authors response  1) The theoretical analysis are not terribly new, which is just a straightforward application of first order Taylor expansion. This idea could be traced back to the very first paper on adversarial examples FGSM (Goodfellow et al 2014). True  2) The novelty of the paper is to replace exact gradient (w.r.t input) by their finite difference and use it as a regularization. However, there is a misalignment between the theory and the proposed algorithm. The theory only encourages input gradient regularization, regardless to how it is evaluated, and previous studies have shown that this is not a very effective way to improve robustness. According to the experiments, the main empirical improvement comes from the finite difference implementation but the benefit of finite difference is not justified/discussed by the theory. Therefore, the empirical improvement are not supported by the theory. Authors have briefly respond to this issue in the discussion but I believe a more rigorous analysis is needed.   This seems okay based on author response  3) Moreover, the empirical performance does not achieve state of the art result. Indeed, there is a non negligible  gap (12%) between the obtained performance and some well known baseline. Thus the empirical contribution is also limited.  Yea, for some cases
The paper attacks the important problem of learning time series models with missing data and proposes two learning frameworks, RISE and DISE, for this problem. The reviewers had several concerns about the paper and experimental setup and agree that this paper is not yet ready for publication. Please pay careful attention to the reviewer comments and particularly address the comments related to experimental design, clarity, and references to prior work while editing the paper.
The authors consider view consistency when learning graph neural networks. However, as mentioned by the reviewers, the novelty of the proposed method is limited and the rationality of the implementation is not convincing. More deep discussions about related papers and analytic experiments are required to support this work. Additionally, I have concerns about the scalability of the method   whether it can deal with more than two views and how it will perform are not studied in this work. I tend to reject it based on its current status. 
This paper received borderline negative scores. The reviewers all agree that the proposed approach is interesting. However, there are also common concerns around the clarity of the paper, as well as lacking sufficient empirical evaluation. One reviewer also argues that technical contribution is relatively limited. The author responses were taken into account but it didn t manage to swing the reviews. Therefore, I recommend reject and wish the authors can incorporate the feedback in the revision. 
This paper proposed a self supervised learning view for sequential recommendation with different forms of model augmentation: neuron masking, layer dropping, and encoder complementing. Overall the scores are negative. The reviewers raised concerns mostly around the motivation of the proposed approach (which wasn t fully supported by the experimental results) as well as the limited contribution (especially considering some of the augmentation strategies have been proposed in the past). One reviewer also brought out an interesting connection between model augmentation and model regularization. The authors responded that they will keep improving the paper and hopefully we will see a much improved version in the next submission.
The paper contributes to the understanding of out of distribution detection by showing that binary discrimination between in  and out distribution examples  is equivalent to several different formulations of the out of distribution detection problem . The paper shows this in an asymptotic setup based on studying likelihood ratios for distinguishing in distribution examples from out of distribution examples. The paper also provides numerical results showing that a simple baseline based on binary classification works well.  The paper got very mixed responses ranging from strong accept to reject:   Reviewer YhZ7 (recommending 3: reject) raises several important concerns, specifically that the paper doesn t explain the significance of its contributions adequately, that experiments are not thorough enough (for example that only one out of distribution dataset is considered), and that to train a binary classifier one needs to have sufficiently many out of distribution examples.  The authors argued in response that the purpose of the paper is to provide an understanding of existing methods that are often empirically driven, made revisions to the exposition, and point out that they actually evaluate on six/seven out of distribution test sets.  After discussion, the reviewer is still concerned that the paper states  We show that when training the binary discriminator between in  and out distribution together with a standard classifier on the in distribution in a shared fashion, the binary discriminator reaches state of the art OOD detection performance  as a contribution and that this claim is not supported by the results in the paper. The authors say they are happy to drop this particular statement and emphasize that their contribution is that that a binary classifier can be a useful tool for OOD detection. The reviewer is not satisfied by this response, as the reviewer feels that this makes the contribution much less impactful.     Reviewer iH61 (recommending 6: marginally above, initially reject) pointed out that the significance of one of the contributions is limited, since the claims resemble the ones by Thulasidasan et al. [2021] and Mohseni et al. [2020], and initially recommends to reject. The authors respond that those two papers only aim at good performance, but do not unify existing approaches, as the paper under review does. The reviewer slightly raised their score, but again points out that the previous works already show that a binary discriminator performs well.     Reviewer Lwwq (recommending 10: strong accept) appreciates the unification of different methods and votes for strong acceptance. The reviewer also points out that he/she is not an expert in the field, and thus this reviewer s rating should be taken with care.     Reviewer YRfA (recommending 8: accept) points out that the authors make notable progress towards a better understanding of OOD methods, but is concerned about what problem the authors are trying to solve and its significance, and states that he/she cannot judge the importance of the paper.    Reviewer vYWv (recommending 6: marginally above, initially recommending reject) finds that the paper provides helpful insights to connect methods for OOD detection tasks, and weakly recommends acceptance.    The reviewer s opinions on this paper vary significantly. Initially, a major selling point of the paper was that  the binary discriminator reaches state of the art OOD detection performance , but after discussion, the authors and reviewers agree that this statement is not supported by experiments, and the idea of using a binary discriminator is also not new, and thus everyone agrees that this statement should be removed.  This leaves as the major contribution an improved understanding of a variety of methods, and casting them as versions of a binary classifier.  This by itself would be sufficient to carry a paper, however the stated equivalence is rather weak as it is based on an asymptotic analysis, and in the asymptotic regime, out of distribution detection is rather trivial because the distributions are given. This also explains why in the paper s experiments all the methods that are asymptotically related behave quite differently in experiments.   I do not recommend this paper for acceptance. I ve read the paper and I ve thought quite a while it and its reviews. I have also discussed the paper with a colleague who works actively on out of distribution detection, since I m not an expert on this topic myself. While in general I find it very valuable to unify and to understand existing out of distribution algorithms better, I don t see how the particular interpretation provided by the paper is impactful, since it is unclear how the connection drawn in an asymptotic setup for Bayes classifiers actually extend to concrete OOD detection algorithms, which operate in the finite sample regime.
In this paper, a new method is proposed to discover diverse policies solving a given task. The key ideas are to (1) learn one policy at a time, with each new policy trying to be different enough from the previous ones, and (2) switch between two rewards on a per trajectory basis: the "normal" reward on trajectories that are unlikely enough under previoiusly discovered policies, and a "diversity inducing" reward on trajectories that are too likely (so as to push the policy being learned away from the previous ones). The main benefit of this switching mechanism is to ensure that the new policy will be optimal, because the reward signal isn t "diluted" by the diversity inducing signal as long as the policy stays far away from the previous ones.  After the discussion period, most reviewers clearly recommended acceptance of the paper. One reviewer remained on the "reject" side though, especially due to an unconvincing theoretical analysis of the method, in spite of several back and forth with authors. I also had my own concerns regarding that part after reading the paper, and further discussions with authors eventualy led to a significant rewrite of the corresponding theorems and proofs. I believe the final version (shared in comments by authors after the dealine for paper revisions) to at least be technically correct, though the relevance of the theory w.r.t. practical usage of the method is still not entirely convincing (e.g., assumptions regarding the number of distinct global optima, and the need for positive rewards).  That being said, in spite of these concerns regarding the practical significance of the theoretical analysis, I believe the paper has a strong enough empirical validation, and the method is (1) simple, (2) intuitively reasonable, (3) original due to the trajectory switching mechanism, which makes me recommend acceptance.
The paper presents a training method for deep neural networks to detect out of distribution samples under perspective of Gaussian discriminant analysis.  Reviewers and AC agree that some idea is given in the previous work (although it does not focus on training), and additional ideas in the paper are not super novel. Furthermore, experimental results are weak, e.g., comparison with other deep generative classifiers are desirable, as the paper focuses on training such deep models.  Hence, I recommend rejection.
The paper proposes a supervised adversarial method for disentangling the latent space of a VAE into two groups: latents z which are independent of the given attribute y, and \hat{y} which contains information about y. Since the encoder also predicts \hat{y} it can be used for classification and the paper shows competitive results on this task, apart from the attribute manipulation task. Reviewers had raised points about model complexity and connections to prior works which the authors have addressed and the paper is on the borderline based on the scores.   Though none of the reviewers explicitly pointed out the similarity of the paper with Fader networks (Lample et al., 2017), the adversarial setup for getting attribute invariant  z  is exactly same as in Fader networks, as also pointed out in an anonymous comment. The only difference is that encoder in the current paper also predicts the attribute itself (\hat{y}), which is not the case in Fader n/w, and hence the encoder can be used as a classifier as well (authors have also mentioned and discussed this difference in their response). However, the core idea of the paper as outlined in the title of the paper, ie, using adversarial loss for information factorization, is very similar to this earlier work, which diminishes the originality of the work.   With the borderline review scores, the paper can go in either of the half spaces (accept/reject) but I am hesitant to recommend an "accept" due to limited originality of the approach. However, if there is space in the program, the paper can be accepted. 
This paper proposes an algorithm for noisy labels by adopting an idea in the recent semi supervised learning algorithm.  As two problems of training noisy labels and semi supervised ones are closely related, it is not surprising to expect such results as pointed out by reviewers. However, reported thorough experimental results are strong and I think this paper can be useful for practitioners and following works.   Hence, I recommend acceptance.
This paper proposes an interesting machinery around Generative Adversarial Networks to enable sampling not only from conditional observational distributions but also from interven­tional distributions. This is an important contribution as this means that we can obtain samples with desired properties that may not be present in the training set; useful in applications such as ones involving fairness and also when data collection is expensive and biased. The main component called the causal controller models the label dependencies and drives the standard conditional GAN. As reviewers point out, the causal controller assumes the knowledge of the causal graph which is a limitation as this is not known a priori in many applications. Nevertheless, this is a strong paper that convincingly demonstrates a novel approach to incorporate causal structure into generative models. This should be of great interest to the community and may lead to interesting applications that exploit causality. I recommend acceptance. 
Reviewers are in a consensus and recommended to reject after engaging with the authors. Further, many additional questions raised in the discussion should be addressed in the submission to improve clarity. Please take reviewers  comments into consideration to improve your submission should you decide to resubmit. 
This paper proposes a method called Federated Bias variance attacks (FedBVA) to generate adversarial examples for federated learning, which can be used to make the model more robust to adversarial attacks.  All the reviewers found the problem and the approach very interesting. Their concerns include the following main points (please see the reviews for more details): * The decomposition of the bias and variance can be made more rigorous * The necessity for a shared dataset of adversarial examples makes the application a little limited * Need fairer experimental baselines * Vanilla federated learning is not guaranteed to preserve privacy   the authors should edit this claim in the motivation * Need compatibility with secure aggregation approaches where the central server cannot access local updates  The authors did do a great job of responding to the reviewers  comments. Given the interest in the problem and the novelty of the idea, I think an improved version of the paper would be quite well received.
This paper proposes to implement posterior sampling for reinforcement learning for MBRL using three types of noisy convolutional layers inspired by object  and event based domain knowledge. These layers are used to augment the SimPLe agent (Kaiser et al, 2020), resulting in the EVaDE SimPLe agent, and experiments demonstrate that the EVaDE SimPLe outperforms SimPLe on average across twelve Atari games.  The reviewers  opinions on the paper were mixed. The reviews highlighted several strengths of the paper: that using posterior sampling for exploration in MBRL is well motivated (Reviewers 9oaA, XiQT) and that the simplicity of the proposed layers is appealing (Reviewers trzPm, XiQT). However, the reviewers also generally felt that the proposed method was overly specific to a particular domain (Reviewer gXzj, XiQT) and that there was not enough analysis demonstrating *why* the proposed layers work, in which cases they would not work, or why these modifications might be better than other similar modifications (Reviewers 9oaA, trzP, gXzj). Initially there were also some concerns raised by Reviewer trzP about the validity of the evaluation due to the number of seeds, though these concerns were addressed by the authors during the rebuttal.  I agree with the reviewers that the approach is interesting and that getting posterior sampling to work well in MBRL is an important problem. But I also find myself agreeing that the present approach is not analyzed in sufficient depth (the results are overly focused on just overall performance, rather than analyzing behaviors exhibited by the agents) and that it is unclear how well it would work in other domains (e.g. 3D settings). I therefore feel this work is not quite ready to be presented at ICLR, and recommend rejection.
An interesting application of graph neural networks to robotics. The body of a robot is represented as a graph, and the agent’s policy is defined using a graph neural network (GNNs/GCNs) over the graph structure.  The GNN based policy network perform on par with best methods on traditional benchmarks, but shown to be very effective for transfer scenarios: changing robot size or disabling its components.  I believe that the reviewers  concern that the original experiments focused solely on centepedes and snakes were (at least partially) addressed in the author response: they showed that their GNN based model outperforms MLPs on a dataset of 2D walkers.  Overall:   an interesting application   modeling robot morphology is an under explored direction   the paper is  well written   experiments are sufficiently convincing (esp. after addressing the concerns re diversity and robustness).  
The paper extends LISTA by introducing gain gates and overshoot gates, which respectively address underestimation of code components and compensation of small step size of LISTA. The authors theoretically analyze these extensions and backup the effectiveness of their proposed algorithm with encouraging empirical results. All reviewers are highly positive on the contributions of this paper, and appreciate the rigorous theory which is further supported by convincing experiments. All three reviewers recommended accept. 
The authors propose a clustering algorithm for users in a system based on their lifetime distribution. The reviewers acknowledge the novelty of the proposed clustering algorithm, but one concern left unresolved is how the results of the analysis can be of use in the real world examples used. 
The submission proposes to train a model to modify objects in an image using language (the modified image is the effect of an action). The model combines CNN, RNN, Relation Nets and GAN and is trained and evaluated on synthetic data, with some examples of results on real images.  The paper received relatively low scores (1 reject and 2 weak rejects).  The authors did not provide any responses to the reviews and did not revise their submission.  Thus there was no reviewer discussion and the scores remained unchanged.  The reviewers all agreed that the submission addressed an interesting task, but there was no special insight in how the components were put together, and the work was limited in the experimental results.  Comparisons against additional baselines (AE, VAE), and ablation studies or examinations of how the components can be varied is needed.  The paper is currently too weak to be accepted at ICLR.  The authors are encouraged to improve their evaluation and resubmit to an appropriate venue.
This paper received some additional discussion between the reviewers and the area chair. The reviewers were largely unswayed by the author responses. One concern was the level of technical novelty, feeling that this was largely a straightforward adaptation of DPSGD (as, admittedly, most works in the DP ML setting are). The primary technical contribution may be the sampling amplification theorem, which one reviewer felt was also straightforward from previous work. Other criticisms was that the privacy parameter epsilon is rather large, and that results are restricted to 1 layer GNNs. Generally, the work did not feel very novel to reviewers from either the privacy or the GNN community. However, they felt that the paper could benefit substantially from exploration and implementation of the comments made in the responses, so the authors are encouraged to pursue those directions. Some of the many suggestions from reviewer Xcpu may help the authors make the paper appeal more to the GNN community.
A number of suggestions have been given about the manuscript. The evaluation raised questions about clarify, placement with respect to other approaches, choices for the design, etc. There are no immediate replies from authors, so I hope the suggestions are useful for future work.
The authors propose an actor critic method for finding Nash equilibrium in linear quadratic mean field games and establish linear convergence under some assumptions. There were some minor concerns about motivation and clarity, especially with regards to the simulator. In an extensive and interactive rebuttal, the authors were able to argue that their results/methods, which appear to be rather specialized to the LQ setting, offer insight/methods beyond the LQ setting.
The authors propose modifications to the Transformer architecture in BERT by using grouped FFN and an additional convolution module. The paper doesn t have all the results and comparison that should be done for a model that has seen similar architecture modification in the previous papers. While it is not necessary to show improvements on multiple hardware systems, it is important to see comparisons to more, stronger baselines and metrics on the full downstream GLUE eval rather than just Squad to establish improvements. A reject.
The paper proposes a regularisation technique based on Shake Shake which leads to the state of the art performance on the CIFAR 10 and CIFAR 100 dataset. Despite good results on CIFAR, the novelty of the method is low, justification for the method is not provided, and the impact of the method on tasks beyond CIFAR classification is unclear.
The paper proposes a sequential meta learning method over few shot sequential domains, which meta learns both model parameters and learning rate vectors to capture task general representations.  Reviewers raised many insightful and constructive comments. The main themes are as follows:   The problem setting needs further motivation and clarifications, to make it more realistic and applicable.   The novelty is relatively weak, e.g. the approach is too simple, and learning the learning rate is a common trick.   The method needs great effort for better presentation and justification. The current presentation simply lists several equations in a dense way without detailed explanation. Some main claims such as mitigating catastrophic forgetting are not elaborated extensively.  AC scanned through the paper and agreed with the reviewers  main points. Authors  rebuttal in general did not address these concerns to the satisfaction. For example, even after revision, the readability of this paper is not good enough. The authors are encouraged to perform a thorough revision.
There was fairly detailed discussion among three of the four reviewers. The fundamental concern of the reviewers is regarding the contribution of the paper. During the rebuttal, the authors clarified the following:  > while the effects of varying uncertainty / horizon lengths is well understood for Bayes optimal policies, it is not understood for existing meta RL approaches, which is the topic of this paper  That is, the contribution of the paper is to understand the effects of varying uncertainty/horizon lengths for meta RL approaches. However, it is known in prior work that meta RL algorithms such as RL^2 can implement Bayes optimal policies in principle. As a result, it s not clear whether this contribution is significant relative to prior knowledge, and this paper does not seem to bring any new insights.  An alternative framing of the paper would be to consider the question of how meta RL solutions compare to Bayes adaptive optimal policies. While this framing would be interesting and novel, the current version of the paper does not sufficiently answer this question, since the only experiments include RL^2 (and such a study would require experimenting with more sophisticated meta RL algorithms beyond RL^2).  As such, this paper isn t suitable for publication at ICLR in its current form.
The paper shows that data augmentation methods work well for consistency training on unlabeled data in semi supervised learning.  Reviewers and AC think that the reported experimental scores are interesting/strong, but scientific reasoning for convincing why the proposed method is valuable is limited. In particular, the authors are encouraged to justify novelty and hyper parameters used in the paper. This is because I also think that it is not too surprising that more data augmentations in supervised learning are also effective in semi supervised learning. It can be valuable if more scientific reasoning/justification is provided.  Hence, I recommend rejection.
Main content:  Blind review #3 summarizes it well:  This paper proposes a new loss for training models that predict where events occur in a sequence when the training sequence has noisy labels. The central idea is to smooth the label sequence and prediction sequence and compare these rather than to force the model to treat all errors as equally serious.  The proposed problem seems sensible, and the method is a reasonable approach. The evaluations are carried out on a variety of different tasks (piano onset detection, drum detection, smoking detection, video action segmentation).     Discussion:  The reviewers were concerned about the relatively low level of novelty, simplicity of the proposed approach (which the authors argue could be seen as a feature rather than a flaw, given its good performance), and inadequate motivation.     Recommendation and justification:  After the authors  revision in response to the reviews, this paper could be a weak accept if not for the large number of stronger submissions.
This paper derives CLT type results for the minimum $\ell_2$ norm least squares estimator allowing both n and p to grow.  Pros: As one reviewer puts it: Asymptotic confidence intervals for different prediction risks are derived. These results seem new.   Cons: It s not clear what has been gained by having these results, other than having them.  Reasoning: Staring at Figure 1 for a while, what jumps out is how little the CI matters. Unless $p\approx n$, the band is essentially uniform around the first order result derived elsewhere. The claim the authors seem to make at the bottom of page 1 is that, "supposing I have 90 observations and 100 predictors, it may not be so bad to collect 8 more observations. Even though on average I m worse off, perhaps not for my data?" The flip side of this argument is "why am I using min norm OLS"? I think that the authors are making the wrong argument in this paper. The point of analyzing this problem is not to understand what happens when $p\approx n$ but to understand why $p \gg n$ is good, and thereby try to justify parameter explosion in deep learning. I should be looking at the left side of Figure 1, not the center. Even the language "more data hurt" is the wrong statement. The point isn t to show that collecting data is bad but to justify adding parameters. We should say "more parameters help". If the authors  proof technique added to the understanding in that case, then this paper would be more convincing. As is, I find it hard to overrule with the reviewers who appear to be mainly on the fence with little enthusiasm. 
Three reviewers agree on the value of the contribution and recommend acceptance. A reviewer votes for rejection but the authors have clarified all the major concerns raised by the reviewer. Therefore, I recommend acceptance. 
The paper introduces a novel dataset condensation technique that generates synthetic samples (images) by matching model gradients with those obtained on the original input samples (images). The authors also show that these synthetic images  are not architecture dependent and can be used to train different deep neural networks. The approach is validated on several smaller datasets like MNIST, SVHN and CIFAR10. This work is well motivated and the methodological contributions convincing. All reviewers were enthusiastic and indicated that there were no flaws in this work. The rebuttal clarified outstanding questions and made the paper stronger.
This paper studies the important problem of efficiently identifying good hyperparameters for convolutional neural networks. The proposed approach is based on using an SVD of unfolded weight tensors to build a response surface that can be optimized with a dynamic tracking algorithm. Reviewers raised a number of concerns which were not fully addressed in rebuttals and lead me to recommend rejecting this work. In particular: focus on single hyperparameter (learning rate) made it unclear whether the proposed approach could actually be used for other hyperparameters or to jointly optimize combination of hyperparameters, empirical improvements even for learning rate are not strong and baselines are weak, and concern that initial success early in training (5 epochs) may not lead to generalization late in training. Additionally, there were several concerns around the clarity of the presentation, which I also found hard to follow: how is KG related to information theoretic metrics, why is the particular form of averaging across layers reasonable, and how is it related to other generalization / performance metrics? With additional experiments on other hyperparameters (for example L2 regularization), I think the work would be greatly strengthened. 
To ensure that a VAE with a powerful autoregressive decoder does not ignore its latent variables, the authors propose adding an extra term to the ELBO, corresponding to a reconstruction with an auxiliary non autoregressive decoder. This does indeed produce models that use latent variables and (with some tuning of the weight on the KL term) perform as well as the underlying autoregressive model alone. However, as the reviewers pointed out, the paper does not demonstrate the value of the resulting models. If the goal is learning meaningful latent representations, then the quality of the representations should be evaluated empirically. Currently it is not clear whether that the proposed approach would yield better representations than a VAE with a non autoregressive decoder or a VAE with an autoregressive decoder trained using the "free bits" trick of Kingma et al. (2016). This is certainly an interesting idea, but without a proper evaluation it is impossible to judge its value.
This paper presents a novel method for synthesizing fluid simulations, constrained to a set of parameterized variations, such as the size and position of a water ball that is dropped. The results are solid; there is little related work to compare to, in terms of methods that can "compute"/recall simulations at that speed. The method is 2000x faster than the orginal simulations. This comes with the caveats that:  (a) the results are specific to the given set of parameterized environments; the method is learning a  compressed version of the original animations; (b) there is a loss of accuracy, and therefore also a loss of visual plausibility.  The AC notes that the paper should use the ICLR format for citations, i.e., "(foo et al.)" rather than "(19)". The AC also suggests that limitations should also be clearly documented, i.e., as seen from the  perspective of those working in the fluid simulation domain.  The principle (and only?) contentious issue relates to the suitability of the paper for the ICLR audience, given its focus on the specific domain of fluid simulations.  The AC is of two minds on this: (i) the fluid simulation domain has different characteristics to other domains, and thu understanding the ICLR audience can benefit from the specific nature of the predictive problems that come the fluid simulation domain;  new problems can drive new methods.  There is a loose connection between the given work and residual nets, and of course res nets have also been recently reconceptualized as PDEs. (ii) it s not clear how much the ICLR audience will get out of the specific solutions being described; it requires understanding spatial transformer networks and a number of other domain specific issues. A problem with this type of paper in terms of graphics/SIGGRAPH is that it can also be seen as "falling short" there, simply because it is not yet competitive in terms of visual quality or the generality of fluid simulators;  it really fulfills a different niche than classical fluid simulators.  The AC leans slightly in favor of acceptance, but is otherwise on the fence.  
This paper presents a NAS method that avoids having to retrain models from scratch and targets a range of model sizes at once. The work builds on Yu & Huang (2019) and studies a combination of many different techniques. Several baselines use a weaker training method, and no code is made available, raising doubts concerning reproducibility.  The reviewers asked various questions, but for several of these questions (e.g., running experiments on MNIST and CIFAR) the authors did not answer satisfactorily. Therefore, the reviewer asking these questions also refuses to change his/her rating.   Overall, as AnonReviewer #1 points out, the paper is very empirical. This is not necessarily a bad thing if the experiments yield a lot of insight, but this insight also appears limited. Therefore, I agree with the reviewers and recommend rejection.
This paper develops a modular system named FILM, for egocentric instruction execution task in the ALFRED environment, which uses structured representations that build a semantic map of the scene, perform exploration with a semantic search policy, to achieve the natural language goal. They achieve strong performance while avoiding both expert trajectories and low level instructions. The reviewers all reasonably liked the paper (all reviewers gave  marginally above the acceptance threshold  score) and appreciated the planner ideas + strong results; but many of them also had concerns about the use of templated mappings from 7 high level goal types to low level instruction sequences, and whether this will make the system specific to ALFRED. The authors did provide some new results in the response period to show that results drop without the templates but not by a large margin. Some reviewers also had concerns about the novelty of the work and said that the semantic map building module and sub goal deterministic policy are motivated by previous work, but their incorporation into the FILM system is novel. Lastly, there was some concerns/debates on whether the system assumes/uses too much domain knowledge / task type taxonomy which might reduce the ability to generalize to other domains / data types, versus on the other hand the results may also serve to highlight the need for improvements in high level planning/control in these types of visual language navigation tasks.
The authors present a simple alternative to adversarial imitation learning methods like GAIL that is potentially less brittle, and can skip learning a reward function, instead learning an imitation policy directly.  Their method has a close relationship with behavioral cloning, but overcomes some of the disadvantages of BC by encouraging the agent via reward to return to demonstration states if it goes out of distribution.  The reviewers agree that overcoming the difficulties of both BC and adversarial imitation is an important contribution.  Additionally, the authors reasonably addressed the majority of the minor concerns that the reviewers had.  Therefore, I recommend for this paper to be accepted.
Main content:  Blind review #2 summarizes it well:  The aim of this work is to improve interpretability in time series prediction. To do so, they propose to use a relatively post hoc procedure which learns a sparse representation informed by gradients of the prediction objective under a trained model. In particular, given a trained next step classifier, they propose to train a sparse autoencoder with a combined objective of reconstruction and classification performance (while keeping the classifier fixed), so as to expose which features are useful for time series prediction.  Sparsity, and sparse auto encoders, have been widely used for the end of interpretability. In this sense, the crux of the approach is very well motivated by the literature.     Discussion:  All reviews had difficulties understanding the significance and novelty, which appears to have in large part arisen from the original submission not having sufficiently contextualized the motivation and strengths of the approach (especially for readers not already specialized in this exact subarea).     Recommendation and justification:  The reviews are uniformly low, probably due to the above factors, and while the authors  revisions during the rebuttal period have improved the objections, there are so many strong submissions that it would be difficult to justify override the very low reviewer scores.
The paper presents a GCN based solution with a distance aware pooling method for diagnosis and prognosis of COVID 19 based on CT scan. It aims to address an important and timely problem. The proposed solution is reasonable.   The paper receives mixed ratings, and therefore we had extensive discussions. It is agreed by all of us that    (1) the novel contribution of the proposed method is relatively low compared with standard ICLR papers;   (2) the evaluation is interesting, but could be improved with state of art baselines on CT scan (not limited to GCN based method);   (3) the authors have improved the writing of the paper significantly, which convinces two reviewers to elevate their scores.   The paper addresses a timely topic, but there is still room for improvement in methodology and evaluation. We hope that the reviews can help the authors prepare a strong publication in the future.  
This paper and revisions have some interesting insights into using ER for catastrophic forgetting, and comparisons to other methods for reducing catastrophic forgetting. However, the paper is currently pitched as the first to notice that ER can be used for this purpose, whereas it was well explored in the cited paper "Selective Experience Replay for Lifelong Learning", 2018. For example, the abstract says "While various methods to counteract catastrophic forgetting have recently been proposed, we explore a straightforward, general, and seemingly overlooked solution – that of using experience replay buffers for all past events". It seems unnecessary to claim this as a main contribution in this work. Rather, the main contributions seem to be to include behavioural cloning, and do provide further empirical evidence that selective ER can be effective for catastrophic forgetting.   Further, to make the paper even stronger, it would be interesting to better understand even smaller replay buffers. A buffer size of 5 million is still quite large. What is a realistic size for continual learning? Hypothesizing how ER can be part of a real continual learning solution, which will likely have more than 3 tasks, is important to understand how to properly restrict the buffer size.  Finally, it is recommended to reconsider the strong stance on catastrophic interference and forgetting. Catastrophic interference has been considered for incremental training, where recent updates can interfere with estimates for older (or other values). This definition does not precisely match the provided definition in the paper. Further, it is true that forgetting has often been used explicitly for multiple tasks, trained in sequence; however, the issues are similar (new learning overriding older learning). These two definitions need not be so separate, and further it is not clear that the provided definitions are congruent with older literature on interference.    Overall, there is most definitely useful ideas and experiments in this paper, but it is as yet a bit preliminary. Improvements on placement, motivation and experimental choices would make this work much stronger, and provide needed clarity on the use of ER for forgetting.
The paper is interested in multi task learning. It introduces a new architecture which condition the model in a particular manner: images features and task ID features are fed to a top down network which generates task specific weights, which are then used in a bottom up network to produce final labels. The paper is experimental, and the contribution rather incremental, considering existing work in the area. Experimental section is currently not convincing enough, given marginal improvements over existing approaches   multiple runs as well as confidence intervals would help in that respect. 
The paper proposed an regret based approach to speed up counterfactural regret minimization. The reviewers find the proposed approach interesting. However, the method require large memory. More experimental comparisons and comparisons pointed out by reviewers and public comments will help improve the paper. 
The general consensus amongst the reviewers is that this paper is not quite ready for publication. The reviewers raised several issues with your paper, which I hope will help you as you work towards finding a home for this work.
This paper treats the task of point cloud learning as a dynamic advection problem in conjunction with a learned background velocity field.  The resulting system, which bridges geometric machine learning and physical simulation, achieves promising performance on various classification and segmentation problems.  Although the initial scores were mixed, all reviewers converged to acceptance after the rebuttal period.  For example, a better network architecture, along with an improved interpolation stencil and initialization, lead to better performance (now rivaling the state of the art) as compared to the original submission.  This helps to mitigate an initial reviewer concern in terms of competitiveness with existing methods like PointCNN or SE Net.  Likewise, interesting new experiments such as PIC vs. FLIP were included.
Reviewers agree that this is a very promising paper, with an excellent overview of existing techniques for semi supervised and neuro symbolic learning. However, reviewers also agree that the paper is not ready. With one more revision for clarity, some limited empirical validation and illustration of the theory, and focus on the essential message, this could become a seminal paper for our understanding of semi supervised learning. Luckily the reviews provided ample feedback, and the authors should be able to submit a very competitive paper next time around.
While the motivation of the paper is interesting the reviewers expressed concerns about the experimental setup, comparison to related work, and paper framing. For experiments, it was unclear why authors compared such disparate methods instead of more fine grained adjustments (e.g., such as corrupting graphs as suggested by R3). For comparison, other methods such as Deep Walk and VGAE (as suggested by R1) seemed missing. I think the biggest issue however was with framing: as the reviewers pointed out, it was not clear enough how looking at downstream performance relates to looking at the manifold. In fact the paper title is much too general and is also well known: manifold learning has been around for 15+ years. I would urge the authors to take the recommendations of reviewers and either design new experiments that explicitly target the manifold or reframe the paper to design new evaluation metrics for latent (possibly structured) generative models.
We want to acknowledge that there has been a tremendous amount of work done during the discussion period on this paper for clarifying multiple points, adding multiple new comparison methods and new analysis. This is a very different draft than what was submitted and the reviewers acknowledged that. The draft is much closer from acceptance at ICLR than it was at submission time. However, despite all those additions, we do not support a publication at ICLR.   The main issues with the current draft are its positioning and motivations.  Right now the draft is in between a paper about Knowledge Base Construction (KBC) and a paper analyzing the knowledge contained within a large language model. This in between came up in multiple places during the discussion and is what causes the biggest confusion around this work. And, since there is no clear choice, the draft has limitation on either side.  * If the main point is around KBC to build general purpose KBs, then one would expect experiments on downstream tasks powered by a KB, language understanding tasks for instance. Indeed, KBs are just a means to an end and the latest advances in very large language models have shown that KBs were not essential to be state of the art in language understanding tasks (GLUE, QA datasets, etc). So we would like to see whether these enhanced KBs could be beneficial. Or the KBs are studied as a way to encode commonsense like in (Bosselut et al., 2019) or (Davison et al., 2019), but this is not the point of the current draft. * If the main impact of the draft is around what the language models learn, bridging the deep language model and knowledge graph communities through enhanced model transparency, as it has been said in the discussion, then the discussion with (Petroni et al. 19) should be more prominent and the introduction, motivation and experiments of the draft should reflect that.  That s why, even if this work is of solid quality, the current draft can not be accepted.
The author response addressed some reviewer concerns, and generally reviewers increased their scores. However, there are important, and unanswered concerns about the generalization of the model. The discussion raised the concerns that despite the paper claim of "a specific class of higher order reasoning" emerging, the result suggests relatively simple strategies. This might not be a limitation of the approach, but of the evaluation scenario. So, this either requires a more nuanced view of the findings, and further empirical evidence to support the claim.
This submission provides a new bound and derived method for unsupervised domain adaptation, based on adversarial training.  The method is then extensively evaluated empirically.  Pro:   the proposed method seems empirically successful  Con:   I agree with one of the reviewers that the presented theoretical justification is not convincing. There should be assumptions on how f_S and and f_T relate in order to drive meaningful guarantees in the proposed framework. The presented results here seem to be a case of a method that was motivated by some theoretical consideration, and worked well after some heuristic approximations were made. It would be nice to see a cleaner analysis of the conditions under which the method will be successful.
This paper explores the hypothesis that bloat can be prevented in Genetic Programming by identifying "winning subtrees" from simplified solutions, and use these to seed new GP runs. This idea is connected with the lottery ticket hypothesis in deep learning.  Reviewer are unanimous that the paper as it stands is not ready to publish. One big issue is that the empirical results are not particularly good. Another is that the conceptual foundations of the paper, in particular the parallell to the lottery ticket hypothesis, might be flawed. Nevertheless, there is much interesting research to do in this direction.
All the reviewers agree that this is an interesting paper but have concerns about readability and presentation. There is also concern that many results are speculative and not concretely tested. I recommend the authors to carefully investigate their claims with stronger experiments and submit it to another venue. I recommend presenting at ICLR workshop to obtain further feedback.
The paper proposes a model based proximal policy optimization reinforcement learning algorithm for designing biological sequences. The policy of for a new round is trained on data generated by a simulator. The paper presents empirical results on designing sequences for transcription factor binding sites, antimicrobial proteins, and Ising model protein structures.  Two of the reviewers are happy to accept the paper, and the third reviewer was not confident. The paper has improved significantly during the discussion period, and the authors have updated the approach as well as improved the presented results in response to comments raised by the reviewers. This is a good example of how an open review process with a long discussion period can improve the quality of accepted papers.  A new method, several nice applications, based on a combination of two ideas (simulating a model to train a policy RL method, and discrete space search as RL). This is a good addition to the ICLR literature.
This paper proposes a framework for artificial life. In the framework, there is no primitive agent construct, but rather a set of basic recurrent network components (such as linear algebra operations). The framework is open ended and objective less. The authors describe the emergence of different organisms out of these building blocks, illustrating the idea with a simplified implementation.  The paper is extremely original, at least in the context of the deep learning/ICLR community. At the same time, together with a majority of reviewers (mostly experts in relevant topics such as neuro evolutionary methods and artificial life), I felt that this work is not ready to be presented at a major conference, for three main reasons. 1) It should make its links to the huge literature on artificial life outside the ICLR community clearer. 2) The empirical work is somewhat limited and 3) this is not counterbalanced by a clear theoretical roadmap.  I thus do not recommend acceptance, although I really hope the authors will present a more thoroughly worked out version of the paper soon. 
The authors propose an alteration to Dreamer that incorporates a swav like objective. The reviewers raised a number of issues with the paper, overall arguing for rejection. In particular, the reviewers felt that the work was not well motivated, weak performance, that a number of baselines were missing, and a lack of analysis of the results, a lack of novelty. While the authors addressed many of these concerns during the rebuttal, the majority of reviewers still felt this was not enough and that the paper did not meet the bar for acceptance. Therefore, I recommend rejection at this stage so that these concerns can be addressed.
This paper proposes a novel method of solving inverse problems that avoids direct inversion by  first reconstructing various piecewise constant projections of the unknown image (using a different CNN to learn each)  and then combining them via optimization to solve the final inversion.  Two of the reviewers requested more intuitions into why this two stage process would  fight the inherent ambiguity.  At the end of the discussion, two of the three reviewers are convinced by the derivations and empirical justification of the paper. The authors also have significantly improved the clarity of the manuscript throughout the discussion period. It would be interesting to see if there are any connections between such inversion via optimization with deep component analysis methods, e.g. “Deep Component Analysis via Alternating Direction Neural Networks ” of Murdock et al. , that train neural architectures to effectively carry out the second step of optimization, as opposed to learning  a feedforward mapping.  
The main contribution is a way of analyzing the generalization error of neural nets by breaking it down into bias and variance components, and using separate principles to analyze each of the two components. The submission first proves rigorous generalization bounds for overparameterized linear regression (motivated in a general sense by the NTK); there are settings where this improves upon existing bounds. It extends the case to a matrix recovery model, showing that it s not limited to the linear regime. Finally, experimental results show that the risk decomposition holds empirically for neural nets.  The numerical scores would place this paper slightly below the cutoff. The reviewers feel that the paper is well written and have not identified anything that looks like a critical flaw. They have a variety of concerns, mostly centered around whether the results apply to practical situations. Specifically, they re worried about (1) the theory not applying directly to neural nets, (2) the high noise setting being less relevant for modern deep learning, and (3) whether there s a realistic situation where it improves over past bounds. Regarding (1), the theory covers not only the linear regime, but also the nonlinear matrix recovery regime; combined with the empirical results, this seems pretty solid by the standards of a DL theory paper. Regarding (2), even though the most common benchmarks indeed have low label noise, the high noise regime still seems worth understanding (after all, we d like our nets to work in domains like medicine). I haven t dug deeply enough to properly evaluate (3), but the author response seems believable to me.  Overall, the paper strikes me as creative and well executed. Regardless of whether the theory is easily extendable to neural nets, this seems like an interesting paper that can be built on in future work. I recommend acceptance.
It seems that the reviewers reached out a consensus that the paper is not ready for publication at ICLR. The reviewers raised concerns including “The empirical observations are not supported by theoretical analysis” , “The proposed algorithm is a simple modification to an existing algorithm”, concerns with “with the novelty of the paper”, “The message of the paper is not new. “ Please see the reviews for more detailed discussions about the paper.
This is an interesting paper, aiming to separate the generalization properties of SGD and GD.  Unfortunately, the reviewers had many significant concerns, primarily on the topic of the relationship to prior work by Wu et al. (which has a similar setting and similar proof techniques), but also regarding presentation and interpretation of results in general.  As such, I recommend the authors continue with this line of valuable work, aiming in particular to further separate it from existing results.
This paper proposes Bayesian quantized networks and efficient algorithms for learning and prediction of these networks. The reviewers generally thought that this was a novel and interesting paper.  There were a few concerns about the clarity of parts of the paper and the experimental results. These concerns were addressed during the discussion phase, and the reviewers agree that the paper should be accepted.
The Authors study the learning dynamics of deep neural networks through the lenses of chaos theory.   The key weakness of the paper boils down to a lack of clarity and precision. Chaos theory seems to be mostly used to computing eigenvalues but is not used to derive meaningful insights about the learning dynamics. R2 noted, "Chaos theory provides a way of computing eigenvalues but does not give much understanding on the neural network optimization.". R4 noted, "The authors use an insight from chaos theory to derive an efficient method of estimating the largest and smallest eigenvalues of the loss Hessian wrt the weight". Hence, statements such as "the rigorous theory developed to study chaotic systems can be useful to understand SGD" seem unsubstantiated.  Reduced to its essence, the key contribution is (1) a method to compute the top and the smallest eigenvalue, (2) the observation that the spectral norm of the Hessian along SGD optimization trajectory is related to the inverse of the learning rate, and (3) a method to automatically tune the learning rate.  Let me discuss these three contributions:  * The significance of the first contribution is unclear, as pointed out by R2. Indeed there are other methods (e.g. power method, Lanczos) for computing these quantities that should achieve either a similar speed or similar stability. Given the rich history of developing estimators of these quantities, a much more detailed evaluation is warranted to substantiate this claim.   * The core insight that the top eigenvalue of the Hessian in SGD is related to the inverse of the learning rate in the training of deep neural networks is nontrivial but is not fully novel. Closely related observations were also shown in the literature. This precise statement however indeed was not stated in the literature. This contribution could be a basis for acceptance, but the paper is not sufficiently focused on it, and the evaluation of this claim is a bit narrow in scope.  * Finally, there is a range array of methods to tune the learning rate. As noted for example by R3, "There are numerous ideas for proposing new optimization and without careful, through comparison to baseline, well known methods", the evaluation is too limited to treat this as a core contribution.  Based on the above, I have to recommend the rejection of the paper. At the same time, I would like to thank the Authors for submitting the work for consideration to ICLR. I hope the feedback will be useful for improving the work.
This submission got 3 rejection and 1 marginally below the threshold. In the original reviews, most of the concerns lie in the limited novelty, the inferior performance to some existing similar works and the limited scalability of the proposed method. Though authors provide some additional experiments, the reviewers still feel the experiments are not convincing and keep their ratings. AC agrees with the reviewers comments on this paper. Though achieving SOTA performance is not necessary for every submission, NAS alike method is purely pursuing better performance (either higher accuracy or better efficiency). Thus, the performance is also important for evaluating a NAS paper. From the reviewers, the proposed method does not show better performance than some existing works, like BiFPN. This makes the value of the paper is not clear, in particular considering the method novelty is limited. The authors could consider to improve the submission in the experiments to better justify the proposed method, either achieving better performance or higher efficiency than existing works. At its current status, AC cannot make accept recommendation. 
although i (ac) believe the contribution is fairly limited (e.g., (1) only looking at the word embedding which goes through many nonlinear layers, in which case it s not even clear whether how word vectors are distributed matters much, (2) only considering the case of tied embeddings, which is not necessarily the most common setting, ...), all the reviewers found the execution of the submission (motivation, analysis and experimentation) to be done well, and i ll go with the reviewers  opinion.
This paper presents two self supervised learning objectives that can be used as intermediate pre training tasks to refine the T5 sequence to sequence model between pre training and task fine tuning. It shows that, at small to moderate model sizes, adding this step significantly improves performance on commonsense oriented target tasks.  Pros:   This appears to be a fairly straightforward improvement in self supervised learning in NLP, with fairly extensive experiments.  Cons:    This model isn t trained at the same extremely large scales (10B+ words) as state of the art models, and it performs significantly below the state of the art. It s not clear that the released model represents a useful model for any application as is, and while it s likely, it s not proven that the ideas in this paper would still be useful at larger scales.   Given that, it seems like the most likely audience for this work is other developers of pretrained models in NLP, which makes the fit to a general ML conference less clear.   The framing around  concepts  and, more importantly, the model name  concept aware LM , gives the unwarranted impression that the new model handles  concepts  in a way that T5 doesn t. It is not reasonable to use the word  concept  to refer to specific parts of speech in your title (even if you later explain that), and whether your model handles concepts in a categorically different way from T5 would take a substantial analysis to show, which doesn t seem to be present. I don t think this paper is up to ICLR s standards with the current name, and urge the authors to change it. 
the reviewers all found the problem to be important, the proposed approach to be interesting, but the manuscript to be preliminary. i agree with them.
This paper improves DeepBugs by borrowing the NLP method ELMo as new representations. The effectiveness of the embedding is investigated using the downstream task of bug detection.   Two reviewers reject the paper for two main concerns: 1 The novelty of the paper is not strong enough for ICLR as this paper mainly uses a standard context embedding technique from NLP. 2 The experimental results are not convincing enough and more comprehensive evaluation are needed.   Overall, this novelty of this paper does not meet the standard of ICLR. 
The paper used the Koopman operator theory to explain and guide the DNN pruning. All the reviewers deemed that such a viewpoint is novel (but at different levels). However, the paper still had some issues, including unclear technical details, vague/overselling statements, being computation and memory expensive, etc. The paper finally got 4 "marginally above threshold" (one being of low confidence), making it on the borderline. The AC read through the paper and agreed that the Koopman operator theory brings new perspective to DNN pruning, with potential for other analysis of DNNs. Although the paper is imperfect and not strong, it does not have severe problems either and the issues pointed out by the reviewers could be easily fixed (except the scalability issue, which can be left as future work). In order to encourage new ideas, the AC recommended acceptance.
This work proposes an improved visualisation techniques for ReLU networks that compensates for filter scale symmetries/invariances, thus allowing a more meaningful comparison of low dimensional projected optimization landscapes between different network architectures.    the visualisation techniques are a small variation over previous works + extensive experiments provide nice visualisations and yield a clearer visual picture of some properties of the optimization landscape of various architectural variants  A promising research direction, which could be further improved by providing more extensive and convincing support for the significance of its contribution in comparison to prior techniques, and to its empirically derived observations, findings and claims. 
After reading the meta reviews and the authors comment, the meta reviewer thinks the paper is not ready for publication in a high impact conference like ICLR. The paper is not well positioned with respect to the literature, and the proposed techniques are not well discussed in relation with predominant paradigms like optimism in the face of uncertainty.
All the reviewers agreed that this was a sensible application of mostly existing ideas from standard neural net initialization to the setting of hypernetworks.  The main criticism was that this method was used to improve existing applications of hypernets, instead of extending their limits of applicability.
This is an interesting, controversial paper that contributes to an ongoing debate in Bayesian deep learning.  Bayesian inference with artificially “cooled” posteriors (e.g., trained with Langevin dynamics with down weighted noise) was recently found to outperform over both point estimation and fully Bayesian treatments (Wenzel et al., 2020). This paper proposes a new explanation for these observed phenomena in terms of a data curation mechanism that popular benchmark data sets such as CIFAR underwent. The analysis boils down to an evidence overcounting/undercounting argument and takes into account that curated data sets only contain data points for which all labelers agreed on a label. The authors claim that, when modeling the true generative process of the data, the cold posterior effect (partially) vanishes.  The paper is well written and provides a consistent analysis by modeling the data curation mechanism in terms of an underlying probabilistic graphical model of the labeling mechanism. Unfortunately, several observed phenomena of (Wenzel et al., 2020) remain unexplained by the theoretical arguments, e.g., the fact that “very cold” (T  > 0) posteriors don’t hurt performance, or the observation that the optimal temperature seems to depend on the model capacity. While the proposed explanation doesn’t capture the full picture (upon which both authors and reviewers agree), the paper’s focus on the data curation process, supported extensive experiments, gives a partial explanation and provides an interesting perspective that will spur further discussion and should be of broad interest to the Bayesian deep learning community.  
This paper proposes an equivariant sequence to sequence model for dealing with compositionality of language. They show these models are better at SCAN tasks.  Reviewers expressed two major concerns: 1) Limited clarity of section 4 which makes the paper difficult to understand. 2) Whether this could generalize to more complex types of compositionality.  Authors responded by revising Section 4 and answering the question of generalization. While the reviewers are not 100% satisfied, they agree there is enough novel contribution in this paper.   I thank the authors for submitting and look forward to seeing a clearer revision in the conference.
The committee feels that this paper presents a simple, yet effective way to adapt language models from various users in a sufficiently privacy preserving way.  Empirical results are quite strong.  Reviewer 3 says that the novelty of the paper is not great, but does not provide any references to prior work that are similar to this paper.  The meta reviewer finds the responses to Reviewer 3 sufficient to address the concerns.  Similarly, Reviewer 2 says that the paper may not be relevant to ICLR, but the committee feels its content does belong to the conference since the topic is extremely relevant to modern language processing techniques.  In fact, the authors provide several references that show that this paper is similar in content to those submissions.  Reviewer 1 s concerns are also not sufficiently strong to warrant rejection.  The responses to each criticism suffices and the meta reviewer thinks that this paper will add value to the conference.
This paper takes on (in my view) one of the most important questions in the lottery ticket literature today: how small are the smallest lottery tickets that exist in our neural networks? Many methods have been proposed for finding weak lottery tickets (those that require training to reach full accuracy) and strong lottery tickets (those that do not), but we have no idea how close they come to finding the smallest lottery tickets. Moreover, in many cases, we only know how to find lottery ticket subnetworks early in training rather than at initialization. Is this a fundamental limitation on the existence of lottery tickets, or is this simply a limitation of our methods for finding them? I am personally very involved in lottery ticket conversations in the literature, and I believe I can speak with some authority when I say that these are vital questions where any progress is important.  Moreover, these are exceedingly difficult research questions, and (again, in my view) the authors should be commended for taking them on. A naive approach to these questions would involve brute force search over all possible subnetworks, which is infeasible even on the smallest of toy examples, let alone the meaningful computer vision tasks where lottery ticket work typically focuses.  I am sharing all of this information to provide background for my confident recommendation to accept this paper over the many legitimate concerns expressed by reviewers and those that I saw when reading the paper in detail. Those include that: * This paper does not solve any of these research problems in their entirety. * It focuses on toy networks smaller than those traditionally studied in the lottery ticket literature, and it is well known that lottery ticket behavior changes in character at larger scales. * Planting good subnetworks may be an unrealistic proxy for the kinds of subnetworks that actually emerge naturally. * There may be multiple good subnetworks in a network, not just the one that was planted. * The graphs are a bit hard to read. * I find the mix of pruning methods studied, which were designed with very different goals (pruning after training, pruning before training, finding strong lottery tickets), a bit confusing.  **The bottom line:** With all of that said, in my view, the paper asks good questions and provides an initial foothold that other researchers will be able to build on as we seek more general answers. This is similar to the contributions made by Zhou et al., which started the conversation on strong lottery tickets, and potentially even Frankle & Carbin, which kicked off the lottery ticket discussion but got many things wrong. Both papers were good first attempts at solving big problems, and both were highly influential despite their flaws. Similarly, even if this submission isn t perfect in every way, this is among the most important kinds of contributions that a paper can make. For that reason, I strongly recommend acceptance under the belief that this paper will help to foster a valuable conversation in the literature.  P.S. I really, truly, strongly beg the authors to redo their graphs following the style of some of the more user friendly lottery ticket or pruning papers they have cited (e.g., Frankle et al., 2021). The graphs in this paper were really hard to parse. Really really really hard to parse. They re too small, the y axis is often squished, gridlines would be helpful, the lines are overlapping in ways that are difficult to distinguish because the colors blend, etc. etc. This is quite possibly the biggest impediment I see to this paper s ability to have broader influence.
This paper initially received borderline reviews. The main concern raised by all reviewers was a limited experimental evaluation (synthetic only). In rebuttal, the authors provided new results on the CelebA dataset, which turned the first reviewer positive. The AC agrees there is merit to this approach, and generally appreciates the idea of compositional concept learning.
The paper addresses an interesting problem (learning in the presence of noisy labels) and provides extensive experiments. However, while the experiments in some sense cover a good deal of ground, reviewers raised issues with their quality, especially concerning baselines and depth (in terms of realism of the data). The authors provided many additional experiments during the rebuttal, but the reviewers did not find them sufficiently convincing.
This paper provides a method for eliminating options in multiple answer reading comprehension tasks, based on the contents of the text, in order to reduce the "answer space" a machine reading model must consider. While there s nothing wrong with this, conceptually, reviewers have questioned whether or not this is a particularly useful process to include in a machine reading pipeline, versus having agents that understand the text well enough to select the correct answer (which is, after all, the primary goal of machine reading). Some reviewers were uncomfortable with the choice of dataset, suggesting SQuAD might be a better alternative), and why I am not sure I agree with that recommendation, it would be good to see stronger positive results on more than one dataset. At the end of the day, it is the lack of convincing experimental results showing that this method yields substantial improvements over comparable baselines which does the most harm to this well written paper, and I must recommend rejection.
After carefully reading all reviews and rebuttal, I actually think the paper provides sufficient new insight in understanding MAML that is worth being accepted. I want to thank the authors for actively engaging with the reviewers, and providing sufficient changes to the paper in order to clarify and improve its contributions.   Theoretical results tend to be harder to judge, as they often need to happen under assumptions that make them tractable. Nevertheless they provide intuitions and understanding of the underlying principle that end up having an impact even in more realistic scenarios where these assumptions might not hold. I think this is such a scenario, and I think better understanding the relationship between ERM and approaches as meta learning is important for the field moving forward.
The paper presents a new attack combining trojans (backdoor attacks) with adversarial examples. The new attack is triggered only if both a trojan and the respective adversarial perturbation are present. Experimental evaluation demonstrates that neither adversarial training (as a defense against adversarial examples) nor defenses against backdoors are effective against the new attack.  The proposed method is original albeit somewhat incremental (combination of two well known attack techniques). The main weakness of the paper, however, is its threat model. It is not clearly explained why the proposed attack would make sense for an attacker. Backdoor attacks are typically executed by model creators in order to force certain decisions on certain data. On the other hand, adversarial examples are generated by model users (or abusers) who have an interest in wrong model predictions (e.g., decisions made in their favor). The paper does not provide a convincing use case in which such combined attacks would be feasible.   Furthermore, paper s clarity can be improved. The introduction does not present a clear picture of poisoning attacks. It essentially treats poisoning attacks as equivalent to backdoor/trojan attacks. This is not true and a substantial body of research (starting from the seminal paper by Barreno et al. in 2006) has addressed indiscriminate poisoning attacks aimed at general deterioration of classifier performance. A distinction between a clean label and a poisoned label attacks is also not clearly presented. The notation of Section 3 is rather complex and confusing.   
This work provides evidence against the hypothesis that ResNets implement iterative inference, or that iterative convergent computation is a good inductive bias to have in these models. The reviewers indicate that they think this hypothesis is interesting and relevant to the ICLR community, but they do not find the current work sufficiently convincing. Both theoretically and experimentally the paper does not fully demonstrate the claim that iterative inference is not useful in ResNets, and the reviewers are unanimous in their recommendation to reject the paper until the evidence for this claim is strengthened. 
This paper introduces a dataset and a trained evaluation metric for evaluating discourse phenomena for MT. Several context aware MT models are compared against a sentence level baseline. The paper develops metrics which evaluate the models according to their performance on four discourse phenomena: anaphora, lexical consistency, coherence and readability, and discourse connective translation. Data is released for three language pairs (all using English as the target language).   First, I’d like to point out that creating datasets and benchmarks for analyzing/evaluating discourse level errors in machine translation is an extremely valuable contribution. This paper is addressing a very relevant problem and even though there is no new model/method/algorithm being proposed, this work *fits* this conference   it is my opinion that the community should welcome and value more than it currently does the efforts spent in creating high quality datasets that can help make progress in the field.  There was substantial discussion among reviewers about this paper.   The main weaknesses raised by the reviewers were:   Limited information about the process to create the anaphora test, which was a contribution of prior work (Jwalapuram et al. (2019)   this was addressed in the updated version; but the anaphora challenge sets seem to be only a minor update over previous work.   All language pairs use English as the target language, and it is not simple to extend this approach beyond English target languages.   Lack of detail on how BLEU scores were computed (tokenised? true cased? My recommendation is to use sacrebleu)   this was clarified in the rebuttal.   The evaluated NMT models all date from 2018 or earlier.   Two of the 4 benchmarks (anaphora and coherence) are evaluated by neural models trained on WMT outputs, which makes the interpretation of scores is opaque, and their validity is unclear.  While the creation of a benchmark for discourse evaluation of MT is a laudable effort as mentioned above, it is my opinion that due to some of the weaknesses above the current version of this work is not yet ready for publication. However, I strongly encourage the authors to improve upon these points and resubmit their work to another venue. I list some suggestions below to improve this paper.  My biggest concern with the current version is the last weakness above. As pointed out by a reviewer, the framework of Jwalapuram et al. (2019) provides empirical support for the model s sensitivity (if there is a pronoun error, does the metric pick it up?). But they don’t necessarily capture model *specificity* (if the metric ranks one output higher, can we be confident that this is because of a pronoun translation error?). For the coherence metric, authors make an argument that their metric is sensitive to coherence issue, but concerns remain about whether it is sufficiently specific to these issues. In the rebuttal, authors argue that BLEURT is sentence level, but they could easily aggregate sentence level judgments and report correlation between BLEURT and human coherence  judgments to show that their metric correlates better with human coherence judgments than BLEURT or even just BLEU. Besides BLEURT, I would add there are other recently proposed metrics that may capture discourse phenomena (neural metrics trained against MQM annotations or sentence level human assessments with document context) and should be compared against: check COMET [1] or PRISM [2] (the latter is sentence based but could be adapted for paragraphs or documents).  There is also prior work comparing various context aware machine translation approaches against a sentence level baseline, some with negative findings [3,4,5]. I suggest the authors look at this related work in future iterations of their paper.  [1] https://arxiv.org/pdf/2009.09025.pdf  [2] https://arxiv.org/pdf/2004.14564.pdf   [3] https://www.aclweb.org/anthology/2020.eamt 1.24.pdf  [4] https://arxiv.org/pdf/1910.00294.pdf   [5] https://www.aclweb.org/anthology/2020.emnlp main.81.pdf  
The paper considers representation learning of 3D molecular graphs. The authors propose a message passing scheme using spherical coordinates. It is tested on three datasets of 3D moleclular graphs. The authors offer an in depth analysis of different aspects, with an extensive experimentation of the method.  Strengths:    The SMP introduces an interesting method to alleviate the computation cost issue in SCS from O(nk^3) to O(nk^2). This method is important and can be generalized to more broad types of tasks.   This is an empirical work, and the experimental results support the effectiveness of SMP.   The proposed MP approach can better distinguish certain structures than some existing models.   Incorporating torsion information when representing 3D molecules is novel and helpful   While message passing methods on graphs exploit only the connectivity, this work shows an interesting method to include the embedding information in the case of geometrical graphs.  Weaknesses:    The proposed SMP scheme in Eq. (1) lacks novelty since it basically enriches the GN framework in [1] with geometry features   the architecture of the proposed SphereNet is similar to DimeNet   Why SMP is better than Cartesian coordinate system (CCS) is not well explained.  Overall, a majority of reviewers are in favor of acceptance and a third reviewer is happy with either acceptance or rejection and does not give strong reasons for rejecting the paper. My recommendation is, therefore, acceptance. I recommend the authors use the reviewers comments to improve the paper for its camera ready version.
The paper has some interesting ideas around auto regressive policies and estimating their entropy for exploration. The use of autoregressive policies in RL is not particularly novel, and the estimate of entropy for such models is straightforward. Finally, the experiments focus on very simple tasks.
This paper studies the statistics of activation norms and Jacobian norms for randomly initialized ReLU networks in the presence (and absence) of various types of residual connections. Whereas the variance of the gradient norm grows with depth for vanilla networks, it can be depth independent for residual networks when using the proper initialization.  Reviewers were positive about the setup, but also pointed out important shortcomings on the current manuscript, especially related to the lack of significance of the measured gradient norm statistics with regards to generalisation, and with some techinical aspects of the derivations. For these reasons, the AC believes this paper will strongly benefit from an extra iteration. 
This paper studies the evolution of the mean field dynamics of a two layer fully connected and Resnet model. The focus is in a realizable or student/teacher setting where the labels are created according to a planted network. The authors study the stationary distribution of the mean field method and use this to explain various observations. I think this is an interesting problem to study. However, the reviewers and I concur that the paper falls short in terms of clearly putting the results in the context of existing literature and demonstrating clear novel ideas. With the current writing of the paper is very difficult to surmise what is novel or new. I do agree with the authors  response that clearly they are looking at some novel aspects not studied by the previous work but this was not revised during the discussion period. Therefore, I do not think this paper is ready for publication. I suggest a substantial revision by the authors and recommend submission to future ML venues. 
The paper proposes a methodological improvement in the Langevin based training of energy based models. The idea is to initialize the Langevin flow used to train an energy based model with a normalizing flow which learns to mimic the Langevin flow as the energy based model is being trained. The method is empirically evaluated on synthetic data and image benchmarks.  The reviewers are currently divided: one argues for strong rejection, two for weak acceptance, and one for strong acceptance. In summary, the reviewers have expressed two main points of criticism: (a) that the motivation is unclear or not experimentally demonstrated; (b) that the convergence properties of the algorithm are unclear. Regarding (a), I believe the authors have adequately addressed this concern, and in my judgement the method is sufficiently motivated. Regarding (b), the authors responses have mostly relied on non rigorous argumentation and appeal to prior work, so I don t think the issue has been addressed to the reviewers  satisfaction. Having said that, in my judgement lack of clarity regarding convergence is not a sufficient reason to reject the paper, as there don t seem to be reasonable doubts that the method doesn t converge in practice.  On balance, although the paper has certain weaknesses, it proposes an interesting and potentially useful method without major technical inadequacies, so I m leaning towards recommending acceptance.
This paper presents CMOW—an unsupervised sentence representation learning method that treats sentences as the product of their word matrices. This method is not entirely novel, as the authors acknowledge, but it has not been successfully applied to downstream tasks before. This paper presents methods for successfully training it, and shows results on the SentEval benchmark suite for sentence representations and an associated set of analysis tasks.  All three reviewers agree that the results are unimpressive: CMOW is no better than the faster CBOW baseline on most tasks, and the combination of the two is only marginally better than CBOW. However, CMOW does show some real advantages on the analysis tasks. No reviewer has any major correctness concerns that I can see.  As I see it, this paper is borderline, but narrowly worth accepting: As a methods paper, it presents weak results, and it s not likely that many practitioners will leap to use the method. However, the method is so appealingly simple and well known that there is some value in seeing this as an analysis paper that thoroughly evaluates it. Because it is so simple, it will likely be of interest to researchers beyond just the NLP domain in which it is tested (as CBOW style models have been), so ICLR seems like an appropriate venue. It seems like it s in the community s best interest to see a method like this be evaluated, and since this paper appears to offer a thorough and sound evaluation, I recommend acceptance.
The authors consider the problem of training a fair classifier on decentralized data, and compare three methods: training locally, training the proposed FedAvg algorithm with local fairness, and a global fairness approach.  The reviewers agreed that the setting was interesting and novel, but had concerns about the writing quality, experimental setup, and, most importantly, the organization of the paper, with several reviewers complaining that necessary information was relegated to the appendix.  Overall, this work is not quite ready for publication. With that said, the reviewers agreed that it was interesting and highly promising (it just needs refinement). Please seriously consider the reviewers  recommendations, which on the whole were very constructive and, if followed, should lead to a significant improvement in your manuscript.
Although the connection between randomized smoothing and PDE revealed in this paper is an interesting direction to explore, the method proposed unfortunately is not certified. The method could work as a good empirical defense since the smoothed classifier could be learned more efficiently. 
The paper develops a method for decomposing 3D scenes into objects by coupling NeRF decoders to representations produced by a slot based encoder.  After the discussion phase, reviewer ratings are mixed with three on either side of above/below threshold, and one higher (but low confidence) accept score.  Drawbacks include limited novelty, as stated by Reviewer 8UAh: "the unsupervised decomposition of the scene, which is somehow incremental, given that it s achieved by applying the slot based approach of [Locatello et al. 2020] to NeRFs".  Reviewer bAmB likewise mentions this issue.  Reviewer VrrK: "some of its contributions are on improving NeRFs while the decomposition part is rather marginal". Reviewer VrrK also raises concerns about lack of experiments on real data: "Since the proposed method is based on NeRF, how well does it work with real photographs?"  The AC agrees with the marginal rating of the reviewers and is particularly concerned with overall novelty of the proposed pipeline and question of applicability beyond simulated data.  More work seems required to solidify an experimental case on real images.
This paper proposes factorized prior distributions for CNN weights by using explicit and implicit parameterization for the prior. The paper suggest a few tractable methods to learn the prior and the model jointly. The paper, overall, is interesting.  The reviewers have had some disagreement regarding the effectiveness of the method. The factorized prior may not be the most informative prior and using extra machinery to estimate it might deteriorates the performance. On the other hand, estimating a more informative prior might be difficult. It is extremely important to discuss this trade off in the paper. I strongly recommend for the authors to discuss the pros and cons of using priors that are weakly informative vs strongly informative.  The idea of using a hierarchical model has been around, e.g., see the paper on "Hierarchical variational models" and more recently "semi implicit Variational Inference". Please include a related work on such existing work. Please discuss why your proposed method is better than these existing methods.  Conditioned on the two discussions added to the paper, we can accept it. 
This paper studies how two layer neural networks can learn DNFs. The paper provides some theoretical analysis together with empirical evidence.  The direction of analyzing how neural networks learn certain concept classes is definitely extremely important, and the authors do make some progress towards this direction. However, there are some major concerns about the paper:   In the main result, the authors seem to only able to prove that the learning process converges with exponentially many neurons (exponential in the input dimension, see 6.1 setup). With this many neurons, it is unclear whether the result is directly covered by existing works such as:    (a). "Learning and generalization in overparameterized neural networks, going beyond two layers". (b). "Fine grained analysis of optimization and generalization for overparameterized two layer neural networks"   These two works provide efficient optimization and generalization bounds w.r.t the complexity of the function. However, these works are still in the NTK regime. It would be nice if the authors can distinguish the current technique from NTKs by providing some theoretical guarantee that their main result is indeed more efficient than kernels (as they argue in the intro), the author can refer to:    (a) "What can ResNet efficiently learn, going beyond kernels" .  (b) "When Do Neural Networks Outperform Kernel Methods?"   With the current form of the draft, it is unclear how the result is better than existing approaches. The authors should address that in the next version of the paper.   Missing reference of NTKs: "A convergence theory for deep learning via over parameterization"   
In this paper, the problem of estimating the average of a moment function that depends on an unknown regression function. It heavily relies on prior papers by e.g. Chernozhukov et al. and the actual novel material consists of making these theoretical  results more practical. Experiments for two practical approaches based on neural networks respectively random forests are also reported.  Initially, the presentation of the paper was heavily criticized by the reviewers, but during the rebuttal phase at least some of the issues were removed. Together with some other improvements this lead to an increased average score. However, it seems fair to say that reading the other papers first, is still kind of necessary.  Despite the still unclear novelty the paper has some merits, which in principle make it acceptable. Compared to the other good papers in my batch, however, it is more incremental and the overall contribution is not as strong. For this reason I vote for rejection, but a comparison to other papers outside my batch is probably a good idea.
This paper studies the problem of dynamically selecting samples to replay given that all previous data is stored. The paper shows that in this setting, selecting which samples to replay outperforms several baselines over a variety of datasets.   I believe that the reviewers understood this work, but their initial opinions were quite mixed.   Two of the reviewers did not "accept" this setting (all past data stored and accessible) as a reasonable one for continual learning. The discussion did not lead to a reconciliation.   I found truth in both views. On one side, I can believe that the proposed setting has applications (recommender systems where historical data is kept seems like a reasonable one). I also find the approach reasonable since "compute" is often the bottleneck and not memory/storage. On the other, I also see that this is specializing the CL problem a bit and so, while immediately useful, may or may not help to improve more general continual learning approaches. This is highly speculative. Another argument against this setting is that it is not absolutely clear that in this setting CL approaches are necessarily required. This really depends on the specifics of the problems.  Several of the questions and weaknesses discussed by other reviewers were also discussed and addressed by the reviewers.   Overall, the final score from the reviewers makes this a very borderline paper. Further, even amongst the positive reviewers, one provides an overall recommendation of a 6 (marginally above the acceptance threshold). In the end, the paper was in the category of papers that were examined closely for possible acceptance, but the broad view of the area chair and the reviewers was that the paper could benefit from additional work before publication.
The authors consider planning problems with sparse rewards.                                                                         They propose an algorithm that performs planning based on an auxiliary reward                                                       given by a curiosity score.                                                                                                         They test they approach on a range of tasks in simulated robotics environments                                                      and compare to model free baselines.                                                                                                                                                                                                                                    The reviewers mainly criticize the lack of competitive baselines; it comes as now                                                   surprise that the baselines presented in the paper do not perform well, as they                                                     make use of strictly less information of the problem.                                                                               The authors were very active in the rebuttal period, however eventually did not                                                     fully manage to address the points raised by the reviewers.                                                                                                                                                                                                             Although the paper proposes an interesting approach, I think this paper is below                                                    acceptance threshold.                                                                                                               The experimental results lack baselines,                                                                                            Furthermore, critical details of the algorithm are missing / hard to find.
This paper proposes to use high dimensional representation for labels to strengthen the adversarial robustness of deep neural networks. Experimental results demonstrate that the proposed method improve adversarial robustness. All reviewer agree that the authors propose an interesting idea and this direction deserves further exploration. On the other hand, the reviewers also raise a serious question: There is a lack of explanation of why high dimensional representation of labels improve adversarial robustness. Therefore, it is not clear if the proposed method can defend refined attacks tailored to such dimensional label representation. The authors are highly encouraged to conduct deeper analysis, especially on the robustness against finer attacks.
As the title states (and reads somewhat like an openreview review title), the authors apply the options framework from the RL community to perform hierarchical RL where the option is the dialogue act and the subproblem is the NLG component in task oriented dialogue (TOD) policy learning. The two technical contributions (beyond the conceptual connection above) is showing that asynchronous updates between the hierarchy levels guarantees convergence and language model based discriminator to densify the reward structure. Empirical results are solid improvements over recent SoTA findings.     Pros   + This is a conceptually appealing application of RL to TOD and they authors had to make additional modifications to get it to work — which will help other researchers in this space. + There are both theoretical and empirical contributions. The theoretical contributions are also insightful and not superfluous to the problem being studied. + Using a language model based discriminator for reward shaping isn’t completely new (although I haven’t seen in this setting and stated exactly the same), but is interesting and effective.    Cons    + The writing could use significant work; while the reviewers/rebuttal cleared up many issues, I actually didn’t appreciate the value of this paper in my first read due to the writing (even if the motivation, etc. is sufficiently clear). + Human evaluation is treated as somewhat of an afterthought and there isn’t a deep dive into error analysis of the results. The visualization is a good first step, but there isn’t really a when/why this method works better than others, which is important for a problem where evaluation isn’t conclusive in the best cases. This is also significant since the authors claim ‘comprehensibility’.  Evaluating along the requested dimensions:    Quality: The conceptual and theoretical contributions are both of high quality. This is a promising approach to TOD and the authors additions (e.g., async optimization, LM reward shaping) are good examples of applied research. The empirical results are sufficient to above average, but not as strong (although this is partially an artifact of TOD evaluation)    Clarity: The motivation is good, but the paper could use some work in writing. Some examples include (1) stating precisely how the option choices are derived (latent variables), (2) mapping out notation in something like a preliminaries section, (3) sketch of proofs in the main body for continuity. If the reader is familiar with the closest cited work, it is a bit easier, but I think some effort in making the paper more self contained would increase its impact.   Originality: Options in HRL is widely known, but applying it to TOD is novel to the best of my (and the reviewers) knowledge. I think many could have come up with the basic idea, but it took some effort to get it to work.   Significance: This is a widely studied problem and the approach is fairly convincing. I don’t think it will be ‘disruptive’ or cross pollinate to other application areas, but will almost certainly be cited within the conversational agent community.  In summary, the reviewers like this paper a bit more than myself personally — I think it is borderline with a preference to accept while the reviews are a more confident accept. However, the reviewers are also experienced experts in this area. I also do think that the authors handled concerns well in the rebuttal stages and addressed my more pressing concerns. I would encourage the authors to improve the writing if accepted, but I would prefer to accept this if possible. 
This paper formalizes the problem of training deep networks in the presence of a budget, expressed here as a maximum total number of optimization iterations, and evaluates various budget aware learning schedules, finding simple linear decay to work well.   Post discussion, the reviewers all felt that this was a good paper. There were some concerns about the lack of theoretical justification for linear decay, but these were overruled by the practical use of these papers to the community. Therefore I am recommending it be accepted.
The paper provides a functional approximation of the error of ResNets and VGGs pruned with IMP and SynFlow on CIFAR 10 and ImageNet, showing that it is predictable in terms of an invariant tying width, depth, and pruning level.  In particular, it formulates the test error as a function of the density of the network after pruning and identifies a low density high error plateau, a high density low error plateau, and a power law behavior for intermediate density. It further demonstrates that networks of different sparsities are freely interchangeable. The paper provides an interesting insight on the power law structure of the error as networks are pruned, however the results are very limited to specific types of networks (ResNets and VGGs), pruning methods (IMP and SynFlow) and datasets (CIFAR 10, ImageNet). Hence, it s not clear if the proposed functional approximation generalizes to other network families, pruning methods, and datasets. I understand that adding a new architecture or dataset is expensive, but fitting the proposed scaling law (the five parameters) requires pruning only a small number of networks, as mentioned by the authors. Comparing the calculated error and the actual error of the pruned network for different architectures and datasets can help verify the findings in the paper, and significantly widens its scope.
This paper deals with a problem of feature compatible learning, where the features produced by new model should be compatible with old features. As pointed out by the reviewers, there are several weaknesses with this paper: (a) the novelty is not strong enough, (b) the experimental results should be better explained and be more thorough, (c) the formulation is not well motivated.
This paper makes a claim that the iid assumption for NN parameters does not hold. The paper then expresses the joint distribution as a Gibbs distribution and PoE. Finally, there are some results on SGD as VI. Reviewers have mixed opinion about the paper and it is clear that the starting point of the paper (regarding iid assumption) is unclear. I myself read through the paper and discussed this with the reviewer, and it is clear that there are many issues with this paper.  Here are my concerns:   The parameters of DNN are not iid *after* training. They are not supposed to be. So the empirical results where the correlation matrix is shown does not make the point that the paper is trying to make.   I agree with R2 that the prior is subjective and can be anything, and it is true that the "trained" NN may not correspond to a GP. This is actually well known which is why it is difficult to match the performance of a trained GP and trained NN.   The whole contribution about connection to Gibbs distribution and PoE is not insightful. These things are already known, so I don t know why this is a contribution.   Regarding connection between SGD and VI, they do *not* really prove anything. The derivation is *wrong*. In eq 85 in Appendix J2, the VI problem is written as KL(P||Q), but it should be KL(Q||P). Then this is argued to be the same as Eq. 88 obtained with SGD. This is not correct.  Given these issues and based on reviewers  reaction to the content, I recommend to reject this paper. 
The paper introduces a framework for learning dynamical system models from observations consisting of discrete spatio temporal series. It is composed of two components trained sequentially.  A first one learns embedding from observations using a seq2seq approach, where the embeddings are constrained to follow linear dynamics. This is inspired by approximation schemes for Koopman operators. These embeddings are then used as the spatio temporal series representions and are fed to a transformer trained as an autoregressive predictor. Experiments are performed on different problems using data generated from PDEs through numerical schemes. Comparisons are performed with different ML baselines.  The paper is well written with experiments on problems with different complexities. The original contributions of the paper are 1) the combination of pretrained embeddings with a transformer auto regressor, 2) a seq2seq architecture for learning time series representations constrained by linear dynamics.  On the cons side, the paper original contribution and significance are over claimed. Closely related ideas for learning approximate Koopman operators and observables have already been developed and used in similar contexts. Besides there is no discussion here on the properties or physical interpretability (which is often an argument for Koopman) of the learned representations. Then the baselines are mainly composed of simple regressors (LSTM, conv LSTM, etc.) and this is not a surprise that they cannot learn dynamics over long term horizons. There is no comparison with dynamical models incorporating numerical integration schemes that could model the temporal dynamics of the system. There is now a large literature on this topic exploiting discrete (ResNet like) or continuous formulations (as started with Neural ODE).
The paper proposes a new spatial temporal point cloud convolution. However, many reviewers suggest the paper can be improved with better baselines and motivations.
This paper proposes an approach for 8 bit fixed point training of NNs, based on a careful analysis of quantization error in fixed point methods. They present convincing and thorough empirical results in addition to a detailed analysis providing insights about their method. Reviews for this paper were quite split. One reviewer was a strong advocate, asserting that the paper will have substantial impact in the area, and that the authors’ approach of minimizing quantization error for fixed point training is of substantial practical interest. Other reviewers were concerned that the proposed method was not novel enough, and that the proposed approach was not practical enough to work in realistic hardware use cases. The authors provided substantial detailed responses addressing the majority of reviewers’ concerns, and after following the discussion in detail I agree with the reviewer advocating for the paper, that the paper presents a practical, novel approach with valuable insights for the field from their analysis and results.   I indicated I am certain about this decision, but I would be ok with the paper being bumped down from oral to poster.
This paper addresses the problem of program synthesis given input/output examples and a domain specific language using a bottom up approach. The paper proposes the use of a neural architecture that exploits the search context (all the programs considered so far and their execution results) to decide which program to evaluate next. The model is trained on policy using beam aware training and the method is evaluated on string manipulation and inductive logic programming benchmarks. The results show that the proposed method outperforms previous work in terms of the number of programs evaluated and accuracy.    Overall, the reviewers found the paper to be well written and the idea proposed to be significantly novel and interesting to be presented at the conference and I agree. Several limitations were pointed out by the reviewers in terms of (i) actual run time performance, (ii) the incompleteness of the search algorithm and the (iii) reproducibility of the approach. I believe the authors have addressed these points satisfactorily in their comments.
Overall, this paper receives positive reviews. The reviewers find the technical novelty and contributions are significant enough for acceptance at this conference. The authors  rebuttal helps address some issues. The area chair agrees with the reviewers and recommend it be accepted at this conference.
This paper studies ensemble calibration and the relationship between the calibration of individual ensemble member models with the calibration of the resulting ensemble prediction.  The main theoretical result is that individual ensemble members should not be individually calibrated in order to have a well calibrated ensemble prediction.  While other recent work has found this to be the case in empirical results, this paper substantiates the empirical results through theoretical results.   Pros: * Theoretical study of ensemble calibration with meaningful insights  Cons: * Contributions limited to theoretical study of known observation and dynamic temperature scaling. * Dynamic temperature scaling is not shown to outperform baseline methods. * Limited experimental validation: CIFAR 10/CIFAR 100.  The authors engaged in a extensive discussion with reviewers and made changes to their paper, including adding standard deviation results over multiple runs and the SKCE calibration measure.  Overall this is solid work and could be accepted to the conference; however, reviewers agree that parts of the work are lacking, in particular: 1. limited experimental evaluation (one type of task, one/two datasets only), and 2. given known literature the benefit of the derived theoretical results to practioners is not clear.  The discussions have been unable to resolve this disagreement. 
This paper proposes two methods to learn the architecture of normalizing flows models; Their framework is inspired by (Liu et al., 2019) which uses ensembles/mixtures with learnable weights for architecture search. The application of these ideas to NFs requires a trivial modification to respect the invertibility constraint; which consists in building a mixture model over all possible sequences of compositions of transformations from a fixed set.  The paper proposes to use an upper bound to the forward KL instead of the fKL directly. The reasoning is that this will lead to a "pure" model after optimization, that is, the mixture weights will be in {0, 1}. Mathematically, this simply corresponds to treating the mixture as a latent variable model and performing MAP inference over discrete latent variables, assuming that all mixture components have the same prior weights in the mixture.  The experimental results across various datasets are very mixed, and the family of transformations considered in the experiments is quite restricted.
To borrow the succinct summary from R1, "the paper suggests a method for generating representations that are linked to goals  in reinforcement learning. More precisely, it wishes to learn a representation so that two states are similar if the  policies leading to them are similar." The reviewers and AC agree that this is a novel and worthy idea.  Concerns about the paper are primarily about the following. (i) the method already requires good solutions as input, i.e., in the form of goal conditioned policies, (GCPs) and the paper claims that these are easy to learn in any case. As R3 notes, this then begs the question as to why the actionable representations are needed. (ii) reviewers had questions regarding the evaluations, i.e., fairness of baselines, additional comparisons, and  additional detail.   After much discussion, there is now a fair degree of consensus.  While R1 (the low score) still has a remaining issue with evaluation, particularly hyperparameter evaluation, they are also ok with acceptance. The AC is of the opinion that hyperparameter tuning is of course an important issue, but does not see it as the key issue for this particular paper.  The AC is of the opinion that the key issue is issue (i), raised by R3. In the discussion, the authors reconcile the inherent contradiction in (i) based on the need of additional downstream tasks that can then benefit from the actionable representation, and as demonstrated in a number of the evaluation examples (at least in the revised version). The AC believes in this logic, but believes that this should be stated more clearly in the final paper. And it should be explained the extent to which training for auxiliary tasks implicitly solve this problem in any case.  The AC also suggests nominating R3 for a best reviewer award.
This paper addresses the problem of differential private data generator. The paper presents a novel approach called G_PATE which builds on the existing PATE framework. The main contribution is in using a student generator with an ensemble of teacher discriminators and in proposing a new private gradient aggregation mechanism which ensures differential privacy in the information flow from discriminator to generator.  Although the idea is interesting, there are significant concerns raised by the reviewers about the experiments and analysis done in the paper which seem to be valid and have not been addressed yet in the final revision. I believe upon making significant changes to the paper, this could be a good contribution. Thus, as of now, I am recommending a Rejection.
This paper improves on previous work (adv BNN) with hierarchical variational inference. It observes that mean field VI training for BNNs often result in close to deterministic approximate posterior distributions for weights, which effectively makes the BNN closer to deterministic neural network, thereby loosing the robustness advantage of stochastic neural networks. To address this, a hierarchical prior is proposed on the weights, which, together with the corresponding approximate posterior design, aims at preventing the collapse of the variances of the weights towards zero. This improved version of adv BNN is shown to be reasonably better than the original adv BNN and their deterministic counter part against the PGD and EOT attacks on a various of benchmark dataset in the adversarial robustness literature.  Reviewers initially had questions about whether the comparison is fair to the original adv BNN since the reported results were very different. This issue has been addressed by the authors during the author feedback period, after that reviewers agreed that the proposed approach is a good extension of adv BNN towards making it more robust. They also agree that the analysis of the original adv BNN in terms of posterior variance collapse is interesting and potentially useful, although they also pointed out the link of increased variance (with the proposed method) and better uncertainty estimation is unclear.  In revision, I would encourage the authors to clear up the confusions of the reviewers by clearly stating the comparison setting with the original adv BNN, and better clarify the methodology.
The paper attempts to make transformers more scalable for longer sequences. In this regards, authors propose a clustering based attention mechanism, where only tokens attends to other tokens in the same cluster. This design reduces memory requirements and allows more information mixing than simple local windows. Using the proposed approach, new state of the art performance is obtained on Natural Questions long answer, although marginal. However, reviewers raised numerous concerns. First, the novelty of the paper compared to prior work like reformer or routing transformer which also conceptually does clustering is not resolved. Second, the claim that k means yields a more balanced/stable clustering than LSH is not well established. Finally, why clustering, i.e. attention between similar vectors is better than dissimilar or randomly chosen vectors or does is it even as expressive is not clear. Thus, unfortunately I cannot recommend an acceptance of the paper in its current form to ICLR.
This paper proposes a new measure to detect memorization based on how well the activations of the network are approximated by a low rank decomposition. They compare decompositions and find that non negative matrix factorization provides the best results. They evaluate of several datasets and show that the measure is well correlated with generalization and can be used for early stopping. All reviewers found the work novel, but there were concerns about the usefulness of the method, the experimental setup and the assumptions made. Some of these concerns were addressed by the revisions but concerns about usefulness and insights remained. These issues need to be properly addressed before acceptance.
this is a meta review with the recommendation, but i will ultimately leave the final call to the programme chairs, as this submission has a number of valid concerns.  the proposed approach is one of the early, principled one to using (fixed) dense vectors for computing the predictive probability without resorting to softmax, that scales better than and work almost as well as softmax in neural sequence modelling. the reviewers as well as public commentators have noticed some (potentially significant) short comings, such as instability of learning due to numerical precision and the inability of using beam search (perhaps due to the sub optimal calibration of probabilities under vMF.) however, i believe these two issues should be addressed as separate follow up work not necessarily by the authors themselves but by a broader community who would find this approach appealing for their own work, which would only be possible if the authors presented this work and had a chance to discuss it with the community at the conference. therefore, i recommend it be accepted. 
Overall, there were significant concerns about the motivation and experiments in this paper, and these were thought not to merit acceptance on their own. Because of this, the reviewers started discussing the theory to see if that would justify acceptance. The reviewers were not able to find a clear advantage over existing approaches, nor sufficient motivation; also the presentation was found to be largely inaccessible. In the rebuttal there was a brief mentioning of background and possible implications, but they were hard to assess and the paper itself did not have such context nor was updated to have such context. For a future version, one recommendation could be to focus significantly more on context, motivation, and improvements over prior work. Also, making the paper more self contained could help. 
This paper proposes a simple approach to improve the robustness of training a sparsely gated mixture of experts model, which at a high level simply consists in training initially as a dense gated model, to better warm start a final phase of sparse training. Results are presented to highlight the potential benefits of this approach.  The authors have provided a detailed response and updated results, in response to the reviews. Each reviewer has also responded at least once to the author response. Despite that engagement, all reviewers are leaning towards rejection (though there is one reviewer with a rating of 6, they regardless state that "I m confident this will make a great resubmission at a future venue", indicating they actually support rejection).  The reviewers point out that the proposed method is not really novel, pointing to an existing recent paper. Even without that prior work, I would also argue that the proposed approach is conceptually straightforward and has benefits that were fairly predictable and not particularly surprising. Given the generally lukewarm reception from the reviewers, I think there is a legitimate concern to be had here about this work s potential for impact.  Though the review process has definitely improved the paper s manuscript since its submission, I unfortunately could not find a reason to dissent from the reviewers  consensus that this submission is not ready to be published. Therefore recommend it be rejected at this time.
There was some slight disagreement on the paper, but the majority of reviewers agree that although some answers of the authors on questions brought good clarification, other issues still remain problematic. Some of the assumptions remain unclear (w.r.t CDTE), and reviewers still have doubts about the global convergence and weak stable fixed point concept, that lack clear math details. The experiments are also still a bit too immature, more comparison is needed, as well as an evaluation on other domains.
The authors argue that directly optimizing the IS proposal distribution as in RWS is preferable to optimizing the IWAE multi sample objective. They formalize this with an adaptive IS framework, AISLE, that generalizes RWS, IWAE STL and IWAE DREG.   Generally reviewers found the paper to be well written and the connections drawn in this paper interesting. However, all reviewers raised concerns about the lack of experiments (Reviewer 3 suggested several experiments that could be done to clarify remaining questions) and practical takeaways.   The authors responded by explaining that "the main "practical" takeaway from our work is the following: If one is interested in the bias reduction potential offered by IWAEs over plain VAEs then the adaptive importance sampling framework appears to be a better starting point for designing new algorithms than the specific multi sample objective used by IWAE. This is because the former retains all of the benefits of the latter without inheriting its drawbacks." I did not find this argument convincing as a primary advantage of variational approaches over WS is that the variational approach optimizes a unified objective. At least in principle, this is a serious drawback of the WS approaches. Experiments and/or a discussion of this is warranted.  This paper is borderline, and unfortunately, due to the high number of quality submissions this year, I have to recommend rejection at this point. 
Overview: This paper introduces a maximum mutual information method for helping to coordinate RL agents without communication.  Discussion: Some reviewers leaned towards accept, but I found the two reviewers recommending rejecting to be more convincing.  Recommendation: This is an important research topic and I m glad this paper is focusing on the problem. Hopefully the reviews will help improve a future version of this paper. I agree that this is a new way of using mutual information, but it seems more like a small improvement rather than a very significant step forward.   In addition, I think the setting needs to be better motivated. This is a centralized training with decentralized execution (CTDE) setting, and this paper helps the agents coordinate. In CTDE, the agents work in the environment and then pool their information to train before deploying on the next episode. I don t understand why, e.g., in multiwalker, agents would not be able to communicate while walking, can communicate after they succeed or drop the object (the episode ends), and then cannot communicate once the next episode starts.
Thanks for your submission to ICLR.  This paper explores zero shot adaptation from a theoretical perspective.  Three of the four reviewers are quite positive about the paper, particularly after the discussion phase.  One reviewer was more negative, citing a lack of compelling experiments and some possibly restrictive assumptions.  The authors responded to these concerns, as well as the concerns of the other reviewers.  One of the more positive reviewers increased their score from 6 to 8.  I did not hear from the negative reviewer, but my feeling is that I tend to agree with the authors that the focus of the paper is more on the theoretical side.  Moreover, the authors did add some additional results to the main paper, so I am of the opinion that the paper should indeed be accepted to the conference.  Even though this is paper is on the theoretical side, please do include as strong a set of empirical results as possible in the final version.  Also keep in mind the other suggestions from the reviewers when preparing the final manuscript.
In federated learning, distributed and resource limited client nodes cooperatively train a model without sharing their local data. The results thus far on analyzing the  convergence of federated learning are restricted to “unbiased” client participation, where the probability of a client c being selected is proportional to c’s data size. This work presents the first convergence analysis of federated learning for biased client selection, and quantifies the impact of selection skew on time to convergence. Specifically, biasing toward clients with higher local loss is shown to be beneficial, and a protocol is developed based on this, to trade between convergence time and solution bias.  The paper is in general well written, and develops a natural idea.   The strong convexity assumption is a concern: how much can it be weakened? The authors are also asked to run experiments systematically on (much) larger datasets. The test accuracy and possible overfitting concerns also need to be addressed in more depth. The authors are also encouraged to see how much Assumption 3.4 uniformly bounded stochastic gradients can be dispensed with.  
This paper connects MAML to contrastive learning under some simplifying assumptions and with slight modifications in the setting.  Specifically, the authors show that if the inner loop updates are only applied on the top linear layer, MAML is equivalent to supervised contrastive learning (SCL). This means that MAML learns a feature transformation that brings in class representations closer and representations across classes far away. The zeroing trick the authors propose seems to give some performance gain in the experiments and is an actionable insight from their theory.   Overall the paper is very interesting and (as far as I know) novel. The proposed zeroing trick is supported by theory and experiments and is justifies the previous theoretical narrative.   Some reviewers raised concerns on motivation and numerous clarification questions that the authors have addressed to a large extent in my opinion.
At a high level, the novelty of this paper is limited: RL2 with transformers instead of RNNs. The emphasis is then placed on the experimental evaluation. Unfortunately, the reviewers felt that the experimental methodology and results were not strong enough at this stage to warrant publication. During the rebuttal, the reviewers did not engage nor discuss the author response, unfortunately, so I do not know what they think of the rebuttal. However, on evaluating the concerns of the reviewers against the updated manuscript, I think the updates do not go far enough to satisfy the concerns raised (experiments + baselines). Therefore, I recommend rejection.
The reviewers were uniformly unimpressed with the contributions of this paper. The method is somewhat derivative and the paper is quite long and lacks clarity. Moreover, the tactic of storing autoencoder variables rather than full samples is clearly an improvement, but it still does not allow the method to scale to a truly lifelong learning setting. 
This paper proposes a GNN that uses global attention based on graph wavelet transform for more flexible and data dependent GNN feature aggregation without the assumption of local homophily.  Three reviewers gave conflicting opinions on this paper. The reviewer claiming rejection questioned the novelty of the paper and the complexity of the global attention mentioned in the paper. Even through the authors  responses and subsequent private discussions, concerns about complexity and novelty were not completely resolved.   Considering the authors  claim that the core contribution of this paper is to design fully learnable spectral filters without compromising computational efficiency, it is necessary to consider why it is meaningful to perform global attention based on graph wavelet transform in the first place. In terms of complexity, although the wavelet coefficient can be efficiently calculated using the Chebyshev polynomials mentioned by the authors, in the attention sparsification part, n log n is required **for each node** in sorting, resulting in complexity of n^2 or more. There may still be an advantage of complexity over using global attention in a message passing architecture, but it will be necessary to clarify and verify that, given that the proposed method uses an approximation that limits global attention within K hops.  Also, this paper modifies the graph wavelet transform in graph theory, which requires a deeper discussion. For example, as the authors mentioned, the original wavelet coefficient psi_uv can be interpreted as the amount of energy that node v has received from node u in its local neighborhood. The psi_uv defined by the learnable filter as shown in Equation 3 has a different meaning from the original wavelet coefficient. There is insufficient insight as to whether it is justifiable to use this value as an attention coefficient.  Overall, the paper proposes potentially interesting ideas, but it seems to require further development for publication.
The paper provides a simple method of active learning for classification using deep nets. The method is motivated by choosing examples based on an embedding computed that represents the last layer gradients, which is shown to have a connection to a lower bound of model change if labeled. The algorithm is simple and easy to implement. The method is justified by convincing experiments.   The reviewers agree that the rebuttal and revisions cleared up any misunderstandings.  This is a solid empirical work on an active learning technique that seems to have a lot of promise. Accept.  
The reviewers expressed some interest in this paper, but overall were lukewarm about its contributions. R4 raises a fundamental issue with the presentation of the analysis (see the D_infty assumption). The AC thus goes for a "revise and resubmit".
This paper has been reviewed with four expert reviewers. The reviewers have reached the consensus that the paper is not yet ready for publication. The main concerns are related to novelty. All reviewers gave substantial and constructive feedback. Following the recommendation of the reviewers, the meta reviewer recommends rejection.
This paper proposes a novel stochastic gradient Markov chain Monte Carlo method incorporating a cyclical step size schedule (cyclical SG MCMC).  The authors argue that this step size schedule allows the sampler to cross modes (when the step size is large) and locally explore modes (when the step size is smaller).  SG MCMC is a very promising method for Bayesian deep learning as it is both scalable and easily to incorporate into existing models.  However, the stochastic setting often leads to the sampler getting stuck in a local mode due to a requirement of a small step size (which itself is often due to leaving out the Metropolis Hastings accept / reject step).   The cyclic learning rate intuitively helps the sampler escape local modes.  This property is demonstrated on synthetic problems in comparison to existing SG MCMC baselines.  The authors demonstrate improved negative log likelihood on larger scale deep learning benchmarks, which is appreciated as the related literature often restricts experiments to small scale problems.  The reviewers all found the paper compelling and argued for acceptance and thus the recommendation is to accept.  Some questions remain for future work.  E.g. all experiments were performed using a very low temperature, which implies that the methods are not sampling from the true Bayesian posterior.  Why is such a low temperature needed for reasonable performance?  In any case a very nice paper.
The submission provides an interesting way to tackle the so called distributional shift problem in machine learning. One familiar example is unsupervised domain adaptation. The main contribution of this work is deriving a bound on the generalization error/risk for a target domain as a combo of re weighted empirical risk on the source domain and some discrepancy between the re weighted source domain and the target domain. The authors then use this to formulate an objective function.  The reviewers generally liked the paper for its theoretical results, but found the empirical evaluation somewhat lacking, as do I. Especially the unsupervised domain adaptation results are very toy ish in nature (synthetic data), whereas the literature in this field, cited by the authors, does significantly larger scale experiments. I am unsure as to how much I value I can place in the IHDP results since I am not familiar with the benchmark (and hence my lower confidence in the recommendation).  Finally, I am not very convinced that this is the appropriate venue for this work, despite containing some interesting results.
In this paper, authors study adversarial examples from a distributional robustness point of view. Reviewers had several concerns about the work and all thought the paper is not above the accept threshold. In particular, they mentioned that the presentation and writing of the paper need to be improved and results (specially the ones presented in Section 2) are not significant contributions and novel. Given all, I think the paper needs more work before being accepted.
The paper presents a domain adaptation approach based on the importance weighting for unsupervised cross lingual learning. The paper first analyzes factors that affect cross lingual transfer and finds that the cross lingual transfer performance is strongly correlated with feature representation alignments as well as the distributional shift in class priors between the source and the target. Then the paper designs an approach based on the observations.  Pros:  + The paper is well written and the proposed approach is well motivated.  + The analysis about which factors affect cross lingual transfer is interesting and provides some great insight.   Cons:    As the reviewer pointed out, the experiments for verifying the proposed approach are relatively weak.  Overall, the paper presents nice insights to connect cross lingual transfer with domain adaptation. All reviewers lean to accept the paper and I also found the paper is in general interesting.
Though the general direction is interesting and relevant to ICLR, the novelty is limited. As reviewers point out it is very similar to Le & Zuidema (2015), with few modifications (using LSTM word representations, a different type of pooling). However, it is not clear if they are necessary  as there is no direct comparison (e.g., using a different type of pooling). Overall, though the submission is generally solid,  it does not seem appropriate for ICLR.  + solid + well written   novelty limited   relation to Le & Zuidema is underplayed
This work presents a simple technique for improving the latent space geometry of text autoencoders. The strengths of the paper lie in the simplicity of the method, and results show that the technique improves over the considered baselines. However, some reviewers expressed concerns over the presented theory for why input noise helps, and did not address concerns that the theory was useful. The paper should be improved if Section 4 were instead rewritten to focus on providing intuition, either with empirical analysis, results on a toy task, or clear but high level discussion of why the method helps. The current theorem statements seem either unnecessary or make strong assumptions that don t hold in practice. As a result, Section 4 in its current form is not in service to the reader s understanding why the simple method works.  Finally, further improvements to the paper could be made with comparisons to additional baselines from prior work as suggested by reviewers.
The paper proposes a discretization of Wasserstein gradient flow with an euler scheme, and propose a way to estimate each step of the euler scheme using ratio estimators from samples regularized with gradient penalties. Statistical bounds are given to bound the estimated flows from the wasserstein flow.    Reviewers have raised concerns regarding the assumptions under which results present in the paper hold, this was clarified by the authors (goedesic lambda convexity, log sobolev constant for the target density . lipchitizity of velocity fields).  The paper needs a revision to incorporate that feedback and to be in shape for publication.   Other concern were on earlier claims in the paper regarding the monge ampere equation and approximation of the optimal mapping this was addressed by the rebuttal.   Other concerns were also on explaining the relation of the work to score based models and energy based models.   Overall the paper needs to state in a clearer way the assumptions for the theoretical results and to acknowledge the limitations of those assumptions in analyzing the euler scheme.   
While all reviewers acknowledge the relevance of such an evaluation work for the MRI reconstruction field, they all agree that the contribution has a limited fit with a ML conference like ICLR. The work is solid experimentally and will surely interest the audience of conferences like ISMRM or MICCAI. For this reason, the work can unfortunately not be endorsed for publication.
This paper proposes a meta learning algorithm that extends MAML, particularly focusing on multimodal task distributions. The paper is generally well written, especially with the latest revisions, and the qualitative experiments show some interesting structure recovered. The primary weakness of the paper is that the experiments are largely on relatively simple benchmarks, such as Omniglot and low dimensional regression problems. Meta learning papers with convincing results have shown results on MiniImagenet, CIFAR, CelebA, and/or other natural image datasets. Hence, the paper would be more compelling with more difficult experimental settings. In the paper s current form, the reviewers and the AC agree that it does not meet the bar for ICLR.
This paper investigates the problem of using zero imputation when input features are missing. The authors study this problem, propose a solution, and evaluate on several benchmark datasets. The reviewers were generally positive about the paper, but had some questions and concerns about the experimental results. The authors addressed these concerns in the rebuttal. The reviewers are generally satisfied and believe that the paper should be accepted.
The authors develop a novel connection between information theoretic MPC and entropy regularized RL. Using this connection, they develop Q learning algorithm that can work with biased models. They evaluate their proposed algorithm on several control tasks and demonstrate performance over the baseline methods.  Unfortunately, reviewers were not convinced that the technical contribution of this work was sufficient. They felt that this was a fairly straightforward extension of MPPI. Furthermore, I would have expected a comparison to POLO. As the authors note, their approach is more theoretically principled, so it would be nice to see them outperforming POLO as a validation of their framework.  Given the large number of high quality submissions this year, I recommend rejection at this time.
Nice language modeling paper with consistently high scores. The model structure is neat and the results are solid. Good ICLR type paper with contributions mostly on the ML side and experiments on a (simple) NLP task.
The paper proposes an algorithm with sublinear regret for the problem of routing users through a network with unknown congestion functions over an infinite time horizon. The reviewers generally appreciated the main contribution of this work. One of the reviewers also felt that, although it may be possible to obtain the main result using more standard techniques, it is not clear whether doing so is an easy extension of the prior work. Following the discussion, all of the reviewers agreed that the paper missed important related work and it needs a major revision that incorporates the extensive feedback of Reviewer 2. For these reasons, I recommend reject.
This paper proposes a communication efficient data parallel SGD with quantization. The method bridges the gap between theory and practice. The QSGD method has theoretical guarantees while QSGDinf doesn t, but the latter gives better result. This paper proves stronger results for QSGD using a different quantization scheme which matches the performance of QSGDinf.  The reviewers find issues with the approach and have pointed some of them out. During the discussion period, we did discuss if reviewers would like to raise their scores. Unfortunately, they still have unresolved issues (see R1 s comment).  R1 made another comment recently that they were unable to add to their review: "The proposed algorithm and the theoretical analysis does not include momentum. However, in the experiments, it is clearly stated that momentum (with a factor of 0.9) is used. Thus, it is unclear whether the experiments really validate the theoretical guarantees. And, it is also unclear how momentum is added for both NUQSGD and EF SGD, since momentum is not mentioned in Algorithm 1 in this paper, or the paper of QSGD, or the paper of EF SignSGD. (There is a version of SignSGD with momentum *without* error feedback, called SIGNUM)."  With the current score, the paper does not make the cut for ICLR, but I encourage the authors to revise the paper based on reviewers  feedback. For now, I recommend to reject this paper.
This paper proposes a general framework to study the limit behavior of neural models with respect to the scaling of hyperparameters in terms of network width, which covers existing mean field (MF) and neural tangent kernel (NTK) limits, as well as other new limit models that were not discovered before. While the reviewers agree that the study of limiting behavior of neural network models is of great importance and could be a good addition to the current understanding of NTK and MF, there is a technical flaw in the proof (regarding Condition 1) pointed by the reviewer. After reviewer discussion, all reviewers agree that this is a serious issue, and needs to be addressed before publication. I believe it could be a strong paper if the technical flaw can be fixed. I encourage the authors to revise the paper and resubmit it to the next conference. 
This submission received 4 diverging ratings: 3, 5, 5, 6. On the positive side, reviewers appreciated the novelty of the approach and strong empirical performance. At the same time, all negatively inclined reviewers mentioned unfair comparisons with baselines (which was partially addressed in the rebuttal), flaws in the evaluation protocols and ablations not fully supporting claims made in the paper. After discussions with the authors most reviewers decided to stick with their original ratings. AC agrees that the remaining open questions around empirical validation will need to be answered more clearly before the paper can be accepted. The final recommendation is reject.
The paper presents a sound and efficient (but not complete) algorithm for verifying that a piecewise linear neural network is constant in an Lp ball around a given point. This is a significant contribution towards practical protection from adversarial attacks with theoretical guarantees. The proposed algorithm is shown to be sound (that is, when it returns a result, that result is guaranteed to be correct) and efficient (it is easily parallelizable and can scale to large networks), but is not complete (there exist cases where the algorithm will return "I don t know"). The experiments show good results in practice. The reviewers are positive about the paper, and most initial concerns have been addressed in the rebuttal, with the paper improving as a result. Overall, this is an important contribution worth communicating to the ICLR community, so I m happy to recommend acceptance.
Reviewers concur that the paper and the application area are interesting but that the approaches are not sufficiently novel to justify presentation at ICLR. 
This paper presents a way to use GNNs to learn edge weights of a coarsened graph given the node mapping from the original graph to the coarsened graph.  The paper is well written and the approach is well motivated as learning makes it easy to adapt the edge weights to different tasks and objectives, as illustrated in the graph Laplacian and Rayleigh quotient examples.  All the reviewers gage positive reviews for this paper, hence I recommend accepting this paper.  The reason for not promoting this paper further to spotlight or oral is that the paper addressed a relatively small problem, learning the edge weights given the node mapping, and the proposed method is quite simple.  Therefore this paper’s impact could be limited.  One suggestion to the authors is to present more results on downstream tasks, i.e. how does the proposed coarsening algorithm improve downstream task performance, instead of just losses defined without a downstream task in mind.  Example things to consider: does this approach improve graph classification accuracy?  Does this improve downstream GNN model’s efficiency without sacrificing accuracy?
The paper proposes a method that does uncertainty modeling over missing data imputation using a framework based on generative adversarial network. While the method shows some empirical improvements over the baselines, reviewers have found the work incremental in terms of technical novelty over the existing GAIN approach which renders it slightly below the acceptance threshold for the main conference, particularly in case of space constraints in the program. 
This paper proposes a method for efficiently training neural networks combined with blackbox implementations of exact combinatorial solvers.  Reviewers and AC agree that it is a well written paper with a novel idea supported by good experimental results. Experimental results are of small scale and can be further improved, but the authors acknowledged this aspect well.  Hence, I recommend acceptance.
The paper considers quantization issues for learned neural network based image compression methods.  Many works on the topic incorporate quantization into the training of the method. The paper provides evidence that post training quantization is effective. Specifically, the paper demonstrates that state of the art learned image compression methods can be quantized post training and retain a very similar level of compression performance. The paper argues that this is important in particular for cross platform applications, where an image is decompressed on different architectures. Finally, the paper proposes an approach to discretize entropy parameters.  The reviewers raised the following concerns.    Reviewer 2XDr is concerned about the application of post training quantization being a contribution, since post training quantization has already been studied in [1] (and in the recent paper [2] that can be considered as concurrent work). The authors response is that the methods in [1] has extra overhead and clarify how the prior work is in fact different. This addressed the reviewer s concern, and the reviewer raised their score.    Reviewer eyVf finds the comparisons with previous methods to be insufficient, and in general find the value of the research unclear, as the goals are not sufficiently specified. The authors clarified, and the reviewer was satisfied with the response and raised the rating to marginal above the threshold.     Reviewer L7dn tends towards acceptance, but has concerns about the technical novelty, that are unspecified unfortunately.    Reviewer oV3R argues that the solution is marginal relative to prior work, and votes to reject the paper. The authors responded why they think it isn t, and also wrote a private letter to the area chair in which they explain why they think that reviewer s oV3R should not be taken into account. I agree with the authors that the paper under review provides a step relative to Balle et al (2019), and that the writing of the paper is not an issue; however, the reviewer s overarching point is that the overall contribution is marginally significant when taking the prior work by Balle et al (2019) into account and this is the sentiment of other reviewers as well.    Reviewer GrpS, an expert on image compression, leans towards acceptance and argues that the results are strong as they show little to no loss due to the quantization technique, but also rates the contribution to only be marginally significant and novel, and raises a few questions and issues, to which the reviewers responded.   This paper is really borderline. Four out of five reviewers rate this paper as marginally above the acceptance threshold. The consensus is that while the experiments and claims are correct, the contribution is only marginally significant or novel, in particular, relative to prior work, and therefore I recommend rejecting the paper. I would, however, not be upset if it would be accepted.
This paper proposes a modular RNN architecture called SCOFF. The work was inspired by cognitive science(object file and schema) and was built upon previous work RIMs. The method is validated on tasks having multiple objects of the same type.  Pros:   It addresses an important problem in DNN   systematic generalization.   The proposal makes sense and is more flexible than RIM.   Experimental results outperform baselines.  Cons before rebuttal:   The presentation of the algorithm is not very clear due to some confusing notations and missing details of algorithm steps.   The comparison with baselines might not be fair due to extra parameters.   The novelty is limited, because the only difference from RIM is weight sharing.  The reviewers raised concerns listed in Cons. The authors successfully addressed concerns: they indicated that the comparison was fair with the same input to both; SCOFF is more flexible than RIM, and there is spatial attention to input. The authors added the missing details in the revised version.  All reviewers agree that the problem is important and the idea is interesting.  Since the authors  rebuttal was very helpful in clarifying the questions raised, I recommend accept. 
This paper proposes a k NN smoothing procedure for dealing with the problem of churn prediction. The idea is interesting and is based on theoretical foundations. The reviews have raised some limitations in the significance and in the experiments. The rebuttal provided by the authors have addressed some concerns. However, the new experimental evaluation have raised new concerns about the results, in particular with respect to results given in Table 3. Some typo may exist, but even some doubts remain on the experimental evaluation and results and thus on the effectiveness of the results. Authors  rebuttal was too late to allow another round of discussion. Considering the current concerns and uncertainties on the paper, I have to recommend rejection.
The main contribution of the paper is to perform a systematic and large study of self training as a method to deal with distribution shifts. Reviewers have appreciated the clarity in the overall writing of the paper, and rigor in the empirical analysis. However the main concern from two of the reviewers is that the technical contributions of the paper are only marginal and incremental in nature. The premise that self learning improves robustness is already somewhat well established (Reviewer PUq6 has pointed out papers that focus on how self training / self learning improves distribution shift and how self training and pre training stack together), and the main contribution of the paper is a systematic application to different datasets. Given the existing work on the relevance of self training in distribution shift, the paper falls below the acceptance bar for ICLR in my opinion.
This paper provides an interesting study on the adversarial robustness comparisons between ViTs and CNNs, and successfully challenges the previous belief that ViTs are always more robust than CNNs on defending against adversarial attacks. Specifically, as revealed in this paper, when the attacker considers the attention mechanisms, the resulting patch attack can hurt ViTs more.   Overall, all the reviewers enjoy reading this paper and appreciate the comprehensive robustness comparisons between ViTs and CNNs. The reviewers were concerned about the missing experiments about adversarial training, vague statements about the inspiration for future defenses, visualization of adversarial examples, etc. All these concerns are well addressed during the discussion period, and all reviewers reach a consensus on accepting this paper.  The final version should include the experiments, visualizations, and clarifications provided in the rebuttal. In addition, please release the code as promised.
This paper proposes a method for compressing weight matrices in large scale pre trained NLP encoders (like BERT) through low rank decompositions of both fully connected and self attention layers. The method is used to compress and speedup pre trained models. Experiments measure timing on a single CPU thread and demonstrate speedups with small loss of accuracy. Reviewers noted the that goal of this paper is potentially impactful. Some reviewers viewed the resulting loss in accuracy as marginal, while others viewed it as more substantial   a potential downside. Reviewers also raised concerns about the methodology used to measure inference speedup, a critical measure of success. Specifically, timing experiments were done only on a single CPU thread   while most practical scenarios would almost certainly rely on GPUs   as a result, positive experimental results are less impactful. Authors updated the paper to include GPU timing experiments, which did show speedups   though only marginal speedups over the baseline, TinyBERT. Further, reviewers pointed out that there are several other relevant baselines on compression approaches that are not compared with, and that further analysis should be done on the timing/accruacy tradeoff of baseline methods. Finally, reviewers felt that the contribution of the proposed method relative to other approaches that also attempt to compress transformers is not clearly outlined. Weighing these concerns, I agree with reviewers that the paper is not ready for acceptance in its current form. 
This work proposes and interesting approach to learn approximate set membership. While the proposed architecture is rather closely related to existing work, it is still interesting, as recognized by reviewers. Authors s substantial rewrites has also helped make the paper clearer. However, the empirical merits of the approach are still a bit limited; when combined with the narrow novelty compared to existing work, this makes the overall contribution a bit too thin for ICLR. Authors are encouraged to strengthen their work by showing more convincing practical benefit of their approach.
The paper trains a classifier to decide if a program is a malware and when to halt its execution. The malware classifier is mostly composed of an RNN acting on featurized API calls (events). The presentation could be improved. The results are encouraging, but the experiments lack solid baselines, comparisons, and grounding of the task usefulness, as this is not done on an established benchmark.
although the way in which the authors characterize existing rnn variants and how they derive a new type of rnn are interesting, the submission lacks justification (either empirical or theoretical) that supports whether and how the proposed rnn s behave in a "learning" setting different from the existing rnn variants.
While there was some interest in the ideas presented, this paper was on the borderline, and was ultimately not able to be accepted for publication at ICLR.  Reviewers raised concerns as to the novelty, generality, and practicality of the approach, which could have been better demonstrated via experiments.
The authors propose a framework for for the certification of reinforcement learning agents against adversarial observation/state perturbations based on randomized smoothing. They develop the theory of the framework, demonstrating that the framework can be used to certify lower bounds on the worst case cumulative reward of an agent. They validate their theoretical bounds experimentally.  The paper is well written and reviewers were mostly in agreement that the contributions are worthy of acceptance. The technical concerns from reviewer zGtv were addressed during the discussion phase, but I strongly encourage the authors to revise the manuscript to address the points raised in the discussion.
The paper proposes a novel  lossless compression scheme that leverages latent variable models such as VAEs. Its main original contribution is to improve the bits back coding scheme [B. Frey 1997] through the use of asymmetric numeral systems (ANS) instead of arithmetic coding. The developed practical algorithm is also able to use continuous latents. The paper is well written but the reader will benefit from prior familiarity with compression schemes. Resulting message bit length is shown empirically to be close to ELBO on MNIST. The main weakness pointed out by reviewers is that the empirical evaluation is limited to MNIST and to a simple VAE, while applicability to other models (autoregressive) and data (PixelVAE on ImageNet) is only hinted to and expected bit length merely extrapolated from previously reported log likelihood. The work could be much more convincing if its compression was empirically demonstrated on larger and better models and larger scale data. Nevertheless reviewers agreed that it sufficiently advanced the field to warrant acceptance.
This paper presents an approach to improving the calculation of embeddings for nearest neighbor search with respect to edit distance.  Reading the reviews, it seems that the paper is greatly improved over its previous version, but still has significant clarity issues. Given that these issues remain even after one major revision, I would suggest that the paper not be accepted for this ICLR, but that the authors carefully revise the paper for clarity and submit to a following submission opportunity. It may help to share the paper with others who are not familiar with the research until they can read it once and understand the method well.  I have quoted Reviewer 3 below in the author discussion, where there are some additional clarity issues that may help being resolved:     Some specifics are clear now with their new edition.  * The [relationship between] cgk  & cgk not as clear as it could be. For example the algorithms are designed for bits. So one should assume that they are applying it on the bits of the characters. But this should be clarified in the manuscript. * Also still backpropagating through f  is not clear to me. * And in the text for inference they still say: "We randomly select 100 queries and use the remainder of the dataset as the base set" which should be "the remainder excluding the training set" or "including?".
All reviewers explain in detail, why they think the paper should not be accepted. Besides fixing an initially criticized format violation, the authors did not respond to any of the concerns raised the reviewers, and in fact, they partially agree that more work in another direction needs to be done.  
This work presents an improved lower bound on the communciation complexity of distributed optimization in some settings. While reviewers agree that the paper is addressing a challenging and important question, all reviewers questioned the significance of the contributions of this work. In particular, two reviewers felt that the novelty of this work is limited. Unfortunately, the author response was unable to adequately address these concerns.
The paper demonstrates a case of federated learning with unlabelled but systematically partitioned data between clients. A title along terms like "FL with unlabelled data" would be much better   the considered setting here is not fully unsupervised but relies on the key assumption that while not the labels, at lease the precise label frequencies have to be known on each client, which is a strong assumption (also iid up to the class shift). Semi supervised FL approaches should also be discusses.  Overall, reviewers all agreed that the paper is interesting, well motivated and deserves acceptance. We hope the authors will incorporate the open points as mentioned by the reviewers.
The paper presents a novel procedure to set the steps size for the L BFGS algorithm using a neural network. Overall, the reviewers found the paper interesting and the main idea well thought. However, a baseline that was proposed by one of the reviewers seems to be basically on par with the performance of the proposed algorithm, at least in the experiments of the paper. For this reason, it is difficult to understand if the new procedure has merit or not. Also, the reviewers would have liked to see the same approach applied to different optimization algorithms.  For the reasons, the paper cannot be accepted in the current form. Yet, the idea might have potential, so I encourage the authors to take into account the reviewers  comments and resubmit the paper to another venue.
Main content:  Blind review #1 summarizes it well:  Recently many language GAN papers have been published to overcome the so called exposure bias, and demonstrated improvements  in natural language generation in terms of sample quality, some works propose to assess the generation in terms of diversity, however, quality and diversity are two conflicting measures that are hard to meet. This paper is a groundbreaking work that proposes receiver operating curve or Pareto optimality for quality and diversity measures, and shows that simple temperature sweeping in MLE generates the best quality diversity curves than all language GAN models through comprehensive experiments. It points out a good target that language GANs should aims at.      Discussion:  The main reservation was the originality of the idea of using temperature sweep in the softmax. However, it turns out this idea came from the authors in the first place, which they have not been able to state directly due to the anonymity requirement. Per the program chair s instruction to direct this to the area chair, I think this has been handled correctly.     Recommendation and justification:  This paper should be accepted. It provides readers with insight in that it illuminates a misconception of how important exposure bias has been assumed to be, and provides a less expensive MLE based way to train than GAN counterparts.
The paper extends Gauge invariant CNNs to Gauge invariant spherical CNNs.  The authors significantly improved both theory and experiments during the rebuttal and the paper is well presented. However, the topic is somewhat niche, and the bar for ICLR this year was very high, so unfortunately this paper did not make it. We encourage the authors to resubmit the work including the new results obtained during the rebuttal period.
Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers  comments into consideration to improve your submission for the camera ready. 
This paper presents a method for speeding up Gaussian process inference by leveraging locality information through k nearest neighbours.   The key idea is well motivated intuitively, however the way in which it is implemented seems to introduce new complications. One such issue is KNN overhead in high dimensions, but R1 outlines other potential issues too. Moreover, the method s merit is not demonstrated in a convincing way through the experiments. The authors have provided a rebuttal for those issues, but it does not seem to solve the concerns entirely. 
The paper considers federated learning in the presence of malicious clients and a semi honest centralized server. The authors provide a novel secure aggregation technique (i.e. split the clients into shards, and securely aggregate each shard’s updates, and the estimating things based on the updates from different shards) to protect clients from the server. Furthermore, an important property of the proposed protocol is that the estimation error is (provably) dimension free against Byzantine malicious clients. The paper is well written.   The reviewers had a number of concerns many of which were addressed during the rebuttal phase. There was also another round of discussion after the rebuttal phase. Overall, the reviewers felt that there are still some issues that need to be resolved (see the updated reviews the main issues are: (i) the assumption of non collusion between the server and the clients, (ii) assumptions and analysis of the non iid case, and (iii)  comparing to attacks that are specifically targeted against the baselines). I believe that once these issues are addressed, the paper will provide an important contribution to the area of federated learning.   
This work replaces the RNN layer of square with a self attention and convolution, achieving a big speed up and performance gains, particularly with data augmentation. The work is mostly clear presented, one reviewer found it "well written" although there was a complaint the work did not clear separate out the novel aspects. In terms of results the work is clearly of high quality, producing top numbers on the shared task. There were some initial complaints of only using the SQuAD dataset, but the authors have now included additional results that diversify the experiments. Perhaps the largest concern is novelty. The idea of non RNN self attention is now widely known, and there are several systems that are applying it. Reviewers felt that while this system does it well, it is maybe less novel or significant than other possible work. 
The paper presents a novel architecture, ModeRNN, for unsupervised video prediction by learning spatiotemporal attention in the latent subspace (slots).  ModeRNN effectively learns modular features using a set of mode slots and adaptively aggregates the slot features with learnable importance weights. The paper has promising results on several benchmark video prediction datasets.   During the post rebuttal discussion, the reviewer Wt6k and VMMf responded to the authors  rebuttal, but there was no discussion among them. The consensus is that even though the paper is a very strong engineering effort, it was not clear how the proposed architecture addresses the spatiotemporal mode collapse problem.  T SNE in Fig. 3/10/13 is insufficient to show disentangled feature space. In fact, PhyDnet was designed to disentangle different factors (physical vs unknown), hence not a good baseline.  [Hsieh et al 2018] is a better fit. In addition, synthetic data examples would be helpful to explain the underlying mechanism of the model and provide more insights for the video prediction community.  Based on this reason, I recommend rejecting this paper as it is now and encourage the authors to revise the draft and submit to future venues.  Hsieh, J. T., Liu, B., Huang, D. A., Li, F. F., & Niebles, J. C. (2018, January). Learning to Decompose and Disentangle Representations for Video Prediction. In NeurIPS.
The reviewers and I agree that the paper is well motivated and that there are good comparisons to prior work. However, the scope of the paper is rather limited, and there were some doubts about the overall conclusions and whether the current results fully support them. As such, I cannot recommend the paper for publication.
While there was some support for the ideas presented, the majority of reviewers did not think this paper was ready for publication at ICLR. In particular the experiments need more work, including the protocol for validation, and attention to overfitting.
This paper studies deep neural network (DNN) learning curves by leveraging recent connections of (wide) DNNs to kernel methods such as  Gaussian processes.   The bulk of the arguments contained in this paper are, thus, for the "kernel regime" rather than "the problem of non linearity in DNNs", as one reviewer puts it.  When it comes to scoring this paper, it has been controversial. However a lot of discussion has taken place. On the positive side, it seems that there is a lot of novel perspectives included in this paper. On the other hand, even after the revision, it seems that this paper is still very difficult to follow for non physicists.   Overall, it would be beneficial to perform a more careful revision of the paper such that it can be better appreciated by the targeted scientific community.  
This paper studies the relationship between flatness in parameter space and generalization. They show through visualization experiments on MNIST and CIFAR 10 that there is no obvious relationship between the two. However, the reviewers found the motivation for the visualization approach unconvincing and further found significant overlap between the proposed method and that of Ross & Doshi. Thus the paper should improve its framing, experimental insights and relation to prior work before being ready for publication.
This paper aims to theoretically understand the the benefit of attention mechanisms. The reviewers agreed that better understanding of attention mechanisms is an important direction. However, the paper studies a weaker form of attention which does not correspond well to the attention models using in the literature. The paper should better motivate why the theoretical results for this restrained model would carry over to more realistic mechanisms.
The paper present results using syntactic information (primarily through constituency trees) on the task of recognizing argument discourse units. No reviewer recommends acceptance of the paper:   The empirical results appear strong, though the reviewers raise questions about some of the experimental choices.    The writing is unclear and reviewers point out many missing or incorrect references in the bibliography.   There is little methodological novelty   known techniques are applied to a topic that has not been studied much. Overall, the area chair agrees with the reviewers that this work does not yet meet the bar for ICLR.
This paper studied Bayesian active regression with Gaussian processes, and proposed two intuitive algorithms inspired by the classical disagreement based and uncertainty sampling criteria. The reviewers appreciate the motivation and overall idea of taking a fully Bayesian approach by utilizing the joint posterior of the hyperparameters for active learning. However, there are shared concerns among the reviewers in the clarify and consistency of several key technical components, including discussion around bias variance tradeoff and its connection to the fully Bayesian approach, as well as in the experimental details, which make the current package insufficient for publication.   Reviewers provide very useful feedback (in particular with a very extensive review by Reviewer hDWW) for improving the current work. The authors acknowledge in their responses that these are valid concerns and they would address these issues in a further version of this work.
The work presented in this study gives a theoretical finite sample generalisation performance of stochastic gradient descent on linear models, for different batch sizes and feature structures. This approach enable the authors to predict the training and test losses of neural networks on real data.  While there were some parts that were initially mis understood by some reviewers in the initial version of the papers, the extensive discussions between the authors and the reviewers led to several updates, both in the reference to prior work, but also in the presentation clarity. The wide impact and relevance to ICLR of this type of contribution made us recommend this work for acceptance at ICLR.
This paper consider a classical multi armed bandit problem (then a more general RL setting) and prove some upper and lower bounds, in cases that were not explicitly studied in the literature.  However, those results are very incremental and do not justify (maybe yet, going beyond the sub Gaussian case could be interesting) yet acceptance.
This paper presents a novel option discovery mechanism through incrementally learning reusasble options from a small number of policies that are usable across multiple tasks.  The primary concern with this paper was with a number of issues around the experiments. Specifically, the reviewers took issue with the definition of novel tasks in the Atari context. A more robust discussion and analysis around what tasks are considered novel would be useful. Comparisons to other option discovery papers on the Atari domains is also required.  Additionally, one reviewer had concerns on the hard limit of option execution length which remain unresolved following the discussion.  While this is really promising work, it is not ready to be accepted at this stage.
This paper proposes a new loss agnostic PU learning method based on uncertainty aware pseudo label selection. I would like to thank the authors for their feedback to the initial reviews, which clarified many uncertain issues and improved our understanding of the current paper. Nevertheless, even if the pseudo labeling technique was applied to PU learning for the first time, given that it is a common practice in many weakly supervised learning tasks, the technical novelty is rather limited.  Therefore I cannot recommend acceptance of this paper.
The paper offers novel insights about memorization, the process by which deep neural networks are able to learn examples with incorrect labels. The core insight is that late layers are responsible for memorization. The paper presents a thorough examination of this claim from different angles. The experiments involving rewinding late layers are especially innovative.  The reviewers found the insights valuable and voted unanimously for accepting the paper. The sentiment is well summarized by R2: "The findings of the paper are interesting. It shows the heterogeneity in layers and training stage of the neural net".  I would like to bring to your attention the Coherent Gradients paper (see also R1 comment). This and other related papers already discusses the effect of label permutation on the gradient norm. Please make sure you discuss this related work. As a minor comment, please improve the resolution of all figures in the paper.   In summary, it is my pleasure to recommend the acceptance of the paper. Thank you for submitting your work to ICLR, and please make sure you address all remarks of the reviewers in the camera ready version. 
This paper proposes a  variational video prediction model FitVid and attains a better fit to video prediction datasets.  The draft was reviewed by four experts in the field and received mixed scores (1 borderline accept, 3 reject). The reviewers raised their concerns on lack of novelty, unconvincing experiment, and the presentation of this paper. For a video prediction model, fitting a dataset is quite important. But AC agrees with the reviewer jPAY. It will be more exciting to build a causal model of the world and enable it to perform future and counterfactual prediction (e.g,  CLEVRER).  The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
The reviewers raised a number of major concerns including the incremental novelty of the proposed and a poor readability of the presented materials (lack of sufficient explanations and discussions). The authors decided to withdraw the paper.
This paper addresses an important issue of AutoML systems, specifically their ability to "cold start" on a new problem. Some of the reviewers initially had concerns about the experimental validation and the theoretical foundations of the method, but during the discussion phase the authors addressed concerns extremely well. The authors already included most of the feedback of reviewers, further strengthening the paper.
The paper proposes a particle based framework for learning object dynamics. A scene is represented by a hierarchical graph over particles, edges between particles are established dynamically based on Euclidean distance. The model is used for model predictive control, and there is also one experiment with a particle graph built from a real scene as opposed to simulation.  All reviewers agree that the architectural changes over previous relational networks  are worthwhile and merit publication. They also suggest to tone down the ``dynamic” part of the graph construction by stating that edges are determined based on a radius. In particular, previous works also consider similar addition of edges during collisions, quoting Mrowca et al. "Collisions between objects are handled by dynamically defining pairwise collision relations ... between leaf particles..." which suggests that comparison against a baseline for Mrowca et al. that uses a static graph is not entirely fair. The authors are encouraged to repeat the experiment without disabling such dynamic addition of edges.   
The paper proposes a new framework for online hypothesis testing aimed at detecting causal effects (of treatments on outcomes) within subgroups in online settings where treatments are randomized.  Such settings occur in online advertising where different versions of the same website may be presented to a set of otherwise exchangeable users via A/B testing.  Under the standard causal assumptions of SUTVA, and sequential ignorability, in addition to a set of regularity conditions, the authors derive a result (Theorem 1) leading to an online test (Theorem 2).  Since the resulting test s limiting distribution does not have an exact analytic form, the authors instead propose a bootstrap approach to determine a set of parameters to properly control the error rate.  The author validate their approach by a simulation study, as well as via a user click log data from Yahoo!  The reviewer opinion was somewhat split on this paper, in particular some reviewers raised concern about some (conceptually significant) typos, interpretability of assumptions, and the need for parametric assumptions (the dichotomy between linear models and neural networks is surely a false one   the semi parametric literature obtains nice parametric style results, although perhaps not always for tests, without assuming parametric likelihoods all the time). 
This paper has a few interesting contributions: (a) a bound for un compressed networks in terms of the compressed network (this is in contrast to some prior work, which only gives bounds on the compressed network); (b) the use of local Rademacher complexity to try to squeeze as much as possible out of the connection; (c) an application of the bound to a specific interesting favorable condition, namely low rank structure.  As a minor suggestion, I d like to recommend that the authors go ahead and use their allowed 10th body page!
The author responses and notes to the AC are acknowledged.  A fourth review was requested because this seemed like a tricky paper to review, given both the technical contribution and the application area.  Overall, the reviewers were all in agreement in terms of score that the paper was just below borderline for acceptance.  They found that the methodology seemed sensible and the application potentially impactful.  However, a common thread was that the paper was hard to follow for non experts on MRI and the reviewers weren t entirely convinced by the experiments (asking for additional experiments and comparison to Zhang et al.).  The authors comment on the challenge of implementing Zhang is acknowledged and it s unfortunate that cluster issues prevented additional experimental results.  While ICLR certainly accepts application papers and particularly ones with interesting technical contribution in machine learning, given that the reviewers  struggled to follow the paper through the application specific language it does seem like this isn t the right venue for the paper as written.  Thus the recommendation is to reject.  Perhaps a more application specific venue would be a better fit for this work.  Otherwise, making the paper more accessible to the ML audience and providing experiments to justify the methodology beyond the application would make the paper much stronger.
This paper proposes a method for visualizing representations of neural networks trained with self supervised learning with conditional denoising diffusion probabilistic models. By generating multiple images conditioned on a representation, one can identify what aspects the representation is and is not sensitive to. The proposed method allows for high fidelity generated images that can be used to compare different self supervised methods and layers.  Reviewers agreed that the paper proposed reasonable methodology, targeted an interesting problem of understanding what is learned by self supervised methods, and presented interesting qualitative evaluations. However, there remained concerns on the novelty of results in comparison to other methods for probing representations (e.g. classification based), subjectiveness of interpretation of the qualitative results, and limited quantifications of the intuition gained from the visualizations. While the authors have argued that the point of the paper is to showcase the merits of qualitative visual analysis method, reviewers found that the presented results were insufficient to demonstrate the value of the proposed approach. A number of ideas were discussed with reviewers on how to highlight the value of visualization which could strengthen the paper in the future. Given the lack of novelty on the conditional generation side, and limited insight gained from the qualitative results, I cannot recommend this paper for acceptance in its current form.
Strengths: Execution of paper well received. Results on new dataset. Convincing demonstration that the proposed approach learns good semantic representations.  Weaknesses: Reviewers felt the positioning with prior work was not as strong as it could be. Reviewers wanted to have seen an ablation study.  Contention: Some general agreement among both the one positive reviewer and negative reviewer that the representation of prior work is skewed.  Consensus: With two 5s and one 6 the numerical average of 5.33 is representative of the aggregated consensus opinion which is that the work is just below threshold in its current form.
This work tackles the problem of catastrophic forgetting by using Gaussian processes to identify "memory samples" to regularize learning.  Although the approach seems promising and well motivated, the reviewers ultimately felt that some claims, such as scalability, need stronger justifications. These justifications could come, for example, from further experiments, including ablation studies to gain insights. Making the paper more convincing in this way is particularly desirable since the directions taken by this paper largely overlap with recent literature (as argued by reviewers). 
The reviewers agree that the proposed method to create a more robust representation of a task for model based RL is interesting and has significant merits. After some revision, more critical reviewers improved their ratings of the paper, such that there is unanimous agreement that the paper can be accepted to ICLR.
After carefully going through the reviews and rebuttal, and looking at the content of the paper as well, I feel there are some issues with the current manuscript. As also pointed out by AnonReviewer5 and AnonReviewer2, the text lacks clarity. From specifically defining what a schema is, to being more explicit about the limitation of the work.  I understand that the authors are interested in a largely unexplored setting, and hence there might not be a lot of prior work to cement the evaluation protocol. Particularly because of this I think such papers need to be upfront and clear not only in what is the setting and what is the evaluation but also what are the limitations and open problems.   I do agree that there is value in this direction of research, and that the idea of re ordering the features using attention (which I have to agree it is reminiscent of Bahdanau et al., ICLR 2015   though the semantics of it and its purpose makes it novel here) might be a way forward. But I do think for the paper to make an impact (and be ICLR ready) it needs more work both in the writing and maybe on the experimental side as well (consider some more complex task, or be more explicit on what is the common aspect between tasks in the distribution that can allow chameleon to work)
The paper presents a new semi supervised boosting approach.   As reviewers pointed out and AC acknowledge, the paper is not ready to publish in various aspects: (a) limited novelty/contribution, (b) reproducibility issue and (c) arguable assumptions.  Hence, I recommend rejection.
This paper uses a free energy formulation to develop an approach to learning "jumpy" transition models, which predict surprising future states. This transition model is used in combination with MCTS and applied to a scavenging task in the Animal AI Olympics, outperforming two baselines.  While the reviewers praised the importance of the problem tackled, and the novelty of using a free energy approach, there was a general sense amongst the reviewers that the paper wasn t totally clear (especially for an RL audience). R1 also felt that some of the claims of the paper weren t sufficiently evaluated enough, and several reviewers indicated that they felt the baselines were insufficient (or, at a minimum, not described in enough detail to evaluate whether they were sufficient). Given these points, I feel the paper is not quite ready for publication at ICLR. I encourage the authors to flesh out their analysis a bit more, better describe the baselines (and possibly compare to other existing approaches as mentioned by R4), and overall to frame the paper a bit more for the RL community.  One additional reference the authors may be interested in: Gregor et al (2018). Temporal difference variational auto encoder.
While authors have updated the draft to address reviewers  concerns, some parts are still not clear enough and the presentation needs further improvements. I encourage authors to revise the draft accordingly and resubmit in the future venues.
The paper describes a production ready neural text to speech system. The algorithmic novelty is somewhat limited, as the fully convolutional sequence model with attention is based on the previous work. The main contribution of the paper is the description of the complete system in full detail. I would encourage the authors to expand on the evaluation part of the paper, and add more ablation studies.
The reviewers agree this is a really interesting paper, with an interesting idea (in particular the use of regret clipping might provide a benefit over typical policy gradient methods). However, there are two major concerns: 1) clarity / exposition and more importantly 2) lack of a strong empirical motivation for the new approach (why do standard methods work just as well on these partially observable domains?).
This paper extends Adam by adding another hyperparameter that allows the second moments to be raised to a power p other than 1/2. This certainly seems worth trying. The paper is well written, and the experiments seem reasonably complete. But some of the reviewers and I feel like the contribution is a bit obvious and incremental. The "small learning rate dilemma" needs a bit more justification: since the denominator has a different scale, the learning rates for different values of p are not directly comparable. It could very well be that Adam s learning rate has to be set too small due to some outlier dimensions, but showing this would require some evidence. From the experiments, it does seem like there s some practical benefit, though it s not terribly surprising that adding an additional hyperparameter will result in improved performance. The reviewers think the theoretical analysis is a straightforward extension of prior work (though I haven t checked myself). Overall, it doesn t seem to me like the contribution is quite enough for publication at ICLR. 
This paper provides a new uncertainty measure of examples called "Variance of Gradients" (VoGs); it demonstrates that VoGs are correlated with mistakes, and can be useful for guiding optimization.   On the positive side, the reviewers generally think that the ideas of this paper is nice and contribute to the research thrust in gradient based uncertainty. In addition, the paper provides valuable empirical insights.   However, the reviewers also pointed out a few important limitations:   A more thorough comparison to prior methods is needed to convince the readers for actual usage. There are many other methods (e.g. predicted entropy) for example difficulty estimation / classifier trustworthiness that need to be compared to.   The stability of individual VoG scores needs to be investigated further  The authors are encouraged to address these limitations in the next iteration.
This paper presents a taxonomic study of neural network architectures, focussing on those which seek to map onto different part of the hierarchy of models of computation (DFAs, PDAs, etc). The paper splits between defining the taxonomy and comparing its elements on synthetic and "NLP" tasks (in fact, babi, which is also synthetic). I m a fairly biased assessor of this sort of paper, as I generally like this topical area and think there is a need for more work of this nature in our field. I welcome, and believe the CFP calls for, papers like this ("learning representations of outputs or [structured] states", "theoretical issues in deep learning")). However, despite my personal enthusiasm, the reviews tell a different story.  The scores for this paper are all over the place, and that s after some attempt at harmonisation! I am satisfied that the authors have had a fair shot at defending their paper and that the reviewers have engaged with the discussion process. I m afraid the emerging consensus still seems to be in favour of rejection. Despite my own views, I m not comfortable bumping it up into acceptance territory on the basis of this assessment. Reviewer 1 is the only enthusiastic proponent of the paper, but their statement of support for the paper has done little to sway the others. The arguments by reviewer 3 specifically are quite salient: it is important to seek informative and useful taxonomies of the sort presented in this work, but they must have practical utility. From reading the paper, I share some of this reviewer s concerns: while it is clear to me what use there is the production of studies of the sort presented in this paper, it is not immediately clear what the utility of *this* study is. Would I, practically speaking, be able to make an informed choice as to what model class to attempt for a problem that wouldn t be indistinguishable from common approaches (e.g. "start simple, add complexity"). I am afraid I agree with this reviewer that I would not.  My conclusion is that there is not a strong consensus for accepting the paper. While I wouldn t mind seeing this work presented at the conference, but due to the competitive nature of the paper selection process, I m afraid the line must be drawn somewhere. I do look forward to re reading this paper after the authors have had a chance to improve and expand upon it.
This paper proposes a distributed containerized multi agent reinforcement learning(CMARL) framework that addresses three challenges in MARL: 1) Demanding data transfer. 2) Inter process communication. 3) Effective Exploration. Using a container that collects environment experiences from parallel actors into buffers and learns local policies, CMARL demonstrates notable performance improvements with respect to time as compared to state of the art benchmarks.  Although the reviewers acknowledge that the paper addresses a relevant topic, proposes an effective method, and is well written, after reading the authors  feedback and discussing their concerns, the reviewers reached a consensus about rejecting this paper in its current form. They feel that the contribution is too incremental and that the experimental comparisons are somehow unfair.  I suggest the authors take into consideration the reviewers  suggestions while preparing an updated version of their paper for one of the forthcoming machine learning conferences.
This paper studies meta learning in hierarchical RL, where the unknown hierarchy is learned during meta training and then applied to a test task. The authors propose an optimistic algorithm for solving this problem and analyze it. The main contribution of the paper is in the first end to end analysis.  This paper has three borderline reviews and one reject. Despite the differences in the scores, all reviewers share the same opinion. The idea is novel and very interesting. However, the algorithm and its analysis rely on many assumptions, many of which are introduced in this work and not properly discussed. Because of this, the paper needs a major revision and is rejected for now.
Paper presents a way in which linear RNNs can be computed (fprop, bprop) using parallel scan. They show big improvements in speedups and show application on really long sequences. Reviews were generally favorable.
   The paper presents a semi supervised data streaming approach. The proposed architecture is made of a layer wise k means structure (more specifically a epsilon means approach, where the epsilon is adaptively defined from the distortion percentile). Each layer is associated a scope (patch dimensions); each patch of the image is associated its nearest cluster center (or a new cluster is created if needed); new cluster centers are adjusted to fit the examples (Short Term Memory); clusters that have been visited sufficiently many time are frozen (Long Term Memory). Each cluster is associated a label distribution from the labelled examples. The label for each new image is obtained by a vote of the clusters and layers.  Some reviews raise some issues about the robustness of the approach, and its sensitivity w.r.t. hyper parameters. Some claims ("the distribution associated to a class may change with time") are not experimentally confirmed; it seems that in such a case, the LTM size might grow along time; a forgetting mechanism would then be needed to enforce the tractability of classification.   Some claims (the mechanism is related to how animal learn) are debatable, as noted by Rev#1; see hippocampal replay.  The area chair thinks that a main issue with the paper is that the Unsupervised Progressive Learning is considered to be a new setting ("none of the existing approaches in the literature are directly applicable to the UPL problem"), preventing the authors from comparing their results with baselines.  However, after a short bibliographic search, some related approaches exist under another name: * Incremental Semi supervised Learning on Streaming Data, Pattern Recognition 88, Li et al., 2018; * Incremental Semi Supervised Learning from Streams for Object Classification, Chiotellis et al., 2018; * Online data stream classification with incremental semi supervised learning, Loo et al., 2015.  The above approaches seem able to at least accommodate the Uniform UPL scenario. I therefore encourage the authors to consider some of the above as baselines and provide a comparative validation of STAM.
This paper proposes a solution for the well known problem of posterior collapse in VAEs: a phenomenon where the posteriors fail to diverge from the prior, which tends to happen in situations where the decoder is overly flexible.  A downside of the proposed method is the introduction of hyper parameters controlling the degree of regularization. The empirical results show improvements on various baselines.  The paper proposes the addition of a regularization term that penalizes pairwise similarity of posteriors in latent space. The reviewers agree that the paper is clearly written and that the method is reasonably motivated. The experiments are also sufficiently convincing.
On the positive side, this is among the first papers to exploit non Euclidean geometry, specifically curvature for adversarial learning. However, reviewers are largely in agreement that the technical correctness of this paper is unconvincing despite substantial technical exchanges with the authors.  
This work applies collocation, a well known trajectory optimization technique, to the problem of planning in learned visual latent spaces. Evaluations show that collocation based optimization outperforms shooting via CEM (PlaNet) and  shooting via gradient descent.  Pros:   I agree with the reviewers that this idea makes sense, and will very likely be built on in future work   the authors have very actively addressed most comments of all reviewers that engaged in discussion  cons:   I agree with the reviewers that this is a very simple and straightforward application of collocation methods to the visual latent space domain. Furthermore, the chosen tasks are fairly simplistic, meta world has a variety of tasks, most of which are more complex than the reaching and pushing task that were chosen for this manuscript.     Even with all the updates, the evaluation is still very shallow. I agree with the reviewers that obtaining results for both settings: a) visual MPC with pre trained (or even ground truth) dynamics model b) in the model based RL setting, for which the model is being learned, is important. While the authors have added some of these experiments, a detailed discussion of how the results change from a) to b) is missing.  Furthermore, when using collocation in this MBRL setting, how should dynamics constraints be enforced (should they even be enforced when the model is still really bad?). How does the comparison between collocation and shooting fare when you use dense/shaped rewards for the sawyer tasks? Many questions come to mind, some of which that have been raised by the reviewers, and my main point is that simple idea + in depth analysis of some of these questions would have created a stronger contribution.   Alternatively, real system experiments would have increased the significance of this work.    I don t see any direct references of gradient based visual latent space planning (shooting), but related work on this does exist.   In my opinion, a simple straightforward idea is no reason to reject a paper. However, currently, the reader does not learn when collocation should be considered over other trajectory optimization methods, when attempting to plan in a learned visual latent space. And what some of the main remaining challenges are. Because of this I lean towards recommending reject, and would encourage the authors to deepen their analysis of collocation in visual latent space.
All reviewers agree the paper proposes an interesting setup and the main finding that "prosocial agents are able to learn to ground symbols using RL, but self interested agents are not" progresses work in this area. R3 asked a number of detail oriented questions and while they did not update their review based on the author response, I am satisfied by the answers. 
The paper provides two new generalization bounds for non linear metric learning with deep neural networks, by extending results of Bartlett et al. 2017 to the metric learning setting. The main contribution of the paper is by extending the techniques of Bartlett et al. from a classification setting to the metric learning setting (which has very different objectives) and consider two regimes. In the first regime the techniques are fairly similar but the second regime is more novel. However, the current version of the paper does not highlight the similarity and differences between the results and techniques with Bartlett et al. 2017; it also does not give sufficient intuition on how the metric learning setting is fundamentally different from the classification setting and how the paper leverage the difference to get improved bounds. All the reviewers had some confusions to different degrees, and the paper would be much stronger if it can explain the intuition and make more explicit comparisons.
The paper presents an approach to feature selection. Reviews were mixed and questions whether the paper has enough substance, novelty, the correctness of the theoretical contributions, experimental details, as well as whether the paper compares to the relevant literature.  
The paper presents a CNN that is trained from human games to predict which actions to take for China Competitive Poker (Dou dizhu).  The paper is poorly written, not because of the English, but because it is hard to understand the details of the proposed solution: it is not straight forward to reimplement a solution from the presentation in the paper. It lacks explanations for several design decisions. This is unfortunate, as the authors point out in the rebuttal that they actually did way more experiments that are presented in the paper. Moreover, the experimental results lack comparisons to baselines, ablations, so that the proposed solution could be evaluated fairly.  In its current state, this paper can not be accepted for presentation at ICLR 2019.
The paper describes a new technique to train an adversarial MDP to perturb the observations provided by the environment.  This adversarial MDP is then used to train an RL agent to be more robust.  Since the adversarial agent essentially defines an observation distribution for the environment, the RL agent needs to optimize a POMDP.  This is nice work that was unanimously praised by the reviewers.  It produces stronger adversaries and more robust RL agents than previous work.  This represents an important contribution to the state of the art of robust RL.  
In this paper, in order to theoretically investigate the relationship between graph structure and labels in GNNs, interaction probabity and frequency indicators are introduced and analyzed, and a new family of GNNs with multiple filters is proposed based on the insights from the theoretical analysis, In the discussion, there was an opinion that the theoretical analysis is interesting, but its novelty and clarity are limited. Although certain contributions are acknowledged, the impact is marginal and the audience for which this paper will matter is rather limited.
This paper makes use of cross attention transformers to extract invariant features for unsupervised domain adaptation. Combined with pseudo label approaches, the proposed method achieves state of the art performance, possibly because the transformer features are more robust to the noise. In addition, a two way centre aware labeling method is proposed to produce more reliable pseudo labels.   However, there are some concerns raised by reviewers. After the discussion period, there is still a concern that is not completely unresolved. The comparison with existing methods might not be fair. It is possible that the performance gain is caused by the generally better representation of transformers, which has been shown in supervised image classification.  Overall, the paper is novel and interesting.  I would recommend acceptance of this paper given its impressive performance, but I highly suggest the authors add more ablation studies, for example, compare the proposed transformer and ResNet on a supervised classification task, to further confirm that the performance gain is solely because the transformer is more robust to label noise. The results can be updated in the supplementary. Also, as promised in the discussion, I hope the authors could release their code as soon as possible. This is because the backbone in this paper is totally new, it will be hard for other researchers to achieve SOTA results if they still use CNNs.
The paper discusses new RL algorithms for solving large. TSP instances. The algorithm is novel and the problem is important however certain technical questions regarding the soundness of the algorithm were raised. Furthermore, it seems that despite much larger computational time, the algorithm provides only very moderate gains over previous baselines. Finally, it is not clear how the proposed methods (e.g. equivariance) can be applied outside of the TSP problem scope. Thus the concern is the limited impact of the method on the field.  The authors addressed some of the concerns of the reviewers in the rebuttal however it is still not clear:  (1) how the presented mechanism can be applied for other combinatorial problems beyond TSP and therefore how useful it can be for the machine learning community,  (2) how novel the paper is (the use of equivariance is as direct as in the regular graph neural network setup).  Furthermore, the experiments show that the deep learning approach to TSP is still not competitive with standard non machine baselines. Thus it is not clear whether the proposed algorithm is a right approach to solve this problem, even though it beats other deep learning techniques. The paper is very well written though and the presented method is definitely of value to the research community working on the TSP. Therefore it seems that at this point the paper is more suited for one of the mathematical journals on combinatorics and graph theory.
This is a good paper with strong results via a set of simple steps for post processing off the shelf words  embeddings.  Reviewers are enthusiastic about it and the author responses are satisfactory.
The reviewers found this work well motivated and the additional experiments conducted during the response phase were greatly appreciated. Anderson s acceleration appears to be a simple device that may be of great value to this field, and therefore this work is a timely contribution. The presented theoretical results justify the authors  modifications, although at times it felt more comparisons would be welcome: (a) Section 3.2 could have compared to a lot of three term recurrences that lead to the optimal dependence on the condition number, including Chebyshev s polynomial and conjugate gradient, as well as the results in Brezinski et al. (2018); (b) Section 3.3 would benefit from some comparison with "Evans, Claire, Sara Pollock, Leo G. Rebholz, and Mengying Xiao (2020). “A Proof That Anderson Acceleration Improves the Convergence Rate in Linearly Converging Fixed Point Methods (But Not in Those Converging Quadratically)”. SIAM Journal on Numerical Analysis, vol. 58, no. 1, pp. 788–810." (c) the results in Section 3.4 seem to be a bit preliminary and it would be great if the authors could compare to standard rates of SGD.   Overall we believe this work will generate more interest on memory based optimization techniques in deep learning, and we encourage the authors to thoroughly polish their draft by incorporating the reviewers  comments and the responses during the discussion phase.
The paper presents a topological complexity measure of neural networks based on persistence 0 homology of the weights in each layer. Some lower and upper bounds of the p norm persistence diagram are derived that leads to normalized persistence metric. The main discovery of such a topological complexity measure is that it leads to a stability based early stopping criterion without a statistical cross validation, as well as distinct characterizations on random initialization, batch normalization and drop out. Experiments are conducted with simple networks and MNIST, Fashion MNIST, CIFAR10, IMDB datasets.   The main concerns from the reviewers are that experimental studies are still preliminary and the understanding on the observed interesting phenomenon is premature. The authors make comprehensive responses to the raised questions with new experiments and some reviewers raise the rating.   The reviewers all agree that the paper presents a novel study on neural network from an algebraic topology perspective with interesting results that has not been seen before. The paper is thus suggested to be borderline lean accept.  
This paper proposes a few architectural modifications to the BERT model for language understanding, which are meant to apply during fine tuning for target tasks.   All three reviewers had concerns about the motivation for at least one of the proposed methods, and none of three reviewers found the primary experimental results convincing: The proposed methods yield a small improvement on average across target tasks, but one that is not consistent across tasks, and that may not be statistically significant.  The authors clarified some points, but did not substantially rebut any of the reviewers concerns. Even though the reviewers express relatively low confidence, their concerns sound serious and uncontested, so I don t think we can accept this paper as is.
This paper is a study in optimizing the Donsker Varadhan lower bound on mutual information focusing on a "drift" problem.  The bound is a difference between terms which appears to have an extra degree of freedom where the two terms increase or decrease together.  They propose a fix for this problem. The authors state that the DV bound is of practical value but in most cases it is replaced by discriminative lower bounds as in contrastive predictive coding (CPC) which are biased but have lower variance. The paper does not address the variance (convergence) issues with the DV bound.  We have a well informed reviewer who feels that the paper is not sufficiently novel and has other issues supporting rejection.  Other reviews are not very enthusiastic.  I will side with rejection.
the problem is interesting, and the reviewers acknowledge it s worth an effort to tackle. unfortunately all the reviewers found the work to be too preliminary without a convincing evidence supporting the proposed approach against other alternatives (or on its own.)
This paper generated a lot of discussion. Paper presents an empirical evaluation of generalization in models for visual reasoning. All reviewers generally agree that it presents a thorough evaluation with a good set of questions. The only remaining concerns of R3 (the sole negative vote) were lack of surprise in findings and lingering questions of whether these results generalize to realistic settings. The former suffers from hindsight bias and tends to be an unreliable indicator of the impact of a paper. The latter is an open question and should be worked on, but in the opinion of the AC, does not preclude publication of this manuscript. These experiments are well done and deserve to be published. If the findings don t generalize to more complex settings, we will let the noisy process of science correct our understanding in the future. 
This paper presents a new graph pooling method, called HaarPooling. Based on the hierarchical HaarPooling, the graph classification problems can be solved under the graph neural network framework.  One major concern of reviewers is the experiment design. Authors add a new real world dataset in revision. Another concern is computational performance. The main text did not give a comprehensive analysis and the rebuttal did not fully address these problems.  Overall, this paper presents an interesting graph pooling approach for graph classification while the presentation needs further polish. Based on the reviewers’ comments, I choose to reject the paper. 
This paper proposes a way to handle the hard negative examples (those very close to positive ones) in  NLP, using a distant supervision approach that serves as a regularization.   The paper addresses an important issue and is well written; however, reviewers pointed put several concerns, including testing the approach on the state of art neural nets, and making experiments more convincing by testing on larger problems.   
The authors use a memory augmented neural architecture to learn solve combinatorial optimization problems.  The reviewers consider the approach worth studying, but find the authors  experimental protocol and baselines flawed.  
This paper examines adversarially trained robust models, and finds that accuracy disparity is higher than for standard models. The authors introduce a method they call Fair Robust Learning using Lagrange multipliers to minimize overall robust error while constraining the accuracy discrepancy between classes.    In discussion, consensus was reached that the observations and approach are interesting but the paper is not yet ready for publication. The main concern is that it is not clear if the class accuracy disparity is due to adversarial training, or simply due to lower accuracy in general. Please see reviews and public discussion for further details.
The paper proposed a new learnable activation function called Padé Activation Unit (PAU) based on parameterization of rational function. All the reviewers agree that the method is soundly motivated, the empirical results are strong to suggest that this would be a good addition to the literature. 
This paper proposed an improved ensemble method called BatchEnsemble, where the weight matrix is decomposed as the element wise product of a shared weigth metrix and a rank one matrix for each member.  The effectiveness of the proposed methods has been verified by experiments on a list of various tasks including image classification, machine translation, lifelong learning and uncertainty modeling.  The idea is simple and easy to follow.  Although some reviewers thought it lacks of in deep analysis, I would like to see it being accepted so the community can benefit from it.
The paper provides a reformulation of the distributionally robust optimization problem into a (difficult in general) transportation map problem. In the new reformulation, the authors provide strong convergence results albeit requiring strong conditions, such as solvability of the auxiliary problems in reasonable time or complexity per iteration. At this point, it is very difficult to accept the paper   given the issues that reviewers pointed. In particular, authors  method which requires an expensive MCMC implementation is not scalable. Based on the run times reported in Table 2, the proposed method is 10x slower than their baseline (Volpi et al 2018) achieving performance within their statistical error.
All reviewers appreciate the good quality of this submission with a good idea and solid execution (as said by R3). The paper is clearly written and the addition during the discussion have greatly improved it as acknowledged by all reviewers.  However, a major weakness of the submission still needs to be addressed before a publication at ICLR.   As said in the paper, the task of question generation is a task whose main impact is to improve downstream tasks, and primarily QA. The evaluations follow that and extra experiments (e.g. BioASQ) and discussion wrt state of the art (e.g. Alberti et al.) reinforce them. Yet, as pointed out by R1 & R4, the effect on downstream QA performance is only shown for weaker models than the current state of the art (e.g. T5, BART). Since the rebuttal period was not long enough to run these experiments, it is impossible to assess how the proposed approach compare to them with the current draft. Adding the experiments on T5 (Small) is a step in the right direction but it is not enough for that. Without those experiments, one can not conclude that this pretraining strategy would also help over the strongest existing pretrained model.  The authors should run those experiments to make the arguments presented in the submission much stronger.          
This paper presents an empirical study of generalization in visual reinforcement learning. This study is carried out in the domain of video games and it addresses the benefits of techniques such as regularization, augmentation and training with auxiliary tasks. The reviewers for this submission were positive about the goal and setups in this paper. They agreed that understanding why present day methods that attempt to improve generalization continue to fall short, is an important problem. However, most reviewers were underwhelmed by the findings presented in the submission. As examples: Reviewer 185P mentions that "the paper does not seem to provide a clear and definite answer to the question" and " I am not convinced the experiments described in this paper support the claims made by the authors." and Reviewer SFef mentions that "Most of the conclusions are already known". Some reviewers also found a lack of clarity and several typos in the initial submission. The authors have provided detailed responses to the reviewers. In particular they have fixed most writing issues. They also detailed why certain algorithms and techniques were benchmarked in this submission and others were left out. I think this is reasonable. One cannot expect a paper to benchmark every algorithm out there, and choosing promising and representative ones is sufficient. My takeaway from detailed discussions about this paper are that: The paper is much improved from a writing point of view and the rebuttal addresses some concerns well. However, I do agree with the reviewers that the findings presented in the paper are for the most part expected. This reduces the value of the paper to readers. When this is the case, it may be beneficial to dig deeper into these findings and present a narrow but deep analysis. Please see Reviewer 185P s suggestions in this regard. Given the above, I am recommending rejection for this conference, but I encourage the authors to take into the reviewers suggestions and resubmit.
This paper proposes a transfer learning approach based on previous works on this area, to build language understanding models for new domains. Experimental results show improved performance in comparison to previous studies in terms of slot and intent accuracies in multiple setups. The work is interesting and useful, but is not novel given the previous work. The paper organization is also not great, for example, the intro should introduce the approach beyond just mentioning transfer learning and meta learning. The improvements over the baselines look good, but the baselines themselves are quite simple. It d be better to include comparisons with other state of the art methods. Also, the improvements over DNN are not consistent, it would be good to analyze and come up with suggestions on when to use which approach. 
The authors propose a method of selecting nodes to label in a graph neural network setting to reduce the loss as efficiently as possible. Building atop Sener & Savarese 2017 the authors propose an alternative distance metric and clustering algorithm. In comparison to the just mentioned work, they show that their upper bound is smaller than the previous art s upper bound. While one cannot conclude from this that their algorithm is better, at least empirically the method appears to have a advantage over state of the art.  However, reviewers were concerned about the assumptions necessary to prove the theorem, despite the modifications made by the authors after the initial round.   The work proposes a simple estimator and shows promising results but reviewers felt improvements like reducing the number of assumptions and potentially a lower bound may greatly strengthen the paper.
This paper received 1 weak accept, 1 accept, and 1 weak reject.  All reviewers questioned the motivation for continuous space/time with respect to biological vision. Obviously, discrete approximations used in machine vision are approximations but it is not clear from the paper or the authors’ response that this severely limits the ability of deep nets to predict neural data in ways that their continuous nets would not.   In addition, I have to confess that I did not really understand the argument made by the authors in their revision. In any case, the burden should be on the authors to go beyond general statements and to really demonstrate that the proposed models provide actual insights for neuroscience since the performance in terms of machine vision on CIFAR10 is underwhelming (the authors have to find a low data regime and even then the reviewers stated that the baselines used are not strong baselines, the reduction in the number of parameters is quite small relative to methods for actually reducing the number of parameters).    Clearly, the work has potential as noted by the reviewers. The authors suggest that “DCNs can be used to model the temporal profiles of neuronal responses, which are known to not be constant even when the experimental stimuli are static images: for example, spatial frequency responses change over time in response to stationary gratings (Frazor et al., 2004). Similar observations are made for the contrast response function (Albrecht et al., 2002). Such temporal profiles cannot be simulated in conventional CNNs. “ This sounds like an interesting set of neuroscience data that the authors could be indeed leveraging to demonstrate the benefit of their approach. My recommendation would be to add those in a revision of this paper which will significantly strengthen the work. I would add that the concepts of temporal and spatial continuity are independent and the authors should consider studying them separately to provide more in depth analyses and convincing results.   As it stands, the paper has clear potential but it does not make a sufficient contribution to either ML or neuroscience and hence, I recommend the paper to be rejected. 
This paper proposes a federated averaging Langevin dynamics (FA LD) for numerical mean prediction with uncertainty quantification under the setting of federated learning. Convergence analysis for the proposed method under the smoothness and strong convex assumptions is also provided, and the results are summarized in Theorems 5.7 5.10, each of which bounds the Wasserstein 2 distance $W_2(\mu_k,\pi)$ between the model distribution $\mu_k$ and the target distribution $\pi$ under different settings.  This paper received 5 reviews in total, with scores 6, 5, 3, 5, and 3. Some reviewers evaluated positively the novelty of the idea of using the Langevin dynamics in the federated setting, which I would also like to acknowledge. Upon reading the paper by myself, however, I find that the mathematical formulations are in some places not correct. What I think problematic is the third equation in equation (3): The right hand side is a function of $N$ variables $\\\{\theta_k^c\\\}$, and they undergo different local updates at different clients when $k\not\equiv 0\mod K$ (i.e., the synchronization does not take place). Also $\nabla\tilde{f}^c$ is in general a nonlinear function of its argument. Therefore, the right hand side cannot be written in general as a function of a single variable $\theta_k$ which is defined as $\theta_k \sum_{c 1}^Np_c\theta_k^c$, making this equation incorrect. This problem would affect various parts of the arguments to follow in this paper, such as the first two equations in equation (16) on page 14, the two inline equations just after equation (16), equation (18), the second equality in the inline equation in page 15, line 1, and the third line in equation (25) on page 18, to mention a few. Thus I have to question the validity of the theoretical development in this paper.  Another point I would like to mention is that I did not understand the definition of Schemes I and II in Section 5.4. It is not stated at all that $\mathcal{S}_k$ is a random quantity here. Furthermore, the conditions "with/without replacement" are not described at all.  Still another point to mention is that I did not understand the claim in page 7, lines 30 31. Does it mean: If one knows the number $T_\epsilon$ of steps to achieve the precision $\epsilon$, then one should set the number $K$ of local steps per synchronization should be set of the order of $\sqrt{T_\epsilon}$. But $T_\epsilon$ depends on $K$, so that it would be unnatural to assume that one knows $T_\epsilon$ irrespective of $K$ in the first place.  Because of these, I would judge that this paper is not yet ready for presentation in its current form. I would therefore not be able to recommend acceptance of this paper.  Minor points:   Citation style: The authors use throughout the paper what are called the *narrative citations* even though there are occasions where what are called the *parenthetical citations* (the author name and publication date are both enclosed in parentheses) should be used.   page 3, line 7: is (the  > an) unbiased stochastic gradient; There are several unbiased estimators for the gradient, and what is mentioned here is only one instance of them.   page 3, lines 23 24: The aggregation should take place not on each client but on the central server.   page 3, line 36: a(n) energy function; a(n) unbiased estimate   page 5, lines 17 20: The contents of Assumptions 5.1 and 5.2 are not assumptions but definitions.   page 6, line 2: to obtain (the  > a)  lower bound   page 6, line 18: $\mathcal{D}^2$ is undefined.   page 8, line 39: (a  > the) probability $p_c$ if it is meant to be the one defined in page 3, line 8. Otherwise, use of the same symbol to represent different quantities should be avoided.   page 14, line 25: mod ($E$  > $K$)  0   page 15, line 30: $H_\rho^2$  > $H_\rho$
This work takes dialogue acts into account to generate responses in a human machine conversation. However, incorporating dialogue acts into open domain dialogue was already the focus of Zhao et al s ACL 2017 paper, Learning Discourse level Diversity for Neural Dialog Models using Conditional Variational Autoencoders, and using dialogue acts in a policy for human machine conversation was also an idea that already appeared in Serban et al 2017, A Deep Reinforcement Learning Chatbot. Despite the authors  response that tries to adjust their claims and incorporate a more thorough overview, I encourage the authors to re work their research with a much more careful and reliable examination of previous work and how their effort should be understood in that more comprehensive context.
This paper introduces an unsupervised learning objective that attempts to improve the robustness of the learnt representations. This approach is empirically demonstrated on cifar10 and tiny imagenet with different network architectures including all convolutional net, wide residual net and dense net.   Two of three reviewers felt that the paper was not suitable for publication at ICLR in its current form.  Self supervision based on preserving network outputs despite data transformations is a relatively minor contribution, the framing of the approach as inspired by biological vision notwithstanding.  Several references, including at a past ICLR include: http://openaccess.thecvf.com/content_CVPR_2019/papers/Kolesnikov_Revisiting_Self Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf and  Gidaris, P. Singh, and N. Komodakis. Unsupervised representation learning by predicting image rotations.  In International Conference on Learning Representations (ICLR), 2018.
This paper proposes to encode positions of nodes in graphs by anchor based GNN with customized message passing steps. All reviewers raised significant concerns on this paper, including novelty of the message passing steps, experiments, writing and clarity, etc. The authors have actively responded to reviewer comments, but many of the concerns are still not addressed. Thus, the paper needs some work in order to be competitive.
Four knowledgeable referees have indicated reject mainly because of limited motivation [R1,R3,R4], limited insights on the proposed approach [R1,R2,R3,R4], and inconclusive results [R1,R2,R3,R4]. The claims of the paper could have been strengthened by e.g. discussing currently missing experimental details [R1], performing statistical significance tests [R2,R4], and including comparisons to baselines/previously introduced normalization strategies [R2,R3]. Unfortunately, there was no rebuttal. The paper is therefore rejected.
This paper proposes to combine rewards obtained through IRL from rewards coming from the environment, and evaluate the algorithm on grid world environments. The problem setting is important and of interest to the ICLR community. While the revised paper addresses the concerns about the lack of a stochastic environment problem, the reviewers still have major concerns regarding the novelty and significance of the algorithmic contribution, as well as the limited complexity of the experimental domains. As such, the paper does not meet the bar for publication at ICLR.
The method proposed and explored here is to introduce small spatial distortions, with the goal of making them undetectable by humans but affecting the classification of the images. As reviewers point out, very similar methods have been tested before. The methods are also only tested on a few low resolution datasets.   The reviewers are unanimous in their judgement that the method is not novel enough, and the authors  rebuttals have not convinced the reviewers or me about the opposite.
This paper extends previous observations (Tsipars, Etmann etc) in relations between Jacobian and robustness and directly train a model that improves robustness using Jacobians that look like images. The questions regarding computation time (suggested by two reviewers, including one of the most negative reviewers) are appropriately addressed by the authors (added experiments). Reviewers agree that the idea is novel, and some conjectured why the paper’s idea is a very sensible one. We think this paper would be an interest for ICLR readers. Please address any remaining comments from the reviewers before the final copy. 
This paper considers the problem of pruning deep neural networks (DNNs) during training. The key idea is to include DNN elements only if they improve the predictive mean of the saliency (efficiency of the DNN elements in terms of minimizing the loss function). The objective of early pruning is to preserve the sub network that can maximize saliency. This optimization problem is NP hard, and even approximation is very expensive. The paper proves that one can simplify the approximation by ranking the network element by predictive mean of the saliency function.  The proposed approach is novel as most of the prior work on pruning has focused on either (i) pruning on network initialization  or (ii) pruning after the network has been fully trained.   Couple of issues with the paper are: 1. Current approach is somewhat complicated with many hyper parameters 2. Experimental results are not very compelling when compared to pruning on network initialization  Overall, my assessment is that the paper takes a new research direction and has the potential to inspire the community, and followup work may be able to overcome the above two issues in future. However, due to the remaining shortcomings, the paper is not judged ready for publication in its present form. I strongly encourage to resubmit the paper after addressing the above two concerns.
This paper highlights the problem of penalizing the total correlation of sampled latent variables for unsupervised learning of disentangled representations. Authors prove a theorem on how sample representations with bounded total correlation may have arbitrarily large total correlation when computed with the underlying mean. As a fix, the authors propose RTC VAE method that penalizes total covariance of sampled latent variables.  R2 appreciated the simplicity of the idea, making it easy to understand and implement, but raises serious concerns on empirical evaluation of the method. Specifically, very limited datasets (initially dsprites and 3d shapes) and with no evaluation of disentanglement performance and no comparison against other disentangling methods like DIP VAE 1. While the authors added another dataset (3d face) in their revised versions, the concerns about disentanglement performance evaluation and its comparison against baselines remained as before, and R2 was not convinced to raise the initial score.  Similarly, while R1 and R3 appreciate author s response, they believe the response was not convincing enough for them, and maintained their initial ratings.  Overall, the submission has room for improvement toward a clear evaluation of the proposed method against related baselines.
This paper introduces a new ECG dataset. While I appreciate the efforts to clarify several points raised by the reviewers, I still believe this contribution to be of limited interest to the broad ICLR community. As such, I suggest this paper to be submitted to a more specialised venue.
This paper proposes novel recurrent models for polyphonic music composition and demonstrates the approach with qualitative and quantitative evaluations as well as samples. The technical parts in the original write up were not very clear, as noted by multiple reviewers. During the review period, the presentation was improved. Unfortunately the reviewer scores are mixed, and are on the lower side, mainly because of the lack of clarity and quality of the results.
This paper introduces supervised contrastive learning loss on top of typical cross entropy loss for fine tuning language model for downstream tasks. While the idea is simple and has been used in vision literature (as pointed out by R1 & R4), its application LM is first introduced in this paper. The experimental gain is small in the regular setting but clearer gains in a few shot learning setting and noisy training dataset (through back translation) setting. Overall the paper is clearly written and experiments are carefully studied. During the discussion phase, the authors provided results on the full GLUE dataset as well as other ablation studies (e.g., CE+CE recommended by R2), improving the paper.  
The paper presents a novel strategy for statistically motivated feature selection i.e. aimed at controlling the false discovery rate. This is achieved by extending knockoffs to complex predictive models and complex distributions via (multiple) generative adversarial networks.   The reviewers and ACs noted weakness in the original submission which seems to have been fixed after the rebuttal period   primary related to missing experimental details. There was also some concern (as is common with inferential papers) that the claims are difficult to evaluate on real data, as the ground truth is unknown. To this end, the authors provide empirical results with simulated data that address this issue. There is also some concern that more complex predictive models are not evaluated.  Overall the reviewers and AC have a positive opinion of this paper and recommend acceptance.
All of the reviewers have found some aspects of the formulation interesting, but they raised concerns regarding the practical use of the experimental setup. 
MAML is a well known gradient based bi level optimization to learn a good initialization over a set of relevant tasks. This paper investigate different variants of MAML, providing empirical analysis of two new algorithms (RDP and MCL). Reviewers agree that it is interesting to see what the change of optimization mechanism on both head and body brings to us in the MAML framework. This is done by only empirical analysis. However, all reviewers have concerns that the current version (or even revised one after the author responses) does not contain substantial contributions over existing work in the sense that: (1) experiments do not support well what s been claimed; (2) writing should be much improved to clearly explain the formulation of RDP and MCL, as well as figures in experiments; (3) the analysis about the importance of multi step adaptation is not clear (Section 4); (4) the proposed method has little improvements over baseline methods.  Without any positive feedback from reviewers, I do not have choice but to suggest rejection.     
This paper introduces a probabilistic generative model which mixes a variational autoencoder (VAE) with an energy based model (EBM). As mentioned by all reviewers (i) the motivation of the model is not well justified (ii) experimental results are not convincing enough. In addition (iii) handling sets is not specific to the proposed approach, and thus claims regarding sets should be revised. 
The paper studies an interesting question of the relationship between the eigenvalues of the Hessian matrix with the probability of the output in the logistic loss, and use this to propose a regularization that can improve the performance of the neural networks.   All the reviewers agree that although the question is interesting, the paper lacks significantly in terms of representation and would benefit from another round of revision.
The reviewers have unanimously expressed concerns about clarity, novelty, sound theoretical justification and intuitive motivation of the proposed approach. 
This paper presents a second order optimization algorithm for neural nets which extends LeCun s classic Lagrangian framework. The paper derives a method for computing the exact Newton step for a single training example for a multilayer perceptron. It then describes approximations that can be used to extend the method to more examples.  The authors claim to have spotted factual errors in the reviews. However, I ve looked into the issues, and I find myself agreeing with the reviewers on each of those points (or, if there are misunderstandings, they result from a lack of clarity in the paper rather than insufficient scientific computing background on the part of the reviewers).  The authors claim to have solved a longstanding problem by giving an efficient method for calculating the stochastic Newton step (for a single training example). However, it s not clear this is very useful; as a reviewer points out, estimating the curvature with a single example can t give a very accurate estimate. Once the method is extended to batches, more approximations are required. I also agree with the reviewers that the later parts of the methods section appear a bit rushed.  As the reviewers point out, in the experimental comparisons, the proposed method seems to underperform SGD with momentum even in terms of epochs, which is the setting where second order methods usually shine. Other second order optimizers (e.g. K FAC) have been shown to outperform first order methods in terms of both wall clock time and epochs, so epochwise improvement seems like the minimum bar for a second order optimization paper.
This paper was reviewed by three experts. After the author response, R2 and R3 recommend rejecting this paper citing concerns of novelty and experimental evaluation. R1 assigns it a score of "6" but in comments agrees that the manuscript is not ready for ICLR. The AC finds no basis for accepting this paper in this state.  
The idea of universal perturbation is definitely interesting and well carried out in that paper.
The paper proposes a simple method for detecting out of distribution samples. The authors  major finding is that mean and standard deviation within feature maps can be used as an input for classifying out of distribution (OOD) samples. The proposed method is simple and practical.  The reviewers and AC note the following potential weaknesses: (1) limited novelty and somewhat ad hoc approach, i.e., it is not too surprising to expect that such statistics can be useful for the purpose. Some theoretical justification might help. (2) arguable experimental settings, i.e., the performance highly varies depending on validation (even in the revised draft), and sometimes irrationally good. It also depends on the choice of classifier.  For (2), I think the whole evaluation should be done assuming that we don t know how it looks the OOD set. Under the setting, the authors should compare the proposed method and existing ones for fair comparisons. AC understands the authors follows the same experimental settings of some previous work addressing this problem, but it s time that this is changed. Indeed, a recent paper by Lee at al. 2018 considers such a setting for detecting more general types of abnormal samples including OOD.  In overall, the proposed idea is simple and easy to use. However, AC decided that the authors need more significant works to publish the work. 
This is an interesting paper discussing the impact of classifier abstention on the performance obtained for different groups of data. The reviewers are either very (scores of 8, 7 and 7) or moderately (score of 5) positive about the paper. The main concern is that the paper does not directly propose a solution for the discovered problems. Nevertheless, it can initiate interesting discussions and research around them.
The paper tackles an interesting problem, which is effectively modeling biological time series data. The advantages of deep neural networks over structured models like HMMs are their ability to learn features from the data, whereas probabilistic graphical models suffer from "model mismatch", where the available data must be carefully processed in order to fit the assumptions of the PGM. Any work advancing this topic would be extremely welcome in the world of machine learning in biology.  However, the reviewers each raised individual concerns about the paper regarding its clarity and quality, and the authors did not respond. Thus, the reviewers scores remain unchanged, and the rough consensus is a rejection.
The paper contains interesting ideas for giving simple explanations to a NN; however, the reviewers do not feel the contribution is sufficiently novel to merit acceptance.
Four reviewers rate this article borderline. R3 finds the paper clearly presented and the method effective, but misses quantitative analysis of the dynamic range problem as well as novelty. Following the discussion and revision, she/he considers the paper improved and updated the score to 5, still being concerned about the novelty. R1 considers the paper makes an important observation but has concerns about experiments, rating it 6. R2 considers that the paper contributes a clear idea, but indicates that more analysis and supporting results are needed. She/he indicated a number of shortcomings in the initial review, and found the update good, hence tending to rate the paper higher after the responses, 6. R4 considers the paper well motivated and the method valid. However, he/she found the writing poor and over claiming results, and that more rigorous mathematical notation would help. After the discussion and revision, he/she found the paper better and increased the score to 5, but still found issues preventing the paper from being accepted. In summary, the reviewers agree that the paper contains an interesting and well motivated method, but they also point at a number of shortcomings. The revision improved several of them but others persisted. Although the ratings improved after the discussion, the overall rating is borderline. This is a very competitive call, and hence I have to recommend reject at this time. 
The paper shows how to make use of a linear program for extracting logical rules for knowledge graph completion. Overall, the reviewers and I agree that this is an interesting and important direction for research. Moreover, the presented approach shows good performance with rather small sets of rules extracted. However, all reviewers point out that the related work is not well discussed. While the authors have improved the related work sections during the rolling discussion, overall the positioning of the new method has still to be improved, including a better empirical comparison across different datasets. Overall, we would like to encourage the authors to polish their line of research based on the feedback from the reviews.
All reviewers acknowledge that the idea of the paper is interesting but have expressed serious concerns on empirical evaluations. The paper is not suitable for publication in its current form.
Using volumetric convolutions, this paper focuses on learning in (rather than on) the unit sphere.  The novelty of the approach is debatable, and the mathematical analysis not strong enough to merit that.  In combination with good but not outstanding results, interest of the research community is doubted.  An extended experimental analysis of the method would greatly improve the paper.
This work proposes to train EBMs using multi stage sampling. The EBMs are then used for generating high dimensional images, performing image to image translation, and out of distribution detection. The reviewers are impressed with the results, but indicate that the novelty is limited. While I agree that the work can be seen as a combination of previously proposed techniques, demonstrating that this combination can be made to work well is still a significant contribution to the field. In addition, the paper demonstrates strong results in using Langevin dynamics to translate between images, which I do think is novel. I therefore recommend accepting the paper for a poster presentation.
The reviewer reactions to the initial manuscript were generally positive.  They considered the paper to be well written and clear, providing an original contribution to learning to cooperate in multi agent deep RL in imperfect domains.  The reviewers raised a number of specific issues to address, including improved definitions and descriptions, and proper citations of related work.  The authors have substantially  revised the manuscript to address most or all of these issues.  At this point, the only knock on this paper is that the findings seemed unsurprising from a game theoretic or deep learning point of view.  Pros: algorithmic contribution, technical quality, clarity Cons: no real surprises 
The paper presents a method for compositional task learning in the continual RL setting, by composing and reconfiguring neural modules. The method is evaluated on mini grids and simulated robot manipulation tasks.  The reviewers agree, and I concur, that the paper proposes an interesting solution to a difficult and important problem. The paper is well presented and would make a good addition to the multi task continual learning. The reviewers appreciate the authors  responses and the improvement to the manuscript, and in particular the extra experiments with the wrong number of modules.  The final version of the paper should:    Clarify the explanation of functional modularity   Move the relevant pieces to the main text.   See Gur et al., NeurIPS 2021, https://openreview.net/forum?id CeByDMy0YTL for a definition of learnable compositional tasks via Petri Net formalism.    Reviewers appreciate the extra experiments with the wrong number of modules.
The reviewers raised a number of concerns including insufficiently demonstrated benefits of the proposed methodology, lack of explanations, and the lack of thorough and convincing experimental evaluation. The authors’ rebuttal failed to alleviate these concerns fully. I agree with the main concerns raised and, although I also believe that the work can result eventually in a very interesting paper, I cannot suggest it at this stage for presentation at ICLR.
This paper proposes a meta attack approach based on meta learning approaches to learn generalizable prior from the previously observed attack patterns. The proposed approach is able to attack targeted models with much fewer queries. After author response, all reviewers are very positive about the paper. Thus I recommend accept. 
This paper proposes a fractional graph convolutional networks for semi supervised learning, using a classification function repurposed from previous work, as well as parallelization and weighted combinations of pooling function. This leads to good results on several tasks. Reviewers had concerns about the part played by each piece, the lack of comparison to recent related work, and asked for better explanation of the rationale of the method and more experimental details. Authors provided explanations and details, and a more thorough set of comparison to other work, showing better performance in some but not all cases. However, concerns that the proposed innovations are too incremental remain. Therefore, we cannot recommend acceptance.
This paper introduces a technique to measure the *expected* robustness of a neural network by measuring the probability random input perturbations will cause the model to make a mistake.  The reviewers are not convinced by the results in this paper. The methods are not carefully evaluated against prior work, and it is not exactly clear what lesson one can draw from the resulting statistical evaluation. The experimental setup is not clearly explained in several places, making the paper difficult to fully follow.  Since the authors do not respond to the reviewer concerns, there was no opportunity to address these concerns.
This paper presents a density ratio estimation approach to make the early decision for sequential data. The main contribution of this paper is the mathematical soundness of the proposed algorithm and all reviewers are unanimously positive about this paper with pretty good scores (7, 8, 6, 9, 7). However, despite the good scores, the verbal comments by the reviewers are not very strong except for one reviewer (R2); the reviewer with the highest score (9) did not provide detailed information about his/her rating. Also, the evaluation of this work is relatively weak because synthetic or simple datasets were employed for the experiment and the baseline methods are too straightforward. Also, it is not clear how the proposed algorithm can handle the data with sparse observations (data with idle times in the middle). Moreover, it does not provide rigorous stopping criteria although the authors proposed a simple method to determine the threshold, which is contradictory to the main objective of the proposed algorithm making early predictions on sequential data because the method requires "plotting the speed accuracy tradeoff curve on the test dataset." This response implies that it at least requires a withheld dataset. Although this issue can be regarded as a separate problem, the paper could have provided an ablation study with respect to the criteria.  Considering all these facts high scores but relatively low supports and confidences, and practical limitations, I would recommend accepting this paper as a spotlight presentation. 
This paper proposes a method to update the learning rate dynamically by increasing it in areas with higher sharpness and decreasing it otherwise. This would the hopefully leads to escaping sharp valleys and better generalization. Authors further provide some related theoretical results and several experiments to show effectiveness of their models.  All reviewers find the proposed method well motivated, novel and interesting. The paper is well written and easy to follow. However, both theoretical results and empirical evaluations could be improved significantly:  1  The theoretical results as is provides little to no insight about the algorithm and unfortunately, authors do not discuss the insights from the theoretical results adequately in the paper. See for eg. R1 s comments about this.  2  Given that the theoretical results are not strong, the thoroughness in empirical evaluation is important and unfortunately the current empirical results is not convincing. In particular, there are two main areas to improve:  a) Based on the Appendix D, the choice of hyper parameters seem to be made in an arbitrary way and all models are forced to use the same hyper parameters. This way, the choice of hyper parameters could potentially favor one method over the other. A more principled approach is to tune hyper parameters separately for each method.  b) It looks like the choice of #epochs has been made in an arbitrary way. For all experiments, it would be much more informative to have a figure similar to the left panel of Fig. 4 but with much more #epochs so that reader can clearly see if the benefit of SALR would disappear with longer training or not.  c) Based on the current results, SALR s performance  is on par with that of Entropy SGD on CIFAR 100 and WP and there is a very small gap between them on CIFAR 10 and PTB. I highly recommend adding ImageNet results to make the empirical section stronger. The other option is to compare against other methods in fine tuning tasks. That is, take a checkpoint of a trained model on ImageNet and compare SALR with other methods on several fine tuning tasks.  Given the above issues, my final recommendation is to reject the paper. I want to thank authors for engaging with reviewers during the discussion period and adding several empirical results to the revision. I hope authors would address the above issues as well and resubmit their work.
This paper extends an earlier work with scalar output to vector output. It establish a relationship of two layer ReLu network and convex program. The result can be used to design training algorithms for ReLu networks with provably computational complexity. Overall, this is an interesting idea, leading to better theoretical insights to computational issues of two layer ReLu networks. 
Overall, the paper proposes an interesting idea to share parameters across words and reduce the size of the embedding which hasn t been explored in the past with promising results on XNLI task. However, all the reviewers agree that the novelty of this paper is not enough. In addition, the clarity and experiments are not sufficient enough too.
This paper introduces and analyses a method to train a population of VAEs with mixed continuous (referred to as "style") and discrete (referred to as "labels") latent variables. The population is trained under the constraint that inferred discrete latent variables  to be the same for all models. The paper also investigates a data augmentation mechanism inspired by  (Antoniou et al., 2017). The presentation is overall clear and the idea is interesting, although the language of "agents" is not standard in generative model literature and is a bit confusing. The experiments also show very good clustering results of the proposed method. Unfortunately the pipeline was determined to be quite complex while the motivation for its design choices were unclear. This, combined with multiple concerns about the experimental validation, led to a reject decision. 
The goal of verification of properties of generative models is very interesting and the contributions of this work seem to make some progress in this context. However, the current state of the paper (particularly, its presentation) makes it difficult to recommend its acceptance.
This work proposes a method for automatic adaptation of the learning rate via a estimating quadratic approximation of the full batch during training. The method motivated by two observed properties of the loss landscape, first the full batch loss along the gradient direction is well approximated by a quadratic polynomial, and second the optimal full batch step size does not change quickly during training. Two primary criticisms raised by reviewers is the weak experimental evidence provided to validate the method and similarities with other approaches for adapting the learning rate. Ultimately reviewers remain unconvinced by the rebuttal and maintained their scores. The AC further stresses the difficulty of properly (and fairly) comparing optimization methods in deep learning. As is consistently shown in the literature, optimizer performance is typically dominated by hyperparameter tuning, this is particularly problematic when submissions tune their own baselines as authors naturally are incentivized to tweak their own methods until the method looks favorable relative to others. Comparing directly against prior published results tuned by other researchers would help alleviate reviewer concerns regarding hyperparameter tuning.
This paper proposes a method for refining distributional semantic representation at the lexical level. The reviews are fairly unanimous in that they found both the initial version of the paper, which was deemed quite rushed, and the substantial revision unworthy of publication in their current state. The weakness of both the motivation and the experimental results, as well as the lack of a clear hypothesis being tested, or of an explanation as to why the proposed method should work, indicates that this work needs revision and further evaluation beyond what is possible for this conference. I unfortunately must recommend rejection.
the paper validates the benefit of multi task learning on MNIST datasets, which is not sufficient for ICLR publication
The authors propose a model which combines a neural machine translation system and a context based machine translation model, which combines some aspects of rule and example based MT.  This paper presents work based on obsolete techniques, has relatively low novelty, has problematic experimental design and lacks compelling performance improvements. The authors rebutted some of the reviewers claims, but did not convince them to change their scores. 
The authors consider control tasks that require "inductive generalization", ie                                                      the ability to repeat certain primitive behaviors.                                                                                  They propose state machine machine policies, which switch between low level                                                       policies based on learned transition criteria.                                                                                      The approach is tested on multiple continuous control environments and compared to                                                    RL baselines as well as an ablation.                                                                                                                                                                                                                                               The reviewers appreciated the general idea of the paper.                                                                            During the rebuttal, the authors addressed most of the issues raised in the                                                         reviews and hence reviewers increased their score.                                                                                                                                                                                                                      The paper is marginally above acceptance.                                                                                           On the positive side: Learning structured policies is clearly desirable but                                                         difficult and the paper proposes an interesting set of ideas to tackle this                                                        challenge.                                                                                                                          My main concern about this work is:                                                                                                 The approach uses the true environment simulator, as the                                                                            training relies on gradients of the reward function.                                                                                This makes the tasks into planning and not an RL problems; this needs to be                                                        highlighted, as it severly limits its applicability of the proposed approach.                                                                               Furthermore, this also means that the comparison to the model free PPO baselines                                                    is less meaningful.                                                                                                                 The authors should clear mention this.                                                                                              Overall however, I think there are enough good ideas presented here to warrant                                                      acceptance. 
This paper proposed to improve the quality of underwater images, specifically color distortion and haze effect, by an unsupervised generative adversarial network (GAN). An end to end autoencoder network is used to demonstrate its effectiveness in comparing to existing works, while maintaining scene content structural similarity. Three reviewers unanimously rated weak rejection. The major concerns include unclear difference with respect to the existing works, incremental contribution, low quality of figures, low quality of writing, etc. The authors respond to Reviewers’ concerns but did not change the rating. The ACs concur the concerns and the paper can not be accepted at its current state.
The paper revisits lossless compression using deep architecture. In contrast to main stream approaches, it suggests to make use of probabilistic circuits, introducing a novel class of tractable lossless compression models. Overall, the reviews agree that this is an interesting direction and a novel approach. I fully agree. Actually, I like that the paper is not just saying well, we could use a probabilistic circuit for ensure tractability but also shows that there is still a benefit of different variable orderings for encoding and decoding. In any case, adding probabilistic circuits to the "compression family" is valuable and also paves the way to novel hybrid approaches, combining neural networks and probabilistic circuits. I have enjoyed reading the paper, reviews, and discussion.
This paper presents a new twist on the typical LSTM that applies several rounds of gating on the history and input, with the end result that the LSTM s transition function is effectively context dependent. The performance of the model is illustrated on several datasets.  In general, the reviews were positive, with one score being upgraded during the rebuttal period. One of the reviewers complained that the baselines were not adequate, but in the end conceded that the results were still worthy of publication.  One reviewer argued very hard for the acceptance of this paper "Papers that are as clear and informative as this one are few and far between. ... As such, I vehemently argue in favor of this paper being accepted to ICLR."
I have serious concerns about how experiments are reported in this paper. Most methods tried to compare at an iteration complexity of roughly 100 epochs because it is known more computation improves performance very significantly but the computational resources are limited for many researchers, especially in academia. While this convention may not be the ideal way to compare different methods, for fairness, this practice has been followed in most of previous papers.   Unfortunately this paper disregarded this practice, and on Imagenet the reported results from previous works were mixed at 100 epochs (e.g. STR) and at 500 epochs (rigL — which was explicitly marked to be 5x in the original paper) without any clarification, and the only other method in the table showing comparable performance to the proposed method, LRR,  also requires many more than 100 epochs. Moreover, the authors did not explicitly disclose the equivalent epochs of their algorithms in the Imagenet experiments, and this is not acceptable. Based on the information inferred from the current writing, it is extremely likely that significant unfair advantages were given to the proposed algorithms.   Since the authors did not report experiments appropriately,  this paper cannot be accepted in its current form regardless of other potential merits of the proposed methods. I hope the authors view this outcome positively, and proactively fix the problem. If in revised versions, the experiments are reported according to the common practice, I am sure the work would become publishable. 
This paper presents a approach to the distributed kernel k means problem using a combination of random features to efficiently approximate the kernel matrix, a distributed stochastic proximal gradient algorithm which calls a distributed lanczos algorithm as a primitive to find a low rank approximation to the kernel matrix, and additional compression to reduce the cost of the communications.   The algorithm is a novel combination of prior ideas, and empirically works well. However, the claimed theoretical convergence rate is not convincing: e.g., the convergence rate depends on the Frobenius norm of the error in approximating the kernel with random feature maps, which is O(n^2) for a problem with n data samples. This implies that O(n^2) iterations must be used in the algorithm, which is already slower than a naive approach to kernel k means.  This paper takes a promising approach to the problem, but as the potential contribution lies in combining prior ideas in order to obtain a provably guaranteed approximate solution to the distributed kernel k means problem, and the proposed algorithm was not shown to satisfy this promise, the recommendation is to reject.
The paper is looking at an interesting problem, but it seems too early. The approach requires training a new language model  from scratch for each new word, rendering it completely impractical for real use. The main evaluation therefore only considers four words   "bonuses", "explained", "marketers", "strategist" (expanded to 20 during the rebuttal). This is not sufficient for ICLR.
Pros   A novel formulation for cross task and cross domain transfer learning.   Extensive evaluations.  Cons   Presentation a bit confusing, please improve.  The paper received positive reviews from reviewers. But the reviewers pointed out some issues with presentation and flow of the paper. Even though the revised version has improved, the AC feels that it can be improved further. For example, as pointed out by reviewers, different parts of the model are trained using different losses and / or are pre trained. It would be worth clarifying that. It might help if the authors include a pseudocode / algorithm block to the final version of the paper.
The paper provides a proof that Transformer networks (a popular deep learning model) are universal approximators for sequence to sequence functions. The theorem relies on the idea of contextual mappings (Definition 3.1), which models the attention layers. The results provide an important starting point for understanding a very widely used architecture.  As with many theoretical papers, the reviewers provided several suggestions as to which are important parts to be presented in the main paper. The authors were very responsive during the discussion period, updating the structure of the paper significantly. This shows nice evidence supporting the need for a long discussion period for ICLR. One reviewer upgraded their score (to 8), which is not reflected in the system.  This is an excellent paper, providing much needed theoretical analysis of a popular neural architecture. Clear accept.  
This paper focuses on the adversarial robustness of deep neural networks against multiple and unforeseen threat models, which proposes a threat model called Neural Perceptual Threat Model (NPTM). The philosophy behind sounds quite interesting to me, namely, approximating human perception with a neural neural "neural perceptual distance". This philosophy leads to a novel algorithm design I have never seen, i.e., Perceptual Adversarial Training (PAT) which achieves good robustness against various types of adversarial attacks and even could generalize well to unforeseen perturbation types.   The clarity and novelty are clearly above the bar of ICLR. While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. Thus, all of us have agreed to accept this paper for publication! Please carefully address all  comments in the final version.
The paper proposes to learn a state representation using bi simulation in an RL setting. The approach is thoroughly evaluated on several benchmarks. In its current form the paper is mainly an empirical contribution, with now some theoretical contributions tucked away in the appendices. Nevertheless, an interesting approach with promising results.  The reviewers appreciated the revised paper and the discussion. The replies and discussions successfully addressed all serious concerns of the reviewers. Please also clarify the discussed points in the next iteration of the paper, and run the experiments with more seeds, as promised.
The paper presents a novel method of fusion of information from two modalities: text (context and question) and a Knowledge Base, for the task of question answering. The proposed method looks quite simple and clear, while the results show strong gains against baseline methods on 3 different datasets. Ablation studies show that the model achieves good performance on more complex questions. While the reviewers raise some concerns, e.g., on the sensitivity of the proposed method, the technical novelty against prior works, they see values in this paper in general. And the authors did a good job in their rebuttal. After several rounds of interactions, some reviewers were convinced to raise their scores by a little bit. As a result, we think the paper is in a good shape and ICLR audience should be interested in it.
All reviewers agree that this is a reasonable contribution but that it is also extremely limited in scope. The authors suggest in one of their response that their technique could apply to "any data mixing method with “batched k sum” structure". Such a larger level of generality might make the paper more interesting, but at the moment it is an extremely niche result.
This paper considered the computational budgets of adversarial training in the context of Federated Learning and studied the propagation of adversarial robustness from affordable parties to low resource parties. Although the authors conducted the extensive experiments to show the effectivenss of FedRBN, there are still important concerns from the reviewers,  (1) The novelty is marginal compared to FedBN, DBN and previous insights, which moves the similar framework to adversarial robustness and changed the rules, especially given the competitive ICLR. More theorectical novelty will be preferred.   (2) Many technical details are not well explained and some parts need to be improved, which make the reviewers not well convinced about FedRBN.   Given above points, I will recommend rejection and encourage the authors to improve the paper in the future.
The authors investigate various class aware GANs and provide extensive analysis of their ability to address mode collapse and sample quality issues. Based on this analysis they propose an extension called Activation Maximization GAN which tries to push each generated sample to a specific class indicated by the Discriminator. As experiments show, this leads to better sample quality & helps with mode collapse issue. The authors also analyze inception score to measure sample quality and propose a new metric better suited for this task.
This was an extremely difficult paper to decide, as it attracted significant commentary (and controversy) that led to non trivial corrections in the results.  One of the main criticisms is that the work is an incremental combination of existing results.  A potentially bigger concern is that of correctness: the main convergence rate was changed from 1/T to 1/sqrt{T} during the rebuttal and revision process.  Such a change is not trivial and essentially proves the initial submission was incorrect.  In general, it is not prudent to accept a hastily revised theory paper without a proper assessment of correctness in its modified form.  Therefore, I think it would be premature to accept this paper without a full review cycle that assessed the revised form.  There also appear to be technical challenges from the discussion that remain unaddressed.  Any resubmission will also have to highlight significance and make a stronger case for the novelty of the results.
This paper proposes that ML models might be better expressed in a way closer to their mathematical representation than to Python code.  This is an attractive proposition, but the paper s development of this proposition is that models might best be expressed in LaTeX, which is not a hypothesis that the reviewers consider proven.  Ultimately, this paper proposes a new language in which to express ML models, and compares that language against one baseline: PyTorch.  However, this is far from a reasonable baseline.  Even within Python, systems such as JAX (which the paper dismisses as a "Program translator") are much closer to the pure functional style; and going further afield, comparisons should be to DEX and Julia, to name but two.  The reviewers appreciate the approach to autobroadcasting, but again note that this does not require a new language, and again, systems like JAX, DEX, Julia all have approaches to broadcasting which are not compared.  Reviewer 8MK2 is concerned that we "still need to write other modules .. in Python", but the authors rebut this well: Kokoyi is not expected to be applied to an entire program, just to the model components.  Even if Kokoyi were to be successful, there is a question of its wider applicability.  A major strength of PyTorch/JAX is that they are used by a much larger community than just ML paper authors.  It is because the authors write in these tools that their work is usable by other practitioners.  The paper explicitly says it is targeted not even at ML paper authors, but at a subset of that community.  The usability analyses are very much lacking.  Lines of code is a notoriously coarse tool to assess programming paradigms.  I would also caution against trying to do any small group user study   the best initial study is to release Kokoyi into the wild, get feedback from users, and then if it proves popular, prepare a paper or monograph.  This is the path of PyTorch and other frameworks.    Until then, the paper may be of interest to a workshop very focused on programming models for ML, but is not currently suitable for the wider ICLR audience.
This work proposes the Federated Matching algorithm as a novel method to tackle the problems in federated learning. The paper is well written and original, and it contributes to the state of the art. 
The reviewers viewed the work favorably, with only one reviewer providing a score slightly below acceptance. The authors thoroughly addressed the reviewer s original concerns, and they adjusted their score upwards afterwards. The low rating reviewer remains skeptical of the significance of the work, but the other two reviewers make firm cases for the appeal of the work to the ICLR audience. In follow up discussion after the author s responses were submitted and discussed, the low rating reviewer did not make a clear case for rejecting the paper, and further, the higher rating reviewers  arguments for the impact of the paper were convincing. Therefore, I recommend accepting this paper.
This paper proposes a regularizer for recurrent neural networks, based on injecting random noise into the hidden unit activations.  In general the reviewers thought that the paper was well written and easy to understand.  However, the major concern among the reviewers was a lack of empirical evidence that the method works consistently.  Essentially, the reviewers were not compelled by the presented experiments and demanded more rigorous empirical validation of the approach.  Pros:   Well written and easy to follow   An interesting idea   Regularizing RNNs is an interesting and active area of research in the community  Cons:   The experiments are not compelling and are questioned by all the reviewers   The writing does not cite relevant related work   The work seems underexplored (empirically and methodologically)
This paper proposes loss functions to encode topological priors during data embedding, based on persistence diagram constructions from computational topology.  The paper initially had some expositional issues and technical questions, but the authors did an exceptional job of addressing them during the rebuttal period nearly all reviewers raised their scores (or intended to but didn t update the numbers on their original reviews).  The AC is willing to overlook some of the remaining questions. For example, concerns that topology isn t well known in the ICLR community (8muq) are partially addressed by the improved exposition (and it s OK to have technically sophisticated papers so long as some reviewers were able to evaluate them).  And, future work can address scalability of the algorithm, which indeed does seem to be a challenge here (ey6b).  In the final "camera ready," the authors are encouraged to address any remaining comments and to consider adding experiments/discussion regarding scalability to larger datasets.
This paper proposes a few shot learning method that learns task adaptive semantic features that can incorporate for both of the support and query sets. Two approaches for modality combination are developed. The additional experiments in the author response addressed some concerns of the reviewers. However, the technical novelty of the proposed method is high enough since the proposed method uses existing techniques.
This paper considers hierarchical reinforcement learning, and specifically the case where the learning and use of lower level skills should not be decoupled. To this end the paper proposes Hierarchical Proximal Policy Optimization (HiPPO) to jointly learn the different layers of the hierarchy. This is compared against other hierarchical RL schemes on several Mujoco domains.  The reviewers raised three main issues with this paper. The first concerns an excluded baseline, which was included in the rebuttal. The other issues involve the motivation for the paper (in that there exist other methods that try and learn different levels of hierarchy together) and justification for some design choices. These were addressed to some extent in the rebuttal, but I believe this to still be an interesting contribution to the literature, and should be accepted. 
The paper received 3,3,1 as reviews. All reviewers have the consensus on the weaknesses, i.e. limited technical novelty and weak boost in performance in datasets that may not be the state of the art anymore. The authors have submitted a rebuttal however the rebuttal did not improve the score of the reviewers. Following the reviewers recommendation, the AC recommends rejection.
This paper addresses a method for unsupervised meta learning where a VAE with Gaussian mixture prior is used and set level inference, taking episode specific dataset as input, is performed to calculate its posterior. In the meta testing phase, semi supervised learning with the learned VAE is used to fast adapt to few show learning. Reviewers are satisfied with the author responses, agreeing that the method is a principled way to tackle unsupervised meta learning.  
This paper is interesting since it goes to showing the role of model averaging. The clarifications made improve the paper, but the impact of the paper is still not realised: the common confusion on the retraining can be re examined, clarifications in the methodology and evaluation, and deeper contextulaisation of the wider literature.
This manuscript studies scaling distributed stochastic gradient descent to a large number of nodes. Specifically, it proposes to use algorithms based on population analysis (relevant for large numbers of distributed nodes) to implement distributed training of deep neural networks.   In reviews and discussions, the reviewers and AC note missing or inadequate comparisons to previous work on asynchronous SGD, and possible lack of novelty compared to previous work. The reviewers also mentioned the incomplete empirical comparison to closely related work. On the writing, reviewers mentioned that the conciseness of the manuscript could be improved. 
Casting domain generalization as a rate distortion problem and developing an information theoretic approach to solving it looks like an interesting idea. While the proposed method is technically sound, the assumption made in the proposed method is too strong to hold in real world applications. Though in the rebuttal the authors provided additional experiments on two benchmark datasets, reviewers  concerns about the strong assumption made in the proposed algorithm still remain. To address this issue, I think besides conducting more extensive experiments, the authors also need to analyze when the assumption does not hold in practice, why the proposed algorithm could still perform well compared with other domain generalization methods.  In summary, this is a borderline paper below the acceptance bar of ICLR.
The paper proposes Data Dependent GCN (D2 GCN), which improves the efficiency of vanilla GCN by node wise skipping, edgewise skipping, and bit wise skipping. Gate functions are learned to prune the unimportant neighbor nodes in combinations, unimportant edge connections, and in the bit precision. The proposed method boosts efficiency while achieving comparable performance over benchmark datasets. Most reviewers agree that the paper is well motivated, and the writing is clear. However, two of the reviewers found the novelty of the paper compared to previous work (for example, [1]) is limited. Three reviewers raised concerns about the lack of theoretical or empirical analysis on how D2 GCN can alleviate the over smoothing problem, and how the proposed method can serve as an implicit regularization.  For the novelty concerns, the authors provided a detailed comparison with previous work during the rebuttal. For the lack of analysis on over smoothing, the authors provided an additional empirical analysis using the distance of different intermediate layers’ output as the metric for measuring over smoothing. But at least one reviewer is still not satisfied with those.  Given the current review scores (3, 5, 5, 6), the paper is below the acceptance threshold for the conference. The AC believes that the proposed method seems to be an effective and simple way towards more efficient graph neural networks and hence encourages the authors to submit the revised paper to another venue after addressing the reviewers’ concerns, especially on theoretical or empirical analysis on over smoothing and implicit regularization.   [1]: Gated graph sequence neural networks
This paper addresses an importnant and more realistic setting of multi task RL where the reward function changes; the approach is elegant, and empirical results are convincing. The paper presents an importnant contribution to the challenging multi task RL problem.
Reviewers agree that this work is promising. The paper is well grounded in the literature and different aspects of the considered methods are investigated through a variety of experiments. Unfortunately, this paper does not provide sufficient details to allow the reader to understand what has been done nor how to adequately build from it. For example, details in the Appendix lack sufficient formalization of the equations or concepts used to train the preference based agents. The paper would benefit from clarifications of the method, procedures, and equations used. Beyond that, a major concern lied within the evaluation of the simulated patients across different initializations. Provided that one of the proposed contributions of this paper is a robust simulation platform for RL research within healthcare, it would be important to report convincing results on the patient physiologies admitted by the simulator and characterizing the behaviors of policies learned using this simulator. Finally, issues regarding the structure of the paper, including the split between the main paper and the Appendix, should be resolved before this paper can be published. Notably, the authors should consider elevating important material from the Appendix into the main paper.
The SketchODE submission is a continuously valued model for chirographic drawing data such as handwritten digits or sketches. It relies on variational sequence to sequence model where the latent code z is a global encoding of the drawing dynamical, and contains a neural controlled differential equation encoder to encode a discrete 2D drawing sequence s, and an augmented neural ODE decoder (conditional on the latent code z) to model both the first order dynamics both of the drawing velocity and of the pen state (effectively modelling second order dynamics on the pen position). The model enables to sample sketches by sampling latent codes, as well as to interpolate between two latent codes, and is evaluated on VectorMNIST (a new task), QuickDraw sketches, and DiDi schematics, where it is compared to discrete RNN based Seq2Seq and two more recent baselines.  Reviewers praised the idea of using continuously valued Neural ODEs for drawing, compelling properties of the model for conditional generation or interpolation, the new VectorMNIST dataset, and the writing. Reviewers had some concerns: overstating the novelty and contribution to general continuous seq2seq given that the evaluation was done only on chirographic drawing tasks (Q3GY, zrrF), some experimental details such as missing ablations, examples from QuickDraw or Didi, or comparisons with transformers (Q3GY, zrrF), clarifications on computational complexity (zrrF, S7jh), situating the work with respect to applications of Neural ODEs to physics (zrrF); most of these concerns were addressed in the rebuttal. Reviewer KvGm had the most concerns about the experimental section, but has increased their score after the discussion with the authors.  There was no discussion among the reviewers, only between the authors and reviewers zrrF and KvGm. After the authors  rebuttal, the scores became 8. 8, 6 and 5, and thus I believe that the paper meets the conference acceptance bar.
This paper introduces a first occupancy representation for reinforcement learning problems, with potential benefits on problems with non stationary rewards.  The representation is defined analogously to the successor representations, but captures the expected discounted time to first arrive at a state instead of measuring discounted visitations.  The paper develops the idea and illustrates some uses for exploration, unsupervised RL, and non stationary reward functions (for example when food rewards are consumed).  The reviews brought forward a number of related older ideas in the literature, where several aspects of the method have been previously developed.  These include dynamic goal learning, option conditional predictions, general value functions, dynamical distance learning, and temporal difference models.  However, from the author response and ensuing discussion, the exact form of the proposed representation has not been studied for the purposes presented in this paper.  The reviewers appreciated the utility of this representation for problems with non Markovian rewards, in particular that “the use of the first occupancy values as an exploration bonus results in much more efficient exploration”.  Multiple reviewers commented on the desire for a stronger empirical evaluation, but they were satisfied with the contribution of the paper.  The reviewers arrived at a consensus that the paper contributes a new representation for RL problems with non stationary rewards, with two reviewers strongly convinced and none opposed.  The paper is therefore accepted.
The paper presents a thorough comparison of different algorithms for multi agent Deep RL methods. The conclusions of the paper is that across a variety of envionment and hyperparameter tuning, multi agent PPO seems to peform well relatively to the competitors.  The reviewers agreed that the paper fills a gap in the literature regarding a fair and thorough comparison of algorithm, and that the paper clearly presents the results. As it stands, the code to reproduce the experiments and the results are a useful contribution to the community. However, the reviewers felt the technical contribution to be below the bar for ICLR, as the paper does not help in understanding the differences between algorithms, or develop insights as to how to further improve algorithms. The large standard deviations of the various algorithms also makes the main experimental insight (MAPPO works consistently well) relatively weak.  
This paper received mixed reviews, 3 positives (7, 6, 6) and 2 negatives (4, 4). Due to the divergence of the reviews, I carefully read the paper and made my best efforts to understand the paper and the review comments. This paper proposes to learn a quantization network using a small calibration set given a network trained with the full precision. The combination of AdaQuant, integer programming, and batch norm tuning makes sense although they do not have substantial novelty. The three components are reasonably tightly coupled and comprise a complete algorithm. However, the sequential AdaQuant distracts the main claim of this work significantly. This is probably added during the review process but looks ad hoc to me. Sequential AdaQuant seems to be effective to improve accuracy, but cannot be applied before the bit allocation was set, which makes it require integer programming no more. Because of this issue, the overall presentation becomes confusing and the argument sometimes sounds unfair (please refer to the last posting by R5.).   In addition, the presentation of this paper could be improved, especially for the details of the integer programming formulation. It is not clear how to define some variables mathematically. The discussion about the size of the calibration set together with the overfitting issue is lacking, and rigorous discussion and analysis would make the paper much stronger. The reviewers are not convinced of the novelty of this paper, and they rather believe that this is an engineering oriented work. Considering this fact,  the evaluation of this paper is not very comprehensive. The ablation study with respect to the size of the calibration set should be conducted more intensively. The experiment fails to show the benefit of mixed precision quantization effectively and it is limited to presenting the compression ratio in Figure 3. The authors used a small calibration set taken from the training dataset, which looks weird because they claim that the post training quantization requires only a small "unlabeled" calibration set at the beginning of the abstract; it is more desirable to use arbitrary examples in the same domain.  Despite the interesting aspects, I believe that this paper needs a focus and substantial improvement for publication, and, consequently, recommend rejection.
This paper is somewhat incremental on recent prior work in a hot area; it has some weaknesses but does move the needle somewhat on these problems.
The paper study to what extent languages are hard to model by a conditional language model based on information theoretic measurements.   Overall, the reviewers value the systematic and extensive controlled experiments present in the paper. However, the presentation of the paper makes it very hard to follow and reviewers all still complain that it is hard to understand the take home message of the paper.    Despite the reviewers also appreciate the authors  effort in improving the paper, submitting the revision, responding to the feedback, they still conclude that significant reorganizing and revising of the paper is needed before it can be published.   In particular, the paper may be able to improve by backing up the empirical study with some linguistic phenomena or by a more careful rewriting in explaining and discussing the empirical results.   Some other strong arguments such as "Our application of statistical comparisons as a fairness measure also serves as a novel rigorous method for the intrinsic evaluation of languages, resolving a decades long debate on language complexity." may need to be carefully revised. In this particular example, it is unclear how this paper "resolve" the debate on language complexity by demonstrating a few experiments.  Several sentences like this one should be revised.    
The paper presents a new dataset, containing around 8k pictures of 30 horses in different poses. This is used to study the benefits of pretraining for in  and out of domain images.  The paper is somewhat lacking in novelty. Others have studied the same type of pre training in the past using other datasets, which makes the dataset the main novelty. But reviewers raised many questions about the dataset, in particular about how many of the frames of the same horse might be similar, and of how few horses there are; few enough to potentially not make the results statistically meaningful. The authors replied to these questions more by appealing to standards in other fields than by explaining why this is a good choice. Apart from these crucial weaknesses, however, the research appears good.  This is a pretty clear reject based on lack of novelty and oddities with the dataset.
All reviewers are very critical about the submitted paper regarding novelty of results, insufficient placement with respect to existing results, and clarity of presentation. The authors also did not submit a rebuttal. Hence I am recommending rejection of the paper.
All reviewers come to agreement that this is a solid paper worth publishing at ICLR; the authors are encouraged to incorporate additional comments suggested by reviewers.
All four reviewers were against accepting the paper. A major point shared by everyone was lack of clarity: this included its overall writing, its discussion toward prior work, and imprecise math to explain the ideas. The paper did improve quite a bit over its revisions. Whether this clarified all of the reviewers  understanding of the paper remains unclear. The work may ultimately need another cycle of reviews to assess its quality.  Another shared point are a number of recommended ablations in the experiments, as well as going through more comprehensively in the set of studied datasets (R3), effect of AE choices (R2), and alternatives to the geodesic (R1, R2).
The paper is proposed a rejection based on majority reviews.
The authors study the theoretical performance of a meta learning in two settings. In the first one the overall number of possible tasks is limited and tasks are close in KL divergence. The second setting is MAP estimation (in a hierarchical Bayesian framework) for a family of linear regression tasks. Lower and upper bounds are provided on minimax parameter estimation error. This paper has spurred a lot of discussion among reviewers and (competent) external commentators. Most of these criticisms were right on target, but the authors managed to convince the reviewers and myself that there was simply an issue of presentation of the main results. I suggest the authors to take into serious considerations all the aspects raised by the reviewers that has generated misinterpretations of the presented results.
Biological memory systems are grounded in spatial representation and spatial memory, so neural methods for spatial memory are highly interesting. The proposed method is novel, well designed and the empirical results are good on unseen environments, although the noise model may be too weak. Moreover, it would have been great to evaluate this method on real data rather than in simulation. 
Although the rebuttal helped clarify the reviewers  confusion on notational confusion and the motivation of problem setup, all reviewers are still in a position of unable to championing the paper:   the technical concerns by Reviewer 4 need to addressed   the paper would have been stronger if baselines such as one class classification / outlier detection have been compared   the algorithm also has at least one short coming over other techniques that it needs to wait some time until it can collect enough test data  We hope the reviews can help the authors strengthen the paper in the next revision. 
This paper explores the contrast in performance between easy and hard tasks (episodes) in few shot image classification and propose mitigating strategies to avoid large performance gaps.  None of the reviewers support the acceptance of this work, despite the authors  detailed rebuttals, with all reviewers confirming their preference for rejection following the author response. Issues raised included lack of clarity of writing and lack of sufficiently convincing experimental results.  I unfortunately could not find a good reason to dissent from the reviewers majority opinion, and therefore also recommend rejection at this time.
This paper proposes a simple generalization to epsilon greedy exploration that induces temporally extended probes and can leverage options.  The idea and analysis are trivial.  Computational results demonstrate when this sort of exploration is helpful.  The paper is well written and the authors offer a fair assessment of when these ideas do or do not address challenging exploration tasks.  A range of computational results support and offer insight into the concepts. 
The paper addresses sample efficient robust policy search borrowing ideas from active learning. The reviews raised important concerns regarding (1) the complexity of the proposed technique, which combines many separate pieces and (2) the significance of experimental results. The empirical setup adopted is not standard in RL, and a clear comparison against EPOpt is lacking. I appreciate the changes made to address the comment, and I encourage the authors to continue improving the paper by simplifying the model and including a few baseline comparisons in the experiments.
The reviewers agree that this paper overcomes a number of difficult algorithmic and technical challenges in parallelizing the RED method for image reconstruction.  
The paper addresses the problem of learning to (re)rank slates of search results while optimizing some performance metric across the entire list of results (the slate). The work builds on a wealth of prior work on slate optimization from the information retrieval community, and proposes a novel approach to this problem, an extension of pointer networks, previously used in sequence learning tasks.   The paper is motivated by an important real world application, and has potential for significant practical impact. Reviewers noted in particular the valuable evaluation in an A/B test against a strong production system   showing that the work has practical impact. Reviewers positively noted the discussion of practical issues related to applying the work at scale. The paper was found to be clearly written, and demonstrating a thorough understanding of related work.  The authors and AC also note several potential weaknesses. Several of these were addressed by the authors, as follows. R3 asked for more breadth on metrics, and additional clarifications   the authors provided the requested information. Several questions were raised regarding the diverse clicks setting and choice of hyperparameter \eta   both were discussed in the rebuttal. Further analysis / discussion of computational and performance trade offs are requested and discussed.  Overall, the main drawback of the paper, raised by all three reviewers, is the size of the contribution. The paper extends an approach called "pointer networks" to the model application setting considered here. The reviewers and AC agree that, while practically relevant and interesting, the research contribution of the resulting approach limited. As a result, the recommendation is to not accept the paper for publication at ICLR in its current form. 
The paper proposes a new method for interpreting the hidden units of neural networks by employing an Indian Buffet Process. The reviewers felt that the approach was interesting, but at times hard to follow and more analysis was needed. In particular, it was difficult to glean any advantage of this method over others. The authors did not provide a response to the reviews.
The paper proposes to use the Huber and absolute loss for value function estimation in reinforcement learning, and optimizes it by leveraging a recent primal dual formulation by Dai et al.   This is a controversial paper. On one hand, it is a well motivated idea to apply robust loss on RL; the paper implemented the idea well by leveraging the saddle point formulation, and empirically demonstrate its advantages in practice.   On the other hand, the technical novelty of this paper is limited. The idea of Huber and standard conjugate formulation are straightforward application of existing techniques (despite being well motivated).   The authors seem to think that there has been no application of Huber loss on RL. But existing implementations of RL already uses Huber loss. For example, in the openAI baselines (https://openai.com/blog/openai baselines dqn/), they said the following:   "Double check your interpretations of papers: In the DQN Nature paper the authors write: “We also found it helpful to clip the error term from the update [...] to be between  1 and 1.”. There are two ways to interpret this statement — clip the objective, or clip the multiplicative term when computing gradient. The former seems more natural, but it causes the gradient to be zero on transitions with high error, which leads to suboptimal performance, as found in one DQN implementation. The latter is correct and has a simple mathematical interpretation — Huber Loss. You can spot bugs like these by checking that the gradients appear as you expect — this can be easily done within TensorFlow by using compute_gradients."   The authors discussed the first approach above on in the rebuttal, but I am not sure if the authors have considered the second method. If not, it would be worthwhile to discuss and compare with it.    See also "Agarwal et al. An Optimistic Perspective on Offline Reinforcement Learning" and "Dabney et al. Distributional Reinforcement Learning with Quantile Regression."  On the other hand, I have not seen the application of saddle point approach by primal dual method of Dai on Huber specially.   It seems that the proposed algorithm is in the end equivalent to MSBE+primal dual+ (h with softmax output). If it is that simple, I think it would help the readers to explicitly point this out upfront in the beginning (which is an interesting conceptual connection).  Because the primal dual approach need to be approximate h with a neural network, the difference of the two methods is vague in the primal dual space.   A side mark:  when we say "an objective for which we can obtain *unbiased* sample gradients", i think that the gradient estimator of the augmented Lagrange is unbiased; the gradient estimates of MHBE and MABE are still biased.    Overall, it is a paper with a well motivated and valuable contribution, but limited in terms of technical depth and novelty.
The paper proposes a novel method to calibrate a knowledge graph embedding method when ground truth negatives are not available. Essentially, the method relies on generating corrupted triples as negative examples to be used by known approaches (Platt scaling and isotonic regression).   This is claimed as the first approach of probability calibration for knowledge graph embedding models, which is considered to be very relevant for practitioners working on knowledge graph embedding (although this is a narrow audience). The paper does not propose a wholly novel method for probability calibration. Instead, the value in experimental insights provided.  Some reviewers would have liked to see a more in depth analysis, but reviewers appreciated the thoroughness of the results in the clear articulation of the findings and the fact that multiple datasets and models are studied.   There was an animated discussion about this paper, but the paper seems a useful contribution to the ICLR community and I would like to recommend acceptance. 
This paper provides a unifying perspective regarding a variety of popular DNN architectures in terms of the inclusion of multiplicative interaction layers.  Such layers increase the representational power of conventional linear layers, which the paper argues can induce a useful inductive bias in practical scenarios such as when multiple streams of information are fused.  Empirical support is provided to validate these claims and showcase the potential of multiplicative interactions in occupying broader practical roles.  All reviewers agreed to accept this paper, although some concerns were raised in terms of novelty, clarity, and the relationship with state of the art models.  However, the author rebuttal and updated revision are adequate, and I believe that this paper should be accepted.
The reviewers agree that the paper needs significantly more work to improve presentation and is not fully empirically and conceptually convincing.
The paper goes over a long list of proposed clustering similarity indices and attempts to provide a taxonomy of those by their different approaches and the extent by which they satisfy a list of "desired properties" proposed by the authors. This is very much in the spirit of earleir work on clustering similaritie by [Meila 2007] and on clustering quality measures [Ackerman, Ben David 2008, 2009].  While there may be some interest in such a compendium, there is no much novelty in this paper and it relevance to practice is also unclear.
This paper proposed an enhanced autoencoder for collaborative filtering by adding another element wise neural network for rating predictions. Overall the scores are negative, where reviewers pointed out concerns around the motivation, time complexities, and most importantly, using rating prediction as the evaluation setting which ignores the missing not at random nature of the recommender systems. The authors didn t provide any response. Therefore, I vote for rejection.
This paper proposes an offline neural method using concrete/gumbel for learning a sparse codebook for use in NLP tasks such as sentiment analysis and MT. The method outperforms other methods using pruning and other sparse coding methods, and also produces somewhat interpretable codes. Reviewers found the paper to be simple, clear, and effective. There was particular praise for the strength of the results and the practicality of application. There were some issues, such as only being applicable to input layers, and not being able to be applied end to end. The author also did a very admirable job of responding to questions about analysis with clear and comprehensive additional experiments. 
I think this paper has more positives than the reviews might indicate. And I do not share all the reviewers  concerns about the content of the paper. I think that there are a few concerns, though, that still suggest this paper should not be accepted as it is, when taken in conjunction with the concerns brought up by the reviewers.  On the positive side:     Asynchronous methods often give significant improvements, and the throughput benefits can even be seen here in Table 6.    The experiments are detailed with a lot of results comparing against many alternatives, including for ImageNet.  On the negative side:     The biggest concern I have with this paper is the scale of the experiments. This is supposedly about "distributed" SGD, but the largest scale experiment was run on only two workstations, and many experiments were run in the S1 and S2 settings which don t seem to be distributed at all (run on a single workstation). That is, there s a mismatch between the scale at which these experiments were run and the scale at which people want to run distributed deep learning.    The theory seems to be only an incremental change to the standard local SGD theory. The paper says "At a naive first glance, studying the convergence properties of locally asynchronous SGD would be an incremental to existing analyses for local SGD" but then it does not satisfactorily explain _why_ the approach is _not_ incremental. Not enough is done in the paper to explain why the analysis is not just a trivial combination of the local SGD with the standard approach to make an algorithmic analysis asynchronous. (Or, if the theoretical result _is_ incremental, the paper should make less of a big deal out of it.)    The description of the algorithm in Section 1.2 is confusing. I think it would benefit from being more concrete.  The paper should also compare against the paper "Asynchronous Decentralized Parallel Stochastic Gradient Descent" (Lian et al, 2017). It is actually not clear to me whether the method proposed here is a subset (or superset) of the method described in that paper, but they seem _very_ similar.
The paper addresses the learning of robot controllers for changing or unknown environments. It makes use of differentiable physics for online system identification and of reinforcement learning for offline policy training. A universal controller is trained on a distribution of simulation parameters in order to ensure its robustness. Differentiable physics is used to estimate the simulation parameters from the recent observation history. These parameters are fed to the controller so as to modulate the policy. This approach is evaluated on three benchmarks (2 + 1 added during the rebuttal).  The main originality of the paper is the use of differentiable physics for the identification of the parameters in the context of varying environments. The topic is of interest and in line with the recent developments for robotics. However, the novelty is limited, and all evaluators were concerned about the limited experimental contribution. The authors have added a new experiment during the rebuttal but this was not considered sufficient to change the evaluation. Overall this is considered as a promising contribution, but the experimental setting should be largely improved with additional problems and comparisons with SOTA methods from the recent RL literature.
After reading the paper, reviews and authors’ feedback. The meta reviewer agrees that this paper addresses an important topic. However, as the reviewers pointed out. The paper mainly builds the technique on simulated setting, and it is unclear how the method will translate to real world speedups. Past work(e.g. [1]) has also shown that many cases there could be a huge gap when the solution is not built carefully.  The paper would benefit from a prototype to demonstrate the applicability of the approach. This paper is therefore rejected.  Thank you for submitting the paper to ICLR.   [1] Riptide: Fast End to End Binarized Neural Networks 
The paper studies the Bayesian persuasion model in a more realistic setting where the sender does not know the receiver’s utility but can interact with the receiver repeatedly to learn the utility. The paper proposes a learning based framework to optimize the sender’s strategy, then analyze the theoretical properties of the proposed framework, and perform extensive experiments. The reviewers acknowledged that the paper investigates an important problem of relaxing the practical shortcomings of the Bayesian persuasion model. However, the reviewers pointed out several weaknesses in the paper, and there was a clear consensus that the work is not ready for publication. The reviewers have provided very detailed and constructive feedback to the authors. We hope that the authors can incorporate this feedback when preparing future revisions of the paper.
The paper proposes a way to find why a classifier misclassified a certain instance. It tries to find pertubations in the input space to identify the appropriate reasons for the misclassification. The reviewers feel that the idea is interesting, however, it is insufficiently evaluated.  Even for the datasets they do evaluate not enough examples of success are provided. In fact, for CelebA the results are far from flattering.
The paper proposes a method to speed up training of deep nets by re weighting samples based on their distance to the decision boundary. However, they paper seems hastily written and the method is not backed by sufficient experimental evidence.
I thank the authors and reviewers for the discussions. Reviewers agreed the work is interesting but there are some aspects of the paper that need improvements. In particular, authors need to better address concerns raised by R5. Given all, I think the work still needs a bit more work before being accepted.  
As the reviewers have pointed out and the authors have confirmed, the original version of this paper was not a significant leap beyond combining recent understanding of Neural Tangent Kernels and previous techniques for kernelized bandits. In a revision, the authors updated their draft to allow the point at which gradients are centered around, theta_0, to now equal theta_t. This seems like a more reasonable algorithm and it is satisfying that the authors were able to maintain their regret bound for this dynamic setting. However, the revision is substantial and it seems unreasonable to expect reviewers to read the revised results in detail the reviewers also felt it may be unfair to other ICLR submissions. All reviewers believe the paper has introduced valuable contributions to the area but should go under a full review process at a future venue. A reviewer would also like to see a comparison to Kernel UCB run on the true NTK (or a good approximation thereof). 
The paper proposes a new pre trained language model for information extraction on documents. It consists of a new pre training strategy with area masking and a new graph based decoder to capture the relationships between text blocks. Experimental results show better performances of the proposed approach.  Pros • The paper is generally clearly written. • Experimental results show better performances on the benchmark datasets.  Cons • Novelty of the work might not be enough. For example, the graph based decoder is not new. The masking technique is also a natural extension of that in BERT. • Significance of the work might not be enough. For example, the improvement from the area masking is not so significant. • There are additional experiments which can be added, as pointed out by Reviewer 3. • Presentation can be further improved. Some of the issues indicated by the reviewers have been addressed in the rebuttal. We appreciate the authors’ efforts.  During the rebuttal, the authors have addressed the clarity issues pointed out by the reviewers. However, the main issues in novelty and significance still exist. The reviewers think that the quality of the work is still not high enough as an ICLR paper.  
This paper proposes a new semiretro algorithm by combining the two major approaches of retrysynthesis, the template based method and the template free method   breaking a full template into several semi templates and embedding them into the two step template free framework.  They also obtained state of the art performance in this task based on the recent GNN architecture. Although all reviewers were satisfied with the idea and excellent performance of this paper, the possibility of information leakage in their experiments was raised through the discussion period, and the authors seem to agree to this to some extent.  In conclusion, it is difficult to say that accurate and rigorous experimental verification of the proposed method has been made yet. I encourage the authors to resubmit the paper after correcting errors in their experiments.
The paper propose a new metric for the evaluation of generative models, which they call CrossLID and which assesses the local intrinsic dimensionality (LID) of input data with respect to neighborhoods within generated samples, i.e. which is based on nearest neighbor distances between samples from the real data distribution and the generator. The paper is clearly written and provides an extensive experimental analysis, that shows that LID is an interesting metric to use in addition to exciting metrics as FID, at least for the case of not to complex image distributions The paper would  be streghten by showing that the metric can also be applied in those more complex settings.  
This paper does not meet the bar for ICLR   neither in terms of the quality of the write up, nor in experimental design. The two confident reviewers agree to reject the paper, the weak accept comes from a less confident reviewer who did not write a good review at all. The rebuttal does not change this assessment.
The paper presents an RL approach to the problem of graph sparsification. The reviewers expressed concerns about novelty, presentation, the correctness of some claims, and experimental validation. While the authors provided rebuttal and addressed some questions (leading to the increase of the score), some reviewers thought the authors focused on a justification of why suggested experiments were not done rather than doing them. We believe the paper in its current state is below the bar and recommend rejection.
The paper considers the setting of distributed optimization and proposes an adaptive gradient averaging and compression scheme to reduce the communication cost. The proposed scheme is shown to achieve the same convergence rate as full gradient AMSGrad algorithm, but due to the reduced cost, it exhibits linear speedup as the number of workers grows.  The reviews appreciated the clear presentation of the results, technical soundness, and convincing numerical experiments. The paper is a solid contribution to distributed optimization. Thus, I recommend acceptance.
This paper presents two novel VAE based methods for semi supervised anomaly detection (SSAD) where one has also access to a small set of labeled anomalous samples. The reviewers had several concerns about the paper, in particular completely addressing reviewer #3 s comments would strengthen the paper.
+ a simple method + producing diverse translation is an important problem     technical contribution is limited / work is incremental   R1 finds writing not precise and claims not supported,  also discussion of related work is considered weak by R3   claims of modeling uncertainty are not well supported   There is no consensus among reviewers.  R4 provides detailed arguments why (at the very least) certain aspects of presentations are misleading (e.g., claiming that a uniform prior promotes diversity). R1 is also negative, his main concerns are limited contribution and he also questions the task (from their perspective  producing diverse translation is not a valid task; I would disagree with this).  R2 likes the paper and believes it is interesting, simple to use and the paper should be accepted. R3 is more lukewarm.   
The paper considers the task of sequence to sequence modelling with multivariate, real valued time series. The authors propose an encoder decoder based architecture that operates on fixed windows of the original signals.  The reviewers unanimously criticise the lack of novelty in this paper and the lack of comparison to existing baselines. While Rev #1 positively highlights human evaluation contained in the experiments, they nevertheless do not think this paper is good enough for publication as is. The authors did not submit a rebuttal.  I therefore recommend to reject the paper.
This paper studies how self supervised objectives can improve representations for efficient RL. The reviewers are generally in agreement that the method is interesting, the paper is well written, and the results are convincing. The paper should be accepted.
This paper tries to establish that LSTMs are suitable for modeling neural signals from the brain.  However, the committee and most reviewers find that results are inconclusive.  Results are mixed across subjects.  We think it would have been far more interesting to compare other types of sequence models for this task other than the few simple baselines implemented here.  It is also unclear what is the LSTM learning extra in contrast with the other models presented in the paper.
This paper presents a two step approach to achieve disentangled representation and good reconstruction at the same time in deep generative models: the first step focuses on good disentanglement (e.g., with beta TCVAE) while possibly sacrificing reconstruction reconstruction, while the second step focuses on high quality reconstruction, conditioned on the low quality reconstruction from disentangled representation. In this paper, each step uses an existing method: beta TCVAE is used for the first step and AdaIN is used for the second, so the paper presents an intuitive combination of two existing methods to achieve both goals. Some useful ablation studies are provided to empirically justify the specific method choices. The concern is whether the two step approach is necessary to achieve both; the authors  argument is that models learning only one set of latent variables may not have the capacity to achieve both goals, and methods jointly learning two set of variables ("disentangled" and "correlated" variables) can not guarantee they represent disjoint structures of data. Some of these statements seem somewhat handwavy (including the d separation argument, I am not very sure if it applies when the variables are learned separately), and shall be made rigorous and justified (theoretically and/or empirically).  The reviewers rate this paper to be borderline.
All the reviewers and I agree that the proposed approach is interesting and the paper is overall well written. However, I agree with R3 that the paper  need further re working the theoretical part (see the post rebuttal comments of R4). Thus, I would encourage the authors to carefully address the comments of the reviewers in the revised version of the paper, which would ultimately improve the quality of the paper. 
This is a good contribution, with the potential to become extremely good and significant if presentation is substantially improved. All reviewers comment on the lack of clarity of the paper, especially concerning its central contributions (Section 4 and 5), as illustrated also by the relatively low confidence scores. Reviewers also mention the current imbalance between the generality of high order compositional networks and the motivation and empirical evaluation of these models. Generalizations of graph neural representations based on higher order local interactions are particularly interesting in contexts such as combinatorial optimization, where heuristics typically exploit high order interactions.  In summary, we believe this work deserves a further iteration before it can be in proceedings in order to improve the exposition and the motivation of compositional networks, that will greatly improve its exposure to the community.  That said, the idea it lays forward is of potential interest, and thus the AC recommends resubmission to the workshop track. 
All reviewers agree that the proposed method interesting and well presented. The authors  rebuttal addressed all outstanding raised issues. Two reviewers recommend clear accept and the third recommends borderline accept. I agree with this recommendation and believe that the paper will be of interest to the audience attending ICLR. I recommend accepting this work for a poster presentation at ICLR.
The paper received borderline and negative reviews but has raised many questions and discussions, showing that the paper has some merit. Many concerns were however raised on various aspects of the paper such as mathematical rigor, clarity, and motivation of manifold regularization that is too disconnected from the robustness to local random perturbation which is encouraged by the method. The rebuttal addresses some of these comments and the reviewers have appreciated the detailed answer. Yet, it was not sufficient to change the reviewer s opinions.  In its current form, the paper is not ready for publication and the area chair agrees with most of the reviewer s comments. He recommends a reject, but encourage the authors to take into account the feedback from the reviewer before resubmitting to a future venue.
The paper proposes a way to use kernel method for multi view generation. The points are mapped into a common subspace (with CNN feature extractor and kernel on top), and then a generation procedure from a latent point is given.  I found the paper not easy to ready and follow; the idea of using CNN + kernel methods have been around for some years (for example, see "Impostor networks" by Lebedev et. al), and explicit feature map shows that kernel is just an additional layer to the network. Overall, the approach is straightforward, the generation can be quite slow and the benefits are not clear. The reviewers are mildly negative, so I think this time this paper can not be accepted.
All reviewers recommended rejection after considering the rebuttal from the authors. The main weaknesses of the submission include poorly motivated claims and designs, and insufficient experimental comparisons. The AC did not find sufficient grounds to overturn the reviewers  consensus recommendation.   
This paper experiments with a combination of Sparse MoEs and Ensembles on the Vision Transformer (ViT), showing improved performance. To efficiently combine Sparse MoEs and Ensembles, the paper presents Partitioned Batch Ensembles (PBE), where the parameters of the self attention layers are shared, and an ensemble of Sparse MOEs are used for the MLP layers of the Transformer blocks.  While reviewers agree that the proposed approach is interesting, they also point out several weaknesses, such as the limited novelty of the proposed method (a simple combination of existing techniques) and small experimental gains. They also pointed out several weakness related to the experimental part. While the authors responded in a very detailed manner to several of these points and presented several additional experiments, I feel this paper will benefit from consolidating all these new results and going through another round of reviews.
There are two parts to this paper (1) an efficient procedure for solving trust region subproblems in second order optimization of neural nets, and (2) evidence that the proposed trust region method leads to better generalization performance than SGD in the large batch setting. In both cases, there are some promising leads here. But it feels like two separate papers here, and I m not sure either individual contribution is well enough supported to merit publication in ICLR.  For (1), the contribution is novel and potentially useful, to the best of my knowledge. But as there s been a lot of work on trust region solvers and second order optimization of neural nets more generally, claims about computational efficiency would require comparisons against existing methods. The focus on efficiency also doesn t seem to fit with the experiments section, where the proposed method optimizes less efficiently than SGD and is instead meant to provide a regularization benefit.  For (2), it s an interesting empirical finding that the method improves generalization, but the explanation for this is very hand wavy. If second order optimization in general turned out to help with sharp minima, this would be an interesting finding indeed, but it doesn t seem to be supported by other work in the area. The training curves in Table 1 are interesting, but don t really distinguish the claims of Section 4.5 from other possible hypotheses. 
The paper received four borderline reviews.  Overall, the manuscript has improved after the rebuttal (in particular, an issue in the convergence proof has been fixed), and a reviewer has increased his score to borderline accept. Yet, the paper did not convince the reviewers that the contribution was significant enough and none of the reviewer got enthusiastic about the paper. The main issue with the paper seems to be the unclear positioning between the optimization literature for stochastic composite optimization, the literature on support identification (e.g., Nutini, 2019), and the (more empirical) deep learning literature.  The paper postulates that the group sparsity regularization is crucial for deep neural networks, which seems to be the main motivation of the paper. Yet, the experiments do not demonstrate any concrete consequence of better group sparsity, wether it is in terms of accuracy or interpretability.  If positioned in this literature, a comparison should be made with classical pruning approaches, where pruning occurs as an iterative procedure that is distinct from optimization. If positioned instead in the stochastic optimization literature, better analysis of the convergence rates should be provided; if positioned in the support identification literature, the paper should explain how the results compare to those of the literature (e.g., Nutini, 2019 and others). In other words, any point of view requires clarifications and additional discussions.   Besides,       the theoretical assumptions need to be discussed:  does the Lipschitz assumption holds for multilayer neural networks ? Certainly not for ReLu networks, but what can we say something useful, even with smooth activation functions?      the experimental setup needs more details. Reproducing the experiments with the current paper seems difficult; in particular, the choice of hyper parameters is not crystal clear.  For these reasons, the area chair recommends to reject the paper, but encourages the authors to resubmit to a future venue while taking into account the previous comments.
The paper presents Simple Recurrent Unit, which is characterised by the lack of state to gates connections as used in conventional recurrent architectures. This allows for efficient implementation, and leads to results competitive with the recurrent baselines, as shown on several benchmarks.  The submission lacks novelty, as the proposed method is essentially a special case of Quasi RNN [Bradbury et al.], published at ICLR 2017. The comparison in Appendix A confirms that, as well as similar results of SRU and Quasi RNN in Figures 4 and 5. Quasi RNN has already been demonstrated to be amenable to efficient implementation and perform on a par with the recurrent baselines, so this submission doesn’t add much to that.
This paper shows a case study of an adversarial attack on a copyright detection system. The paper implements a music identification method with a simple convolutional neural network, and shows that it is possible to fool such CNN with an adversarial learning. After the discussion period, two among three reviewers incline to the rejection of the paper. Although the majority of the reviewers agree that this is an interesting problem with an important application, they also find many of their concerns remain unaddressed. These include the generality of the finding as the current paper is more like a proof of concept that black/white box attack can work for copyright system. The reviewers are also concerned that the technique solution/finding is not novel as it is very similar to prior work in other domains (e.g., image classification). One reviewer was particularly concerned about that the user study is missing, making it difficult to judge whether the quality of the modified audio is reasonable or not.
The paper proposed an operation called StructPool for graph pooling by treating it as node clustering problem (assigning a label from 1..k to each node) and then use a pairwise CRF structure to jointly infer these labels. The reviewers all think that this is a well written paper, and the experimental results are adequate to back up the claim that StructPool offers advantage over other graph pooling operations. Even though the idea of the presented method is simple and it does add more (albeit by a constant factor) to the computational burden of graph neural network, I think this would make a valuable addition to the literature.
The paper suggests a new way to learn a physics prior, in an action free way from raw frames. The idea is to "learn the common rules of physics" in some sense (from purely visual observations) and use that as pre training. The authors made a number of experiments in response to the reviewer concerns, but the submission still fell short of their expectations. In the post rebuttal discussion, the reviewers mentioned that it s not clear how SpatialNet is different from a ConvLSTM, mentioned the writing quality and the fact that the "physics prior" is really quite close to what others call video prediction in other baselines.  
The manuscript describes a procedure for prioritizing the contents of an experience replay buffer in a UVFA setting based on a density model of the trajectory of the achieved goal states. A rank based transformation of densities is used to stochastically prioritize the replay memory.  Reducing the sample complexity of RL is a worthy goal and reviewers found the overall approach is interesting, if somewhat arbitrary in the implementation details. Concerns were raised about clarity and justification, and the restriction of experiments to fully deterministic environments.  After personally reading the updated manuscript I found clarity to still be lacking. Statements like "... uses the ranking number (starting from zero) directly as the probability for sampling"   this is not true (it is normalized, as confusingly laid out in equation 2 with the same symbol used for the unnormalized and normalized densities), and also implies that the least likely trajectory under the model is never sampled, which doesn t seem like a desirable property. Schaul s "prioritized experience replay" is cited for the choice of rank based distribution, but the distribution employed in that work has rather different form. The related work section is also very poor given the existing body of literature on curiosity in a reinforcement learning context, and the new "importance sampling perspective" section serves little explicatory purpose given that an importance re weighting is not performed.   Overall, I concur most strongly with AnonReviewer1 that more work is needed to motivate the method and prove its robustness applicability, as well as to polish the presentation.
The authors compare different model based R algorithms to see whether observation prediction is important. They show that, as expected, it is. On the other hand, they seem to show that latent space prediction is not very useful. The study is limited to domains with image data: Does this domain have something particularly special? Perhaps experiments with smaller scale POMDP problems might actually have shown something different. It is very difficult to do a study of this type properly, and although the authors have tried, it s hard to see how this paper can be accepted. I agree with some the positive points some reviewers have raised, but I think that, at the end of the day, the paper is trying to draw too general conclusions from a handful of datapoints. Were I writing this paper, I would first try the simplest version of the hypothesis with very basic environments that are, however, more varied than the ones shown here. Would the hypothesis hold, I d scale up to more complex environments and try to also run with more seeds to get a clearer signal. 
This paper proposes a method for aligning an input text with the frames in a video that correspond to what the text describes in a weakly supervised way. The main technical contribution of the paper is the use of co attention at different abstraction levels.  Among the four reviewers, one reviewer advocates for the paper while the others find this paper to be a borderline reject paper. Reviewer3 who was initially positive about the paper, during the discussion period, expressed that he/she wants to downgrade his/her rating to weak reject after reading the other reviewers  comments and concerns. The main concern of the reviewers is that the contribution of the paper incremental, particularly since the idea of co attention has been used in many different area in other context. The authors responded to this in the rebuttal that the proposed approach incorporate different components such as Positional Encodings and is different from prior work, and that they experimentally perform superior compared to other co attention usages such as LCGN. Although the AC understands the authors response, the majority of the reviewers are still not fully convinced about the contribution and their opinion stay opposed to the paper.
In standard message passing GNNs (MPNNs), one step at any node u involves receiving state/embedding information from all of u’s neighbors, and then updating u’s state as a function of these messages and of u’s own current state. Thus, the communication pattern at every step is that of a star topology (a graph with u at its “center”, and with u connected to all its neighbors, and with no other edges). However, it is well known that the expressive power here is bounded by that of the 1st order Weisfeiler Leman isomorphism test (1 WL). This paper then takes the natural step of generalizing the star topology to more general ones (e.g., k hop egonets, the subgraph induced by the nodes of distance at most k from u). It is shown that this framework is strictly more powerful than 1 WL and 2 WL (however, as pointed out by a referee, this is actually a weaker version of 2 WL that is equivalent to 1 WL), and is at least as powerful as 3 WL. Subgraph sampling approaches that improve efficiency are also introduced. It is shown that this method beats the SOTA for some number of well known graph ML problems.  It looks like this paper has a very strong overlap with the NeurIPS 2021 paper "Nested Graph Neural Networks" (https://openreview.net/forum?id 7_eLEvFjCi3). Both papers use rooted subgraphs (k hop ego nets) to replace the k hop rooted subtrees in traditional GNNs, and both use a base GNN over the rooted subgraph to compute a subgraph representation as the node representation while pooling the node representations into a graph representation. Both papers claim to outperform 1 WL in expressive power; both use distance to center node in order to enhance subgraph node features. The authors are urged to compare and contrast the two papers in the camera ready version.
All reviewers rated this submission as a weak reject and there was no author response. The AC recommends rejection.
The paper proposes an architecture of a model for multiple correlated time series that has application in anomaly detection.  The main idea is to use attention both along time to capture trends, seasonality, etc and along series types to capture their correlation.  Training loss attempts to reconstruct masked ranges of values, and thus the discrepancy between predicted and actual values can be used to flag anomalies.  While the ideas proposed in the paper are useful and lead to a well engineered model while combining current architectural elements to best suit their task, the paper fails to impress as a research contribution.   The writing requires improvement, and the experiments on two datasets are not entirely convincing.  The authors have realized these limitations, as can be seen in the author feedback.  However, the changes entailed are too extensive to  revise the ratings across all reviewers. I hope these will help the authors submit a better next version to another conference.
Understanding neural networks once they have been trained is a big open problem for machine learning. This manuscript designed graph theoretic and information theoretic measures aimed at helping us understand community structure and function in trained networks. In particular, they measure community structure (modularity) and entropy for trained networks and related these to the performance of the networks. The manuscript runs experiments with fully connected networks on problems such as MNIST and CIFAR. Both community structure and entropy measures are shown to correlate (Spearman and Pearson correlation coefficients) with performance metrics in the networks studied.  Reviewers tended to agree that the paper was well written and motivated by an interesting and timely question (understanding trained networks). However, on the whole, most of the reviewers believe that the manuscript is too preliminary for publication at ICLR and I agree. A central issue cited by most of the reviewers is that the experiments are performed on small/toy models for small tasks and under particular hyperparameter regimes. It is therefore unclear to what extent the results would generalize to other situations. E.g. would the results hold for larger dataset or for convolutional neural networks? Connected to this complaint, reviewers worry that there is not enough connection to the literature and baseline methods that could be used to predict performance given measures of trained network activity. Even allowing that the observed correlations are true and generalizable, are these measures better than those covered elsewhere in the literature? Additionally problematic, the measures are not theoretically justified either. Thus, we are missing both reasoned arguments for the metrics and robust quantification beyond a limitted experimental setting. One reviewer, Xmnm, is compelled by the work and recommends acceptance. However, they do not present a compelling case for acceptance, and even repeat several of the concerns raised by other reviewers.  In sum, the work is on an interesting subject and timely, but needs further work to be ready for publication.
The paper initially had mixed reviews (4,5,6).  The main issues raised were: 1) limited novelty (re using/integrating components) [R2]; 2) limited generalization ability since the model needs to be retrained on every video [R2, R3]; 3) limited applicability   experiments limited to certain domain of video, while results on videos with large motion are not convincing [R2, R3]; 4) missing ablation studies / experiments [R3, R4].  The author response partially addressed some concerns, but the main points 1 3 are still problematic. In addition, the AC noted that the technical aspect was lacking:    Training with contrastive loss on a single video may likely overfit the embedding to the video, which leads to a meaningless embedding where all non neighboring segments are orthogonal in the embedding space. While changing the softmax temperature can yield higher entropy transition probabilities, the induced probability distribution is probably highly noisy. It would be better to train this on a large video corpus, which will prevent overfitting. Also contrastive loss is typically used to build a discriminative embedding space for classification/recognition, not a smooth embedding space for generation (where distances between embedding vectors are strongly correlated to similarity). Thus some other embedding smoothness terms could be added during contrastive learning.   The learning is only on the transition probabilities, while the video generation is separate. It would have been more convincing to learn the transition probabilities with the video generation process in an end to end manner. Perhaps a discriminator could be placed after the video generator so that the transition probabilities could be learned so as to better mimic real video. Other loss terms based on video temporal smoothness could also be added ensure smoother transitions between clips (e.g., motion consistency).  The negative reviewers remained unconvinced by the author response, and the AC agreed with their concerns. Thus, the paper was recommended for rejection.
 pros:   Identification of several interesting problems with the original DNC model: masked attention, erasion of de allocated elements, and sharpened temporal links   An improved architecture which addresses the issues and shows improved performance on synthetic memory tasks and bAbI over the original model   Clear writing  cons:   Does not really show this modified DNC can solve a task that the original DNC could not and the bAbI tasks are effectively solved anyway.  It is still not clear whether the DNC even with these improvements will have much impact beyond these toy tasks.  Overall the reviewers found this to be a solid paper with a useful analysis and I agree.  I recommend acceptance.  
This paper uses known methods for learning a differentially private models and applies it to the task of learning a language model, and find they are able to maintain accuracy results on large datasets. Reviewers found the method convincing and original saying it was "interesting and very important to the machine learning ... community", and that in terms of results it was a "very strong empirical paper, with experiments comparable to industrial scale". There were some complaints as to the clarity of the work, with requests for more clear explanations of the methods used.  
This paper proposes to quantize the weights of neural networks that can minimize the L_2 loss between the quantized values and the full precision ones. The paper has limited novelty, as many of the solutions presented in the paper have already been discovered in the literature. During the discussion, the reviewers agree that it is an incremental contribution. Parts of the paper can also be clarified, particularly on the optimality of the solution, assumptions used in the approximation, and some of the experimental results. Experimental results can also be made more convincing by adding comparision with the more recent quantization methods.
The paper proposes to use covariance of the approximate posterior to induce a metric on the latent space of the VAE and use it to sample from the Riemannian manifold learned by a VAE. Experiments on MNIST and CelebA show the method outperforms vanilla VAE in terms of sample quality (FID and PR scores). It is also shown to work better than baseline VAE models on a medical imaging classification task. While the reviewers have acknowledged the contributions of the paper, the novelty in the contributions and their importance/impact was seen to be rather limited. The main concern from the reviewers is   while the paper is mainly based on the use of inverse covariance as the metric for manifold, it doesn t give a reasonable theoretical justification on it is a sensible metric that captures the intrinsic geometry of data. Authors in their response justify it as   since the covariance matrices are learned from the data and favor through the posterior sampling some direction in the latent space, it is a natural choice as metric. This is not very convincing. A more technical justification for this will certainly make the paper more convincing. I suggest the authors to look at "Kumar, Abhishek, and Ben Poole. "On Implicit Regularization in $ β $ VAEs." International Conference on Machine Learning. PMLR, 2020" which theoretically connects inverse covariance and the Riemannian metric in Sec 5.2, and see if it can be adapted in their context.
The consensus among the reviewers is that this is a borderline paper: its main idea is sensible and natural. Unfortunately, while the reviewers appreciated the authors  responses to their comments, they felt that the paper failed to demonstrate the usefulness of the idea beyond toy datasets. The latter would considerably strengthen this paper.
This paper addresses the problem of exploration in challenging RL environments using self imitation learning. The idea behind the proposed approach is for the agent to imitate a diverse set of its own past trajectories. To achieve this, the authors introduce a policy conditioned on trajectories. The proposed approach is evaluated on various domains including Atari Montezuma s Revenge and MuJoCo.  Given that the evaluation is purely empirical, the major concern is in the design of experiments. The amount of stochasticity induced by the random initial state alone does not lead to convincing results regarding the performance of the proposed approach compared with baselines (e.g. Go Explore). With such simple stochasticity, it is not clear why one could not use a model to recover from it and then rely on an existing technique like Go Explore. Although this paper tackles an important problem (hard exploration RL tasks), all reviewers agreed that this limitation is crucial and I therefore recommend to reject this paper.
The submission presents a differentiable take on classic active contour methods, which used to be popular in computer vision. The method is sensible and the results are strong. After the revision, all reviewers recommend accepting the paper.
The authors propose a regularized for convolutional kernels that seeks to improve adversarial robustness of CNNs and produce more perceptually aligned gradients. While the topic studied by the paper is interesting, reviewers pointed out several deficiencies with the empirical evaluation that call into question the validity of the claims made by the authors. In particular:  1) Adversarial evaluation protocol: There are several red flags in the way the authors perform adversarial evaluation. The authors use a pre defined adversarial attack toolbox (Foolbox) but are unable to produce successful attacks even for large perturbation radii   this suggests that the attack is not tuned properly. Further, the authors present results over the best case performance over several attacks, which is dubious since the goal of adversarial evaluation is to reveal the worst case performance of the model.   2) Perceptual alignment: The claim of perceptually aligned gradients also does not seem sufficiently justified given the experimental results, since the improvement over the baseline is quite marginal. Here too, the authors report failure of a standard visualization technique that has been successfully used in prior work, calling into question the validity of these results.  The authors did not participate in the rebuttal phase and the reviewers maintained their scores after the initial reviews.   Overall, given the significant flaws in the empirical evaluation, I recommend that the paper be rejected. I encourage the authors to rerun their experiments following the feedback from reviewers 1 and 3 and resubmit the paper with a more careful empirical evaluation.
All reviewers unanimously accept the paper.
This paper presents an ensemble method for conversation systems, where a retrieval based system is ensembled with a generation based system.  The combination is done via a reranker.  Evaluation is done on one dataset containing query reply pairs with both BLEU and human evalutations.  The experimental results are good using the ensemble model.  Although this presents some novel ideas and may be useful for chatbots (not for goal oriented systems), the committee feels that the approach and the presented material does not have enough substance for publication at ICLR:  it will be interesting to evaluate this system in a goal oriented setting; many prior papers have built generation based conversation systems (1 step)   this paper does not present any comparison with those papers.  Addressing these issues may strengthen the paper for a future venue. 
This paper offers "natural attribute" methods for shift detection.  Several reviewers are positive, but reviewer czxP is the most authoritative in the eyes of the area chair.  In particular the AC is concerned that the task that this paper defines is artificial and not useful. Naturally occurring shifts are real, happen all the time and meaningfully affect model performance... Natural shifts should be defined over distributions not singular instances. The authors create artificial instantiations of natural shifts to illustrate a well known flaw in OOD detection algorithms. To demonstrate the usefulness of this approach to "natural shifts" the authors should should show how this algorithm performs in settings like "WILDS"... where the shift are meaningful and not artificial, in the opinion of the AC.  The paper should cite and contrast WILDS (https://arxiv.org/abs/2012.07421) and Mandoline (http://proceedings.mlr.press/v139/chen21i.html) in a future revision.
This paper proposes model based reinforcement learning algorithms that have theoretical guarantees. These methods are shown to good results on Mujuco benchmark tasks. All of the reviewers have given a reasonable score to the paper, and the paper can be accepted.
The reviewers agree the paper is not ready for publication at ICLR.
The submission presents an approach to uncovering causal relations in an environment via interaction. The topic is interesting and the work is timely. However, the experimental setting is quite simplistic and the approach makes strong assumptions that limit its applicability. The reviewers are split. R2 raised their rating from 3 to 6 following the authors  responses and revision, but R1 maintained their rating of 3 and posted a response that justifies this position. In light of the limitations of the work, the AC recommends against accepting the submission.
The paper presents a quantization aware training method for GNNs. The problem is very well motivated, the method is well executed, and experiments are also well designed. The paper does seem relatively low on technical novelty.   All the reviewers are positive about the paper, and the paper has certainly improved significantly over the rebuttal phase.   So, we would like to see the paper accepted at ICLR. 
The paper presents a new perspective on recommendation systems, categorizing them as linear predictors where the main difference between the various methods is the regularizer. The authors then propose an objective function that aims at optimizing the Frobenius norm while maintaining a low rank solution, and present algorithm that have closed form solutions based on the SVD.  The reviewers noted the novelty of the framework, but the overall assessment after the discussion was that the theoretical contribution was limited. The algorithm proposed by the authors does not provide any improvement on standard criteria (performance, computational complexity), which makes the algorithmic/experimental contribution limited as well.
The reviewers raised several theoretical and empirical questions about the paper. During the rebuttals, the authors seem to  successfully address the experimental issues, in particular those raised by Reviewers 1 and 2. However, the theoretical concerns have mainly remained unanswered. Reviewer 2 has a major concern about the convergence of Algorithm 1, both Reviewers 3 and 4 (in particular Reviewer 3) have several questions about the theoretical analysis and see the technical contribution of the work relatively low. These are not minor issues and require a major revision to be properly addressed. So, I suggest that the authors take the reviewers comments into account, revise their work and properly address the issues raised by the reviewers, and prepare their work for an upcoming conference. 
This paper studies deep non linear infinite width neural networks that go beyond the NTK and learn features. This paper extends the prior result on shallow neural networks to deep neural networks and empirically evaluates the deep inf wide nn. The reviewers find the contributions in the paper valuable. The meta reviewer agrees and thus recommends acceptance.
This paper proposes a new method for learning symmetric representations for equivariant world models. All reviewers recognized the interesting results in the paper. The reviewers have raised some concerns, which were not addressed well yet after the rebuttal. For example, Reviewers LG1G and Uu3z mentioned about the limitation of using the group for a task and the generality of the approach. Reviewer 5ro3 mentioned about the lack of novelty. Though they gave 6, they were quite neutral about the paper acceptance. Eventually, after a second round of deiscussions, we had to make this difficult decision: The current form of this paper is not ready for publications.
The authors propose a new dataset to evaluate the robustness of image classifiers. The dataset consists of data from three sources: a crowdsourced dataset collected by the authors called ImageNet Renditions, images from Google street view, and data sampled from DeepFashion2. This new dataset allows the authors to test robustness to different renditions of an object (e.g. artistic depictions of an object category) and robustness to changes in geography and camera type. In addition, they propose a new augmentation strategy called DeepAugment which consists of an encoder/decoder style network that transforms the appearance of the input image by simply applying different random perturbations of the weights of the augment network. Robustness results are presented on the previously described datasets where the proposed augmentation strategy in combination with an existing approach (AugMix) performs best in some cases. However, the results are not convincing and AugMix often outperforms the new method.    In general, the authors did a good job addressing many of the comments (e.g. they provided more detail about how ImageNet R was collected), but there were still several lingering concerns. R4 was the most positive about the paper, but unfortunately was one of the least vocal during the discussion. R1 was concerned that the paper did not do a great job of defining what was meant by robustness. This AC doesn t agree fully with their concerns, but does agree that more care could have been taken to position the paper better in light of the existing datasets that are already available (see R1’s comments). As the reviewers and authors note, collecting new datasets is a lot of work so care should be taken to ensure that this is not duplicate effort. The authors addressed these concerns in their response to some extent, but more discussion is needed in the paper.   There was a lot of discussion between the authors and reviewers about this paper. The new dataset has a lot of merit, but there is some concern that the paper does not do a great job of clearly presenting its findings and conclusions. In addition, the proposed augmentation technique is slightly underwhelming performance wise and not very clearly described in the main paper. R2 sums up the opinion of this AC: “I think this work is interesting and is in principle asking the right questions. However, the analysis and conclusions currently do not provide robust and generalizable insights that advance the field.” There is clearly a lot of promise here, and the current recommendation is a weak reject. The authors are strongly encouraged to take the detailed feedback they have received on board and to revise the paper to further improve it for a future submission.  
The majority of reviewers suggest that this paper is not yet ready for publication. The idea presented in the paper is interesting, but there are concerns about what experiments are done, what papers are cited, and how polished the paper is. This all suggests that the paper could benefit from a bit more time to thoughtfully go through some of the criticisms, and make sure that everything reviewers suggest is covered.
Description of paper content:  The authors propose a dynamics model that can generalize to novel environments. The train and test MDPs have the same state and action spaces but different dynamics. Environment specific inference is achieved by estimating latent vectors Z that describe the non stationary or variable part of the dynamics. These Z s are inferred from trajectory segments in unlabeled environments. The Z s are learned contrastively: Z s from the same trajectory are pulled together, and Z s from separate trajectories are pushed apart. However, to mitigate the error of distancing Z s from different trajectories but the same environment, Z s on trajectories with similar transitions are also pushed together using a soft clustering penalty. These losses are justified based on ideas from Pearl’s causal inference.  Summary of paper discussion:  The reviewers concluded that the contributions are conceptually interesting and “somewhat” novel. The reviewers felt that the empirical performance gains of the method over baselines were demonstrated but not extremely impressive.
This is a solid paper and considers the problem of training a wide neural network with a single hidden layer. This can be framed as an optimization problem in the space of probability distributions with a suitable entropy regularization, where each atom in the distribution corresponds to a hidden neuron. The dual of this problem (for finite data) is a finite dimensional optimization problem and the paper proposes a particle based coordinate ascent scheme. The paper provides some convergence rate results. After the rebuttal, the authors have also included more experimental/numerical results.  The authors have answered the concerns raised by the reviewers and overall, the paper can be accepted: The presented approach appears to be sufficiently novel and might be useful in other settings. The presentation is clear and easy to follow for such a technical paper; the paper is well organized. The limitations of the approach are clearly stated (dependence on the regularization parameter for entropy term that may be hard to select)
Two reviewers recommended rejection, and one is slightly more positive. The main concern is that the experiments are not convincing (ie, the number of base and added classes is very small). Furthermore, while the paper introduces several interesting ideas, the AC agrees with the second reviewer that each of these could be explored in more detail. This work seems preliminary. The authors are encouraged to resubmit to a future conference.
This paper proposes SimpleBits, for simplifying input images to remove irrelevant details but keep relevant details for classification. This idea can be applied during/after training. Authors have significantly revised the draft to address reviewer concerns, to improve the readability and clarify concerns on complexity analysis, for which reviewers have raised scores post rebuttal. However, even with score changes, there are commonly expressed concerns, that manuscript still needs some more improvements to be ready for publication in their post rebuttal comments: findings are not very nontrivial or significant (reviewer eYVm), still incomplete (RfmX) or optimization algorithm is yet to be found (reviewer agcx) .
The paper proposes a simple approach for computing a sentence embedding as a weighted combination of pre trained word embeddings, which obtains nice results on a number of tasks.  The approach is described as training free but does require computing principal components of word embedding subspaces on the test set (similarly to some earlier work).  The reviewers are generally in agreement that the approach is interesting, and the results are encouraging.  However, there is some concern about the clarity of the paper and in particular the placement of the work in relation to other methods.  There is also a bit of concern about whether there is sufficient novelty compared to Arora et al. 2017, which also compose sentence embeddings as weighted combinations of word embeddings, and also use a principal subspace of embeddings in the test set.  This AC feels that the method here is sufficiently different from Arora et al., but agrees with the reviewers that the paper clarity needs to be improved, so that the community can appreciate what is gained from the new aspects of the approach and what conclusions should be drawn from each experimental comparison.
This paper proposes a family of new methods, based on Bayesian Truth Serum, that are meant to build better ensembles  from a fixed set of constituent models.  Reviewers found the problem and the general research direction interesting, but none of the three of them were convinced that the proposed methods are effective in the ways that the paper claims, even after some discussion. It seems as though this paper is dealing with a problem that doesn t generally lend itself to large improvements in results, but reviewers weren t satisfied that the small observed improvements were real, and urged the authors to explore additional settings and baselines, and to offer a full significance test.
The authors develop a technique for unsupervised learning of neurosymbolic encoders. Some of the difficulty with the paper came from the accessibility to a broader machine learning audience, though there is related work such as Shah 2020 in machine learning. The other difficulty came from the experiments: there was both a question about the metrics and the task. Quoting a reviewer  "Current evaluation seems not very convincing to me. The authors only show that with the help of symbolic program, the method could get representations with better cluster quality (program helps representation learning). But I think a more intersting perspective is to see whether the learned program itself is helpful. For example, whether it could be used to predict future trajectory (such as 3 body problem), or even help solving some high level reasoning tasks."  and another reviewer   "Maybe something comparing the programs of experts to what the latent representation learned?"  Making the paper more accessible and improving the experiments will improve its quality.
The authors study the problem of open ended knowledge grounded natural language generation, in the context of free form QA or knowledge grounded dialogue, focusing on improving the retrieval component of the retrieval augmented system. By retrieving more relevant passages, the generations are more grounded in retrieved passages.   Pros: + The paper is clearly written and motivated. + Presents a straightforward approach that shows improvement over a  strong baseline.   + A strong paper focuses on a rather under explored problem of knowledge grounded open ended generation, proposing novel objective, significant empirical improvements on two datasets in multiple metrics. + The authors included human evolution results to support their findings.  + The authors did a good job addressing several questions raised during review period and added several new experiment results and discussions to strengthen their findings. The reviewer team was generally satisfied.  Cons:  + Several related work on knowledge guided dialog response generation is missing in the paper. Although the paper s focus is on retrieval based QA systems, the main focus is on open domain generation, which has overlaps with dialog response generation research. So the authors should cite the following papers in their paper: [1] Dinan, Emily, et al. "Wizard of wikipedia: Knowledge powered conversational agents." arXiv preprint arXiv:1811.01241 (2018). [2] Zhou, Kangyan, Shrimai Prabhumoye, and Alan W. Black. "A dataset for document grounded conversations." arXiv preprint arXiv:1809.07358 (2018). [3]Zhan, Haolan, et al. "CoLV: A Collaborative Latent Variable Model for Knowledge Grounded Dialogue Generation." Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021. [4]Zhao, Xueliang, et al. "Knowledge grounded dialogue generation with pre trained language models." arXiv preprint arXiv:2010.08824 (2020).  + There are several related work concerning with generation of a response given a relatively small set of evidence text such as the following ones:  [5]Lian, Rongzhong, et al. "Learning to select knowledge for response generation in dialog systems." arXiv preprint arXiv:1902.04911 (2019). [6]Kim, Byeongchang, Jaewoo Ahn, and Gunhee Kim. "Sequential latent knowledge selection for knowledge grounded dialogue." arXiv preprint arXiv:2002.07510 (2020). Although these work do not include a retrieval part, the authors should cite and discuss similarities and differences to [5] & [6] in their paper.
This paper investigates using "curiosity" to improve representation learning. This paper is not ready for publication. The main issues was the reviewers found the paper did not support the claim contributions in terms of (1) evaluating the new representations and improvement due to the representation, and (2) the novelty of the method compared to the long literature in this area. In general the reviewers found the empirical evidence unconvincing, and the too many missing details.  The results in this paper have many issues: claims of performance based on three runs; undefined error measures; bolding entries in tables which appear not significantly better without explanation; unclear/informal meta parameter tuning.   Finally, there are some terminology issues in this paper. I suggest an excellent paper on the topic: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3858647/  
Authors present a method for representing DNA sequence reads as one hot encoded vectors, with genomic context (expected original human sequence), read sequence, and CIGAR string (match operation encoding) concatenated as a single input into the framework. Method is developed on 5 lung cancer patients and 4 melanoma patients.   Pros:   The approach to feature encoding and network construction for task seems new.   The target task is important and may carry significant benefit for healthcare and disease screening.  Cons:   The number of patients involved in the study is exceedingly small. Though many samples were drawn from these patients, pattern discovery may not be generalizable across larger populations. Though the difficulty in acquiring this type of data is noted.   (Significant) Reviewer asked for use of public benchmark dataset, for which authors have declined to use since the benchmark was not targeted toward task of ultra low VAFs. However, perhaps authors could have sourced genetic data from these recommended public repositories to create synthetic scenarios, which would enable the broader research community to directly compare against the methods presented here. The use of only private datasets is concerning regarding the future impact of this work.   (Significant) The concatenation of the rows is slightly confusing. It is unclear why these were concatenated along the column dimension, rather than being input as multiple channels. This question doesn t seem to be addressed in the paper. Given the pros and cons, the commitee recommends this interesting paper for workshop.
The paper tackles program synthesis using a discrete latent code approach, enabling two level beam search decoding. The approach is well motivated, as program synthesis requires high level choices that affect long subsequences of the output, and discrete codes are amenable to heuristic search. Empirical results show improvements over methods with no latent variables and methods with continuous latent variables. However, the review process reveals that some of the claims made about the nature and necessity of the discrete latent codes is not sufficiently justified by the current analysis. With borderline assessments in a very competitive venue, *I cannot recommend acceptance*. I would like to encourage the authors to pursue this direction further, shedding more light on the nature of the latent representations learned as an interpretable planning mechanism.  The discussion was rich and surfaced a lot of concerns and issues with the paper, that I strongly encourage the authors to take into account.  After the author responses and internal discussion, many initial concerns were settled, but some remain. The two main concerns raised are: (1) that the paper makes overly ambitious claims about high level planning which is not backed by an analysis of the latent codes themselves, and (2) that the improvement may be due to increased generation diversity (which could be possible in continuous LV models too) rather than meaningful high level planning. I believe that after clarifying such remaining loose ends,  this work would be of great interest to both the field of program synthesis as well as the discrete representation learning community. 
This work a "Seatbelt VAE" algorithm to improve the robustness of VAE against adversarial attacks. The proposed method is promising but the paper appears to be hastily written and leave many places to improve and clarify. This paper can be turned into an excellent paper with another round of throughout modification.    
The paper presents several related results. The initial main result consists in relating GPCA to GCN, showing that GPCA can be understood as a first order approximation of some specific instance of GCN where the W matrix is directly defined on data. This result is then exploited to define a supervised version of GPCA. As a follow up the authors propose a novel GPCA based network (GPCANet) and a GPCANet initialisation for GNNs. The paper is well written and easy to read. Empirical results are reported to verify the above mentioned connection between  GPCA and GCN, as well as the performances of  GPCANet  and the proposed initialisation for GNNs. Overall, while the mentioned connection was never explicitly reported in the literature, its existence is not surprising and thus its significance seems to be limited. Also the performances of GPCANet do not seem to be significant from a statistical point of view. The novel initialisation procedure for GNNs seems to be interesting and promising, although the used datasets may not make evident its full power. Authors rebuttal and discussion did not change the reviewers  initial assessment.
Dear authors,  the paper contains many interesting and novel ideas. Indeed, tuning step size is very time and energy consuming, and deriving and analyzing new adaptive algorithms has not only theoretical benefits but, more importantly, is a key when training more complicated ML models.  The paper contains many weaknesses as noted by reviewers. I know that you have addressed many of them one of the reviewers is still concerned about the other issues involving Theorem 1 and the assumption of the bounded preconditioner. He thinks the preconditioner bound is troublesome. In the overparameterized regime, he would expect the gradients to become near zero as the algorithm converges, which would actually cause the preconditioner to NOT be bounded below. It seems that the analysis might actually improve if the authors abandoned AMSGrad/Adam and instead just considered SGD for which the preconditioner assumption is not an assumption but just a property of the algorithm.      Thank you  
The paper proposes a method for training GANs in few shot setting. Two key components of the method are: a skip layer channel wise excitation (SLE) module that encourages gradient flow across resolutions, and a self supervised loss of autoencoding to regularize the discriminator. The results presented in the paper are indeed impressive in the few shot setting. Reviewers had some concerns about training set memorization which have been addressed by the authors with additional evaluations using LPIPS metric. Overall, the paper tackles an important problem of few shot learning of GANs and will be a good addition to the ICLR program. 
This paper presents a method for distilling pretrained models (such as BERT) into a different student architecture (CMOW), and extend the CMOW architecture with a bidirectional component.  On a couple of datasets, results are comparable to DistilBERT a previous baseline. This paper is nice, but can be stronger with more empirical experiments on non GLUE tasks (TriviaQA, Natural Questions, SQUAD for example).  Furthermore, I agree with Reviewer M3tk that there are many empirical comparisons with baselines such as TinyBERT missing and the argument of not needing the teacher model to be super convincing.
In this paper, the authors propose a method for generating high quality synthetic datasets, and use their methods to evaluate a variety of causal effect estimators.  In general, the paper was not received very favorably by reviewers.  The primary concerns were: (a) issues with built in bias in the algorithm that generates synthetic data (due to collider stratification bias induced by conditioning on causally "downstream" variables, (b) issues with "replicating underlying counterfactuals," which indeed is a difficult problem, and (c) lack of "technical novelty."  First, I am personally very sympathetic to what the authors are trying to do.  Regardless of current reviewer reception, I think the causal inference community really needs more high quality benchmarks, and (semi)synthetic datasets, and validation approaches.  I urge the authors to continue this line of work.  That said, I think it is important (for causal benchmarks) to be clear about the distinction between the observed data distribution (e.g. p(C,A,Y) for the backdoor model), and the full data distribution (e.g. p(C, A, Y(0), Y(1)) for the backdoor model with a binary treatment).  Generally what makes a benchmark interesting is preserving some features of the _full_ data distribution, and allowing "knobs" that make the problem easier and harder.  Much of what the ACIC competition organizers did was provide such knobs.  Mimicking features of just the observed data distribution, even if they are complicated, isn t enough to make a causal benchmark interesting, since the problem is all about how full and observed data relate.  When revising the paper, please keep this difference in mind, and consider what features of p(C, A, Y(0), Y(1)) (or more complex versions of this) make for an interesting benchmark, while also generating p(C,A,Y) that "mimics observed data" in some way.
The paper introduces a framework for enforcing constraints into deep NNs used for modeling spatio temporal dynamics characterizing physical systems. The authors consider different types of constraints (pointwise, differential and integral). They start from a formulation approximating PDEs as set of ODEs (method of lines). Their main idea is to approximate the solution of the equations using an interpolant between observations and imposing the constraints on this approximation function. The interpolant is built using basis functions located at observation points. The formalism considers irregular spatial grids and both soft and hard constraints. The main claim is then the introduction of a general formalism for considering different types of constraints on irregular grids. Experiments illustrate the behavior of the proposed method on different types of evolution equations and constraints.  The reviewers agree that the proposed approach is interesting and that some of the ideas are original. However, they also consider that the paper is not convincing enough to demonstrate the interest and novelty of the approach, compared to alternative methods. The experimental section mainly considers (except for one application) regular grids and constraints that could be handled by other methods as well. The authors should present cases where their method provides a clear advantage, distinct from existing solutions. The authors provided a well argued rebuttal, clarifying several points. However, all reviewers retained their original scores and encourage the authors to further develop the experimental analysis to present a stronger paper. In addition, the presentation could be improved, and some technical aspects better explained (e.g., description of interpolation methods, and some advice on which interpolant to choose for a given problem).
The paper proposes a curriculum approach to increasing the number of agents (and hence complexity) in MARL.  The reviewers mostly agreed that this is a simple and useful idea to the MARL community. There was some initial disagreement about relationships with other RL + evolution approaches, but it got resolved in the rebuttal. Another concern was the slight differences in the environments considered by the paper compared to the literature, but the authors added an experiment with the unmodified version.  Given the positive assessment and the successful rebuttal, I recommend acceptance.
In this paper, the authors work on improving the interpretability of CNNs following distillation methods. The paper is written in such a convoluted way and with many changes in the notation that makes it hard to understand what they are proposing and what for. This is a major impediment for the paper going forward. But also, the reviewers point out some clear aspects of the paper and the interpretability that it provides for CNNs, for example, the entropy analysis and its variability or referring to the discrimination part of the CNN or mentioning an explainable alternative to CNNs, while still relying on the convolutional filters. The authors need to dedicate more time to explain this paper carefully before it can be properly reviewed and published at any major conference.
The paper presents an actor critic type of method consisting of two types of features   dynamics and tasks, in the multi task continuous control setting. While the topic of the research is interesting and relevant to ICLR, the reviewers have concerns with the novelty and technical significance of the work. Specifically, the proposed method is very similar to several other works leading to an incremental novelty. In addition, the method is evaluated only on simple environments. The concerns remain after the discussion period.   In the next version of the manuscript, the authors are encouraged to pursue more difficult settings and modify the method to work on those problems. That would make the paper stronger, and lead to a more novel method evaluated on harder problems.
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.   The paper tackles an interesting and relevant problem for ICLR: incremental classifier learning applied to image data streams.  2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    The proposed method is not clearly explained and not reproducible. In particular the contribution on top of the baseline iCaRL method is unclear. It seems to be mainly the use of CAE which is a minor change.   The experimental comparisons are incomplete. For example, in Table 4 the authors don t discuss the storage requirements of GAN and FearNet baselines.   The authors state that one of their main contributions is fullfilling privacy and legal requirements. They claim this is done by using CAEs which generate image embeddings that they store rather than the original images. However it s quite well known that a lot of data about the original images can be recovered from such embeddings (e.g. Dosovitskiy & Brox. "Inverting visual representations with convolutional networks." CVPR 2016.). These concerns all impacted the final decision.  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  There were no major points of contention and no author feedback.   4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers reached a consensus that the paper should be rejected. 
The paper presents an interesting idea for making self attention efficient. Several reviewers were not satisfied with the experiments because it did not include runtime and sought after benchmarks.  Rebuttal did a good job of clarifying a few of those with newly added experiments that make the paper stronger. However, the new experiments are in limited settings as well as the real advantage over LSH baselines require more investigation.   This could make a good paper in the future if the experiments are made more rigorous with standard tasks and benchmarks. 
Overall, the work is borderline with no reviewer feeling strongly for or against the paper.  The paper is well written and proposes a simple approach, along with code for reproducibility. Criticism stems primarily in the work s technical novelty, being an incremental improvement of ideas from ANP and BANP, and related work like Neural Bootstrapper. In addition, the experimental validation involves regression on 1 to 2D functions, Bayesopt on synthetic functions, and contextual bandits on the synthetic wheel bandit problem. This is fairly toy, and multiple reviewers raise unaddressed concerns on the regression experiments. Ignoring orginality in and of itself (which is overvalued in conferences), the work does not yet provide a sufficiently convincing demonstration of its practical importance.  I recommend the authors use the reviewers  feedback to enhance their preprint should they aim to submit to a later venue.
This work proposes a stochastic process variant that extends existing work on neural ODEs. The resulting method allows for a fast data adaptive method that can work well fit to sparser time series settings, without retraining. The methodology is backed up empirically, and after the response period, the reviewers  concerns are sufficiently addressed and reviewers are in agreement that the contributions are clear and correct.
The paper introduces an idea that was found interesting by all reviewers (including Gxxe who recommends a marginal reject). A majority of the reviewers also point out a few weaknesses of the paper, notably in terms of clarity of several statements that were found to be hand wavy (see the reviews of Gxxe and oSPE for more precise details). The area chair agrees with those statements, but overall, the originality of the idea introduced in this paper outweighs these weaknesses, and the experimental study is conducted in a reasonably convincing manner.  Even though there is room for improvements, the area chair is happy to recommend an accept, but encourages the authors to follow the constructive feedback provided by the reviewers for the camera ready version.
It is important to develop efficient training methods for BERT like models since they have been widely used in real world natural language processing tasks. The proposed approach is interesting. It speeds up BERT training via identifying lottery tickets in the early stage of training. We agree with the authors s rebuttal that autoML is not that related to the work here. Our main concern on this work  is its worse than BERT performance showed in Table 2. The performance gap is significant. Sufficiently more training steps would fill the performance gap but the proposed method may have no advantage any more over the normal training procedures. To make this work more convincing, we would like to suggest to include experiments on comparing different methods under similar prediction performance.  In addition, since the main claim of this work is for training efficiency,  it will be helpful to show the advantage of this method by directly presenting the training curves/ results of different methods.   Overall this paper is pretty much on the boundary. We encourage the authors to resubmit this work once these issues are well resolved. 
R4 of this submission was slightly positive on this submission while all other reviewers expressed quite significant concerns in their reviews. R4 also agreed that the originality and experimental results as presented in this submission are not sufficient during discussion, although he/she pointed out the incorporation of long range structural information is novel. Given the above recommendations and discussions, a reject is recommended.  
Two reviewers recommend acceptance. One reviewer is negative, however, does not provide reasons for rejection. The AC read the paper and agrees with the positive reviewers. in that the paper provides value for the community on an important topic of network compression.
The manuscript discusses weaknesses in previous sample selection criteria in learning with noisy labels, and proposes a new selection criterion by incorporating the uncertainty of losses, together with theoretical justification. To select samples, the manuscript uses the lower bounds of the confidence intervals derived from concentration inequalities instead of using point estimation of losses. By incorporating uncertainty about large loss samples, the method is able to distinguish: truly mislabeled samples, or clean yet underrepresented samples that are less frequently selected or learned by the model so far. Experiments are performed on four benchmark datasets (MNIST, F MNIST, CIFAR 10, CIFAR 100) and use a diverse set of possible noise functions.   Reviewers agreed on several positive aspects of the manuscript, including: 1. This manuscript addresses weaknesses in previous sample selection methods and potentially impacts various applications (e.g., worst group generalization and fairness); 2. The technical steps are clearly explained with theoretical justification; 3. Many datasets and comparison methods used and thus the proposed approach is also validated empirically.  Reviewers also highlighted several major concerns, including: 1. The space complexity of the proposed method; to implement the proposed method, it seems that naively one will need to keep track of the history of losses of every sample in the training set; 2. The validity of Markov process assumption on the training losses when momentum is used in optimization. Also, the position of the parameter in parameter space depends on the starting points and search paths, but not merely the last positions. 3. The experiments mostly are designed with relatively low noise (20% and 40%); 4. The evaluation matric as well as the analysis should be diverse. The performance of the proposed approach appears to have larger variances, which should be further explained.  Many of the major concerns have been addressed during the rebuttal including: experiments with 50%, 60%, and 70% symmetric noise, extension to NLP data by showing results on the NEWS dataset, and further discussion on the space complexity and Markov process assumption.
The authors take a closer look at widely held beliefs about neural networks. Using a mix of analysis and experiment, they shed some light on the ways these assumptions break down. The paper contributes to our understanding of various phenomena and their connection to generalization, and should be a useful paper for theoreticians searching for predictive theories.
This paper provides a novel approach for addressing ill posed inverse problems based on a formulation as a regularized estimation problem and showing that this can be optimized using the CycleGAN framework. While the paper contains interesting ideas and has been substantially improved from its original form, the paper still does not meet the quality bar of ICLR due to a critical gap between the presented theory and applications. The paper will benefit from a revision and resubmission to another venue.
The authors have addressed the issues raised by the reviewers. All the reviewers think that the paper deserves to be published at ICLR 2021. The authors should implement all the reviewers’ suggestions into the final version, especially for clarity issues and clear explanations. The reviewer also encourages authors to investigate $m$’s effects on the convergence rate more to see whether there is a structural limitation in federated learning settings for future work. 
The submission formulates self paced learning as a specific iterative mini max optimization, which incorporates both a risk minimization step and a submodular maximization for selecting the next training examples.  The strengths of the paper lie primarily in the theoretical analysis, while the experiments are somewhat limited to simple datasets: News20, MNIST, & CIFAR10.  Additionally, the main paper is probably too long in its current form, and could benefit from some of the proof details being moved to the appendix.  
The paper contributes to a body of empirical work towards understanding generalization in deep learning. They do this  through a battery of experiments studying "single directions" or selectivity of small groups of neurons. The reviewers that have actively participated agree that the revision is of high quality, impact, originality, and significance. The issue of a lack of prescriptiveness was raised by one reviewer. I agree with the majority that this is not necessary, but nevertheless, the revision makes some suggestions.  I urge the authors to express the appropriate amount of uncertainty regarding any prescriptions that have not been as thoroughly vetted!  
While this was a borderline paper, concerns about the novelty and significance of the presented work exist on the part of all reviewers, and no reviewer was willing to argue for acceptance. Many good points to the work exist, and a stronger case on these issues would greatly strengthen the paper overall. I look forward to a future submission.
Summary: This paper provides comprehensive empirical evidence for some of the systemic issues in the NAS community, for example showing that several published NAS algorithms do not outperform random sampling on previously unseen data and that the training pipeline is more important in the DARTS space than the exact choice of neural architecture. I very much appreciate that code is available for reproducibility.  Reviewer scores and discussion:  The reviewers  scores have very high variance: 2/3 reviewers gave clear acceptance scores (8,8), very much liking the paper, whereas one reviewer gave a clear rejection score (1). In the discussion between the reviewers and the AC, despite the positive comments of the other reviewers, AnonReviewer 2 defended his/her position, arguing that the novelty is too low given previous works. The other reviewers argued against this, emphasizing that it is an important contribution to show empirical evidence for the importance of the training protocol (note that the intended contribution is *not* to introduce these training protocols; they are taken from previous work).  Due to the high variance, I read the paper myself in detail. Here are my own two cents:   It is not new to compare to a single random sample. Sciuto et al clearly proposed this first; see Figure 1 (c) in https://arxiv.org/abs/1902.08142    The systematic experiments showing the importance of the training pipeline are very useful, providing proper and much needed empirical evidence for the many existing suggestions that this might be the case. Figure 3 is utterly convincing.   Throughout, it would be good to put the work into perspective a bit more. E.g., correlations have been studied by many authors before. Also, the paper cites the best practice checklist in the beginning, but does not mention it in the section on best practices (my view is that this paper is in line with that checklist and provides important evidence for several points in it; the checklist also contains other points not being discussed in this paper; it would be good to know whether this paper suggests any new points for the checklist).  Recommendation: Overall, I firmly believe that this paper is an important contribution to the NAS community. It may be viewed by some as "just" running some experiments, but the experiments it shows are very informative and will impact the community and help guide it in the right direction. I therefore recommend acceptance (as a poster).
The paper proposed a meta learning method for tuning the learning rate. In the discussion, reviewers agreed that the key issue is that the empirical evaluation is not yet sufficient to demonstrate the efficacy of the method. In particular, this is an especially pressing issue given that there are now many meta learning methods for tuning the learning rate (none popular in practice though), and the paper does not compare to any of them. Relatedly, most reviewers found that the novelty of the method is not clearly established and discussed in the paper.   Based on the above, I have to recommend rejecting the paper. I would like to thank the authors for submitting the work for consideration to ICLR. I hope the feedback will be useful for improving the work. 
This paper was referred to the ICLR 2021 Ethics Review Committee based on concerns about a potential violation of the ICLR 2021 Code of Ethics (https://iclr.cc/public/CodeOfEthics) raised by reviewers. The paper was carefully reviewed by two committe members, who provided a binding decision. The decision is "Significant concerns (Do not publish)". Details are provided in the Ethics Meta Review. As a result, the paper is rejected based Ethics Review Committee s decision .  The technical review and meta reviewing process moved proceeded independently of the ethics review. The result is as follows:  This paper studies the problem of evaluating optimiser s performance, which is important to show whether real progress in research has been made. It proposes several evaluation protocols, and used Hyperband (Li et al. 2017) to automate the tuning of each optimiser in the bench marking study. Evaluations have been conducted on a wide range of deep learning tasks, and the paper reaches to a conclusion that none of the recently proposed optimisers in evaluation can uniformly out perform Adam in all the tasks in consideration.  Reviewers agreed that the evaluations are extensive, however there are some shared concerns among reviewers. The paper argues that manual hyper parameter tuning by humans is the right behavior to target for, which is the motivation to use Hyperband as an automating tool, and there is a human study to demonstrate that Hyperband tuning resembles human tuning behaviour. Some reviewers questioned about this desiderata choice that favours human tuning behaviour, also concerns on how the human study is conducted (and to what extend the human study itself is reflective enough for the human tuning behaviour in general).  Personally I welcome any empirical study that aims at understanding the real progress of a research topic, and I agree it is important to make rigorous automation tools in order to enable such a large scale study. Therefore, while the presented results are extensive, I would encourage the authors to incorporate the feedback from the reviewers to better examine their assumptions. 
This paper addresses the problem of domain generalization. The proposed solution, DIVA, introduces a domain invariant variational autoencoder. The latent space can be decomposed into three components: category specific, domain specific, and residual. The authors argue that each component is necessary to capture all relevant information while keeping the latent space interpretable.  This work received mixed scores. Two reviewers recommended weak reject while one reviewer recommended weak accept. There was extensive discussion between the reviewers and authors as well as amongst the reviewers. All reviewers agreed this is an important problem statement and that this work offers a compelling initial approach and experiments for domain generalization. There was disagreement as to whether the contributions as is was sufficient for acceptance. Some reviewers were concerned over similarity to [ref1], this work appears close to the time of ICLR submission and is therefore considered concurrent. However, despite this, there was significant confusion over the proposed solution and whether it is uniquely useful for domain generalization or for other areas like adaptation or transfer learning with reviewers arguing that experiments in these other settings would have helped showcase the benefits of the proposed approach. In addition, there was inconclusive evidence as to whether the two latent components were necessary.   Considering all discussions, reviews, and rebuttals the AC does not recommend this work for acceptance. The contribution and proposed solution needed substantial clarification and the experiments need additional analysis to explain under what conditions each latent component is needed either to improve performance or for interpretability. 
Thanks for your submission to ICLR.  Three of the four reviewers are ultimately (particularly after discussion) very enthusiastic about the paper, and feel that their concerns have been adequately addressed.  The fourth reviewer has not updated his/her score but has indicated that their concerns were at least somewhat addressed.  I took a look at their review and agree that the authors have addressed these concerns sufficiently.  I am happy to recommend this paper for acceptance at this point.  Note that I really appreciate the time and effort that the authors went into adding additional results and clarifications for the reviewers.
The authors study planning problems with sparse rewards.                                                                            They propose a tree search algorithm together with an ensemble of value                                                             functions to guide exploration in this setting.                                                                                     The value predictions from the ensemble are combined in a risk sensitive way,                                                       therefore biasing the search towards states with high uncertainty in value                                                          prediction.                                                                                                                         The approach is applied to several grid world environments.                                                                                                                                                                                                             The reviewers mostly criticized the presentation of the material, in particular                                                     that the paper provided insufficient details on the proposed                                                                        method. Furthermore, the comparison to model free RL methods was deemed somewhat                                                    lacking, as the proposed algorithm has access to the ground truth model.                                                            The authors improved the manuscript in the rebuttal.                                                                                                                                                                                                                    Based on the reviews and my own reading I think that the paper in it s current                                                      form is below acceptance threshold. However, with further improved presentation                                                     and baselines for the experiments, this has potential to be an important contribution.
Four experts reviewed the paper and provided mixed recommendations. All reviewers found the experimental results strong, but they have different views about the technical novelty. Three reviewers considered the technical novelty as a weakness of the paper, but Reviewer z4BR was less concerned about it than the other two. After AC carefully read the paper and the authors  responses, AC agreed with the reviewers that the combination of InfoLOOB and modern Hopfield networks, which were both existing works, is incremental despite the empirical results. Besides, AC agreed with Reviewer jmHN that the theoretical results are not significant enough and could be moved to Appendices. While the empirical results are strong, they could not answer how the trend would change with bigger models and bigger datasets. Hence, while the paper clearly has merit, the decision is not to recommend acceptance. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
This paper offers a novel perspective for learning latent multimodal representations. The idea of segmenting the information into multimodal discriminative and modality specific generating factors is found to be intriguing by all reviewers and the AC. The technical derivations allow for an efficient implementation of this idea.  There have been some concerns regarding the experimental section, but they have all been addressed adequately during the rebuttal period. Therefore the AC suggests this paper for acceptance. It is an overall nice and well thought work.  
After rebuttal the reviewers unanimously agree that this is a strong paper and should be accepted
The paper considers the important problem of performance degradation under distribution shift and proposes a simple yet effective method to alleviate this problem. They do so by considering feature statistic to be non deterministic and rather a multivariate Gaussian distribution.  The model can be integrated into networks without additional parameters and experiments show that it works better than BN as well as if the assumed distribution was uniform. The latter was added during rebuttal period.  There were two main concerns regarding distinguishing the work from AdaIN and baseline that were addressed during rebuttal and some parts of the paper were re written to address repetition.
The authors consider the problem of unconditional image generation in the low data regime, such as learning from frames of a single video or even from a single image. The main idea is to apply GANs with a specific two branch discriminator architecture such that the content features and layout features are handled independently. Secondly, to improve the variability of generated images the authors apply diversity regularization. The authors show that the proposed model is able to, to a certain extent, generate diverse high quality samples.  The paper is well written, the authors described their method and the evaluation protocol thoroughly and clearly. The reviewers felt that this submission was borderline, with questionable novelty and significance. In an extensive rebuttal and discussion phase the authors addressed several raised challenges and improved their paper. However, two points remain:   **Technical novelty**: content and layout separation as well as diversity regularisation previously appeared in many contexts and papers.   **Motivation and practicality**: One of the main arguments for the utility of the proposed method is to use it for data augmentation. While it may indeed result in content based augmentations, it nevertheless necessitates training of a GAN for every single image, which is severely limiting in practice.  After reading the manuscript, reviews, and the rebuttals, my view is that the paper is below the acceptance bar and I agree with the points on novelty and significance. In particular, the main application to data augmentation seems to be "unexciting" and the proposed method impractical. At the same time the proposed method is a combination of already known techniques, albeit in a different setting. I suggest the authors condense the arguments in the extensive rebuttal to improve the points raised above and resubmit.
This paper extends work on neural architecture search by introducing a new framework for searching and experiments on new domains of NMT and QA. The results of the work are  beneficial and show improvements using this approach. However the reviewers point out significant issues with the approach itself:    There is skepticism about the use of NAS in general, particular compared to using the same computational power for other types of simpler hyperparameter search.   There is general concern about the use of such large scale brute force methods in general. Several of the reviewers expressed concerns about ever possibly being able to replicate these results.   Given the computational power required, the reviewers feel like the gains are not particularly large, for instance the Squad results not being compared to the best reported systems.  
All the reviewers found interesting the use of Shapley values to provide feature attributions for fairness, however, the reviewers brought up a number of issues, particularly in terms of presentation and clarity. While the authors  responses did clarify some of these concerns, this was not enough for the reviewers to broadly support acceptance.
Although the reviewers found the idea of the work interesting, they all think it is not ready for publication. The experiments do not properly support the claims. Discussion on the connection to some related work is missing. And also the proposed method is not well motivated. I suggest the authors to take the reviewers  comments into account, revise their work and prepare it for future venues.
The results reported in this paper and the model checkpoints released are of interest and broad utility to the community in the opinion of the NLP.  While one reviewer was somewhat negative, most reviewers were in favor of acceptance of this paper, which expands the results from [1] to downstream tasks. The AC therefore recommends acceptance.
This manuscript personalization techniques to improve the scalability and privacy preservation of federated learning. Empirical results are provided which suggests improved performance.  The reviewers and AC agree that the problem studied is timely and interesting, as the tradeoffs between personalization and performance are a known concern in federated learning.  However, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty and clarity of the conceptual and empirical results. Reviewers were also unconvinced by the provided empirical evaluation results. 
This paper proposes training Gaussian mixture models using SGD, creating an algorithm appropriate for streaming data. However, we feel that the current manuscript does not sufficiently support the proposed method, and lacks insight into its workings. The reviewers believed the method lacked justification (while the authors claim to have added theoretical justification to the revised manuscript, I did not see any such new theory), and were not convinced that the method offered a significant improvement on existing methods. 
The main idea of this paper is to use nearest neighbor search to to accelerate iterative thresholding based sparse recovery algorithms. All reviewers were  underwhelmed by somewhat straightforward combination of  existing results in sparse recovery and nearest neighbor search.  While the proposed method seems effective in practice, the paper has the feel of not being a fully publishable unit yet. Several technical questions were asked but no author feedback was provided to potentially lift this paper up.
Unfortunately, the reviewers of the paper are all not certain about their review, none of them being RL experts.  Assessing the paper myself—not being an RL expert but having experience—the authors have addressed all points of the reviewers thoroughly.   
This paper studies the problem of uncertainty estimation under distribution shift. The proposed approach (PAD) addresses this under estimation issue, by augmenting the training data with inputs that the network has unjustified low uncertainty estimates, and asking the model to correct this under estimation at those augmented datapoints. Results show promising improvement over a set of common benchmark tasks in uncertainty estimation, with comparisons to a number of existing approaches.  All the reviewer agreed that the experiments are well conducted and the empirical results are very promising. However, they also had a shared concern on the justification of the approach. Reviewers are less willing to accept a paper merely for commending its empirical performance.  I share the above concern as the reviewers, and I personally found the presentation of the approach a bit rush and disconnected from the motivation. For example, the current presentation feels like the method is motivated by BNNs but it is not clear to me how the proposed objective connects to the motivation. Also no derivation of the objective is included in either main text or appendix.   In revision, I would suggest a focus on improving the clarity and theoretical justification of the proposed objective function.
In this paper, the authors show the effect of RNI on the expressive power of GNN for the first time, where the RNI was initially proposed in Sato et al. 2020. Overall, I like the idea of random node initialization because it is simple, effective, and theoretically well founded. The key concern was that the novelty over the Sato s paper and the reviewers were still not convinced by the response. Therefore, the paper is still below the acceptance threshold.  I strongly encourage authors to revise the paper based on the reviewer s comments and resubmit it to a future venue.  
The paper introduces a version of approximate policy iteration (API), called Autodidactic Iteration (ADI), designed to overcome the problem of sparse rewards.  In particular, the policy evaluation step of ADI is trained on a distribution of states that allows the reward to easily propagate from the goal state to states farther away.  ADI is applied to successfully solve the Rubik s Cube (together with other existing techniques).  This work is an interesting contribution where the ADI idea may be useful in other scenarios.  A limitation is that the whole empirical study is on the Rubik s Cube; a controlled experiment on other problems (even if simpler) can be useful to understand the pros & cons of ADI compared to others.  Minor: please update the bib entry of Bottou (2011).  It s now published in MLJ 2014.
The paper received four negative reviews. The overall idea was found to be interesting, but several concerns were raised. There is a general consensus that the experimental part and the results are not convincing. Several comments have also been made regarding the clarity and motivation, which needs to be strengthened. R4 also mentions references from the sparse estimation literature that would help for positioning the paper. The rebuttal did address some of these points, but it was not sufficient to change their opinion.  Overall, the area chair agrees with the reviewers and follows their recommendation.
This paper is an analysis of the phenomenon of example forgetting in deep neural net training. The empirical study is the first of its kind and features convincing experiments with architectures that achieve near state of the art results. It shows that a portion of the training set can be seen as support examples. The reviewers noted weaknesses such as in the measurement of the forgetting itself and the training regiment. However, they agreed that their concerns we addressed by the rebuttal. They also noted that the paper is not forthcoming with insights, but found enough value in the systematic empirical study it provides.
The paper proposes a discriminator dependent rejection sampling scheme for improving the quality of samples from a trained GAN. The paper is clearly written, presents an interesting idea and the authors extended and improved the experimental analyses as suggested by the reviewers.
The diffusion maps framework is used to embed a given collection of datasets into diffusion coordinates that capture intrinsic geometry. Then a correspondence map is constructed between datasets by  finding rotations that align these coordinates. The approach is interesting. The reviewers, however, found the empirical analysis somewhat simplistic with inadequate comparisons to other correspondence construction methods in the literature. 
Split opinions on paper: 6 (R1), 3 (R2), 6 (R3). Much of the debate centered on the novelty of the algorithm. R2 felt that the paper was a straight forward combination of CycleGAN with S+U, while R3 felt it made a significant contribution. The AC has looked at the paper and the reviews and discussion. The topic is very interesting and topical. The experiments are ok, but would be helped a lot by including the real/synth car data currently in appendix B: seeing the method work on natural images is much more compelling. The approach still seems a bit incremental: yes, it s not a straight combination but the extra stuff isn t so profound. The AC is inclined to accept, just because this is an interesting problem.
The method under consideration uses parallel convolutional filter groups per layer, where activations are averaged between the groups, forming "inner ensembles".  Reviewers raised a number of concerns, including the increased computational cost for apparently little performance gain, the choice of base architecture (later addressed with additional experiments using WideResNet and ResNeXt), issues of clarity of presentation (some of which were addressed). One reviewer was unconvinced without direct comparison to full ensembles. Another reviewer raised the issue of a missing direct comparison to the most similar method in the literature, maxout (Goodfellow et al, 2013). Authors rebutted this by claiming that maxout is difficult to implement and offering vague arguments for its inferiority to their method.  The AC agrees that a maxout baseline is important here, as it is extremely close to the proposed method and also trivially implemented, and that in light of maxout (and other related methods) the degree of novelty is limited.  The AC also concurs that a full ensemble baseline would strengthen the paper s claims. In the absence of either of these the AC concurs with the reviewers that this work is not suitable for publication at this time.
Reviewers unanimously accepted this paper. 
The paper aims to improve generalization and sample efficiency in robotic control. In this regard authors note that modular robot systems, which provide building blocks for a task specific morphology, can be considered just another domain where transformers can be used. The authors propose to learn a universal controller over this modular design space and leverage successful “large scale pre training and fine tuning” scheme for transformer. We thank the reviewers and authors for engaging in an active discussion. Reviewers found that methodological novelty is limited ("The contributions on algorithmic and network designs are marginal. Their approach is a direct application of Transformers and Reinforcement Learning approaches.") and there are very similar approaches in the literature (e.g., AMORPHEUS approach (AM) by Kurin et al., 2021.) Despite these shortcomings, overall the reviewers found the experimental results to be strong and analysis to be thorough, which will be valuable to the community.
Pros:   good results on Montezuma  Cons:   moderate novelty   questionable generalization   lack of ablations and analysis   lack of stronger baselines   no rebuttal   The reviewers agree that the paper should be rejected in its current form, and the authors have not bothered revising it to take into account the detailed reviews.
There is overall consensus about the paper s lack of novelty and clarity.  Reviewer 1 has detailed comments that can be used to strengthen the paper.  Reviewer 3 suggests that this paper is very close to Anandkumar et al 2012, and it is not clear where the novelty lies.  Addressing these concerns of the reviewers will make the paper more acceptable to future venues.
This paper proposes a new benchmark to evaluate natural language processing models on discourse related tasks based on existing datasets that are not available in other benchmarks (SentEval/GLUE/SuperGLUE). The authors also provide a set of baselines based on BERT, ELMo, and others; and estimates of human performance for some tasks.  I think this has the potential to be a valuable resource to the research community, but I am not sure that it is the best fit for a conference such as ICLR. R3 also raises a valid concern regarding the performance of fine tuned BERT that are comparable to human estimates on half of the tasks (3 out of 5), which slightly weakens the main motivation of having this new benchmark.   My main suggestion to the authors is to have a very solid motivation for the new benchmark, including the reason of inclusion for each of the tasks. I believe that this is important to encourage the community to adopt it. For something like this, it would be nice (although not necessary) to have a clean website for submission as well. I believe that someone who proposes a new benchmark needs to do as best as they can to make it easy for other people to use it.  Due to the above issues and space constraint, I recommend to reject the paper.
This paper builds on the success of the FermiNet neural wave function framework by pairing it with a graph neural network which predicts the parameters of neural wave function from the geometry. The resulting PESNet trains significantly faster, with no loss of accuracy. This method constitutes an important advance in ML powered quantum mechanical calculations.  The reviewers unanimously recommend acceptance.
The authors present an interesting approach for combining finite state automata to compose new policies using temporal logic. The reviewers found this contribution interesting but had several questions that suggests that the current paper presentation could be significantly clarified and situated with respect to other literature. Given the strong pool of papers, this paper was borderline and the authors are encouraged to revise their paper to address the reviewers’ feedback.   
This paper considers the information bottleneck Lagrangian as a tool for studying deep networks in the common case of supervised learning (predicting label Y from features X) with a deterministic model, and identifies a number of troublesome issues. (1) The information bottleneck curve cannot be recovered by optimizing the Lagrangian for different values of β because in the deterministic case, the IB curve is piecewise linear, not strictly concave. (2) Uninteresting representations can lie on the IB curve, so information bottleneck optimality does not imply that a representation is useful. (3) In a multilayer model with a low probability of error, the only tradeoff that successive layers can make between compression and prediction is that deeper layers may compress more. Experiments on MNIST illustrate these issues, and supplementary material shows that these issues also apply to the deterministic information bottleneck and to stochastic models that are nearly deterministic. There was a substantial degree of disagreement between the reviewers of this paper. One reviewer (R3) suggested that all the conclusions of the paper are the consequence of P(X,Y) being degenerate. The authors responded to this criticism in their response and revision quite effectively, in the opinion of the AC. Because R3 failed to participate in the discussion, this review has been discounted in the final decision. The other two reviewers were considerably more positive about the paper, with one (R1) having basically no criticisms and the other (R2) expression some doubts about the novelty of the observations being made in the paper and their importance for practical machine learning scenarios.  Following the revision and discussion, R2 expressed general satisfaction with the paper, so the AC is recommending acceptance. The AC thinks that the final paper would be clearer if the authors were to carefully distinguish between ground truth labels used in training and the labels estimated by the model for a given input.  At the moment, the symbol Y appears to be overloaded, standing for both.  Perhaps the authors should place a hat over Y when it is standing for estimated labels?
The paper presents a spatial temporal prediction framework with causal effects of predictors for better interpretability. The idea is interesting and the touch on modeling causal relations could be useful in practical applications. The paper receives mixed ratings and therefore there has been extensive discussion. We agree that while the paper has some merits, it falls short on the following aspects:   1, One central issue pointed out by all reviewers is the evaluation. For example, the contribution on efficient attention was not compared to any previous work; most of the baselines do no have access to the spatial information, which makes the comparison unfair. The authors did add two more baselines with access to spatial information. However, there are not enough details and discussions to make the results convincing; In addition, other stronger baselines should be added.   2. The notation and technical presentation was extremely lacking in the submitted version, the amount of unintroduced notations. Even in their core contribution equations had major issues with norm and vectors mixed together (see the difference between the corrected equation in the Taylor equation and the one in the original submission)  After the discussion, all reviewers agree that the paper fails to provide a fair and convincing evaluation, and the ratings will be adjusted to reflect the discussion. We hope that the reviews can help the authors improve the draft for a stronger submission in the future. 
The authors introduce a new activation function which is similar in shape to ELU, but is faster to compute.   The reviewers consider this to not be a significant innovation because the amount of time spent in computing the activation function is small compared to other neural network operations.
The reviewers have agreed this paper is not ready for publication at ICLR. 
This paper adapts the mixup data augmentation strategy to the case of metric learning. The main challenge addressed is the fact that in metric learning, the loss function does not treat each example as an IID sample. The paper takes the view of metric learning as learning over positive and negative pairs (those belonging to the same/different classes) and uses this to develop a fairly general metric mixup formulation. To measure the effectiveness of the approach for metric learning, the paper introduces a new measure called utilization that looks at the distance of a query point to its nearest training point in embedding space.  The reviewers (5 of them) all favour acceptance on the grounds of novelty, and the performance of the method. During the discussion, some issues were raised around whether utilization is a useful measure, improvements to the paper clarity, whether the clean loss in eq. 10 is necessary, and potential limitations on the generality of the approach. However, additional experiments and clarification during the discussion period has resolved these issues to their satisfaction.
The paper proposes to use the sum of training losses during training, or a variant where the sum of training losses begins to be computed after the first E epochs, to estimate the generalization performance of the corresponding network. Although the results seem promising for query based NAS strategies, the reviewers agree that as the paper proposes something that is fundamentally opposite to the common practice, it requires more careful and thorough analysis. Besides, while the connection made by authors to the Bayesian marginal likelihood is interesting, it s not a rigorous argument that convinces the audience about the applicability of the proposed method. I strongly encourage the authors to add more analysis and discussion to the revised version to strengthen their claim and clarify its scope.  
This paper presents an analysis of the robustness of self supervised learning (SSL) features to noisy labels in downstream supervised learning, and provides empirical verification of the results (mostly in the symmetric noise setup); a SSL regularization scheme is also analyzed (section 4). While the paper contains plausible insights, the reviews share similar concerns that the analysis is mainly based on the noise being symmetric, and that the SSL features already have good class separation and Gaussian clusters, which are strong assumptions. Given that the assumptions are not theoretically verified, and that there is not sufficient empirical results in heavy non symmetric noise scenario on large benchmark datasets, the reviewers think the paper does not provide practical guidance for noise label learning in its current form.
The Reviewers noticed that the paper undergone many editions and raise concern about the content. They encourage improving experimental section further and strengthening the message of the paper. 
All reviewers gave either borderline or negative scores; unfortunately, discussion was not lively, so scores remained the same. No reviewers voice strong support for acceptance, but acknowledge several merits of the work.
This paper proposes to approximate arbitrary conditional distribution of a pertained VAE using variational inferences. The paper is technically sound and clearly written. A few variants of the inference network are also compared and evaluated in experiments.  The main problems of the paper are as follows: 1. The motivation of training an inference network for a fixed decoder is not well explained. 2. The application of VI is standard, and offers limited novelty or significance of the proposed method. 3. The introduction of the new term cross coding is not necessary and does not bring new insights than a standard VI method.  The authors argued in the feedback that the central contribution is using augmented VI to do conditioning inference, similar to Rezende at al, but didn t address reviewers  main concerns. I encourage the authors to incorporate the reviewers  comments in a future revision, and explain why this proposed method bring significant contribution to either address a real problem or improve VI methodology.
As one of the reviewers  comment, the paper presents "a mixed of tricks" for the multilingual speech recognition, which includes 1) the use of a pretrained mBERT, 2) dual adapter and 3) prior adjusting.  First, the relative gains of the pretrained mBERT is marginal (Section 3.3.1). Secondly, using 1) on top of 2) is unnecessary.  These confuses the reader about what the conclusion of the paper is.  It would be better if choosing one aspect of the problem and investigate it deeper.   The decision is mainly because of the lack of novelty and clarity. 
The authors propose a modification of the statistical recurrent unit for modelling mutliple time series and show that it can be very useful in practice for identifying granger causality when the time series are non linearly related. The contributions are primarily conceptual and empirical. The reviewers agree that this is a useful contribution in the causality literature.
The paper addresses the problem of generating descriptions from structured data. In particular a Variational Template Machine  which explicitly disentangles templates from semantic content. They empirically demonstrate that their model performs better than existing methods on different methods.   This paper has received a strong acceptance from two reviewers. In particular, the reviewers have appreciated the novelty and empirical evaluation of the proposed approach. R3 has raised quite a few concerns but I feel they were adequately addressed by the reviewers. Hence, I recommend that the paper be accepted. 
The paper studies an important newly identified problem in continual learning of rapid adaptation, and proposes the use of a generate and test method to continually inject random features alongside SGD, enabling better learning on non stationary data streams. Unfortunately the paper remained borderline in the discussions. While reviewers liked the overall research direction and contributions, they also agreed the paper in current form still would benefit from deeper insights into the proposed method, stronger empirical evidence. Experiments cover broad applications including RL, but don t seem to give very clear advantages over other weight regularization schemes, and other metrics of quality could be added. We appreciate the authors have added additional experiments testing it both for the two important regimes of under  and over parameterized networks, though those can be expanded. We are sorry that this good paper remained narrowly below the bar in this case, and hope the detailed feedback helps to strengthen the paper for a future occasion.
main summary: sparse time LSTM  discussions; reviewer 4: technical description of the proposed method insufficient, reviewer 2, 3: same paper sent to ICLR 2019 and rejected recommendation: rejected, based on all reviewers comments
This paper explores an interpretation of generative models in terms of interventions on their latent variables.  The overall set of ideas seems novel and potentially useful, but the presentation is unclear, the goal of the method seems poorly defined, and the qualitative results (including the videos) are unconvincing.  I recommend you put work into factoring the ideas in this paper into smaller ones.  For instance, definition 1 is a mess.  I would also recommend the use of algorithm boxes.
This paper analyzes a mechanism of the implicit regularization caused by nonlinearity of ReLU activation, and suggests that the learned DNNs interpolate almost linearly between data points, which leads to the low complexity solutions in the over parameterized regime. The main objections include (1) some claims in this paper are not appropriate; (2) lack of proper comparison with prior work; and many other issues in the presentation. I agree with the reviewers’ evaluation and encourage the authors to improve this paper and resubmit to future conference. 
This paper explores the performance of Q learning in the presence of either one sided feedback or full feedback. Such feedbacks play an important role in improving the resulting regret bounds, which are (almost) not affected by the dimension of the state and action space. The motivation of such feedback settings stems from problems like inventory control. However, the assumptions underlying the theory herein are often quite strong, which might limit the applicability of the theory. The dependency on the length per episode H can also be improved.   
In this paper, the authors proposed a reinforcement learning based model for aspect based sentiment analysis. As raised by the reviewers, 1) the writing needs to be improved: e.g., presenting the details of the proposed method clearly, citing the references properly, etc. 2) related methods need to be implemented for comparison, 3) the reported results are not SOTA compared with existing methods. Moreover, some technical claims are not convincing, which need to be stated more carefully.  In summary, based on its current shape, this paper is not ready to be published in ICLR.
I thank the authors and reviewers for the lively discussions. Although reviewers mentioned the work has potentials to improve adversarial robustness, they agreed that the current draft needs a bit more work specially to strengthen its experimental results and comparisons with related works.   
Dear Authors,  Thank you very much for your detailed feedback to the initial reviews and also for further answering additional questions raised by a reviewer. Your effort has been certainly contributed to clarifying some of the concerns raised by the reviewers and improving their understanding of this paper.  Overall, all the reviewers found a merit in this paper and thus I suggest its acceptance. However, as Reviewer #2 suggested, investigating the convergence in the stochastic case is very important. More discussion on this would be a valuable addition to the paper, which the authors can incorporate in the final version.
The paper provides a high probability analysis for Adagrad for smooth non convex optimization and shows its rate of convergence to critical points. Both rates for deterministic optimization and for stochastic optimization are provided. The main contribution of the paper is that unlike for SGD they don’t require knowledge of smoothness parameter in advance and second, they prove high probability results.   The reviewers lean positively towards the paper. One of the reviewers comments about the comparison with SGD which has some merit. The main comparison of this paper is w.r.t. ward et al 2019 and Zhou et al 2018 both of which prove high probability results. However, both these works require prior knowledge of smoothness parameter. The other axis of comparison is w.r.t algorithms like spider by Fang et al 2018 which uses variance reduction type techniques to obtain the optimal rate for critical point (here it is 1/sqrt{T} for norm square which is T^{ 1/4} for norm and spider is T^{ 1/3} for norm). Of course, an argument can be made for the fact that the algorithm here is closer to what is used in practice and more importantly, the assumptions there are somewhat stronger.   In any case, the paper still has interesting results and I am leaning towards an accept.
This paper proposes methods to improve the performance of the low precision neural networks. The reviewers raised concern about lack of novelty. Due to insufficient technical contribution, recommend for rejection. 
The paper describes an autoencoder based approach to anomaly detection.  The main weakness—not untypical for papers in this application area—is the experimental section.  The problem itself may be not well defined, and of course that makes practical comparison difficult. Perhaps different measures—e.g., remaining life—may be better to compare on, and give better data sets.
This paper introduces a simple baseline for few shot image classification in the transductive setting, which includes a standard cross entropy loss on the labeled support samples and a conditional entropy loss on the unlabeled query samples.  Both losses are known in the literature (the seminal work of entropy minimization by Bengio should be cited properly). However, reviewers are positive about this paper, acknowledging the significant contributions of a novel few shot baseline that establishes a new state of the art on well known public few shot datasets as well as on the introduced large scale benchmark ImageNet21K. The comprehensive study of the methods and datasets in this domain will benefit the research practices in this area.  Therefore, I make an acceptance recommendation.
This paper proposes a new contribution in the recent literature on learning distributions of sketches. While all reviewers have recognized the overall good quality of the presentation, two factors seem to weight heavily on a negative decision: clarifications on the contribution s scope (presented as a tool for general Hessians in the introduction, but ultimately only applied to least square errors of linear predictors, to recover an explicit factorization of the Hessian matrix) and links with existing literature; weakness of experiments whose small scale does not justify using sketches in the first place. Since this is a "learning" approach, I am particularly sensitive to the latter point, and therefore am inclined to reject, but I encourage the authors to address these two issues with the current draft.
This paper shows that batch normalization can be cast as approximate inference in deep neural networks.  This is an appealing result as batch normalization is used in practice in a wide variety of models.   The reviewers found the paper well written and easy to understand and were motivated by underlying idea.  However, they found the empirical analysis lacking and found that there was not enough detail in the main text to verify whether the claims were true.  The authors empirically compared to a recent method showing that dropout can be cast as approximate inference with the claim that by transitivity they were comparing to a variety of recent methods.  AnonReviewer1 casts significant doubt on the results of that work.  This is very unfortunate and not the fault of the authors of this paper.  The authors have since gone to great length to compare to Louizos and Welling, 2017.  Unfortunately, that comparison doesn t appear to be complete in the manuscript.  The main text was also lacking specific detail relating to fundamental parts of the proposed method (noted by all reviewers).  Overall, this paper seems to be tremendously promising and the underlying idea potentially very impactful.  However, given the reviews, it doesn t seem that this paper would achieve its potential impact.  The response from the authors is appreciated and goes a long way to improving the paper.  Taking the reviews into account, adding specific detail about the methodology and model (e.g. the prior) and completing careful empirical analysis will make this a strong paper that should be much more impactful.
This paper explores the effect of poorly sampled episodes in few shot learning, and its effect on trained models. The improvements from the additional attention module (CEAM) and regularizer (CECR) are strong, and the ablations are thorough. The reviewers are not fully convinced that poor sampling is indeed the main issue. That is, it could be that CEAM and CECR improve performance for other reasons, but the hypothesis is sensible, and the reviewers believe a more thorough investigation is beyond the scope of this work.  During discussions, one note that came up is whether CEAM works because of cross episode attention, or if the idea of an instance level FEAT is itself a good one. One ablation to sort this out would be to apply FEAT and an instance level FEAT on episodes that are twice as large as those seen by CEAM so that the effective episode size is the same. This would help answer: is it the reduced noise due to effectively larger episodes, a stronger attention mechanism using instance level information, or is the idea of crossover episodes indeed the important factor? The reviewers agree that this baseline, or an analogous baseline, should be included in the final version. 
This paper proposes a method for differentiable pruning that replaces the hard thresholding of standard pruning, with a soft version that permits taking the gradients of the pruning threshold. The proposed benefits are an accuracy that is better or competitive with alternative methods as well. Moreover, the paper suggests the technique to be efficient.  The pros of this paper are that it is working in an interesting setting of differentiable pruning, with the hope of   in some sense   simplifying the pruning process or at least unifying the process with standard training.  The technique is plausibly justified in its technical development. The paper also follows with a significant number of experiments.   The cons of this paper are that the conceptual framework   beyond the initial idea   is not fully clear. In particular, this paper does not elucidate a clear set of claims and hence, results in the difficulty on the Reviewers part in detangling the claims and identifying the appropriate comparisons.  For example, the paper doesn t take up a simple claim that it is state of the art in accuracy vs parameter measures (and would seem not to given the results of Renda et al. (2020)).  It need not necessarily make this claim, but there are suggestions to such a claim early in the paper. If this is not an intended claim, then the paper can remove any suggestions to such (i.e., the claims around new SoTA for networks not evaluated in prior work).   The paper has a somewhat tentative claim that it is more efficient (in the total number of epochs of training) versus other techniques (Table 3).  However, the presented results are only at a single point versus other methods.  Renda et al. (2020) directly consider accuracy versus retraining cost trade offs. Appendix E of that paper provides one shot pruning results for ResNet 50 showing accuracy on par with that presented here.  The number of retraining epochs is also similar to here. This paper, however, only compares against the most expensive iterative pruning data point in the other paper.  In sum, my recommendation is Reject. This is promising work that needs only (1) to include a few testable claims and (2) to re organize the results (and perhaps run a limited set of new results) to thoroughly explore those claims. For example, if the most important claim is accuracy vs retraining cost, then it needs to show a more complete trade off curve of the two results.  Of course, this, in principle, opens the door to comparisons to many other techniques in the literature. 
meta score: 8  The paper explores mixing 16  and 32 bit floating point arithmetic for NN training with CNN and LSTM experiments on a variety of tasks  Pros:    addresses an important practical problem    very wide range of experimentation, reported in depth  Cons:    one might say the novelty was minor, but the novelty comes from the extensive analysis and experiments
This paper studies the problem of how to use 3D molecular geometry information during training to improve performance during prediction time when 3D information is not available. This is a highly interesting problem as obtaining 3D molecular geometry information requires expensive calculations and such information is usually not available in practice during prediction, while there are some training data with both 2D and 3D information. The work proposes to use self supervised, predictive and generative approaches to make use of such information. The reviewers overall expressed mixed recommendations. One of the reviewers who scored 5 did not provide further feedback after author response even being prompted multiple times. The other reviewer who scored 5 actively participated in discussion and it seems most of the concerns have been addressed. Given the importance of this problem, and this work seems to be among the first to address this problem, I lean toward accept.
This paper is about unsupervised translation between programming languages. The main positive is that it introduces the idea of using a form of unit test generation and execution behavior within a programming language back translation setup, and it puts together together a number of pieces in an interesting way: text to text transformers, unit test generation, execution and code coverage. Results show a substantial improvement. The main weaknesses are that there are some caveats that need to be made, such as the (heuristic, not learned) way that test cases are translated across languages is not fully general, and that limits the applicability. There are also some cases where I find that the authors are stretching claims a bit beyond what experiments support, e.g., in the response to zd7L about applicability to COBOL.  All in all, though, it s a good implementation of an idea that should have a lasting place in this line of work, so it s worth accepting.
All four referees have indicated reject. Severe points of criticism have been raised, concerning the lacking novelty, the  experimental setup and the significance and interpretation of results. I fully agree with the reviewers in all important points, so I recommend rejection.
The paper studies attacks on the self supervised training pipeline of multi modal models, e.g., CLIP and related models.  The reviewers agree that the poisoning results are impressive in that they achieve good poisoning success with a fairly small number of samples.  The threat model is fairly specific to one (high profile) type of self supervised training, but the concepts presented are likely portable to the study of other related training pipelines.
In this paper, the authors propose a new GNN architecture based on Generalized PageRank to handle two weaknesses in some existing GNNs. The novelty of this approach is that it works well for both homophilic and heterophilic graphs (due to the use of GPR).  Overall the paper is interesting and well written. Moreover,  the authors addressed the concerns of reviewers during the rebuttal period. Thus, I vote for acceptance.
The reviewers had a number of concerns which seem to have been addressed by the authors in the discussion phase.  All the reviewers are in favor of accepting the paper. The paper provides an interesting/novel idea for federated learning with heterogenous clients/devices. 
The paper contributes to the literature of deep kernel learning by proposing a deep probabilistic model based on inverse Wishart distributions. This is an interesting addition to the literature, and the authors have provided some experimental evidence of the superiority of their method compared to state of the art on deep GPs and NNGPs. The major concerns with the paper, though, are related to the clarity of writing and its readability. Additionally, the experimental comparison is thin and lacks exploration for alternative configurations for DGPs and NNGPs. 
The paper proposes a hybrid VAE normalizing flow for extracting local and global representations of images.  While the reviewers found the model itself to be "conceptually simple" and "straightforward", all were convinced by the empirical evaluation that, indeed, interesting representation learning is going on, resulting in a unanimous vote to accept.
This submission proposes a new manner to learn ordinary differential equations, aiming to improve their efficiency. While judging it interesting, the reviewers are quite split on this work. Overall there was no strong consensus to accept, nor anyone willing to champion this work.  The main stated weaknesses are    The reliance on the existence of a diffeomorphism (and its choice in the method)   The choice of the base and its expressiveness   A somewhat limited experimental section, not indicating strongly how amenable this would be to more complex problems.
The paper addresses the problem of interpreting recurrent neural networks by quantizing their states an mapping them onto a Moore Machine. The paper presents some interesting results on reinforcement learning and other tasks. I believe the experiments could have been more informative if the proposed technique was compared against a simple quantization baseline (e.g. based on k means) so that one can get a better understanding of the difficulty of these task.  This paper is clearly above the acceptance threshold at ICLR. 
This paper presents experimental evidence that learning with privacy requires optimization of the model settings (architectures and initializations) that are not identical to those used when learning without privacy. While acknowledging potential usefulness of this work for practitioners, the reviewers expressed several important concerns such as (1) lack of SOTA baseline comparisons, (2) lack of clarity of the empirical evaluation protocols, (3) large models (that are widely used in practice) have not been studied in the paper, (4) low technical novelty. The authors have successfully addressed some of the concerns regarding (1) and (2). However (3) and (4) make it difficult to assess the benefits of the proposed approach for the community and were viewed by AC as critical issues. We hope the detailed reviews are useful for improving and revising the paper. 
This paper analyzes neural recording data taken from rodents performing a continual learning task using demixed principal component analysis, and aims to find representations for behaviorally relevant variables. They compare these features with those of a deep RL agent.  I am a big fan of papers like this that try to bridge between neuroscience and machine learning. It seems to have a great motivation and there are some interesting results presented. However the reviewers pointed out many issues that lead me to believe this work is not quite ready for publication. In particular, not considering space when analyzing hippocampal rodent data, as R2 points out, seems to be a major oversight. In addition, the sample size is incredibly small (5 rats, only 1 of which was used for the continual learning simulation). This seems to me like more of an exploratory, pilot study than a full experiment that is ready for publication, and therefore I am unfortunately recommending reject.  Reviewer comments were very thorough and on point. Sounds like the authors are already working on the next version of the paper with these points in mind, so I look forward to it. 
This work introduces a trainable signal representation for spherical signals (functions defined in the sphere) which are rotationally equivariant by design, by extending CNNs to the corresponding group SO(3). The method is implemented efficiently using fast Fourier transforms on the sphere and illustrated with compelling tasks such as 3d shape recognition and molecular energy prediction.  Reviewers agreed this is a solid, well written paper, which demonstrates the usefulness of group invariance/equivariance beyond the standard Euclidean translation group in real world scenarios. It will be a great addition to the conference. 
This paper proposes an end to end deep reinforcement learning based algorithm for the 2D and 3D bin packing problems. Its main contribution is conditional query learning (CQL) which allows effective decision over mutually conditioned action spaces through policy expressed as a sequence of conditional distributions. Efficient neural architectures for modeling of such a policy is proposed. Experiments validate the effectiveness of the algorithm through comparisons with genetic algorithm and vanilla RL baselines.  The presentation is clear and the results are interesting, but the novelty seems insufficient for ICLR. The proposed model is based on transformer with the following changes: * encoder: position embedding is removed, state embedding is added to the multi head attention layer and feed forward layer of the original transformer encoder; * decoder: three decoders one for the three steps, namely selection, rotation and location. * training: actor critic algorithm
This paper proposes a simple method to discover latent manipulations in trained text VAEs. Compared to random and coordinate directions, the authors found that by performing PCA on the latent code to find directions that maximize variance, more interpretable text manipulations can be achieved.   This paper receives 4 reject recommendations with an average score of 3.75. The reviewers have raised many concerns regarding the paper. (i) The idea is straightforward with limited novelty. (ii) There are only mostly qualitative results presented. More in depth analysis and more solid evaluations are needed. (iii) Human evaluation is too small to draw any reliable conclusion. (iv) The proposed method is only tested on one text VAE, how well it can be generalized to other models remains unclear.  The rebuttal unfortunately did not address the reviewers  main concerns. Therefore, the AC regrets that the paper cannot be recommended for acceptance at this time. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere. 
The reviewers generally found the paper s contribution to be valuable and informative, and I believe that this paper should be accepted for publication and a poster presentation. I would strongly recommend to the authors to carefully read over the reviews and address any comments or concerns that were not yet addressed in the rebuttal.
This paper provides empirical evidence on synthetic examples with a focus on understanding the relationship between the number of “good” local minima and number of irrelevant features. The reviewers find the problem discussed to be important. One of the reviewers has pointed out that the paper does not present deep insights and is more suitable for workshops. The authors did not provide a rebuttal, and it appears that the reviewers opinion has not changed.  The current score is clearly not sufficient to accept this paper in its current form. Due to this reason, I recommend to reject this paper.  
This paper proposes a simple meta algorithm to speed up data thinning algorithms with good theoretical guarantees. The method is both theoretically interesting and useful for practical applications.
The paper compares MAML and NAL for meta learning, and provides theoretical explanations on some very simple models when MAML can be significantly better than NAL, related to a definition of task hardness. The findings are also supported by experimental results. While the results are plausible and can mark the starting point of a useful analysis, the models analyzed in the paper are too simplistic to warrant publication at ICLR. The authors are encouraged to extend their methodology to more complicated task models, as well as to, e.g., multi step versions of MAML (since the considered version of MAML makes a single step, the proposed problem hardness may not be applicable in more general situations). It is also not clear how the derived insights can guide the practical applications of MAML.
This paper introduces a new dataset and method for a "semantic parsing" problem of generating logical sql queries from text.  Reviews generally seemed to be very impressed by the dataset portion of the work saying "the creation of a large scale semantic parsing dataset is fantastic," but were less compelled by the modeling aspects that were introduced and by the empirical justification for the work.  In particular:    Several reviewers pointed out that the use of RL in particularly this style felt like it was "unjustified", and that the authors should have used simpler baselines as a way of assessing the performance of the system, e.g. "There are far simpler solutions that would achieve the same result, such as optimizing the marginal likelihood or even simply including all orderings as training examples"    The reviewers were not completely convinced that the authors  backed up their claims about the role of this dataset as a novel contribution. In particular there were questions about its structure, e.g. "dataset only covers simple queries in form of aggregate where select structure" and about comparisons with other smaller but similar datasets, e.g.  "how well does the proposed model work when evaluated on an existing dataset containing full SQL queries, such as ATIS"  There was an additional anonymous discussion about the work not citing previous semantic parsing datasets. The authors noted that this discussion inappropriately brought in previous private reviews. However it seems like the main reviewers issues were orthogonal to this point, and so it was not a major aspect of this decision. 
Average score of 3.33, highest score of 4. The AC recommends rejection. 
This paper proposes a few shot (untargeted) backdoor attack (FSBA) against siamese network based visual object tracking. Contributions can be summarized as follows: First, this paper treats the attack task as an instance of multi task learning and can be regarded as the first backdoor attack against VOT. Besides, a simple yet effective few shot untargeted backdoor attack is proposed and achieves significant effectiveness in both digital and physical world scenarios. This paper reveals the vulnerability of VOT to backdoor attacks caused by outsourced training or using third party pre trained models. One weakness is that threat model requires a very strong attacker with ability to modify the training algorithm, but only very simple defenses are considered. Overall, this is a good first attempt at showing vulnerability of VOT approaches.
This paper proposes algorithms for learning (coarse) correlated equilibrium in multi agent general sum Markov games, with improved sample complexities that are polynomial in the maximum size of the action sets of different players. This is a very solid work along the line of multi agent reinforcement learning and there is unanimous support to accept this paper. Thus, I recommend acceptance.
This paper proposes to solve the fuel optimization problem in hybrid electric vehicles using reinforcement learning. The work is interesting, but the reviewers consider it lacks novelty and it has different concerns on the assumptions of the modeling. The paper is quite difficult to follow. 
This paper received 5 quality reviews. The rebuttal and discussion were effective and addressed many concerns from the reviewers, after which most reviewers increase their ratings of this paper. The final rating is 6 from 4 reviewers, and 8 from 1 reviewer. The AC concurs with the positive recommendation from the reviewers and recommends acceptance.
The manuscript proposes an autoencoder architecture incorporating two recent architectural innovations from the GAN literature (progressive growing & feature wise modulation), trained with the adversarial generator encoder paradigm with a novel cyclic loss meant to encourage disentangling, and procedure for enforcing layerwise invariances. The authors demonstrate coarse/fine visual transfer on generative modeling of face images, as well as generative modeling results on several Large Scale Scene Understanding (LSUN) datasets.   Reviewers generally found the results somewhat compelling and the ideas valuable and well motivated, but criticized the presentation clarity, lack of ablation studies, and that the claims made were not sufficiently supported by the empirical evidence. The authors revised, and while it was agreed that clarity was improved, some reviewers were still not satisfied with the level of clarity (the revision appeared at the very end of the discussion period, unfortunately not allowing for any further refinement). Ablation studies were added in the revised manuscript, which were appreciated, but seemed to suggest that the proposed loss function was of mixed utility: while style mixing quantitatively improved, overall sample quality appeared to suffer.  As the reviewers remain unconvinced as to the significance of the contribution and the clarity of its presentation, I recommend rejection at this time, while encouraging the authors to further refine the presentation of their ideas for a future resubmission.
This paper offers an improved attack on 3 D point clouds. Unfortunately the clarity of the contribution is unclear and on balance insufficient for acceptance.
The submission considers a new acquisition function for active learning. The method considers the sensitivity of the prediction for a given datapoint with respect to parameter perturbations. Points with the largest variance under these perturbations are selected for labelling.  The method is simple and the empirical results are reasonable. Some weaknesses are the clarity of writing, and somewhat limited experimental comparisons.  The discussion was useful and helped improve the clarity. Additional experiments also helped improve the paper, although some reviewers still felt the experimental comparisons were lacking, including using entropy as a baseline acquisition function. Despite these improved scores, the overall average score remains below threshold I m afraid.  I feel this is a useful paper, but perhaps needs a little more polishing in the writing and some additional experiments. As such, it just falls short of the acceptance threshold.
This paper proposes a criterion for representation learning, minimum necessary information, which states that for a task defined by some joint probability distribution P(X,Y) and the goal of (for example) predicting Y from X, a learned representation of X, denoted Z, should satisfy the equality I(X;Y)   I(X;Z)   I(Y;Z). The authors then propose an objective function, the conditional entropy bottleneck (CEB), to ensure that a learned representation satisfies the minimum necessary information criterion, and a variational approximation to the conditional entropy bottleneck that can be parameterized using deep networks and optimized with standard methods such as stochastic gradient descent. The authors also relate the conditional entropy bottleneck to the information bottleneck Lagrangian proposed by Tishby, showing that the CEB corresponds to the information bottleneck with β   0.5. An important contribution of this work is that it gives a theoretical justification for selecting a specific value of β rather than testing multiple values. Experiments on Fashion MNIST show that, in comparison to a deterministic classifier and to variational information bottleneck models with β in {0.01, 0.1, 0.5}, the CEB model achieves good accuracy and calibration, is competitive at detecting out of distribution inputs, and is more resistant to white box adversarial attacks. Another experiment demonstrates that a model trained with the CEB criterion is *unable* to memorize a randomly labeled version of Fashion MNIST. There was a strong difference of opinion between the reviewers on this paper. One reviewer (R1) dismissed the work as trivial. The authors rebutted this claim in their response and revision, and R1 failed to participate in the discussion, so the AC strongly discounted this review. The other two reviewers had some concerns about the paper, most of which were addressed by the revision. But, crucially, some concerns still remain. R4 would like more theoretical rigor in the paper, while R2 would like a direct comparison against MINE and CPC. In the end, the AC thinks that this paper needs just a bit more work to address these concerns. The authors are encouraged to revise this work and submit it to another machine learning venue.
After the revision, the reviewers agree on acceptance of this paper.    Let s do it.
This paper is concerned with finding causal relations from temporal processes and extends the Convergent Cross Mapping (CCM) method.  It focuses on finding information of chaotic dynamical systems from short, noisy and sporadic time series, and the idea of using the latent space of neural ODEs to replace the delay embeddings in CCM seems interesting. All reviewers like the idea. Please try to make the paper more self contained and provide some of the justifications suggested by the reviewers.
A multi scale hierarchical variational autoencoder based technique is developed for unsupervised image denoising and artefact removal. The method is shown to achieve state of the art performance on several datasets. Further, the multi scale latent representation leads to an interpretable visualization of the denoising process.  The reviewers unanimously recommend acceptance.
This paper is lacking in terms of clarity and experimentation, and would require a lot of additional work to bring it to the standards of any high quality venue.
The novelty of the proposed work is a very weak factor, the idea has been explored in various forms in previous work.
The majority of reviewers recommend acceptance for this paper, and the average score is in the acceptance rate. Only one reviewer (reviewer 2) recommend rejection, and from the reading the review, the authors answer, and the paper, I think it is possible that the reviewer missed the motivation behind this architecture, which is partly reason for rejection. Unfortunately the reviewer did not answer the authors so I cannot be sure if the reviewer is aware of that. Therefore, I am confident in my recommendation to follow the majority of the reviewers.  The reviewers generally believe this paper is well written. The paper has a good structure and although quite technical, is still easy to follow.  Concerns were raised about the scale of the experiments and motivations for some experimental choices.  I appreciate that the authors have extended the set of evaluations on the Starcraft task, and added ablations studies, which partly address some of these concerns. 
This paper analyzes random auto encoders in the infinite dimension limit with an assumption that the weights are tied in the encoder and decoder. In the limit the paper is able to show the random auto encoder transformation  as doing an approximate inference on data. The paper is able to obtain principled initialization strategies for training deep autoencoders using this analysis, showing the usefulness of their analysis. Even though there are limitations of paper such as studying only random models, and characterizing them only in the limit, all the reviewers agree that the analysis is novel and gives insights on an interesting problem. 
Below is a summary of the pros and cons of the proposed paper:  Pros: * Proposes a novel method to tune program synthesizers to generate correct programs and prune search space, leading to better and more efficient synthesis * Shows small but substantial gains on a standard benchmark  Cons: * Reviewers and commenters cited a few clarity issues, although these have mostly been resolved * Lack of empirical comparison with relevant previous work (e.g. Parisotto et al.) makes it hard to determine their relative merit  Overall, this seems to be a solid, well evaluated contribution and seems to me to warrant a poster presentation.  Also, just a few notes from the area chair to potentially make the final version better:  The proposed method is certainly different from the method of Parisotto et al., but it is attempting to solve the same problem: the lack of consideration of the grammar in neural program synthesis models. The relative merit is stated to be that the proposed method can be used when there is no grammar specification, but the model of Parisotto et al. also learns expansion rules from data, so no explicit grammar specification is necessary (as long as a parser exists, which is presumably necessary to perform the syntax checking that is core to the proposed method). It would have been ideal to see an empirical comparison between the two methods, but this is obviously a lot of work. It would be nice to have the method acknowledged more prominently in the description, perhaps in the introduction, however.  It is nice to see a head nod to Guu et al. s work on semantic parsing (as semantic parsing from natural language is also highly relevant). There is obviously a lot of work on generating structured representations from natural lanugage, and the following two might be particularly relevant given their focus on grammar based formalisms for code synthesis from natural language:  * "A Syntactic Neural Model for General purpose Code Generation" Yin and Neubig ACL 2017. * "Abstract Syntax Networks for Code Generation and Semantic Parsing" Rabinovich et al. ACL 2017 
Understanding global optimality conditions for deep nets even in the restricted case of linear layers is a valuable contribution. Please add clarifications to ways in which the paper goes beyond the results of Kawaguchi 16, which was the main concern expressed by the reviewers.
Strengths:  This paper proposed to use graph based deep learning methods to apply deep learning techniques to images coming from omnidirectional cameras.  Weaknesses:  The projected MNIST dataset looks very localized on the sphere and therefore does not seem to leverage that much of the global connectivity of the graph All reviewers pointed out limitations in the experimental results. There were significant concerns about the relation of the model to the existing literature.  It was pointed out that both the comparison to other methodology, and empirical comparisons were lacking.   The paper received three reject recommendations.  There was some discussion with reviewers, which emphasized open issues in the comparison to and references to existing literature as highlighted by contributed comment from Michael Bronstein.  Work is clearly not mature enough at this point for ICLR, insufficient comparisons / illustrations
Improving the staleness of asynchronous SGD is an important topic. This paper proposed an algorithm to restrict the staleness and provided theoretical analysis. However, the reviewers did not consider the proposed algorithm a significant contribution. The paper still did not solve the staleness problem, and it was lack of discussion or experimental comparison with the state of the art ASGD algorithms. Reviewer 3 also found the explanation of the algorithm hard to follow.
This paper develops a new few shot image classification algorithm by using a metric softmax loss for non episodic training and a linear transformation to modify the model towards few shot training data for task agnostic adaptation.  Reviewers acknowledge that some of the results in the paper are impressive especially on domain sift settings as well as with a fine tuning approach. However, they also raise very detailed and constructive concerns on the 1) lack of novelty, 2) improper claim of contribution, 3) inconsistent evaluation protocol with de facto ones in existing work. Author s rebuttal failed to convince the reviewers in regards to a majority of the critiques.  Hence I recommend rejection.
The work proposed multi view learning framework that combines diversity and consistency objectives for semi supervised learning. While reviewers appreciated that simplicity of the proposed method, they raised concerns on the limited contribution on top of the original Bayesian Co Training work. Although authors provided detailed rebuttals that addressed some of the reviewers  concerns, and one reviewer did raise their score, the other reviewers  scores remained unchanged. Given the work is closely based off the BCT work, I would like to see more detailed analyses on the importance of the changes brought in this work, such as changing the base learners and introduction of diversity objectives as pointed out by the authors.
The paper studies the robustness of binary neural networks (BNNS), showing how quantized models suffer from gradient vanishing. To solve this issue, the authors propose temperature scaling approaches that can overcome this masking, achieving near perfect perfect success in crafting adversarial inputs for these models. The problem is interesting and important. However, the major concerns are that the technical novelty is limited raised by two Reviewers, small improvements for linear loss functions. The most related work is not compared in the experiment.  
This work presents ChIRo, a method that incorporates 3D torsion angles of a molecular conformer to specifically handle chirality. Specifically ChIRo uses trigonometric functions to encode the torsion angles, which are invariant to bond torsion but sensitive to chirality, thus capable of distinguishing between enantiomers.  Overall, although not groundbreaking, we found the idea presented in the paper to be sufficiently novel and its ability to handle chirality to be of significant practical values.
This paper suggests a NAS approach for quantization which focuses on expanding the number of channels in problematic layers, given some uniform quantization level for all the layers. The reviewers were initially all negative, but the authors added more experiments and the scores changed to borderline (6/6/5). I think that though the reviewers appreciated the effort made by the authors and clarifying many of the issues. Yet, I think two concerns still remained. First, the method is demonstrated convincingly in only one large scale case (ImageNet)   on ResNet50 W2A2. The other case (ResNet18) is less impressive since it uses more parameters and has a small gap from EdMIPS. I think using more architectures (and possibly also precision levels) is important to convincingly demonstrate the utility of the method (e.g. EdMIPS used 5 architectures). Second, the novelty of the method in comparison to the previous method was not completely clear. Though the authors added more experiments on CIFAR to compare with previous methods, the significance of the results is not clear (small differences in the figures, and no error bars), and also the explanation of why the results of NCE should be better than TAS.
The work studies the transferability of perturbations/adversarial attacks on DRL agents. As a way to mitigate the cost of generating individual perturbation for each state in each episode, the authors proposed several variants to use same perturbation across different states across different episodes. While reviewers recognize the potential of the direction, they are not comfortable accepting the paper at its current state. The experiment results in its current form does not provide enough support to the claim. In particular, it is unclear how much the shared perturbation changes the original perception in comparison to the individual comparison, and how should the impact number differences be interpreted. Reviewers brought up concerns regarding all the experiments being evaluated on a DDQN agent, and not enough clarities has been provided on the different design choices. If perceptual similarity is not the indicator of environment transferability, do the authors have intuition on what does? 
The paper analyzes the behavior of random search based NAS and provided new insights (e.g., a low ranking correlation among top 20% candidate architectures in the search phase). An extensive set of experiments were also conducted. However, most reviewers found the incremental nature and similarity with previous works to be a concern. I would encourage the authors to better position their work and better explain the novel methodological aspects. 
This work proposes a method for both instance and feature based transfer learning.  The reviewers agree that the approach in current form lacks sufficient technical novelty for publication. The paper would benefit from experiments on larger datasets and with more analysis into the different aspects of the proposed model.  
This paper mostly received negative scores. A few reviewers pointed out that the idea of modeling user preference in the frequency domain seems novel and interesting. However, there are a few concerns around the clarity of the paper, the motivation of the proposed approach, as well as the experimental results being unconvincing (both in terms of execution as well as exploration of the results). The authors did not provide a response. Therefore, I recommend reject. 
Reviewer E8z9 remained with a number of serious concerns, including efficacy of the defense in higher poisoning setting, overclaiming contributions in terms of L0 defenses (which are mostly achieved by the baseline CutMix), as well as novelty and generalizability of the approach. The author responses were unconvincing, and all other reviewers participated in the discussion, conceding that they too were unable to provide compelling arguments against E8z9 s comments. Other reviewers claimed that these drawbacks may be "acceptable" for a first step, but were not willing to defend it very strongly.   We note that E8z9 claims they are a reviewer for a previous version of the paper and these issues were present before. The authors claim that E8z9 s key points had been addressed in this version, but the reviewer maintains that the issues still persist in the latest version. The authors are advised to take their comments into account for further versions of this paper.
The paper studies a defense against adversarial examples that re trains convolutional networks on adversarial examples constructed to attack pre trained networks. Whilst the proposed approach is not very original, the paper does present a solid empirical baseline for these kinds of defenses. In particular, it goes beyond the "toy" experiments that most other studies in this space perform by experimenting on ImageNet. This is important as there is evidence suggesting that defenses against adversarial examples that work well on MNIST/CIFAR do not necessarily transfer well to ImageNet. The importance of the baseline method studied in this paper is underlined by its frequent application in the recent NIPS competition on adversarial examples.
The reviewers agree that this paper suffers from a lack of novelty and does not make sufficient contributions to warrant acceptance.
The paper presents a domain specific language for RNN architecture search, which can be used in combination with learned ranking function or RL based search. While the approach is interesting and novel, the paper would benefit from an improved evaluation, as pointed out by reviewers. For example, the paper currently evaluated coreDSL+ranking for language modelling and extendedDSL+RL for machine translation. The authors should use the same evaluation protocol on all tasks, and also compare with the state of the art MT approaches.
This paper proposes a joint multi agent trajectory prediction framework for multiple agents using a "heatmap" estimation approach employing a hierarchical strategy and sparse image generation for for efficient inference. The method takes a set of predicted trajectories for each agent produces reorderings. The work yields a top result on a competitive leaderboard.   While multiple reviewers were initially concerned about the paper not making a single major contribution, the author response discussion helped to clear up the degree of novelty. Further experiments provided during the review also led to multiple reviewers increasing their score. In the end, all reviewers recommend acceptance of this paper.  As such the AC recommends accepting this paper.
This work introduces a new neural network model that can represent hyperedges of variable size, which is experimentally shown to improve or match the state of the art on several problems.   Both reviewers were in favor of acceptance given the method s strong performance, and had their concerns resolved by the rebuttals and the discussion. I am therefore recommending acceptance. 
The paper proposes an interesting idea of identifying repeated action sequences, or behavioral motifs, in the context of hierarchical reinforcement learning, using sparsity/compression.  While this is a fresh and useful idea, it appears that the paper requires more work, both in terms of presentation/clarity and in terms of stronger empirical results. 
The paper shows that adversarial training can be fooled to have robust test accuracy < 1% with a new type of poisoning attack ADVIN on the CIFAR 10 dataset, even though the robust training accuracy > 90%. This requires 100% poisoning rate, though the claim is that the poisoned data is  semantically similar  to the original data. This is an interesting research direction. Questions were raised about novelty as well as whether these poisoned data could be detected. During the rebuttal phase the authors provide some evidence that with adaptive attacks detection could be evaded (as expected). The authors are encouraged to take all comments into account and update the paper as indicated in the rebuttal for any future revision.
This paper provides a theoretical analysis for the Feedback Alignment (FA) algorithm, an alternative to backpropagation for training deep linear neural networks. The main drawback of the analysis is that it assumes that the initial weight matrices are diagonal, which makes the dynamics of the algorithm reduce to K independent one dimensional dynamic. Most of the reviewers feel that this assumption is too strong. Note that in many existing papers on the implicit bias/regularization of gradient descent (GD) for optimizing deep linear networks, they do not assume the initial weight matrices are diagonal. The authors provide some additional proof in appendix B during the rebuttal to try to relax the assumption on the diagonal initialization, but it is not critical clear if the same results still hold. While this paper studies a very important problem, I suggest the authors take into account the reviewers’ comments and improve the presentation/results. In addition, a comparison with the implicit bias of GD would help better position this work, as one of the reviewers suggested.
This paper revisits the under explored "implicit" variant of Variational Intrinsic Control introduced by Gregor et al. They identify a flaw that biases the original formulation in stochastic environments and propose a fix.  Reviewers agree that there is a [at least a potential, R4] contribution here: "even the description of what implicit VIC is trying to do is a novel contribution of this work", in the words of R2, and "the derivation has theoretical value and is not a simple re derivation of VIC", in R4 s post rebuttal remarks. Several reviewers raised significant concerns around clarity, which were addressed in an updated manuscript, which also provided new visualizations and new experiments which reviewers found compelling. All reviewers agreed that the revised manuscript was considerably improved.  R4 s score stands at the 5, with the other reviewers all standing at 6. R4 s main concerns are around whether the missing term in the mutual information identified by the authors is a problem in practice on non toy tasks (echoing somewhat R3 s concerns re: high dimensional tasks). While this is a valid concern, the function of a conference paper needn t necessarily be to (even attempt) to provide the final word on a matter. Identifying subtle issues such as the one brought forth in this manuscript and re examining old ideas is a valuable service to the community, and this paper will serve as a beginning to a conversation rather than an end. The AC also considers themselves rather familiar with the original VIC paper, and found the results herein somewhat surprising and noteworthy.  I recommend acceptance, but encourage the authors to incorporate remaining feedback in the camera ready.
The paper generalizes several existing results for structured linear transformations in the form of K matrices. This is an excellent paper and all reviewers confirmed that.
This submission builds on recent work by Bengio et al. (2020) to learn disentangled representations of causal mechanisms. The main innovation in this work is that the authors propose to use compare the generalization gap between a model A >B and its causal inverse B >A to determine the true causal mechanism.   Reviewers  are in consensus that this paper in its current form is not ready for publication. Aside from issues with writing, and comparatively weak experimental results, the reviewers noted significant technical problems in the writing and proofs. As noted by R3, Proposition 1 mentions "unbiased estimator of correct causality direction", which is a nonsensical combination of words in absence of a definition of the causality direction as a random variable. Moreover the proof in the appendix only shows that that 0   KL(P(A|B),Q(A|B)) < KL(P(B|A),Q(B|A)) when P(A|B)   Q(A|B) and P(B) !  Q(B). As R3 also notes, the authors also do not clearly distinguish between the true data distributions (which are not known), datasets that are sampled from these distributions, and approximations to these distributions that are learned from finite sample sets.    The metareviewer would suggest that the authors either improve the level of precision of the formalization of their approach, and/or more explicitly position it as an empirical study, in which case more substantive experimental evaluation would strengthen the contributions.
The paper explores how to effectively conduct negative sampling in learning for text retrieval. The paper shows that negative examples sampled locally are not informative, and proposes ANCE, a new learning mechanism that samples hard negative examples globally, using an asynchronously updated ANN index.  Pros • The problem studied is important. • Paper is generally clearly written. • Solid experimental results. • There is theoretical analysis.  Cons • The idea might not be so new. The contribution is mainly from its empirical part.  During rebuttal, the authors have addressed the clarity issues pointed out by the reviewers. They have also added additional experimental results.
This paper proposes a labeling trick for subgraph representation learning with GNNs. The proposed method, GLASS, improves on subgraph level tasks. The topic of subgraph representation learning is relatively new, and this paper makes progress in that community which would be appreciated by other researchers interested in the same problem.   The paper in the original submission state raised some concerns from the reviewers about unclear writing of the motivation and potential applications, technical novelty, and comparisons with existing approaches (even one that are not specifically designed for subgraph representation learning). It is good that the authors conducted additional experiments to show the effect of SSL (that the approach makes improvements without SSL). This and other clarifications from the authors convinced the reviewers to recommend acceptance.
The paper proposes a quantity to monitor learning on an information plane which is related to the information curves considered in the bottleneck analysis but is more reliable and easier to compute.   The main concern with the paper is the lack of interpretation and elaboration of potential uses. A concern is raised that the proposed method abstracts away way too much detail, so that the shapes of the curves are to be expected and contain little useful information (see AnonReviewer2 comments). The authors agree to some of the main issues, as they pointed out in the discussion, although they maintain that the method could still contain useful information.   The reviewers are not very convinced by this paper, with ratings either marginally above the acceptance threshold, marginally below the acceptance threshold, or strong reject.   
While the reviewers have some outstanding issues regarding the organization and clarity of the paper, the overall consensus is that the proposed evaluation methods is a useful improvement over current standards for meta learning.
The paper generated a lot of discussion. After reviewing all of the opinions, and my own reading of the paper, we have concluded that the theoretical innovation is too incremental for ICLR. It is possible that the idea of "residual feedback" could be helpful, but for this to be demonstrated effectively one would need to consider concrete models where the assumptions are verified.
The equivalence rules for the margin are quite interesting, but I have two main concerns with the current paper (1) the theory does not seem to justify why increasing the number of negative examples helps in contrastive learning   in fact Table 9 shows that the bound gets smaller as K increases. (2) The experiments use a large batch size (N) but use a smaller number of negative examples (K), which does not reduce the computation cost by much. The theoretical issue (1) can be a matter of improving the writing to put less emphasis on the theory. I invite the authors to address these issues and resubmit to other ML venues.   Detailed feedback: I believe you are missing a negative sign in your definition of optimal "loss" $\mathcal{L}_{opt}$ in Eq. (3), which resulted in AnonReviewer4 s final comment.
Authors propose an approach to generation of adversarial examples that jointly examine the effects to classification within a local neighborhood, to yield a more robust example. This idea is taken a step further for defense, whereby the classification boundaries within a local neighborhood of a presented example are examined to determine if the data was adversarially generated or not.   Pro:   The idea of examining local neighborhoods around data points appears new and interesting.   Evaluation and investigation is thorough and insightful.   Authors made reasonable attempts to address reviewer concerns.  Con    Generation of adversarial examples an incremental improvement over prior methods  
Previous approaches to model based offline RL require carefully tuning the trade off between model return and uncertainty. The authors propose an approach that produces a diverse pool of policies on the Pareto front of this tradeoff. On the D4RL offline RL benchmark, P3 outperforms competing approaches when the experience is collected with low or medium return policies.  Before the rebuttal, reviewers identified the following primary concerns: * The experimental evaluation of P3 uses many policy evaluations to select the policy, which results in an unfair comparison with existing methods. * P3 underperforms existing methods on some datasets. Why?  Overall, reviewers were satisfied by the response and raised their scores accordingly. The authors responded by including a modification of P3 that uses FQE to select the policy for evaluation, resolving the first concern. The authors explain that P3 underperforms on high return datasets because it splits its updates across the pool of policies. The authors state, "We believe (and in theory it does hold) that P3 can achieve the same performance of UWAC on high quality datasets, if provided with more computational budget." I suggest that the authors conduct at least one experiment to verify this claim.  The proposed idea is interesting and the revisions the authors have made resolved the primary concerns from reviewers, so I recommend acceptance. The reviewer/author discussion has many substantial points that I recommend the authors integrate into the revision.
The reviewers remained concerned about the overall novelty of the paper, finding the contributions somewhat incremental. The authors are encouraged to better substantiate design choices that they make, to improve the overall presentation, and to contrast with the works/line of research brought up by the reviewers.
This paper proposes to reintroduce bipartite attractor networks and update them using ideas from modern deep net architectures.   After some discussions, all three reviewers felt that the paper did not meet the ICLR bar, in part because of an insufficiency of quantitative results, and in part because the extension was considered pretty straightforward and the results unsurprising, and hence it did not meet the novelty bar. I therefore recommend rejection. 
This paper’s contribution is twofold: 1) it proposes a new meta RL method that leverages off policy meta learning by importance weighting, and 2) it demonstrates that current popular meta RL benchmarks don’t necessarily require meta learning, as a simple non meta learning algorithm (TD3) conditioned on a context variable of the trajectory is competitive with SoTA meta learning approaches.   The reviewers all agreed that the approach is interesting and the contributions are significant. I’d like to thank the reviewers for engaging in a spirited discussion about this paper, both with each other and with the authors. There was also a disagreement about the semantics of whether the approach can be classified as “meta learning”, but in my opinion this argument is orthogonal to the practical contributions. After the revisions and rebuttal, reviewers agreed that the paper was improved and increased their ratings as a result, with all recommending accept.  There’s a good chance this work will make an impactful contribution to the field of meta reinforcement learning and therefore I recommend it for an oral presentation. 
The paper studies the benign overfitting phenomenon for linear models with adversarial training. The main issue is that the result is quite expected for experts versed in the benign overfitting papers, and indeed the reviewers pointed out that they could not see much technical novelty. However, even more importantly, the original benign overfitting papers had the advantage of proposing of simpler model (linear!!!) with the same behavior as the complex ones in practice. This is not the case here, as the result diverge from empirical observations on deep networks. The authors argue that it is a valuable finding that the empirical observation is not "universal", but this is a somewhat moot point as linear models are a priori very very different from the setting in which these empirical observations were made. For these reasons I believe the paper does not meet the bar for ICLR (yet it could still be publishable elsewhere).
This paper proposes a method to quantify transference, which is a measure of information transfer across tasks, for multi task learning framework. Specifically, the transference is measured as the change in the loss for a specific task after performing a gradient update for another. The proposed transference measure is used to both understand the optimization dynamics of MTL and improve the MTL performance, either by grouping tasks or combining task gradients based on the transference. The method is validated on multiple datasets and is shown to bring in some performance gains over the base MTL model (PCGrad, UW MTL).   The majority of the reviewers were negative about this paper (4, 4, 5), while one reviewer gave it a positive rating (6). The reviewers in general agreed that the idea of measuring transference as the change in the loss with gradient updates is novel and intuitive. Yet, the reviewers had common concerns on the 1) weak performance improvements, and the 2) high cost of computing the transference. While computing the transference requires additional computations with linear time complexity, which may be problematic with a large number of tasks, the performance gains using it were rather marginal (less than 0.5% over the baselines).  Another common concern from the reviewers was its insufficient experimental validation, as a comparative study against existing works that perform task grouping is missing. Both the authors and reviewers actively participated in the interactive discussion. However, the reviewers found that the two critical limitations persist even after the authors’ feedback, and in a subsequent internal discussion, they reached a consensus that the paper is not yet ready for publication.   Thus, although the proposed method is novel and appears to be promising, it may need more developments to make it both more effective and efficient. Moreover, there should be more in depth analysis of its time efficiency, and other benefits (e.g. interpretability) that could be achieved with the proposed transference measure. Finally, while there exist many works on learning both hard or soft task grouping, the authors do not reference or compare against them. To name a few, [Kang et al. 11] propose how to learn the discrete task groupings, [Kumar and Daume III 12] propose to learn a soft grouping between tasks, [Lee et al. 16] propose to learn soft grouping based on asymmetric knowledge transfer direction across the tasks, and [Lee et al. 18] proposes the extension of [Lee et al. 16] to a deep learning framework. I suggest the authors to discuss and compare against the above mentioned works, and fortify the related work section by searching for more classical works on multi task learning.     [Kang et al. 11] Learning with Whom to Share in Multi task Feature Learning, ICML 2011   [Kumar and Daume III 12] Learning Task Grouping and Overlap in Multi task Learning, ICML 2012   [Lee et al. 16] Asymmetric Multi task Learning based on Task Relatedness and Confidence, ICML 2016     [Lee et al. 18] Deep Asymmetric Multi task Feature Learning, ICML 2018.
The authors propose in this manuscript to use spiking neural networks (SNNs) as an efficient alternative to dilated temporal convolutions. They propose to utilize the membrane time constant of neurons instead of synaptic delays for memory efficiency. Training such networks with BPTT achieves better performance than other SNN based methods and achieve close to SOTA compared to ANN solutions for keyword spotting.  Pros:   The manuscript addresses an interesting problem.   Performance is good  Cons:   Limited evaluations regarding efficiency, although this is a main point of the paper.   The technical novelty is limited.   One reviewer noted that the model is not actually an SNN, due to the use of multiple spikes per time step.   Benchmarking is weak. Little comparison with previous work.   Structure and writing of the paper needs improvement.  The authors did not reply to any of these critical points. In summary, although the idea seems interesting, the manuscript is not ready for publication.
This paper proposes to use context based metric learning, where an attention/Transformer based mechanism is used to incorporate neighborhood information for deep learning based metric learning. This was initially demonstrated on two simpler datasets, although larger ones were added during the rebuttal. On the whole, reviewers appreciated the simplicity and intuition behind the idea, but the consensus among all of the reviewers found several aspects lacking, including: 1) clarity of the descriptions in the paper, 2) novelty compared to existing work, especially that of Set Transformer for clustering, 3) lack of convincing results compared to baselines, or at least analysis/justification for negative results. While the reviewers appreciated the authors  rebuttal and experiments, it did not address many of these concerns. The idea is interesting and seems to hold some promise, so the authors are encouraged to refine these aspects in order to fully explore this idea and submit to a future venue. 
The paper received weak scores: 4,4,5. R2 complained about clarity. R3 s point about the lack of fully connected layers in current SOA deepnets is very valid and the authors response far from convincing. Unfortunately the major revision provided by the authors was not commented on by the reviewers, but many of the major shortcomings of the work still remain. Generally, the paper is below the acceptance threshold, so cannot be accepted.
Metareview:  This paper proposes a transformer based automated program debugger, called DeepDebug. All reviewers agree that the addressed problem is interesting. However, there was a consensus among reviewers regarding concerns in terms of the novelty and the lack of comparisons with other.  In general, all reviewers consistently gave a score that is below the acceptance threshold. This paper is of interest to the ICLR audience, but current form is not ready for acceptance.    Summary Of Reasons To Publish:   Good Result on QuixBug   Synthetic bug generation  Summary Of Suggested Revisions   Comparisons in/with other datasets/tools
The paper proposes a simple approach to quantify uncertainty in "deterministic" neural networks, not unlike the works of SNGP, DDU, and DUE, where one only performs one forward pass rather than in an ensemble or Monte Carlo sample. In particular, they propose a kernel based method on a network s logits to estimate uncertainty, obtaining data and model uncertainty estimates separately using a bound on Bayes risk.  While I agree with the relevance of the problem, there s a shared concern among reviewers across both technical novelty and experimental validation particularly compared to prior work that can be difficult to understand the key distinguishing factor. I recommend the authors use the reviewers  feedback to enhance their preprint should they aim to submit to a later venue.
Dear authors,  I agree with the reviewers that the paper tries to do several things at once and the results are not that convincing. Overall, this work is mostly incremental, which is fine if there is no issue in the execution. Thus, I regret to inform you that this paper will not be accepted to ICLR.
This paper investigate an interesting problem of multi agent RL with self play. We agree with the reviewers that the paper requires more work before it can be presented at a top conference.  We would  encourage the authors to use the reviewers  feedback to improve the paper and resubmit to one of the upcoming conferences. 
The paper proposes an explanation method based on message flows, and shows better performance than the state of the art methods. The authors addressed most of the reviewer s comments but the reviewers are not enthusiastic.  So I give my evaluation (some concerns are shared with reviewers and were not well addressed in the rebuttal)  Pros:   State of the art results on edge scoring.  Concerns:    The main claim is not supported.  The authors say "we argue that message flows are more natural for performing explainability.  To this end, we propose..."  But I see no such argument after the proposed method is introduced.  Also, no advantage of the flow based approach is shown.  The experiments only show edge scoring, which ignores the layer wise edge scoring.  For this task, many existing methods are similar to the proposed approach in the sense that they measure how much information goes through subgraphs.  Although the proposed method shows good performance in edge scoring, this is not necessarily because the proposed method is flow based, and cannot be evidence of the superiority of flow based methods.  Fine details of the algorithm can contribute to the performance.  In the rebuttal, the authors mentioned a virus infection dataset as a situation where the flow based method can do beyond what existing methods can do.  This kind of experiment should be shown in the paper to support the main claim.     Difference between flows and walks is unclear.  The authors imply that this paper is the first paper based on the flows, and reviewers understood so.  The authors say a walk is "similar" to a flow but the difference is not explained.  (the authors only talk about the difference in how to compute the score in Schnake et al.)  Essential difference between walks and flows should be explained.    The reason why the proposed method performs better than the existing methods is not analyzed.  The authors say they "believe" that this is because the proposed method is based on flows, but what readers want to see is evidence.   Presentation should be improved.  Some formulations are unclear, e.g., I have no idea what F_?{t} means.  If this would be the best notation the authors think of, it should be explained with a figure.  Use another character if t is not the layer id.  Notation is not consistent, e.g., edges are denoted by e in Section 2.1 while they are denoted by a later.     Marginal technical contributions.   High complexity.  The proposed method seems not scalable even with the crude Monte Carlo approximation with a small number of samples.  With my concerns above and the reviewer s evaluations, I would not recommend acceptance.
The paper proposes to apply Mixup to Federated Learning (FL) for addressing the challenge of non iid data. The idea is very simple, but seems to work well in empirical evaluation. Some concerns were raised regarding the communication costs and privacy. The authors rebuttal and revised draft provide reasonable answers to these concerns.   For the final version, it is suggested that the authors can address the following issues:  1) Improve the writing   especially the formulation of the proposed method  2) Provide more discussions and experiments on the communication costs. 
The authors consider the interesting and important problem of how to train a robust driving policy without allowing unsafe exploration, an important challenge for real world training scenarios. They suggest that both good and intentionally bad human demonstrations could be used, with the intuition being that humans can readily produce unsafe exploration such as swerving which can then be learnt using both positive and negative regressions. The reviewers all agree that the paper would not appeal to or have relevance for the wider community. The reviewers also agree that the main ideas are not well presented, that some of the claims are confusing, and that the writing is not technical enough. They also question the thoroughness of the empirical validation. 
A new software framework fo Deep RL is introduced. This is a useful work for the community, but it is not a research work. I agree with Reviewer4 that somehow it is not a right venue: other papers need to have technical contributions, SOTA, and here   it is difficult but it is another type of work   accurate technical implementation and commenting. I do not feel right to have as it a paper on ICLR. 
Paper reviewed by three experts who have provided detailed feedback. All three recommend rejection, and this AC sees no reason to overrule their recommendation. 
The authors argue that uniform priors for the high level latent representations improve transferability, which is beneficial in a number of tasks involving transference. The approach is evaluated on deep metric learning, zero shot domain adaptation and few shot meta learning.  Pro:   A simple yet effective method   Signifiant gains in experimental study  Cons:   Close variants of this approach were proposed in previous works, and so the novelty of the current work is limited.   There is no accompanying analysis which may shed new light on the advantages of the approach.
The paper introduces a method to train neural networks based on so called stability regularisation. The method encourages the outputs of functions of Gaussian random variables to be close to discrete and does not require temperature annealing like the Gumbel Softmax. All reviewers agreed that the proposed method was novel and of interest. The authors conducted extensive experiments. They also adequately addressed the concerns raised by the reviewers (e.g., theoretical foundation and computational cost).
This paper considers deep reinforcement learning skill transfer and composition, through an attention model that weighs the contributions of several base policies conditioned on the task and state, and uses this to output an action. The method is evaluated on several Mujoco tasks.  There were two main areas of concern. The first was around issues with using equivalent primitives and training times for comparison methods. The second was around the general motivation of the paper, and also the motivation for using a BiRNN. These issues were resolved in a comprehensive discussion, leaving this as an interesting paper that should be accepted.
This paper proposes a novel and interesting approach called co distillation for distributed training. The main idea is to add a regularizer in order to encourage local models to be consistent with the global objective. Although the idea is a promising alternative to local update SGD, the approach is mostly empirical. The claim that co distillation helps reduce overfitting could be better justified by theoretical analysis, in addition to the experimental results. I hope that the reviewers  constructive comments will help improve the paper for future re submission.
This paper presents work on classification with a background class.  The reviewers appreciated the important, standard problem the paper considers.  However, concerns were raised regarding presentation, empirical evaluation, clarity, novelty, and signficance of the work.  The reviewers considered the authors  response in their subsequent discussions but felt the concerns were not adequately addressed.  Based on this feedback the paper is not yet ready for publication in ICLR.
This paper proposes a self supervised learning algorithm to compute object centric representations for efficient RL in the context of robot manipulation tasks.  The key idea is to learn an object centric representation (using prior work on SCALOR) and use this to intrinsically generate goals for a SAC policy to achieve. The policy is a goal conditioned attention policy. The evaluation metric is a set of tasks to manipulate objects for a visual rearrangement task.   ${\bf Pros}: $ 1. The baselines are reasonable and consist of other unsupervised RL algorithms in recent literature.   2. Object oriented RL is a growing area of interest and this paper proposes a reasonably novel and validated set of ideas in this domain. I believe it will be of significant interest and potentially make an impact on research in robotics and deep reinforcement learning.  3. The goal conditioned attention policy can handle realistic scenarios, namely   multi object manipulation tasks  4. The attention mechanism also provides a reasonable solution to mitigate combinatorial hardness in multi object environments  ${\bf Cons}$:   1. Some of the reviewers felt that the experimental results from pixel inputs could have been pushed further. However, since the setup and algorithm is relatively novel, there are already many moving parts and this paper seems like a step in that direction  2. Experiments with larger set of objects would have been interesting to investigate and report.      
The paper proposed an end to end network architecture for graph matching problems, where first a GNN is applied to compute the initial soft correspondence, and then a message passing network is applied to attempt to resolve structural mismatch. The reviewers agree that the second component (message passing) is novel, and after the rebuttal period, additional experiments were provided by the authors to demonstrate the effectiveness of this. Overall this is an interesting network solution for graph matching, and would be a worthwhile addition to the literature.
Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers  comments into consideration to improve your submission for the camera ready. 
The paper studied defenses against adversarial examples by training a GAN and, at inference time, finding the GAN generated sample that is nearest to the (adversarial) input example. Next, it classifies the generated example rather than the input example. This defense is interesting and novel. The CelebA experiments the authors added in their revision suggest that the defense can be effective on high resolution RGB images.
The paper proposes a novel explanation for the ineffectiveness of active learning (AL), namely, that AL will select unlearnable collective outliers.   Reviewers generally find the finding is interesting, but the paper lacks in depth analyses. There re additional concerns on the experimental setups.
The paper has initially received mixed reviews, with two reviewers being weakly positive and one being negative. Following the author s revision, however, the negative reviewer was satisfied with the changes, and one of the positive reviewers increased the score as well.   In general, the reviewers agree that the paper contains a simple and well executed idea for recovering geometry in unsupervised way with generative modeling from a collection of 2D images, even though the results are a bit underwhelming. The authors are encouraged to expand the related work section in the revision and to follow our suggestion of the reviewers.
An interesting paper, with non trivial results. The reviewers all agree that the paper is above bar (with two of them indicating strong vote for acceptance). The simplicity of the proposed approach (noted by some of the reviewers) is in my view a positive. Overall, a worthy contribution.
The reviewers all agreed that the problem application is interesting, and that there is little new methodology, but disagreed as to how that should translate into a score. The highest rating seemed to heavily weight the importance of the method to biological application, whereas the lowest rating heavily weighted the lack of technical novelty. However, because the ICLR call for papers clearly calls out applications in biology, and all reviewers agreed on its strength in that regard, and it was well written and executed, I would recommend it for acceptance.
The paper analyzes the flow of information in convolutional neural networks with parallel pathways by using logistic regression probes and similarity indices.  Following the analysis, the authors concluded that:   pathways of similar size have similar contributions to learning and have high redundancy   shorter pathways directly improve solution quality to a greater extent than longer pathways   pathways of different lengths also lead to grater variety amid features in the  downstream  layers of the network  The novelty in this type of analysis and its thoroughness was appreciated by the reviewers. The insight about the benefits of pathways of different length is also valuable; although there is a sense in the community that long pathways without skip connections bring diminishing returns, as pointed by reviewer ac9X, there is still some benefit in quantifying it through such an analysis and establishing the correct mix of long/short pathways. [note: This paper is not there yet, but has the potential.]  On the other hand, the experiments were performed on a single network, a network selected such that this instrumentation is possible, so there is the issue of whether these conclusions generalize, as pointed out by reviewer Hchc.  There were other comments, in terms of structure, errors and typos, as pointed out by reviewer PRea.   The authors have not responded to the comments, nor updated their manuscript. In its current form, the paper is not ready for acceptance.
This paper was reviewed by four experts in the field. Based on the reviewers  feedback, the decision is to recommend the paper for acceptance to ICLR 2021. The reviewers did raise some valuable concerns that should be addressed in the final camera ready version of the paper. The authors are encouraged to make the necessary changes.  It is also very important to think about how to extend this framework to the more challenging CLEVERER dataset (http://clevrer.csail.mit.edu/). 
A GNN model is developed for the supervised, real time learning of optimal solutions for an order fulfillment problem. GNNs with fast forward computations are naturally one good choice given the real time nature of the problem.  While the complexity of the problem and formulation were generally appreciated by the referees, there were major concerns about the experimental setup, datasets, technical claims, sample complexity, and suitability for ICLR. Overall, the paper does not seem ready for publication in ICLR, and the authors are encouraged to consider and work on the reviews carefully.
This paper tries to bridge early stopping and distillation.  1) In Section 2, the authors empirically show more distillation effect when early stopping. 2) In Section 3, the authors propose a new provable algorithm for training noisy labels.  In the discussion phase, all reviewers discussed a lot. In particular, a reviewer highlights the importance of Section 3. On the other hand, other reviewers pointed out "what is the role of Section 2", as the abstract/intro tends to emphasize the content of Section 2.  I mostly agree all pros and cons pointed out by reviewers. I agree that the paper proposed an interesting idea for refining noisy labels with theoretical guarantees. However, the major reason for my reject decision is that the current write up is a bit below the borderline to be accepted considering the high standard of ICLR, e.g., many typos (what is the172norm in page 4?) and misleading intro/abstract/organization. In overall, it was also hard for me to read the paper. I do believe that the paper should be much improved if the authors make more significant editorial efforts considering a more broad range of readers.   I have additional suggestions for improving the paper, which I hope are useful.  * Put Section 3 earlier (i.e., put Section 2 later) and revise intro/abstract so that the reader can clearly understand what is the main contribution.  * Section 2.1 is weak to claim more distillation effect when early stopping. More experimental or theoretical study are necessary, e.g., you can control temperature parameter T of knowledge distillation to provide the "early stopping" effect without actual "early stopping" (the choice of T is not mentioned in the draft as it is the important hyper parameter). * More experimental supports for your algorithm should be desirable, e.g., consider more datasets, state of the art baselines, noisy types, and neural architectures (e.g., NLP models). * Softening some sentences for avoiding some potential over claims to some readers. 
The paper made a solid theoretical contribution on the adversarial  generalization bounds of multi layer neural networks.  However, the paper, at the current form, has many issues in the claim that "the product of the norm can explain the generalization gap":  (1). Weight decay. The authors uses the weight norm as the proxy for generalization gap, however, it is unclear to me that "adversarial trained networks have a larger generalization gap" can be explained by the product of weight norms. To carefully verify this, the authors have to at least carefully tune the weight decay, to the largest possible extend so the generalization error is not hurt, and compare the product of the weight norms in this scenario.  Without weight decay, the neural networks might learn a lot of redundancies in the weights (especially with adversarial training)  which makes the product of the norm to be too large.   The authors do perform experiments showing that with weight decay, the generalization gap becomes smaller and the norms become smaller, however, it is totally unclear to me that the weight decay considered in the experiments are actually optimal   It could still be the case that with proper weight decay, the product of the norms in adversarial training is actually smaller comparing to that of the clean training.    Moreover, the authors should also clarify that **the product of the norms, according to the experiments, are simply too large and they can not be used in the theoretical result to get any meaningful generalization bounds**.    (2). The product of the norm in Rademacher complexity  is tight: This  claim only holds for neural networks with 1 neurons per layer. Once there are more than one neurons, there can simply be one neuron that learns f(x) and the other learns  f(x) and they completely cancels each other. So the product of the norm is obviously NOT TIGHT for any neural network with MORE than ONE NEURON per layer. In fact, the gap can be INFINITELY large.    Unfortunately, I like the paper very much and I hope this paper could be published, however, the claims  "the product of the norm can explain the generalization gap" is simply too misleading and ill supported. I encourage the authors to completely remove this claim and submit the paper to COLT.
The reviewers agree that the problem being addressed is interesting, however there are concerns with novelty and with the experimental results. An experiment beyond dealing with class imbalance would help strengthen this paper, as would experiments with other kinds of GANs.
After much discussion, all reviewers agree that this paper should be accepted. Congratulations!!
This paper presents an approach to tackle visual reasoning by combining MONET and transformers. All reviewers agree that there is some performance improvement shown. But there are several concerns including clarity/writing (multiple reviewers point it), experiments (baselines) and most importantly missing insights from experiments (why it works). While some of the concerns have been handled in rebuttal, the paper still falls short on primary concern of insights/why it works (which reviewers argue is critical for a paper on reasoning). AC agrees that the paper is not yet ready for publication.
This paper is generally very strong. I do find myself agreeing with the last reviewer though, that tuning hyperparameters on the test set should not be done, even if others have done it in the past. (I say this having worked on similar problems myself.) I would strongly encourage the authors to re do their experiments with a better tuning regime.
The paper provides a uniform generalization bound for overparameterized neural networks using the notion of maximal information gain. The analysis relies on the eigendecay of the eigenvalues of the NTK, which has recently been the object of a lot of work in the literature, including the work of Bietti and Bach (the proof actually uses one of their key lemma).  The paper originally received a set of reviews with a large disagreement between the reviewers (including two reviewers with a negative opinion and three reviewers being more positive). After the discussion period, two reviewers kept a very negative opinion, while other reviewers slightly lowered their score. Some of the problems raised by the reviewers include the restrictions imposed on the data, a missing proof (which was eventually added by the authors), the discussion of prior work being inadequate (including for instance the differences with more classical generalization bounds), and the novelty of the analysis.  Overall, the paper clearly has some merits but some of the concerns above are too important at this stage to accept the paper. I recommend the authors address the concerns mentioned in the reviews before re submission.
The paper investigates interference in reinforcement learning and introduces a novel measure that can be used in value based methods. Although the reviewers acknowledge that the paper has merits (the topic is relevant and the paper is well written), they feel that the contribution is not sufficiently supported by either a theoretical or empirical analysis. The authors  responses have solved some of the reviewers  concerns, but they agree that this paper is not ready for publication in its current form. I encourage the authors to update their paper following the reviewers  suggestions, in particular by improving the empirical analysis where comparisons with alternative methods (e.g., AVI/API methods that introduce regularization) need to be added.
The paper proposes a variational dequantization method for categorical data, based on flows with learned truncated support. The problem has been studied before, but the paper makes it clear how the proposed method differs from existing ones. The method is empirically evaluated on a large variety of diverse tasks.  The reviews were initially borderline. In general, the reviewers did not identify major quality of technical issues with the paper, and appreciated the clarity of writing. On the other hand, the reviewers were not fully convinced by the motivation or the empirical performance of the proposed method. After discussion with the authors, some concerns were allayed (especially regarding motivation) and all three reviewers decided to recommend weak acceptance.  Seeing as there are no major technical or quality issues with the paper, and the paper is clearly written and well executed, I m leaning towards recommending acceptance, although some doubts remain about the significance of the contribution.
The authors propose TD updates for Truncated Q functions and Shifted Q functions, reflecting short  and long term predictions, respectively. They show that they can be combined to form an estimate of the full return, leading to a Composite Q learning algorithm. They claim to demonstrated improved data efficiency in the tabular setting and on three simulated robot tasks.  All of the reviewers found the ideas in the paper interesting, however, based on the issues raised by Reviewer 3, everyone agreed that substantial revisions to the paper are necessary to properly incorporate the new results. As a result, I am recommending rejection for this submission at this time. I encourage the authors to incorporate the feedback from the reviewers, and believe that after that is done, the paper will be a strong submission. 
The authors proposed a two stage algorithm for exploiting label smoothing and provided some analysis based on how label smoothing may have reduced the variance in the stochastic gradient. While the authors provided substantial experiments to justify their work (with additional ones during the response stage), none of the reviewers was very excited in the end, for obvious reasons perhaps: (a) the two stage algorithm is a straightforward combination of existing practices (first run with label smoothing and then run without label smoothing), without any new, interesting insight from the authors  side; (b) the analysis is a direct consequence of the authors  assumptions. Basically, if label smoothing reduces variance, SGD would converge faster and vice versa, which is nothing surprising or insightful. The key is to understand when and how any particular way to smooth the label would lead to significant reduction of the variance, which the authors did not provide any guidance or insight other than offering some empirical results. Overall, we do not believe this work, in its current form, adds significant value to our understanding of label smoothing.
This paper focuses on adversarial robustness with unlabeled data. The philosophy behind sounds quite interesting to me, namely, utilizing unlabeled data to enforce labeling consistency while reducing adversarial transferability among the networks via diversity  regularizers. This philosophy leads to a novel algorithm design I have never seen, i.e., ARMOURED, an adversarially robust training method based on semi supervised learning.  The clarity and novelty are clearly above the bar of ICLR. While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. Thus, all of us have agreed to accept this paper for publication! Please carefully address all  comments in the final version. 
The paper presents a new method for detection of model extraction attacks. It is based on the intuition that typical model extraction attacks involve samples submitted by users that are harder to classify than "benign" samples submitted by users. By introducing the notion of hardness, a metric is developed for identifying malicious users submitting their samples for the purpose of model extraction. While the proposed method is original, it incurs a substantial overhead. Experimental evaluation of the proposed method also has several deficiencies, in particular, in the assessment of its overhead as well as in modeling of benign users.
This paper proposes a self attention based autoregressive model called Axial Transformers for images and other data organized as high dimensional tensors. The Axial Attention is applied within each axis of the data to accelerate the processing.  Most of the authors claim that main idea behind Axial Attention is widely applicable, which can be used in many core vision tasks, such as detection and classification. However, the revision fails to provide more application for Axial attention.  Overall, the idea behind this paper is interesting but more convincing experimental results are needed. 
The reviewers were split on this paper: the positive review appreciated (a) how adaptive weighing can be viewed as part of energy minimization, (b) the flexibility of the model to work with different model backbones, (c) the demonstration that even in no noise settings the method generates noticeable improvements. However, all reviews saw important shortcomings in the (a) few out of distribution results, (b) limited ablation studies, (c) clarity of the writing, particularly in notation, (d) explanations of experimental results (e.g., why using pseudolabels sometimes deteriorates performance), (e) assumptions behind the proposed method, (f) lack of self training baselines, (g) limited technical novelty. Ultimately, the number and severity of the shortcomings outweigh the positive parts of the paper. If the authors take the reviewer’s recommendations into account the paper will be a much stronger submission.
This paper introduces a novel convolution like operator called "optimal separable convolution" which is based on minimizing number of operations given a fixed receptive field.  Authors provide further empirical results to show the effectiveness of their proposed operator.  Overall, this is a very interesting work. There is a consensus among reviewers that this work is well motivated, novel and principled. However, reviewers have pointed to several issues that makes this a borderline paper and consequently none of the reviewers were willing to argue for the acceptance. After reading the paper, reviewers  comments and authors  response, I would summarize the main areas of improvements as follows:  1  The "optimal separable convolution" is derived theoretically using "volumetric receptive field condition". However, this condition is not discussed and motivated enough in the paper. For example, different parametrization with the same volumetric receptive field could impose very different expressive power or implicit bias. Why is this not important? Adding discussions/experiments to motivate this condition would improve the paper.  2  The derivations in Sections 2.3 and 2.4 are not well presented and are hard to follow. I suggest authors to use the convention of having a formal Theorem statement followed by the proof. This is important since one of the main contributions of the paper is a principled derivation.  3  All reviewers were concerned with the wall clock time. Authors responded that theoretical #FLOPs is more important because wall clock time is hardware dependent. However, authors reported the wall clock time using CPUs. I understand that wall clock time is hardware dependent but that only means algorithms that can have better wall clock time on the current hardware are more likely to be useful because there is no guarantee that the hardware would be adjusted based on one algorithm especially if the promised improvement is not large enough. Therefore, I think reporting Wall clock time on GPUs is important which was not done here.  4  Even though authors mention several operators in Table 1, they only compare against depth separable conv in the experiments. Even based on FLOPs, the current empirical results are not very promising. For example:   a) The gap between o ResNet (the proposed method) and d ResNet is not significant in Fig 3. In particular when #FLOPs is low, d ResNet and o ResNet have similar performance.   b) In Tables 2 and 4, o ResNet shows small improvements but uses more FLOPs. Even if authors can t exactly match #FLOPs, they should make sure that the proposed method uses less FLOPs than others not the other way around.  c) In Table 3, authors only compare to ResNet and d ResNet is removed.  Considering the above issues, I think the paper is marginally below acceptance threshold. Given the novelty of the work, I want to encourage the authors to improve the paper by taking Reviewers  comments into account and resubmit their work.  
This paper introduces a new RL benchmark that is a simplified 2D version of Minecraft   it is designed to support complex behaviors but reduce the training complexity. It is very well written and clear, positioned well with respect to other benchmarks, and is likely to improve the speed of development/testing of some RL algorithms. It is likely to appeal to a subset of the community and drive research in some cases, while others may prefer to stick with full 3D Minecraft. As such, there are some mixed reviews on the paper, with open questions as to whether it would be welcomed by people who work on Minecraft style domains, whether behaviors learned in the simplified 2D environment would generalize to other settings/domains, and the potential for agents to game the environment. The authors are encouraged to take these aspects and perspectives into consideration when revising the paper.
Meta Review for Recurrent Parameter Generators  This work investigates a method for reducing the parameters of a deep CNN by having a recurrent parameter generator (RPG) produce the weights, in effect achieving this compression via parameter sharing across layers (similar to earlier works, such as the 2016 Hypernetworks paper, as discussed in between xUeP and the author during the review period). But unlike previous work, this work conducts extensive empirical experiments on classification and even pose estimate tasks, and proposes an additional method, such as the use of pseudo random seed to perform element wise random sign reflection in the weight sharing. The novelty and experimental results are clearly displayed in this work, and shows a lot of promise, but after much discussion, I currently cannot recommend acceptance for ICLR 2022.  In my assessment, and also looking at reviewers and discussion, I believe this work is a great workshop paper at present, but there are a few items that would make it much stronger. There are outstanding issues in the paper that need to be improved. In particular, during discussions, reviewers noted that the paper has a problem with the design and presentation of the experiments. It somehow shifts the reader’s focus to the compression task (3 of the 4 reviewers raised concerns about the compression performance and questioned the baselines). In their rebuttal, the authors emphasized that their contribution is not limited to compression but is rather more fundamental, and the authors propose an approach for understanding the relationship between the model DoF and the network performance. But if that s the main narrative of the paper, rather than the compression aspects, the authors need to clearly articulate why decoupling the DoF from the underlying architecture is advantageous (and also make the narrative more clear in the writing). While there are novel innovations in the method proposed, the authors also need to explain clearly why their method works well, why the even weight assignment and random sign flipping are so effective?  There is discussion between the authors and reviewers about what constitutes vector quantization, and I believe the author has clarified their position effectively (with regard to cgCS s review), and I believe this will be explained in great clarity in future revisions. But even with that disagreement out of the way, we still believe that this work needs improvement to meet the bar of ICLR 2022. Reviewers, including myself, do acknowledge the novelty and are excited about the method proposed, and we look forward to seeing an updated version of this work published or presented at a future journal or conference. Good luck!
This paper presents an approach to learn state representations of the scene as well as their action conditioned transition model, applying contrastive learning on top of a graph neural network. The reviewers unanimously agree that this paper contains a solid research contribution and the authors  response to the reviews further clarified their concerns.
This paper adopts the recently developed MLP based architectures for image classification to Automatic Speech Recognition with 3 different modifications to handle variable length sequences. The three architectural modifications are: C MLP (w/ depthwise convolution), TS MLP (w/ shift operator), and F MLP (w/ Fourier transform and w/o gating). The approaches are then evaluated on the Librispeech 100h and Tedlium 2 corpora and are compared to baselines from the literature. The proposed approaches are shown to yield better performance than Transformer based models.  As pointed by the reviewers, there are 3 major concerns: clarity: the initial version of the paper needed more improvement in writing, the authors did improve the writing a lot, which led to increased ratings by the reviewers; reproduction: many experimental details were missing in the initial version, but the authors added those in the revision and shared the code novelty: as agreed by all the reviewers that The novelty of the paper is somewhat weak. It is mainly an application of a known technique to a new use case and the modifications are commonly used in ASR.   The decision is mainly due to the limited novelty.
This paper proposes to apply the Koopman operator theory framework for analyzing sequence neural models. The authors considered two particular applications, namely sentiment analysis and ECG (electrocardiogram) classification.  Reviewers generally agree that the results obtained on the two tasks are interesting. However, there are concerns that the paper lacks methodological novelty (concerning the Koopman operator framework, which the authors agreed) and that the paper would be more suited for an applied conference and/or journal.
The paper presents a new GNN+ architecture and provide interesting theoretical observations about the architecture. The paper is quite promising and has several interesting insights. However, most of the reviewers believe that the paper is not ready for publication and can be significantly improved by: a) more formal and precise statements, b) clarifying the key points of the paper, c) more thorough experimental validation of the framework on real world datasets.   
This paper proposes a general purpose continuous relaxation of the output of the sorting operator. This enables end to end training to enable more efficient stochastic optimization over the combinatorially large space of permutations.  In the submitted versions, two of the reviewers had difficulty in understanding the writing. After the rebuttal and the revised version, one of the reviewers is satisfied. I personally went through the paper and found that it could be tricky to read certain parts of the paper. For example, I am personally very familiar with the Placket Luce model but the writing in Section 2.1 does not do a good job in explaining the model (particularly Eq 1 is not very easy to read, same with Eq. 3 for the key identity used in the paper).   I encourage authors to improve writing and make it a bit more intuitive to read.  Overall, this is a good paper and I recommend to accept it. 
This paper proposes a heuristic for removing privacy sensitive attributes and replacing them with sythetically generated ones. The technique is closely related to an existing work and, as pointed out in the reviews, the experimental evaluation is insufficient for properly evaluating the approach.
The premise is an exciting observation: Differential privacy in federated learning might imply being certified against poisoning attacks. While this may be considered not surprising by some, the connection between differential privacy and robustness is interesting to many. The relationship was characterized both theoretically and empirically.  The reviewers discussed the paper extensively with the authors, and while many issues were clarified, issues on correctness still remained: it is unclear if the proposed DP mechanism actually is DP, and subsampling amplification also had issues. Clarity needs to be added in the writing, and the extensive comments by the reviewers hopefully help the authors in that.
The paper combines several recent optimizer tricks to provide empirical evidence that goes against the common belief that adaptive methods result in larger generalization errors. The contribution of this paper is rather small: no new strategies are introduced and no new theory is presented. The paper makes a good workshop paper, but does not meet the bar for publication at ICLR. 
This paper studies the role of positional and relational embedding s for multi task reinforcement learning with transformer based policies, The paper is well motivated, the experiment shows its effectiveness against other competitive methods. In the rebuttal period, the authors solved most of the reviews’ questions such as novelty and ablation studies. There are still some concerns about the generalizability of this approach for other tasks and more experiments are needed.
Unfortunately the paper is confusingly written, and there is only agreement by all reviewers on the rejection of the paper.  Indeed, if all reviewers and the area chair do not interpret the paper well, the authors  best response would be to rewrite the papers rather than disagree with all reviewers.  In the area chair s opinion, the current form the paper does not merit publication.  The authors are advised to address the reviewers  concerns, rework the paper, and submit to a conference again.
This paper tackles an important problem: understanding if different NN solutions are similar or different. In the current form, however, the main motivation for the approach, and what the empirical results tell us, remains unclear. I read the paper after the updates and after reading reviews and author responses, and still had difficulty understanding the goals and outcomes of the experiments (such as what exactly is being reported as test accuracy and what is meant by: "High test accuracy means that assumptions are reasonable."). We highly recommend that the authors revisit the description of the motivation and approach based on comments from reviewers; further explain what is reported as test accuracy in the experiments; and more clearly highlight the insights obtain from the experiments. 
The paper proposes some interesting ideas about decomposing the global symmetries of multi agent MDP to local symmetries using a method called the Homomorphic Networks. The paper is well writing and can be followed easily. However, there are a number of weaknesses of the paper. Below, we list some of the outstanding ones.  (1) Both some of the reviewers and the AC find there are some clarity issues in the paper, for instance, it is hard to see why using keeping relative communication and local transformation would guarantee symmetry (it would be good to show this with a theorem); it is hard to understand the algorithmic structure due to the use of codes instead of diagram in Appendix D, especially when some definitions in the code are not defined in this paper; the AC also finds it is hard to understand how communication is performed during both training and executing. After the rebuttal, the structure of the paper has been improved but we encourage the authors to keep improving the presentation.  (2) Another weakness is the experiment implementations, which the reviewers found were a bit too simple and contrived   the traffic light example may not be symmetric in the real world. It might be good to demonstrate the effectiveness of the setting in a more realistic setting   perhaps show that the method also works with a slight violation of symmetry.  That being said, the contribution of the paper remains significant, and the AC recommends borderline but slightly leaning toward acceptance.
The authors present a method for creating a curriculum for goal conditioned reinforcement learning. In particular, they propose to use reachability traces to define a sequence of sub goals that aid learning. During the review process, the reviewers mentioned the novelty of the proposed approach and the intuitive explanations provided by the authors. However, the reviewers also pointed out that the experiments could be more thorough, errors in the theoretical justification of the method as well as simplicity of the evaluation environments, among others. Some of the reviewers increased their score after the authors  rebuttal but it was not enough to advocate for acceptance of the paper. I encourage the authors to incorporate reviewers  feedback in the next version of the paper.
This clearly written paper has been constructively evaluated by three expert reviewers who provided at least two very detailed and informative summaries. The authors have addressed the inquiries raised by the reviewers in a comprehensive fashion, and at least one reviewer has updated their score as a result of those detailed rebuttals. In spite of some outstanding limitations, including a somewhat limited view of the relation of the proposed approach to existing alternatives, the reviewers are consistent in suggesting that this work is sufficiently mature to be considered for the inclusion in the program of ICLR 2021. I concur with that and recommend accepting this paper.
The authors present a new method for leveraging multiple parallel agents to speed RL in continuous action spaces. By monitoring the best performers, that information can be shared in a soft way to speed policy search. The problem space is interesting and faster learning is important. However, multiple reviewers [R2, R1] had significant concerns with how the work is framed with respect to the wider literature (even after the revisions), and some concerns over the significance of the performance improvements which seem primarily to come from early boosts. There is also additional related work on concurrent RL (Guo and Brunskill 2015; Dimakopoulou, Van Roy 2018 ; Dimakopoulou, Osband, Van Roy 2018) which provides some more formal considerations of the setting the authors consider, which would be good to reference.  
The paper proposes a neural network architecture for video compression. The reviewers point out lack of novelty with respect to recent neural compression works on static images, which the present paper extends by adding a temporal consistency loss. More importantly, reviewers point our severe problems with the metrics used to measure compression quality, which the authors promise to take into account in a future manuscript. 
This is a borderline paper on the well researched theme of Topic models. The strongest point of the paper is that it proposes a new topic modelling framework where both word and topic embeddings live in the same space.  It then appeals to optimal transport theory to do the necessary training using SGD. However, this is not the first paper to examine Topic models and Optimal Transport theory. Several papers[1,2,3] in the recent past have started investigating this line of research.  In the rebuttal phase, the author(s) justify the choice of state of the art methodologies and also discuss the key conceptual difference between  existing literature and the submitted one. The major difference seems to  they approach the problem differently leading to better quality topics(as measured by several metrics) and computational efficiency existing state of the art requires more complicated iterations whereas proposed approach works with SGD.  The manuscript, if accepted, needs to be updated considerably to reflect some of these aspects.      [1] Hongteng Xu, Wenlin Wang, Wei Liu, and Lawrence Carin. Distilled wasserstein learning for wordembedding and topic modeling. In NeurIPS2018.  [2] Viet Huynh, He Zhao, and Dinh Phung. OTLDA: A geometry aware optimal transport approach fortopic modeling. In NeurIPS2020.   [3] He Zhao, D Phung, V Huynh, T Le, and W Buntine. Neural topic model via optimal transport, 2020
The reviewers unanimously agreed the paper did not meet the bar of acceptance for ICLR. They raised questions around the technical correctness of the paper, as well as the experimental setup. The authors did not address any reviewer concerns, or provide any response. Therefore, I recommend rejection.
This paper received the initial scores with large variance. During the intensive discussion (Number of Forum replies is up to 60), the opinions reached the consensus. I have read all the materials of this paper including manuscript, appendix, comments and response. Based on collected information from all reviewers and my personal judgement, I can make the recommendation on this paper, *rejection*. Here are the comments that I summarized, which include my opinion and evidence.  **Research Problem and Motivation**  (1) It seems that the authors aimed to address the question that “are negative examples necessary for deep clustering?” This research problem has been proposed and addressed in BYOL and SimSiam (If I remembered correctly, some reviewer pointed this out). What the authors actually did is to add two more components, positive sampling strategy and prototypical contrastive loss, on the top of BYOL. In my eyes, it is like putting two patches on BYOL, where one of them does not work (I will explain later).   (2) Moreover, the authors failed to clearly illustrate the drawback of BYOL. In the last sentence of the third paragraph in the Introduction part, the authors mentioned that “BYOL only optimize the alignment term, leading to unstable training and suffering from the representation collapse.” This sentence is too general, which lacks strong motivation.   Therefore, the research problem addressed here is an incremental problem over BYOL, rather than brings new insights into the contrastive learning community.  **Philosophy**  Without a clear motivation, it is difficult to catch the philosophy of this paper, i.e., how their proposed components tackle BYOL’s drawbacks. Moreover, the relationship between two components is also unclear.   **Novelty**  I believe Reviewer Na7g has a thorough analysis of the novelty of this paper. I will not go into details here. The difference does not mean novelty.   **Technique**  Positive sampling strategy does not work. If we take a closer look at Table 4, the rows of BYOL and NCC with PS, there is no significant performance gain. The p values of t test on ACC results on CIFAR 10 and CIFAR 20 are 0.92 and 0.32, respectively. Actually, the prototypical contrastive loss is the key element to boost performance over BYOL.  **Misleading Title**  Based on the above point, the title is misleading. Although no negative data sample pairs are used in the training, the contractiveness on the cluster level should also belong to the scope of contrastive learning.  **Experiments**  (1) In the Introduction part, the authors mentioned that SimSiam is in the same non contrastive category with this paper. However, this paper is not included in the comparison.  (2) The competitive methods in Table 2 and 3 are not consistent. The authors even did not report the performance of BYOL on ImageNet 1K.  (3) Positive sampling strategy does not work. See the above Technique point.  (4) The authors only reported the running time on CIFAR 10 and CIFAR 20.  Therefore, the experimental results are not very convincing and solid to me.  **Presentation**  I believe the presentation also needs many efforts to smooth the logic. For example, “Even though Grill et al. (2020); Richemond et al. (2020) have proposed to use some tricks such as SyncBN (Ioffe & Szegedy, 2015) and weight normalization (Qiao et al., 2019) to alleviate this issue, the additional computation cost is significant.” Actually, Grill et al. (2020) is BYOL, where the authors added their components on. The computational cost of the proposed method should be heavier than BYOL.   Based on the above points, this paper suffers from several severe issues, which makes it not self standing.
The paper presents a new approach to learning human behavior by observing a small number of interactions. To this end, it proposes a Bayesian learning framework where a Boltzmann type prior over human policies, based on an available reward function, governs default behavior. The prior is updated by observing actual trajectories taken by (human) agents, in principle. In practice, a full fledged implementation using Gaussian priors and features from a neural architecture is proposed and shown to be effective in practical benchmarks.   The reviewers are all positive about the paper s contributions. One remaining concern is that the effect of the quality of the prior (Boltzmann vs. other type vs. features designed in a different way) on the learning process is not explored to a significant depth. Yet, the results and approach proposed in the paper are valuable to merit acceptance.
The paper considers new notions of adversarial accuracy and risk which are called "genuine" with an aim to fix issues with the existing definitions in the literature. A number of issues in the paper, including lack of motivation and intuition, and poor formalism were identified by the reviewers. The paper also fails to cite some of the previous literature that has identified similar issues. The authors have only responded to some of the questions raised by the reviewers. 
While it appears that the authors have done significant amount of work to investigate this topic, there are concerns that the theorems are not rigorously/precisely presented, and it is unclear how they can guide the design and training of neural network models in practice. The response and revision of the authors do not provide sufficient materials to address these concerns. 
The paper proposes a unification of three popular baseline regularizers in continual learning. The unification is realized through a claim that they all regularize (surprisingly) related objectives.  The key strengths of the paper highlighted by the reviewers were: 1. The established connection is valuable and interesting, even if weaker than suggested originally 2. Good motivation (unifying different regularization methods is useful for the community) 3. Clear writing  The key weakness of the paper is a weak empirical validation of the claim that these three regularizers work *because* they regularize the norm of the gradient (as mentioned in the discussion by R3). Rather, the key claims are correlational. The authors correctly say that (1) the three regularizers all regularize related objects (namely different norms of the gradient) and (2) they reduce forgetting. However, it is not sufficiently well demonstrated that (1)  > (2). Relatedly, given that the paper does not have a very clear theoretical contribution, it would be really helpful to demonstrate utility. It would be useful to extend experiments that apply these insights to developing novel regularizers or improving/simplifying hyperparameter tuning.  Additionally, in the review process, the link was discovered to be weaker than originally suggested. The paper casts the relation in terms of the Fisher Information Matrix, which suggests it is theoretical and sound. After the discussion, it seems that viewing this relationship in terms of the Fisher Information Matrix is somewhat misleading. The three different regularization methods all regularize different norms of the gradient (L1 or L2), which are empirically, and under some assumption theoretically, related. More precisely, EWC regularizes the trace of the *Empirical* Fisher, which is equivalent to the L2 norm of the gradient of the loss function. SI regularizes a term similar to the L1 norm of the gradient. These effects were seen by the reviewers to be somewhat loosely related to the Fisher Information Matrix.  Based on the above, I have to recommend rejection. I would like to thank the Authors for submitting the work for consideration to ICLR. I hope the feedback will be useful for improving the work.
This paper explores the role of excitatory and inhibitory neurons, and how their properties might differ based on simulations.  A few issues were raised during the review period, and I commend the authors for stepping up to address these comments and run additional experiments.  It seems, though, that the reviewer s worries were born out in the results of the additional experiments: "1. The object classification task is not really relevant to elicit the observed behavior and 2. Inhibitory neurons are not essential (at least when training with batch norm)."  I hope the authors can make improvements in light of these observations, and discuss their implications in a future version of this paper. 
Dear authors,  Thank you for submitting your work to ICLR. The original goal of using smaller models to train a bigger one is definitely interesting and has been the topic of a lot of works.  However, the reviewers had two major complaints: the first one is about the clarity of the paper and the second one is about the significance of the tasks on which the algorith is tested. For the latter point, your rebuttal uses arguments which are little known in the ML community and so should be expanded in a future submission.
The paper proposes a method for learning to optimize (L2O) by distilling a numerical L2O optimization rule into a simple mathematical rule, mathematical equation, using special purpose student learning algorithm.  The motivation for using a symbolic distillation is to provide interpretability and scalability of the trained optimizers.   Pros    The paper addresses an important problem (better understanding learned optimizers).    The experiments give good evidence that learned black box optimizers can be mapped to mathematical rules.  Cons    The symbolic regression student learning algorithm, and many details of the experimentation, remain hard to understand.  Overall, after discussion, the paper was viewed as a solid contribution by the reviews, with some slight disagreement about whether the clarity was sufficient after revisions.  However, I believe that the main points of the paper and the general approach are quite clear, and that the details of the experiments and student learner are sufficiently well explained for other researchers to build on, at least given the appendix and the supplementary material.
This paper proposes a Bayesian alternative to dropout for deep networks by extending the EM based variable selection method with SG MCMC for sampling weights and stochastic approximation for tuning hyper parameters. The method is well presented with a clear motivation. The combination of SMVS, SG MCMC, and SA as a mixed optimization sampling approach is technically sound.  The main concern raised by the readers is the limited originality. SG MCMC has been studied extensively for Bayesian deep networks and applying the spike and slab prior as an alternative to dropout is a straightforward idea. The main contribution of the paper appears to be extending EMVS to deep net with commonly used sampling techniques for Bayesian networks.  Another concern is the lack of experimental justification for the advantage of the proposed method. While the authors promise to include more experiment results in the camera ready version, it requires a considerable amount of effort and the decision unfortunately has to be made based on the current revision.
Overall this paper seems to make an interesting contribution to the problem of subtask discovery, but unfortunately this only works in a tabular setting, which is quite limiting.
 pros:   interesting application of graph networks for relational inference in MARL, allowing interpretability and, as the results show, increasing performance   better learning curves in several games   somewhat better forward prediction than baselines  cons:   perhaps some lingering confusion about the amount of improvement over the LSTM+MLP baseline  Many of the reviewer s other issues have been addressed in revision and I recommend acceptance.
Positives:  The paper proposes an interesting idea: to study the effect on vulnerability to adversarial attacks of training for invariance with respect to rotations. Experiments on MNIST, FashionMNIST, and CIFAR10. An interesting hypothesis partially borne out in experiments.  Negatives:  no accept recommendation from any reviewer insufficient empirical results not a clear enough message very limited theoretical contribution  Although additional experimental results on FashionMNIST and CIFAR10 were added to the initial very limited results on MNIST, the main claim of the paper seems to be somewhat weakened.  The effect of increased vulnerability to adversarial attacks as invariance is increased is less pronounced on the additional datasets.  This calls into question how relevant this effect is on more realistic data than the toy problems considered here.  The size of the network is not varied in the experiments.  If increased invariance results in poorer performance with respect to attacks, one possible explanation is that the invariance taxes the capacity of the network architecture.  Varying architecture depth could partially answer whether this is relevant.  Given the lack of theoretical contribution, more insights along these lines would potentially strengthen the work.  The title uses the term "equivariance," which strictly speaking is when the inputs and outputs of a function vary equally, e.g. an image and its segmentation are equivariant under rotations, but classification tasks should probably be called "invariant."  The reviewers were unanimous in not recommending the paper for acceptance.  The key concerns remain after the author response. 
The paper received low ratings and the reviewers pointed out a number of issues. The authors  short response failed to address these concerns. 
Some reviewers expressed concerns on soundness of the theory in the paper. Specifically, theorem 3 does not seem to be correct.  There are other concerns such as the significance of the theoretical contributions, little empirical value and existence of much stronger results. Unfortunately the authors did not provide responses to the concerns raised by the reviewers.   
The paper proposed the use of a shallow layers with large receptive fields for feature extraction to be used in stereo matching tasks. It showed on the KITTI2015 dataset this method leads to large model size reducetion while maintaining a comparable performance.  The main conern on this paper is the lack of technical contributions: * The task of stereo matching is very specialized one, simply presenting the model size reduction and performance is not interesting to general readers. Adding more analysis that help understanding why the proposed method helps in this particular task and for what kind of tasks a shallow feature instead a deeper one is perferred. In that way, the paper would be addressing much wider audiences.  * The discussions on related work is not thorough enough, lacking of analysis of pros and cons between different methods.
The submission develops a rotationally equivariant scattering transform on the sphere.  Many developments in deep learning make use of spherical representations, and the development of a rotationally equivariant scattering transform is an important if not unexpected development.  The reviews are split with half of the reviewers believing it to be slightly above the threshold for acceptance, and half believe it to be slightly below the threshold for acceptance.  In the papers favor, it solves an important case of the scattering transform framework, which has been demonstrated to be important in diverse machine learning applications such as learning with small data sets, differentially private learning, and network initialization.  As such, continued fundamental development in this area is valuable, especially in the context of representation learning, the focus of ICLR.
The paper proposes an energy consumption attack to neural ODE models. There are two complains from the reviewers:    Although this is a new application to energy consumption attack, most of the attack techniques are simple extensions to the previous attack papers, so the novelty is questioned by some of the reviewers.    The paper is poorly written.  We therefore decide to reject the paper and encourage the authors to address the concerns in their next submission. Reviewers also think a careful discussion about the defense or detection mechanism against the proposed attack will be a good thing to add.
The authors propose a hybrid model free/model based policy gradient method that attempts to reduce sample complexity without degrading asymptotic performance. They evaluate their approach on a collection of benchmark tests.  The reviewers raised concerns about limited novelty of the proposed approach and flaws in the evaluation. The authors need to compare to more baselines and ensure that the baseline algorithms are performing as previously reported. Even then, the reported improvements were small.  Given the issues raised by the reviewers, this paper is not ready for publication at ICLR.
The work investigates the decision boundary of neural networks by quantifying in various ways the shape and curvature of the error set local to correctly classified inputs, dubbed the "adversarial subspace". First, a method is introduced which seeks to find the largest set of orthogonal directions starting at in input x which will all intersect the error set local to an image. This is motivated as a certain geometric measure of the error set, large sprawling error sets may have many orthogonal directions which intersect it local to the given input while small narrow error sets may have relatively few. Using this geometric measure, the authors compare the shape of adversarial subspaces of various image models both with and without adversarial training. After the rebuttal period, reviewers all felt that the work was borderline, with no one strongly advocating for the work. As noted by some reviewers, while some experiments may be interesting it was unclear what new insights the work contributes. For example, the authors argue that the change in the geometry of the error set explains why adversarial training works. It is unclear how this is an explanation more than it is simply an observation that the error set geometry has changed. An analogy would be trying to explain why Resnet 50 performs better than AlexNet by showing that it has higher test accuracy this only shows that it is better, but doesn t explain why.  During the discussion period the AC raised additional concerns regarding a sanity check that the author s main algorithm should pass. In particular, consider an error set x_1 >  K(x_2^2 + ... + x_n^2) + C, parameterized by constants K and C > 0. For all choices of K and C and starting point x   (0, ..., 0), the authors main algorithm will always return 1 as the dimensionality of this error set. It will find the vector (1, 0, ..., 0) and then terminate. However, this is problematic because we can choose K and C to make this error set either very narrow (e.g. K C 100) or very wide (K   .000001, C   .00001) the proposed algorithm will be unable to distinguish between this two extremes. Given this, it seems that greedily selecting the set of orthogonal directions starting at x can be very suboptimal if the intent is to find a maximum sized set of orthogonal directions.   To conclude, the work would be substantially improved if it addresses two major weaknesses. First, there needs to be a clearer motivation for studying this notion of geometry of the error set, what new insights can the authors provide other than adversarial training changes the shape of the error set? Second, the method doesn t seem to be principled given it is unable to distinguish between the two extreme cases discussed above.
This papers proposed an interesting idea for distributed decentralized training with quantized communication. The method is interesting and elegant. However, it is incremental, does not support arbitrary communication compression, and does not have a convincing explanation why modulo operation makes the algorithm better. The experiments are not convincing. Comparison is shown only for the beginning of the optimization where the algorithm does not achieve state of the art accuracy. Moreover, the modular hyperparameter is not easy to choose and seems cannot help achieve consensus.
The manuscript proposes benchmarks for studying generalization in reinforcement learning, primarily through the alteration of the environment parameters of standard tasks such as Mountain Car and Half Cheetah. In contrast with methodological innovations where a numerical argument can often be made for the new method s performance on well understood tasks, a paper introducing a new benchmark must be held to a high standard in terms of the usefulness of the benchmark in studying the phenomenon under consideration.  Reviewers commended the quality of writing and considered the experiments given the set of tasks to be thorough, but there were serious concerns from several reviewers regarding how well motivated this benchmark is and restrictions viewed as artificial (no training at test time), concerns which the updated manuscript has failed to address. I therefore recommend rejection at this stage, and urge the authors to carefully consider the desiderata for a generalization benchmark and why their current proposed set of tasks satisfies (or doesn t satisfy) those desiderata.
This paper analyzes the effects of the weight decay hyperparameter, and based on this analysis, proposes methods to schedule the weight decay. Overall, while I m glad that more work is being done on understanding the effects of weight decay, I don t think this submission is of sufficient quality for ICLR.  Theorem 1 is simply re expressing the well known fact that if the regularization version of weight decay is used, then (simply because it s based on a single objective function) the stationary points are invariant to the choice of learning rate. This may not be apparent due to the misused terminology: "invariant" is referred to as "stable", but "stable stationary point" has a technical meaning very different from the one used here.  Corollary 2 essentially shows that the optimum of the regularized loss is different from the optimum of the unregularized loss. The authors conclude from this that the optimal value of lambda is 0 from the perspective of test error, which is unwarranted.  Overall, the paper centers around the interaction between learning rates and the weight decay parameter. However, as various reviewers point out, this interaction has been analyzed in detail for networks with normalization layers, and normalization completely changes the nature of the interaction. So any analysis would either need to take this into account or limit the scope to networks without normalization.  I encourage the authors to take the reviewers  feedback into account and improve the paper for the next submission cycle.
The paper proposes a new method to combine global and local image features, targeted at image retrieval applications. The main idea is a model branch where both spatial and channel attention are used. The local feature branch undergoes supervision directly and this branch’s output is concatenated to the global feature branch’s output in order to eventually produce the final image embedding.   The reviewers appreciated the care in the evaluation (ablative analysis) and the promise of the approach compared to existing baselines. The reviewers also expressed concerns about several claims, for instance that the proposed approach is able to learn homography transformations, the quality of the exposition, and missing baselines. The reviewers also pointed out that several parts of the paper were hard to follow and important details were missing.   The authors submitted responses to the reviewers  comments. After reading the response, updating the reviews, and discussion, the reviewers considered that ‘the concatenation of local features with global ones works does not mean at all that some geometric transformation is learned’ and the justification provided for omitting baselines (suggested by the reviewers) were unconvincing. The feedback provided was already fruitful, yet major issues still remain.  We encourage the paper to pursue their approach further taking into account the reviewers  comments, encouragements, and suggestions. The detailed feedback lays out a clear path to generate a stronger submission to a future venue.  Reject.
This paper presents a novel approach for integrating time into deep neural network models based on the Gaussian process limit view of a neural network model. Specifically, the approach augments an a temporal neural network designed to process a single time point with a temporal kernel that relates data points across time. The composition of the a temporal neural network kernel with with the temporal kernel is accomplished efficiently using a random features representation of the temporal kernel. The authors propose to represent the temporal kernel via its spectral decomposition, which makes the approach quite flexible. Learning leverages re parameterization. While random features have been used to approximate temporal kernels in prior work [1], the approach in this paper is significantly more general in that it can be composed with any a temporal deep architecture and the authors show results for RNNs, CNNs, and attention based models. The predictive performance of the approach also appears to be consistently better than baselines and it works particularly well on the challenging case of irregularly sampled data.  In terms of weaknesses, the reviewers had a number of questions about the paper. The authors updated the paper to include some more recent models including ODE RNNs. This material is currently presented in the appendices and needs to be moved into the main paper. Several of the reviewers also had technical questions questions that are in fact addressed in the manuscript; however, the authors are relying heavily on the appendices to present many important details and the paper is currently over 30 pages long. The frequent references to the appendix for additional details makes the paper a challenging read. The authors have already done some work to address clarity by adding a new figure, but should prioritize moving additional key details into the main body of the paper to improve readability.   [1] http://auai.org/uai2015/proceedings/papers/41.pdf
All reviewers rate the paper as below threshold. While the authors responded to an earlier request for clarification, there is no rebuttal to the actual reviews. Thus, there is no basis by which the paper can be accepted.
The paper presents an attempt to learn interaction based representations by taking advantage of body part movements and gaze attention. Video representations are learned by benefiting from additional supervisory signals, which are not the ones commonly used, making the paper more interesting.  R3 expresses a concern that the supervisory signal does not come "for free" and that the paper is misleading. The ACs do agree with R3 that the paper benefits from additional signals and is not a pure self supervised learning paper, strictly speaking. The authors also agreed to this in their response to the R3’s comment. R1 also mentioned (after the rebuttal phase) that the proposed approach is not a practical self supervised learning solution and that it does not perform as effectively as conventional self supervised learning methods like InfoNCE on Moco.  Simultaneously, the AC and the majority of the reviewers believe that the paper itself has a value as a multi modal learning paper. We strongly suggest the authors revise the paper to remove the  self supervision  claim. As mentioned above, the paper is not a self supervised learning paper and the authors are asked to correct the details of the paper to reflect this. We also recommend adding analysis on each body signal qualitatively in the final manuscript, as suggested by R4.  It will be great if the authors can consider this as a "conditional accept". In particular, the  self supervision  claim in the current version of the paper is misleading and this must be corrected in the final version. Note that this was also pointed out by the Program Chairs.
This paper investigated the effect of network width on learned features using activation atlases. From the current view of deep learning, the novelty of the paper is limited.  As all reviews rejected the paper and the authors gave up rebuttal, I choose to reject the paper. 
This paper proposes to explore nonnormal matrix initialization in RNNs.  Two reviewers recommended acceptance and one recommended rejection.  The reviewers recommending acceptance highlighted the utility of the approach, its potential to inspire future work, and the clarity and quality of writing and accompanying experiments.  One reviewer recommending weak acceptance expressed appreciation of the quality of the rebuttal and that their concerns were largely addressed.  The reviewer recommending rejection was primarily concerned with the novelty of the method.  Their review suggested the inclusion of an additional citation, which was included in a revised version for the rebuttal but not with a direct comparison of results.  On the balance, the paper has a relatively high degree of support from the reviewers, and presents an interesting and potentially useful initialization in a clear and well motivated way.
The authors provide an analysis of a cross lingual data augmentation technique which they call XLDA. This consists of replacing a segment of an input text with its translation in another language. They show that when fine tuning, it is more beneficial to train on the cross lingual hypotheses than on the in language pairs, especially for low resource languages such as Greek, Turkish and Urdu. The paper explores an interesting idea however they lack comparison with other techniques such as backtranslation and XLM models, and would benefit from a wider range of tasks. I feel like this paper is more suitable for an NLP focussed venue. 
Reviewers found the construction is very clever and the empirical results are interesting. However, a more thorough theoretical explanation is needed for acceptance. 
There was a significant amount of discussion on this paper, both from the reviewers and from unsolicited feedback.  This is a good sign as it demonstrates interest in the work.  Improving exploration in Deep Q learning through Thompson sampling using uncertainty from the model seems sensible and the empirical results on Atari seem quite impressive.  However, the reviewers and others argued that there were technical flaws in the work, particularly in the proofs.  Also, reviewers noted that clarity of the paper was a significant issue, even more so than a previous submission.    One reviewer noted that the authors had significantly improved the paper throughout the discussion phase.  However, ultimately all reviewers agreed that the paper was not quite ready for acceptance.  It seems that the paper could still use some significant editing and careful exposition and justification of the technical content.  Note, one of the reviews was disregarded due to incorrectness and a fourth reviewer was brought in.
The paper proposes the use of topological similarity between conditional submanifolds for a given latent dimension as a metric for measuring disentanglement in generative models. To estimate the topological similarity between conditional submanifolds, the authors build upon an earlier work of Relative Living Times (RLT).   R5 and R4 had concerns on the paper, particularly about the lack of enough novelty in the actual technique (R5) and about the lack of convincing experiments (R4). One of the concerns raised by R4 was around the discrepancies between MIG and FactorVAE. However as noted by other reviewers (R2 and R5), these discrepancies between different popular metrics are well acknowledged in the literature and authors have responded to this point. R2 and R5 also appreciate that avoiding the rotation issue faced by most of these disentanglement metrics is one of the strengths of the proposed metric.  While I tend to agree with R5 that the actual technique is inspired from the earlier work on "Geometry Score", I also think the application to measuring disentanglement in generative models is a novel contribution in itself, especially because current metrics have issues as pointed out by other reviewers   the paper provides a fresh conditional sub manifold perspective on disentanglement and a theoretically sound metric for measuring disentanglement.   Considering this novel perspective and a resulting theoretically sound metric for measuring disentanglement that addresses some of the issues with current metrics, I recommend accepting the paper.  
The authors of this work introduced new metrics for node embedding that can measure the evolution of the embeddings, and compare them with existing graph embedding approaches, and experimented on real datasets.  All reviewers agreed that the work addresses interesting problem and that the proposed measures are nove, but there are too many flaws in the initial version of the paper, and despite the thorough responses of the authors, it is believed that there are still too many open questions for this paper to be accepted this year ICLR.
During the discussion phase, although the reviewers acknowledge the effectiveness of the proposed approach, they raised the concern about the novelty of the paper.  In my opinion, I also agree that the novelty is not well justified in this paper. In the related work section, although the authors put an effort to review the existing studies of subspace learning and feature selection, their relationship (similarity and/or difference) to the proposed method is not discussed. Since the idea of using subspace learning and feature selection in clustering is standard, the novelty of this work should be introduction of the integration step into neural networks, which is not significant enough in its current state. The paper becomes more significant if, for example, theoretically discuss the unique characteristics of the integration into NNs which does not appear in the usual setting.   In addition, the motivation of face clustering is not convincing. I recommend either (1) use and discuss the domain specific property of the problem of face clustering in the proposed method, or (2) construct a general clustering method. Since the authors present additional experiments in the author response, I guess (2) fits. Then, however, the paper should be re organized.  The readability can be improved. For example, Algorithm 1 receives training data {X, A}, but I cannot find the definition of A. Also, please italicize mathematical symbols.  Overall, the paper is still not ready for publication, I will therefore reject the paper. 
The paper is aimed at providing an explaining the perceived lack of generalization results for Adam as compared to SGD. To this end the paper decouples the effect of adaptive per parameter learning rate and the momentum aspect of Adam. The paper shows that the while adaptive rates help escape saddle points faster   they are worse when consider the flatness of minima being selected. Further momentum has no effect on the flatness of minima but again leads to better optimization by providing a drift leading to saddle point evasion. They also provide a new algorithm Adai (based on inertia) targeted at better generalization of adaptive methods.   The paper definitely provides an interesting perspective and the approach to decouple the effect of momentum and adaptive LR and study their efficacy in escaping saddle points and flatness of minima seems a very useful perspective. The primary reason for my recommendation is the presentation of the paper in terms of the rigor its assumptions to establish the results. These aspects have been highlighted by the reviewers in detail. I suggest the authors to carefully revisit the paper and improve the presentation of the assumptions, adding rigor to the presentation as well as adding justifications where appropriate especially in light of non standardness of these assumptions in optimization literature.
This paper introduces a consistency loss for instance discrimination by adding a term to maximize the squared dot product between two views of the same image. The impact of the proposed approach is evaluated on a variety of settings with mixed improvements. While reviewers generally found the proposed method to be interesting, there were concerns regarding the novelty of the approach, the size of the performance improvement, and the choice to focus on instance discrimination vs. more recent approaches based on contrastive instance discrimination.   While I do not share the reviewer concerns regarding novelty, I am sympathetic to the concerns regarding the size of the improvement and the focus on instance discrimination. As such, I recommend that the paper be rejected in its current form. I would encourage the authors to apply their analysis and method to more recent contrastive instance discrimination approaches such as SimCLR and SWaV as well as non explicitly contrastive, but high performing methods like BYOL. I would also encourage the authors to provide quantitative empirical analyses demonstrating the impact of the consistency term on large models rather than just toy models to demonstrate the impact of the consistency term in representational space. 
This work provides a theoretical analysis of Prioritized Experience Replay (PER ) in a supervised learning setting, points out limitations of PER and proposes a model based approach to address these shortcomings for continuous control problems.    Strengths:   The overall problem was motivated well Reviewers agree that this proposed algorithm has promise Overall the paper is well written a diverse set of experiments is provided  Weaknesses:   reviewers point out some clarity issues The theoretical analysis is performed in a supervised learning setting, and it is unclear how the resulting analysis transfers to the RL setting There are some concerns (theoretical/technical) wrt to the proposed algorithm.  The analysis of the experiments is lacking in depth. For instance, no analysis of why the proposed algorithm outperforms very related baselines. Furthermore, it s unclear why for the autonomous driving experiment the algorithms achieve the same return, but the proposed method leads to less crashes.   Rebuttal:   The authors have addressed many of the clarity issues. However, I agree with the reviewers theoretical concerns and deeper analysis requests were not addressed in a significant manner.   Summary:   Overall this manuscript investigates an important problem and provides a promising algorithm. However, some theoretical/technical concerns remain and a deeper analysis of results is required. Hence my recommendation is that in it s current form the manuscript is not quite ready yet for publication.
This paper introduces an alternative to Langevin sampling and also the idea of adversarial score sampling.  The reviewers are generally supportive of the paper.  Pros:   The idea behind improving Langevin sampling is theoretically justified and leads to a simple algorithm.    The idea behind adversarial score matching is also shown to be effective    Improvement over baseline  Cons:    Two ideas packed  into one paper, which is reflected by the title as well.     From the narrative it could be thought that using EDS on the last step of CAS is the contribution of the paper. 
+ clearly written and thorough empirical comparison of several metrics/divergences for evaluating GANs, prominently parametric critic based divergences.    little technical novelty with respect to prior work. As noted by reviewers and an anonymous commentator:  using an Independent critic for evaluation has been proposed and used in practice before.  +  the contribution of the work thus lies primarily in its well done and extensive empirical comparisons of multiple metrics and models    
Three out of four reviewers are positive about the paper after the author response and during the discussion.  Strengths include * The proposed method for parameter reduction in transformers allows end 2 end learning cross modal representations especially on long videos, which has not been possible before * Good performance on audio and video understanding * Extensive set of ablations  Concerns include a somewhat incremental nature of the paper and the still large computational resources to run the experiments. I think, both, the ideas and results are interesting to the community and recommend accept.
The paper introduces a distributed algorithm for training deep nets in clusters with high latency (i.e. very remote) nodes. While the motivation and clarity are the strengths of the paper, the reviewers have some concerns regarding novelty and insufficient theoretical analysis. 
In this paper the authors highlight the role of time in adversarial training and study various speed distortion trade offs. They introduce an attack called boundary projection BP which relies on utilizing the classification boundary. The reviewers agree that searching on the class boundary manifold, is interesting and promising but raise important concerns about evaluations on state of the art data sets. Some of the reviewers also express concern about the quality of presentation and lack of detail. While the authors have addressed some of these issues in the response, the reviewers continue to have some concerns. Overall I agree with the assessment of the reviewers and do not recommend acceptance at this time.
The focus of the paper is stochastic backpropagation for both continuous and discrete random variables. By using standard results from Fourier analysis the authors rewrite the corresponding gradients in an infinite weighted sum form ((3) and (9)), extending the results of (Rezende et al. 2014) and (Fellows. et al., 2018). The efficiency of the approach is illustrated in 2 toy examples.  As summarized by the reviewers, the problem tackled is interesting. However, they also pointed out that the novelty of the approach is quite limited and its practical usefulness is not clear (it should by demonstrated against state of the art baselines, on realistic benchmarks).
A somewhat new approach to growing sparse networks.  Experimental validation is good, focussing on ImageNet and CIFAR 10, plus experiments on language modelling.  Though efficient in computation and storage size, the approach does not have a theoretical foundation.  That does not agree with the intended scope of ICLR.  I strongly suggest the authors submit elsewhere.
This work studies the question of increasing the expressive power of GNNs by adding positional encodings while preserving equivariance and stability to graph perturbations.  Reviewers were generally positive about this work, highlighting its judicious problem setup, identifying the right notion of stability and how it should drive the design of positional encodings. Despite some concerns about the discrepancy between the theoretical results and the empirical evaluation, the consensus was ultimately that this work is an interesting contribution, and therefore the AC recommends acceptance.
This paper proposes a parametrisation of Euclidean distance matrices amenable to be used within a differentiable generative model. The resulting model is used in a WGAN architecture and demonstrated empirically in the generation of molecular structures.   Reviewers were positive about the motivation from a specific application area (generation of molecular structures). However, they raised some concerns about the actual significance of the approach. The AC shares these concerns; the methodology essentially amounts to constraining the output of a neural network to be symmetric and positive semidefinite, which is in turn equivalent to producing a non negative diagonal matrix (corresponding to the eigenvalues). As a result, the AC recommends rejection, and encourages the authors to include simple baselines in the next iteration. 
The paper proposes a framework, named Disentaglement via Contrast (DisCo), to learn disentangled representations via contrastive learning on well pretrained generative models. The method aims at simultaneously discovering semantically meaningful directions in pretrained generative models and training and encoder to extract them. The method uses contrastive learning where random samples perturbed along the discovered directions are regularised to be similar. The method is versatile and can be applied to various pretrained non disentangled generative models including GAN, VAE, and Flow. Extensive experimental evaluation shows the benefits of the approach.  The authors provided a strong rebuttal addressing many of the concerns raised by the reviewers, including running new experiments (such as adding the JEMMIG metric to measure disentanglement as requested by Reviewer sBQs). This led to all reviewers recommending to accept the work.  The paper provides an exhaustive empirical evaluation testing several models and results are convincing. This was highlighted by all reviewers.  While the high level description of the method is clear, in practice the method is quite sophisticated requiring many heuristics (e.g. entropy based domination loss or flipping hard negatives). This requires tuning several hyperparameters and complicates the message. This is mitigated by an ablation study presented by the authors highlighting the importance of each component. This was highlighted by Reviewer j95X and the AC agrees. The paper does provide implementation details, and reproducibility is not a concern.  Related to this point, Reviewer Go6R points out that the paper falls short in providing clear explanations on why the method is able to find meaningful semantic directions, and on where do the gains of the proposed model come from. While the paper could improve in this direction, the proposed empirical validation is convincing.  Overall the paper presents an interesting method that performs well in practice. All reviewers recommend accepting the work. The AC agrees with this recommendation.
This paper proposes permutation invariant loss functions which depend on the distance of sets. This has an interesting interpretation as the roots of a polynomial, and potentially leads to a more efficient method.  It is not clear, however, whether the method works well in practice for multiple reasons: (i) the experiments are performed in a limited setting, and the rebuttal specifically declined to consider more realistic datasets, (ii) there is an open question about the stability of the resulting gradients, which has been pointed out both in the paper and the reviews.   There was initially a majority vote for rejection. After author response, the only reviewer recommending acceptance wrote "As the other reviews (and my original review) say, the experimental results are not totally convincing. So I would not champion the paper in its present form."
This paper computes channel attention by considering feature maps across different layers, and named it the previous knowledge channel attention module (PKCAM). The reviewers find the proposed idea too straightforward and naive. Lack of technical contribution is one of the major criticisms. There are also correctness concerns with the submission. The authors have not provided any rebuttal.  We recommend rejecting the paper.
The reviewers agree that the method is simple, the results are quite good, and the paper is well written. The issues the reviewers brought up have been adequately addressed. There is a slight concern about novelty, however the approach will likely be quite useful in practice.
The reviewers are split.  Two reviewers consider the technical contribution of the paper to be insufficient, and raise concerns about comparisons with Transformers or using more standard benchmarks for GNN experiments.   The other considers the experiments convincing and the method worth publishing.   My own view is that this work is not ready for inclusion in the conference.  In particular, I think this paper would be much stronger with either:   1: a more practical task to illustrate where this method might be applied in earnest, 2: more analysis and baselines on the synthetic data.  Synthetic data can be enough for a new method if it illuminates the functioning and the benefits and drawbacks.  In this paper, we have synthetic data with little analysis, and imo (concurring with R5) insufficient baselines.  For example, while a vanilla Transformer probably could not do the matrix problems (with the matrices encoded naively), one might expect Transformers with sparse attention to do quite well on e.g. transpose and 90 degree rotation, especially given the training curriculum and proper positional embeddings; a convolutional network seems like a strawman.  I also agree with R5 that standard benchmarks for GNN exist, and these might be appropriate (or at least there should be some discussion of why they are not).  3: some theoretical discussion of what the proposed model can do that other methods fundamentally cannot.  I do think this is interesting work, and encourage the authors to revise and resubmit.
This paper proposes a framework which qualifies how well given neural architectures can perform on reasoning tasks. From this, they show a number of interesting empirical results, including the ability of graph neural network architectures for learn dynamic programming.  This substantial theoretical and empirical study impressed the reviewers, who strongly lean towards acceptance. My view is that this is exactly the sort of work we should be show casing at the conference, both in terms of focus, and of quality. I am happy to recommend this for acceptance.
Addressing the initialization issue in DNNs is an important topic, and the proposed approach is found by the reviewers to be interesting. However, the reviewers feel that to clearly promote this research beyond the  proof of concept  phase, deeper investigation in multi layer architectures would be required. This would raise the significance of the paper. Besides extending the study to deeper networks, the paper could also benefit from elaborate experiments to increase convincingness, in particular by addressing R4 s concerns regarding robustness of performance e.g. on small dataset sizes. Finally, the methodology is sound and the authors clarify the significance of the ReLU associated covariance; however, overall the paper does not offer significant technical advancements that could make up for the shortcomings in the areas discussed above. 
The current paper presents a new method for communication and cooperation in multi agent settings. Specifically, the authors propose to model other agents  intentions and internal states using ToM nets and using these predictions to then decide how to communicate/coordinate. The authors present experiments in two multi agent cooperation tasks (multi sensor multi target coverage and cooperative navigation), compare against 4 previous methods (TarMAC, I2C, MAPPO and HiT MAC) and perform the necessary ablations studies and find that their method achieve better rewards in both environments. All reviewers have found the present study to be novel with convincing experimental findings. Reviewers have raised some concerns however a great deal of those have been addressed by the authors during the rebuttal and many of these points have now been incorporated in the paper.   Having read the paper and considering the reviews I agree with the reviewers that this manuscript will make a good addition to the program of ICLR and as such I recommend its acceptance.
The approach proposed here have raised major concerns from multiple reviewers especially concerning the novelty and the experimental validation procedure. Authors did not succeed in convincing reviewers of the value of their work for ML or calcium imaging processing.
This paper introduces a perceptual similarity on top on the commonly used perceptual loss in the literature (LPIPS). The authors draw experiments highlighting that human perceptual similarity is invariant to small shifts, whereas standard metrics are not. The paper studies several factors (anti aliasing, pooling, striding, padding, skip connection) in order to propose a measure on top of LPIPS achieving shift invariance.   This paper initially received mixed reviews. RLHuY was positive about the submission, pointing out the relevance of the real human data and the studied factors for measuring the impact on shift invariance. RGQvy was slightly positive, but also raised several concerns on justification of the claimed properties, human perception experiments, and positioning with respect to data augmentation (PIM). RLHuY, an expert on the field, recommended clear rejection, pointing out missing references (including DISTS), the limited scope of the paper (shift invariance and tiny shifts). After rebuttal, RLHuY and RLHuY stuck to their positions ; RGQvy were inclined to borderline reject  because of unconvincing answers on comparison to data augmentation techniques.   The AC s own readings confirmed the concerns raised by RGQvy and RLHuY, and points the following limitations:     The submission includes limited contribution and expected results: the studied modifications on neural networks  architecture, although meaningful, directly follow ideas borrowed from the literature. They are not supported by stronger theoretical analysis, and several insights related to accuracy or robustness remain unclear.    Experimental results are contrasted, e.g. compared to data augmentation: although these approaches are more demanding at train time, they do not induce any overhead at test time   in contrast to the proposed approach.  Therefore, the AC recommends rejection.
 The paper proposes ALFWorld, which combined TextWorld and ALFRED to create aligned scenarios (one that is text only, and the other in an embodied visual simulator) so that high level policies in language can be learned in a simpler world, and then transferred to the embodied one (using the proposed BUTLER architecture).  The proposed BUTLER model consists of three components: 1) a perceptual module (converts environment observation to specification of objects and relations using text), 2) goal planning module for generating textual specification of subgoals (from observed environment state) and 3) controller module which takes outputs from 1) and 2) and generates a sequence of actions.  Experiments show that using the textual specification, it is possible to models pretrained in the text world can generalize better to embodied settings.  Review Summary: The submission received slightly divergent reviews with R2, R3 recommending acceptance (score 7) and R1 recommending reject (score 4).  All reviewers recognized the novelty of the work, and the potential for follow up work based on the submission.  After considering the author response and discussion between reviewers, both R2 and R3 agreed that there are indeed flaws with the work as pointed out by R1 (R3 lowered their rating to 6).  Despite the concerns, both R2,R3 remained on the positive side.  Pros:    The work and proposed framework can stimulate further research on transferring policies from simple text environments to more realistic visual environment. (R2)   The decomposition of high level goals into low level actions sequences is a good direction for future research (R3)   Good set of experiments and comparisons (R3)   the paper is clearly written and easy to understand (R2)  Cons:   The main claim of the work (high level policies learned in a text based environment can be transferred to a physically simulated environment) is not properly substantiated by the experimental results. (R1)   The proposed method is a complex system and simpler baselines should be considered (R2)   Some assumptions are made in ALFWorld need to be hand designed and may miss important aspects of perception (R3)    More experiments and ablations are needed to properly evaluate the framework  Despite the issues pointed out by R1, the AC believe that the work can inspire future work in this area, and thus recommend acceptance.  The paper is also well written and easy to understand. 
The topic of this paper is timely and important.  However, ultimately the reviewers remained unconvinced that this paper provides a sufficiently clear and sufficiently significant advance to lifelong RL.  As an additional note, the setting under investigation here is not the full lifelong learning setting.  E.g., several of the challenges outlined by Schaul et al. [1] are not treated, and this work is, instead, situated in a somewhat typical multi task setting with substantlal structure.  That is not bad, but it would be good if this is reflected clearly in all the statements, and, e.g., in the title of the work.  The authors are encouraged to carefully take the provided feedback and see how they can use it to improve their work.  This is an important research direction.  It was just felt the current submission was not quite ready for publication yet.  [1] https://arxiv.org/abs/1811.07004
This paper shows a nice idea to transfer knowledge from larger sequence models to small models. However, all the reivewers find that the contribution is too limited and the experiments are insufficient. All the reviewers agree to reject.
The reviewers have supported the acceptance of this paper (R3 and R5 were particularly excited) so I recommend to accept this paper.
This paper presents a method for providing uncertainty for deep learning regressors through assigning a notion of evidence to the predictions.  This is done by putting priors on the parameters of the Gaussian outputs of the model and estimating these via an empirical Bayes like optimization.  The reviewers in general found the methodology sensible although incremental in light of Sensoy et al. and Malinin & Gales but found the experiments thorough.  A comment on the paper pointed out that the approach was very similar to something presented in the thesis of Malinin (it seems unfair to expect the authors to have been aware of this, but the thesis should be cited and not just the paper which is a different contribution).  In discussion, one reviewer raised their score from weak reject to weak accept but the highest scoring reviewer explicitly was not willing to champion the paper and raise their score to accept.  Thus the recommendation here is to reject.  Taking the reviewer feedback into account, incorporating the proposed changes and adding more careful treatment of related work would make this a much stronger submission to a future conference.
The paper explores the Birkhoff von Neumann decomposition in order to propagate gradients through a bi partite matching. The task is very relevant to the community but the reviewers raised concerns both about the theory and the practice of the work. Unfortunately the work is not ready for publication at ICLR.  
The paper clearly has merits, presenting a reasonable approach to zero shot cross lingual learning with good results, but with limited novelty, perhaps. I am sympathetic to the departure from XTREME on NER, agreeing with the authors that using CoNLL data is more interesting than WikiANN.   The post rebuttal discussion centered on novelty and baselining   and specifically, whether other approaches to unsupervised data augmentation exist that should be used to baseline the proposed work. The authors argued that most of the approaches mentioned by the reviewers were in some way supervised. I personally think the confusion is a result of the paper being somewhat poorly framed:  Reviewer 2, for example, suggests a bunch of baselines. Some of these require gold labels for supervised fine tuning to condition the MLM, but this seems like a trivial difference, which is orthogonal to using the augmentation strategies as baselines? Also, other papers have been presented that do not require gold labels, e.g. https://www.aclweb.org/anthology/D18 1100.pdf  Also, on the discussion of Täckström et al. (2012): Older approaches relying on distributional clusters *are* in fact data augmentation methods. Training on augmented data with words replaced is, in the limit, equivalent to training with clusters, when replacement words are sampled from clusters. Others have in the past proposed to use FSAs or clusters induced from static embeddings.  What the authors suggest is a form of co training procedure, so similarly, semi supervised algorithms   e.g., tri training   could have been used as baselines.    In sum, I think the sentiment shared across the reviewers is that the results are largely unsurprising, and could likely be obtained in different ways, including jointly training with a target language modeling objective, tri training, etc. Finally, I agree with Reviewer 2 that a “detailed comparison and discussion of the trade off” between the different approaches to data augmentation, even beyond what’s apples to apples, would benefit the paper. Maybe there s other advantages to the proposed approach over other baselines (effectiveness, robustness)?  
The reviewers overall liked the paper and the mathematical contribution seems substantial and elegant. The two dominant concerns were whether this is really applicable to GANS, and whether the increment from symmetric to normal matrices (e.g., real to complex eigenvalues, still unitary eigenvectors) was significant enough. Our consensus is that this result is a step toward analyzing practical GANS, and (based on the authors  response) that the extension to complex eigenvalues was substantial enough. Hence I m happy to recommend the paper.
The aim of this paper is to interpret various optimizers such as RMSprop, Adam, and NAG, as approximate Kalman filtering of the optimal parameters. These algorithms are derived as inference procedures in various dynamical systems. The main empirical result is the algorithms achieve slightly better test accuracy on MNIST compared to an unregularized network trained with Adam or RMSprop.  This was a controversial paper, and each of the reviewers had a significant back and forth with the authors. The controversy reflects that this is a pretty interesting and relevant topic: a proper Bayesian framework could provide significant guidance for developing better optimizers and regularizers. Unfortunately, I don t think this paper delivers on its promise of a unifying Bayesian framework for these various methods, and I don t think it s quite ready for publication at ICLR.  There was some controversy about relationships to various recently published papers giving Bayesian interpretations of optimizers. The authors believe the added value of this submission is that it recovers features such as momentum and root mean square normalization. This would be a very interesting contribution beyond those works. But R2 and R3 feel like these particular features were derived using fairly ad hoc assumptions or approximations almost designed to obtain existing algorithms, and from reading the paper I have to say I agree with the reviewers.  There was a lot of back and forth about the correctness of various theoretical claims. But overall, my impression is that the theoretical arguments in this paper exceed the bar for a primarily practical/empirical paper, but aren t rigorous enough for the paper to stand purely based on the theoretical contributions.   Unfortunately, the empirical part of the paper is rather lacking. The only experiment reported is on MNIST, and the only result is improved test error. The baseline gets below 99% test accuracy, below the level achieved by the original LeNet, suggesting the baseline may be somehow broken. Simply measuring test error doesn t really get at the benefits of Bayesian approaches, as it doesn t distinguish it from the many other regularizers that have been proposed. Since the proposed method is nearly identical to things like Adam or NAG, I don t see any reason it can t be evaluated on more challenging problems (as reviewers have asked for).   Overall, while I find the ideas promising, I think the paper needs considerable work before it is ready for publication at ICLR. 
This paper propose an approach to efficient Bayesian deep learning by applying Laplace approximations to sub structures within a larger network architecture. In terms of strengths, scalable approximate Bayesian inference methods for deep learning models are an important and timely topic. The paper includes an extensive set of experiments with promising results.   In terms of issues, the reviewers originally raised many concerns and the authors provided a large update to the paper. However, following that update and the discussion, several concerns remain. First, the reviewers noted that the originally submitted draft made claims about the optimality of the sub network selection procedure that were incorrect due to the use of a diagonal approximation. The authors subsequently retracted these claims and re focused on the idea that the subset selection approach is theoretically well motivated heuristic that performs well empirically. Following the discussion, the reviewers continued to express concerns about the heuristic nature of this procedure.   A second point has to do with scalability. The reviewers noted that the authors had only evaluated their approach on small data sets, leaving open the question of how scalable the method is. The authors responded by adding experiments on the same data sets using larger models, which does not squarely address the issue raised. Third, an additional point was raised regarding the lack of control of resource use in the experiments. The authors note that their approach can use more resources when available while many other methods can not. However, some methods including deep ensembles can also expand to use more resources, as can posterior ensembles produced using MCMC methods like SGLD and SGHMC. The authors need to consider quantifying space performance and time performance trade offs in the same units for different approaches to satisfactorily address this issue. While the authors added one set of experiments looking at deep ensembles in isolation, their conclusions that performance saturates for these models at low ensembles sizes seems to be hasty in some cases (e.g., deep ensembles show continued improvement for large corruptions in Figure 5(right) despite the claim by the authors that the models saturate after 15 epochs).   In summary, this appears to be a promising approach. While the authors made significant efforts to correct issues and address questions with the original draft, the majority view of the reviewers following discussion is that this paper requires additional work to more carefully expand on the revised results and to address the heuristic status of the sub network selection approach.
The paper introduces a pipeline to discover PDEs from scarce and noisy data. Reviewers engaged in a very thoughtful discussion with the authors. I read the extensive rebuttal, and I believe the authors have addressed the major concerns claimed by the reviewers. I ask the authors to make sure to include all the changes and additional experiments in the camera ready version.
The paper presents a new architecture that achieves the advantages of both Bi encoder and Cross encoder architectures. The proposed idea is reasonable and well motivated, and the paper is clearly written. The experimental results on retrieval and dialog tasks are strong, achieving high accuracy while the computational efficiency is orders of magnitude smaller than Cross encoder. All reviewers recommend acceptance of the paper and this AC concurs.
This paper discusses how one can equip reinforcement learning agents with an intrinsic reward function that helps identifying factors of variation within a family of MDPs, effectively allowing agents to do experiments in the environment. This is interpreted as causal factors that control important aspects of the environment dynamics.  Although this is a very relevant topic and there was extensive discussion during the discussion phase, with reviewers acknowledging that the final version of the submitted manuscript substantially improved over the original submission, most reviewers still recommend the rejection of the paper. This is mainly due to the assessment that there are still several unclear technical aspects related to the paper. Shortly, the reviewers felt that the paper had important clarity issues, that the claims being made were imprecise, and that there was a dearth of details about the empirical results, making them not fully convincing.  I strongly recommend the authors to take the reviewers suggestions into consideration to have a much stronger submission to future venues. 
This paper tackles the problem of how to utilize a network from the source domain to benefit target domain training in terms of sample/training efficiency. In contrast to prior methods (e.g. that perform fine tuning or distillation), this paper poses it as a bandit problem that decides how to wire intermediate representations of the source model into the target model as well as what aggregation function to use. An alternating/mixed discrete continuous optimization is proposed to perform this decision making, and results are shown across a mix of source target pairs and network architectures.     The reviewers overall found the method interesting and paper topic both interesting and extremely practically useful, presenting an opportunity to save significant energy, compute, and labeling requirements when training on target domains. The results also show very significant improvements, on the conditions tried. However, a number of concerns were raised including comparison to simple same architecture fine tuning (3TiT), comparison benchmarks e.g. VTAB and newer methods in the area (u325, 7soo, 3TiT), need for the adversarial bandit formulation (3TiT, 7soo, 8d9w), and the added storage/inference costs required (3TiT).     Based on these reviews, the authors provided a thorough rebuttal, additional baselines and experiments demonstrating the efficacy of the method (especially the full version) over both reasonable simple baselines (same architecture fine tuning) as well as simpler versions (fixed aggregation), and time/inference time matched performances. Importantly, the advantage of the full method comes out a lot more in the new experiments. Overall, through the rebuttal process the paper has been made much stronger.     Given that the paper provides a nice principled approach to the problem and now has strong compelling results, I recommend acceptance.
This article sets out to study the advantages of depth and overparametrization in neural networks from the perspective of function space, with results on univariate shallow fully connected ReLU networks and some experiments on deep networks.  The article presents results on the concentration /dispersion of the slope / break point distribution of the functions represented by shallow univariate ReLU networks for parameters from various distributions. The reviewers found that the article contains interesting analysis, but that the presentation could be improved. The revision clarified some aspects and included some experiments illustrating breakpoint distributions in relation to the curvature of some target functions. However, the reviewers did not find this convincing enough, pointing out that the analysis focuses on a very restrictive setting and that that presentation of the article still could be improved. The discussion of implicit regularisation in section 2.4 seems promising, but it would benefit from a clearer motivation, background, and discussion. 
This paper attends to the problem of how to implement dense associative memories (i.e. modern Hopfield networks) using only two body synapses. This is interesting because modern Hopfield networks have much higher capacity, but at face value, they require synapses with cubic interactions between neurons, which to the best of our knowledge, is not a common feature in neurophysiology (though it should be noted: it is not by any means impossible from a physiological perspective to have cubic interactions at synapses, see e.g. Halassa, M. M., Fellin, T., & Haydon, P. G. (2007). The tripartite synapse: roles for gliotransmission in health and disease. Trends in molecular medicine, 13(2), 54 63.).   The authors show how the use of a layer of hidden neurons, akin to a restricted Boltzmann machine architecture, coupled with the right energy function, can be used to recover dense associative memory models using only two body synapses. They also demonstrate how this connects to recent work on the relationship between attention mechanisms in modern ML models and Hopfield network dynamics.  Overall, the reviewers were positive on this paper. The most common critique related to the question of "biological plausibility". The authors addressed these concerns by adding some more recognition as to the lack of biological plausibility and more discussions of the relevance to neuroscience. To be candid with the authors, if the goal is indeed to make a more biologically plausible model of modern Hopfield networks, than a fair bit more work would be needed to connect the paper to biology well. As it stands, the only connection is the shift to two body synapses by using hidden neurons, but this provides limited insight for most neuroscientists, as noted by Reviewer 2. Also, some of the biological examples provided seem strained (e.g. the colour memory example, where there is no physiological reason to posit that we store colour memories using our retina, or the MNIST example, since there is no reason to suppose that animals can memorise thousands of specific MNIST images). But overall, the critique regarding biological plausibility was attended to. The other concerns raises were also largely addressed.   Given the interesting contributions from this paper, the overall positive reviews, and the decent job at addressing reviewer concerns, the AC believes that this paper should certainly be accepted. A decision of "Accept (Poster)" seems appropriate, though (as opposed to an oral or spotlight), given the lack of biological connections in a paper with a stated goal of achieving a more biologically realistic model.
This paper proposes a transferable adversarial attack method for object detection by using the relevance map. Four reviewers provided detailed reviews: 2 of them rated “Ok but not good enough   rejection”, 1 rated “Marginally below” and 1 rated “Marginally above”. While reviewers consider the paper well written and using relevance map novel, a number of concerns are raised, including limited novelty, the lack of theoretical results, no use of the proposed dataset, insufficient ablation, etc. During the rebuttal, the authors made efforts to response to all reviewers’ comments. However, the major concerns remain, and the rating were not changed. The ACs concur these major concerns and agree that the paper can not be accepted at its current state.
The authors present a new set of trigger based backdoor attacks that use dynamic patterns that make detection harder. These attacks seem to be stronger with regards to state of the art attacks.   Some weaknesses is the need for full whitebox access of the model. Several key references are missing, and the comparison with other backdoor attacks in unclear.  Moreover, although the authors compare with trigger based backdoors, there are plenty of triggerless backdoors that can be viewed as dynamic, eg the attacks by  Bagdasaryan et al. http://proceedings.mlr.press/v108/bagdasaryan20a.html and works referencing this paper.  The paper indeed provides an interesting path towards dynamic attacks, but the lack of comparisons with state of the art literature, and also the need for whitebox access/high poisoning rate significantly limit the novelty of this work.   
The paper describes the use of tactile sensors for exploration.  An important topic which has been addressed in various previous publications, but is unsolved to date.  The research and the paper are unfortunately in a raw state.  Rejected unanimously by the reviewers, without rebuttal chances used by the authors.
This paper derives a PAC Bayes generalization bound for SGD and uses the results to postulate a functional form for the generalization error as a function of the ratio of the learning rate to the batch size. This functional form is then leveraged to develop a kernel function GP hyperparameter optimization.  The reviewers favorably viewed the novel PAC Bayes bound, but were not convinced by the subsequent analysis. In particular, the reviewers expressed some skepticism about the soundness and generality of the proposed functional form, and were unconvinced that the method would be useful in practice. As such, I cannot recommend the paper for acceptance.
This paper proposes a method to solve the inverse problem of identifying parameters of a dynamic physical system from image observations. The main idea is to train a rendering invariant state prediction (RISP), which estimates the inverse mapping from the pixel to the state domain. The authors introduce a new loss to this end, and an efficient gradient computation of the loss.  The paper received three clear accept recommendations. The reviewers discussed the potential improvement of RISP when combined to disentanglement methods, and also raise several concerns regarding experiments, e.g. rendering conditions during training and testing, or evaluation on real data. The rebuttal did a good job in answering reviewers  concerns, and the reviewers especially appreciated the new results on real videos. Eventually, all reviewers recommended a clear acceptance of the paper.  The AC s own readings confirmed the reviewers  recommendations. The paper is introduces very solid contributions for solving the complex task of physical parameter identification in the unobservable setting. The paper is also clear and well written, and validated with convincing experimental results. Therefore, the AC recommends acceptance.
There is a clear consensus among the reviews to accept this submission thus I am recommending acceptance. The paper makes a clear, if modest, contribution to language modeling that is likely to be valuable to many other researchers.
The reviewers and this AC agree that the paper is not of acceptable form due to several issues: (1) limited novelty, (2) limited/unclear experimental validation, and (3) presentation issues.
The authors study the training settings that may affect active deep learning performance, including code/warm start, leveraging unlabeled data, and initial set selection, for each active learning strategy. The findings on several data sets help understand AL more, with some pieces of insights to inspire future research.  The reviewers were at best lukewarm about the work prior to the rebuttal. Some turned more positive but none were willing to strongly champion for the paper s acceptance, even after the authors provided a decent rebuttal. This leaves the paper to be a borderline case, and the recommendation comes from carefully checking the latest revision and calibrating its score with other submissions.  The reviewers are generally positive about the breadth of the study, the potential impact of the codebase and the systematic study that can inspire future works. Some clarified issues include comments on future research directions and the labeling efficiency plot (which is, however, not analyzed deeper in the main text), and results on additional settings like transfer learning (somewhat preliminary). In the end, two remaining concerns surround whether the technical contribution and the conclusions are sufficiently solid, including  * limited insights: Some reviewers comment that the insights are on the lighter side. The authors identify several issues that may affect the performance of the underlying tasks of active learning, and find that the best setting differs across different active learning strategies. But given that the paper offers at best "best practices of training models on actively queried labels", it is not clear whether the authors achieve their claimed goal of "compare different strategies in a fair way" in particular, the conclusion for this particular comparison seems to be missing (e.g. which is recommended in practice, BADGE or LL4AL or others?). Also, given that only three data sets (5 after rebuttal) have been studied in this work (see item below), the "generalization ability" of the conclusions in this paper cannot be clearly established. While the authors provided some additional pieces in the rebuttal, the pieces can use more study to be fully conclusive. Some reviewers are also concerned that the conclusions are rather scattered.  From a practical perspective, it appears to be a chicken egg problem on whether to fix the active strategy first (and then train the model with the best setting/practice), or fix the training setting first (and then select the best strategy). The authors may want to add more arguments on why they focus on the former rather than the latter.  * limited experiments: several reviewers point out that the few data sets used could not fully justify the "best practice", and demand data sets like ImageNet. The authors offered some new results on TinyImageNet and CIFAR100, but those are not studied as deeply as other data sets at the current point. A more careful study on the two (and other) data sets are thus strongly recommended.
The paper propose a universal technique that enables weak supervision over any label type while still offering desirable properties, including practical flexibility, computational efficiency, and theoretical guarantees.  Over the course of the rebuttal, the authors have made a substantial overhaul on writing and experimentation. The universality claims are now better supported by bounds, and experiments cover comparison to snorkel, majority vote and supervised learning, on multiple applications. The authors are encouraged to move the related work section to the main body of the paper. The authors should also clarify to what extent the contributions they make pertain to Snorkel as opposed to weak supervision more generally. This may require revisiting both the introduction as well as perhaps the title.
This paper first examines a multi domain separation phenomenon, where different types of adversarial noise lead to different running statistics, and then introduces Gated Batch Normalization (GBN), a building block for deep neural networks that improves robustness against multiple perturbation types. GBN consists of a gated subnetwork and a multi branch BN layer, and each BN branch handles a single type of perturbation. Results were reported on MNIST, CIFAR 10, and Tiny ImageNet. Though the idea and methodology are valid and of interest, some concerns regarding the experimental section remain after rebuttal discussions.
This paper considers inter domain policy transfer in reinforcement learning. The proposed approach involves adapting existing policies from a source task to a target task by adding a cost related to the difference between the dynamics and trajectory likelihoods of the two tasks.  There are three major problems with this paper as it stands, as pointed out by the reviewers. Firstly, the "KL divergence" is not a real KL divergence and seems to be only empirically motivated. Then, there are issues with the derivative of the policy gradient. Finally, the theory is not well connected to the proposed algorithm. The rebuttals not only failed to convince the reviewer that raised these issues, but another reviewer lowered their score as a result of these raised points.  This is a really interesting idea with compelling experiments, but must be rejected at this point for the aforementioned reasons.
The paper investigates questions around adversarial attacks in a continual learning algorithm, i.e., A GEM. While reviewers agree that this is a novel topic of great importance, the contributions are quite narrow, since only a single model (A GEM) is considered and it is not immediately clear whether this method transfers to other lifelong learning models (or even other models that belong to the same family as A GEM). This is an interesting submission, but at the moment due to its very narrow scope, it seems more appropriate as a workshop submission investigating a very particular question (that of attacking A GEM). As such, I cannot recommend acceptance.
The paper addresses lifelong/continual learning (CL) by combining reusable components. The algorithm is based on, updating components, updating how they are combined for a given task and adding new components.   Reviewers had concerns about the learning workflow, how it could scale to harder CL streams and how it differs from existing LL/CL work.  They also asked for clarifications about compositionality. They highlighted the experiments as a point of strength.  After the rebuttal, all reviewers found the paper to be above the acceptance bar. 
Two referees support accept and two indicate reject. Despite the author s rebuttal, reviewers determined through subsequent private discussions that the paper was insufficient to satisfy the high standards of ICLR due to the lack of diverse evaluations on various models/datasets  and increased computational overhead. Even the two positive reviewers agree on the weakness of the paper in terms of experimental evaluations and do not have a strong opinion on the acceptance. 
This paper provides some theoretical perspective on the use of data augmentation in consistency regularization based semi supervised learning. The framework used in the paper argues that high quality data augmentation should move along the data manifold. This generic view allows the paper s ideas to be applied across datasets (as opposed to image specific data augmentation used in state of the art semi supervised learning algorithms). I am not aware of any other work raising these points, and indeed this paper is significant in that it provides a new and potentially useful perspective on the most performative semi supervised learning approach. Reviewers agreed that the paper was clear and useful. The main concern was that the paper only included experiments in toy settings. Indeed, it would have been much more impactful to apply these ideas to state of the art semi supervised learning methods, but I think it can be excused given the theoretical focus of the work.
there have been many variants of memory augmented neural nets since around 2014 when NTM, attention based NMT and MemNet were proposed. it is indeed still an interesting and important direction of research, but the bar for introducing yet another variant of memory augmented neural nets has been significantly raised, which is a sentiment shared by the reviewers. the author s response had not swayed the reviewers  opinion, and i am sticking to the reviewers  decisions.   i believe more streamlined and systematic comparison among different memory augmented networks across many different benchmarks (e.g., use the same set of latest variants of memory nets across all the benchmarks) in this submission would make it a better paper and increase the chance of acceptance. 
Although the initial scores of the paper were not positive, the authors managed to properly address the questions/concerns of the reviewers and the changes they made to the paper convinced the reviewers  to update their scores. This clearly shows that there were flaws in the original presentation of the paper. So, I would recommend the authors to take the reviewers  comments into account when they prepare the camera ready version of their work.
The reviewers agreed that there were a few issues with the current version of this work, mainly:    Some missing baselines that are mentioned in the paper, but not sufficiently compared to    Problems with the presentation that did not make it easy to understand.    Not an optimal fit with the intended audience of this conference.   
This paper extends previous models for monotonic attention to the multi head attention used in Transformers, yielding "Monotonic Multi head Attention." The proposed method achieves better latency quality tradeoffs in simultaneous MT tasks in two language pairs.  The proposed method is a relatively straightforward extension of the previous Hard and Infinite Lookback monotonic attention models. However, all reviewers seem to agree that this paper is a meaningful contribution to the task of simultaneously MT, and the revised version of the paper (along with the authors  comments) addressed most of the raised concerns.  Therefore, I propose acceptance of this paper.
The paper presented a detailed discussion on the implementation of a library emulating Atari games on GPU for efficient reinforcement learning. The analysis is very thoroughly done. The major concern is whether this paper is a good fit to this conference. The developed library would be useful to researchers and the discussion is interesting with respect to system design and implementation, but the technical depth seems not sufficient.
While the reviewers generally appreciated the ideas presented in the paper and found the overall aims and motivation of the paper to be compelling, there were too many questions raised about the experiments and the soundness of the technical formulation to accept the paper at this time, and the reviewers did not feel that the authors had adequately addressed these issues in their responses. The main concerns were (1) with the correctness and rigor of the technical derivation, which the reviewers generally found to be somewhat questionable   while the main idea seems reasonable, the details have a few too many question marks; (2) the experimental results have a number of shortcomings that make it difficult to fully understand whether the method really works, and how well.
This paper characterizes the induced geometry of the latent space of deep generative models. The motivation is established well, such that the paper convincingly discusses the usefulness derived from these insights. For example, the results uncover issues with the currently used methods for variance estimation in deep generative models. The technique invoked to mitigate this issue does feel somehow ad hoc, but at least it is well motivated.  One of the reviewers correctly pointed out that there is limited novelty in the theoretical/methodological aspect. However, I agree with the authors’ rebuttal in that characterizing geometries on stochastic manifolds is much less studied and demonstrated, especially in the deep learning community. Therefore, I believe that this paper will be found useful by readers of the ICLR community, and will stimulate future research. 
This paper is not aimed at introducing new methodologies (and does not claim to do so), but instead it aims at presenting a well executed empirical study. The presentation and outcomes of this study are quite instructive, and with the ever growing list of academic papers, this kind of studies are a useful regularizer. 
This paper propose a novel CNN architecture for learning multi scale feature representations with good tradeoffs between speed and accuracy. reviewers generally arrived at a consensus on accept.
This paper studies adversarial training in the linear classification setting, and shows a rate of convergence for adversarial training of o(1/log T) to the hard margin SVM solution under a set of assumptions.   While 2 reviewers agree that the problem and the central result is somewhat interesting (though R3 is uncertain of the applicability to deep learning, I agree that useful insights can often be gleaned from studying the linear case), reviewers were critical of the degree of clarity and rigour in the writing, including notation, symbol reuse, repetitions/redundancies, and clarity surrounding the assumptions made.  No updates to the paper were made and reviewers did not feel their concerns were addressed by the rebuttals. I therefore recommend rejection, but would encourage the authors to continue refining their paper in order to showcase their results more clearly and didactically.
The paper proposes a novel protocol for examining the inductive biases in learning systems, by quantifying the exemplar rule trade off (as measured by the exemplar vs rule propensity (EVR) defined in Eq. (2)) while controlling for feature level bias.   Reviewers mostly agree that the problem studied in this paper is practically relevant and that the two bias measures are potentially interesting and (jointly) more informative than existing measures such as spurious correlation. However, a shared concern among the reviewers (with confidences scores > 3) is the clarity of the exposition (e.g., many key concepts such as the data conditions are informally specified [Section 2 (Reviewer TPBn)], some key messages not clearly conveyed in the main paper [Section 3 (Reviewer RJtk)], and results inconclusive or not sufficiently supported by the experimental results [for both the synthetic setting (Reviewer RJtk) and the real world setting (Reviewer yoH5)]. Based on the above concerns, the reviewers were not convinced that this work is well supported in its current state to merit acceptance for publication.
This paper proposes a framework for novel object captioning by combining BERT and CLIP.  The model improves fluency, fidelity, and adequacy of generated captions. However, as reviewers mentioned, the novelty is limited, combining large models and big data to solve a downstream task does not make useful insights at this moment.
The paper presents novel model stealing attacks against BERT API. The attacks are split in two phases. In the first phase, the black box BERT model is recovered by submission of specially crafted data. In the second phase, the inferred model can be used for identifying sensitive attributes or to generate adversarial examples against the basic BERT model.  Despite the novelty of presented attacks against BERT models, the current version of the paper has some problems with clarity and motivation. The presentation of attacks is very short, and some technical details are not adequately covered. The practical motivation of adversarial example transfer attacks is not very clear, and the authors  response on this issue did not provide a convincing clarifications. Furthermore, creation of surrogate models for generation of adversarial examples is a well known technique and the difference of the proposed AET attack from this conceptual approach is not clear.  Overall, the paper reveals a solid and interesting work but a substantial revision would be necessary to make it suitable for the ACLR audience.  
The paper suggests a non random strategy for selecting minibatches of nodes for training graph neural networks. The main argument is that consecutive memory accesses are faster than random accesses, and thus they claim a 20x speedup per epoch by precomputing batches at a small cost to accuracy.   There are a number of discussion points. One reviewer finds the results hard to believe because previous work has shown that runtime sampling can be fully pipelined. The authors agree but say their speedups are still better, which isn’t a fully satisfying response, and it calls into question the quality of the baseline implementation. Another concern is about the effect of deterministic minibatches. The authors argue that the empirical results speak for themselves, while the reviewer worries about robustness. There also are some concerns about methodology around hyperparameters and special casing of preprocessing for one dataset, though those appear mostly resolved.  On the whole, this is a borderline paper that lands just on the side of rejection. I’d encourage the authors to more thoroughly address the questions about quality of the baseline implementation and the reviewer’s concern about robustness of deterministic minibatches, and then resubmit to the next conference.
This paper proposes an expansion strategy for both task agnostic and task boundary aware CL. The authors demonstrate the quality of their method using two standard scenarios with the Split MNIST and CIFAR datasets.   Enabling CL for task agnostic and task boundary aware is important and an active area of research. The proposed approach is an interesting method that adds an expert for each new task. Experts are then combined (Mixture of Experts) for prediction. One disadvantage of a MoE approach is that the model size and compute will grow linearly with the number of tasks. This effect is partly limited in the paper as the authors show that experts can be small neural networks.  There was a bit of confusion in the original reviews regarding the exact setting this paper works under. As far as I understand this paper mostly deals with the class incremental setting (task IDs available at training time, but not at test time). The task agnostic setting (task IDs never given) is also explored in Section 5.1. I think this confusion is partly a reflection of the state of the CL literature and the authors provided clear and concise replies to the reviewers.  The main limitation that remains is regarding the experiments. I agree with the reviewers that the current experiments seem somewhat preliminary and showing results on larger scale datasets and/or compared to a wider diversity of baselines is important. Reviewer sgG4 made precise comments about this. Other minor comments by the reviewers including providing a detailed report of the memory usage and computational costs of the various methods (partly done in Figure 5.3).  I think this method is interesting and could be impactful. I strongly encourage the authors to polish their manuscript and consider adding some of the additional empirical results that were suggested.
This paper proposes to optimize the code optimal code in DNN compilers using adaptive sampling and reinforcement learning. This method achieves  significant speedup in compilation time and execution time. The authors made strong efforts in addressing the problems raised by the reviewers, and promised to make the code publicly available, which is of particular importance for works of this nature.    
This work uses a graph representation of the protein backbone and a GNN for model quality assessment (MQA) and protein design. The proposed GNN has the property that the vector and scalar outputs are equivariant and invariant with respect to composition of 3D rotations and reflections. Overall speaking, the reviewers like this paper very much (especially its technical novelty), and provide quite positive comments. On the other hand, there are also some concerns being mentioned:  1)	The datasets used in the experiments are a little old – experiments on CASP 13 are preferred. 2)	Some technical details are not very clear and the paper writing needs improvements 3)	Experimental comparison with some recent baselines is missing.  The authors did a good job in their rebuttal and paper revision. Most of the above concerns have been addressed. Therefore, we think the current version of the paper is clearly beyond the bar of ICLR. 
### Summary  This work demonstrates that it is possible to identify lottery tickets with some manner of structural sparsity. The work finds success through refilling (perhaps better termed infilling) and regrouping, two techniques that have found previous homes in other parts of the literature.   ### Discussion  #### Strengths    Tackles an interesting problem.     At face value, I believe the paper achieves its claimed goal, though not with full clarity as written.  #### Weaknesses    There is room to clarify the atypical form of structure here for readers. The suggested title change would appropriately set expectations. However, refinements in the text as well would be welcomed.    As I discuss below, the claims should be settled with respect to the strongest baselines.    Random reinitialization should play a primary role in the presentation of the text. As the authors note, the original lottery ticket paper did not require besting the performance of random reinitialization. However, the random reinitialization results are central to the main figures of the original paper and demonstrate that the result is not merely happenstance. This paper should follow that practice.  ### Recommendation  I recommend Reject and I do not do so lightly, given the scores. The work here is promising because finding a path to better performing lottery tickets remains an open challenge. However, Reviewer YWUh has voiced reasonable concerns about the evaluation methodology.   I ve read the detailed authors  responses and agree with the authors that the presented results may depend on choices in the hyperparameters and training strategy. Having said that, it is critically important for the paper to include in the primary text the strongest baselines available, despite this dependence. On such baselines, the results are worse than originally reported and hence, the primary claims must either be revised to be these results or revised to include these results, with the primary claims providing a range of results. Though the authors offer to make revisions for the camera ready, the required revisions here are substantial enough to require additional review, which is out of the scope of the current process.  An additional oversight in this methodology   at least as reported   is that the primary target of the lottery ticket hypothesis is sparse training, not sparse inference. Evaluating solely the inference performance   rather than training performance, which includes both the forward and backward pass   is, therefore, inconsistent with the purpose of lottery tickets. This methodological error will too need to be repaired in the final version of this work.
This paper proposes Conv TT LSTM for long term video prediction. The proposed method saves memory and computation by low rank tensor representations via tensor decomposition and is evaluated in Moving MNIST and KTH datasets.  All reviews argue that the novelty of the paper does not meet the standard of ICLR. In the rebuttal, the authors polish the experiment design, which fails to change any reviewer’s decision.  Overall, the paper is not good enough for ICLR.
The manuscript develops a new kind of graph neural network (a Graph Mechanics Network; GMN) that is particularly well suited to representing and making predictions about physical mechanics systems (and data with similar structure). It does so by developing a way to build geometric constraints implicitly and naturally into the forward kinematics of the network, while still allowing for effective learning from data. The manuscript proves some essential properties of the new architecture and runs experiments both with simulated particles, hinges, sticks (and their combination), as well as with motion capture data.  Reviewers were generally impressed by the writing and clarity of the work, as well as the main results. In addition, in those cases where reviewers thought that the experiments were lacking, the authors delivered effective new experiments to address those concerns (e.g. looking  at mocap and molecular datasets). One reviewer initially scores the manuscript as a Reject/3 on the basis of concerns about novelty and the scope of the theoretical and experimental contributions of the paper. However, they adjust their score 3 >5 based on the rebuttal presented by the authors (including new experiments). The reviewer also downgrades their certainty (from 3 >2) on the basis of the engagement from reviewers offering higher scores. Overall, the manuscript presents a promising contribution to the graph networks literature and I agree with the general consensus in favour of publication.
The paper proposes to use importance resampling (IR) as an alternative to the more popular importance sampling (IS) approach to off policy RL.  The hope is to reduce variance, as shown in experiments.  However, there is no analysis why/when IR will be better than IS for variance reduction, and a few baselines were suggested by reviewers.  While the authors rebuttal was helpful in clarifying several issues, the overall contribution does not seem strong enough for ICLR, on both theoretical and empirical sides.  The high variance of IS is known, and the following work may be referenced for better 1st order updates when IS weights are used: Karampatziakis & Langford (UAI 11).  In section 3, the paper says that most off policy work uses d_mu, instead of d_pi, to weigh states.  This is true, but in the current context (infinite horizon RL), there are more recent works that should probably be referenced:   http://proceedings.mlr.press/v70/hallak17a.html   https://papers.nips.cc/paper/7781 breaking the curse of horizon infinite horizon off policy estimation
This paper was reviewed by four experts in the field. Based on the reviewers  feedback, the decision is to recommend the paper for acceptance to ICLR 2021. The reviewers did raise some valuable concerns that should be addressed in the final camera ready version of the paper. The authors are encouraged to make the necessary changes and include the missing references.
This paper studies the problem of multivariate mean estimation with a focus on the heavy tailed setting. The authors give an algorithm for this estimation task and then use it (in essentially a black box manner) to obtain heavy tailed estimators for various supervised learning tasks. As pointed out by one of the reviewers and my own reading, the theoretical contributions of the paper are weak and are subsumed by related work (some of which is not cited in the submission).  More generally, the extensive recent literature on the topic is not accurately represented in both the submission itself and the response to the reviewers comments. On the other hand, the experimental results of the paper hold some promise. However, at this stage, these experimental contributions by themselves are in my opinion insufficient to merit acceptance.  
This paper proposes a metric for the safety and interpretability of supervised learning models based on the maximum deviation from interpretable white box models. The safety and interpretability of black box models is an important topic, and many reviewers agree that the approach proposed by the authors is interesting. However, the maximum deviation from popular models such as decision trees, generalized linear and additive models have been intensively studied in the context of robust statistics/learning. Without explicit discussion on the connections with these existing studies, the novelty of the proposed approach cannot be properly evaluated. We thus have to conclude that the paper cannot be accepted in its current form.
All the reviewers recommend acceptance. The reviews found the paper to be interesting with substantial insights. 
This paper proposes a new NAS methods that when doing architecture search, returns flat minima using based on a notion of distance defined for two cells (Eq. (2)). Authors then evaluation the effectiveness of the proposed methods against prior work on several benchmarks.  As authors have discussed in the paper, the idea of using flatness notion in architecture search is not new and has been first proposed by Zela et al 2020. This paper is building on Zela et al 2020 but the proposed algorithm is novel and different than Zela et al 2020. Even though the introduced algorithm is interesting, there are several concerns/areas of improvements:  1  The proposed method s performance is highly dependent to the notion of distance defined in eq. (2). However, the current choice is not well motived and does not seem like a well thought out choice. See for example the issue raised by R1. I think authors need to spend more time on this choice. One other option is to meta learn the vector representation of each operation.  2  All reviewers agree that the improvements marginal and in some cases not statistically significant. Authors have responded by arguing that this is typical for this area of research. I don t find this answer satisfying. For example, consider P DARTS (Chen et al., 2019). P DARTS improves over NA DARTS (the proposed method) on CIFAR 10 and ImageNet and on CIFAR 100 they are on par given the standard deviation of NA DARTS (see Tables 4 and 5). Moreover, the search cost of P DART is 0.27% of NA DARTS (Table 4). So P DARTS has clear advantage over NA DARTS.  Given the above issues, I recommend rejecting the paper. I hope authors would take feedbacks from the reviewing process into account to improve the paper and resubmit. 
This paper presents a marginally interesting idea   that of an interaction tensor that compares two sentence representations word by word, and feeds the interaction tensor into a higher level feature extraction mechanism.  It produces good results on multi NLI and SNLI datasets.  There is some criticism about comparing with several baselines for multi NLI where there was a restriction of not using inter sentence comparison networks, but the authors do compare with a similar approach without that restriction and shows improvements.   However, there is no solid error analysis that shows what type of examples this interaction tensor idea captures better than other strong baselines such as ESIM. Overall, the committee feels this paper will add value to the conference.
This paper aims to improve on the intrinsically motivated goal exploration framework by additionally incorporating representation learning for the space of goals. The paper is well motivated and follows a significant direction of research, as agreed by all reviewers. In particular, it provides a means for learning in complex environments, where manually designed goal spaces would not be available in practice. There had been significant concerns over the presentation of the paper, but the authors put great effort in improving the manuscript according to the reviewers’ suggestions, raising the average rating by 2 points after the rebuttal. 
Main content:  [Blind review #3] The authors propose a metric based model for few shot learning. The goal of the proposed technique is to incorporate a prior that highlight better the dissimilarity between closely related class prototype. Thus, the proposed paper is related to prototypical neural network (use of prototype to represent a class) but differ from it by using inner product scoring  as a similarity measure instead of the use of euclidean distance. There is also close similarity between the proposed method and matching network.  [Blind review #2] The stated contributions of the paper are: (1) a method for performing few shot learning and (2) an approach for building harder few shot learning datasets from existing datasets. The authors describe a model for creating a task aware embedding for different novel sets (for different image classification settings) using a nonlinear self attention like mechanism applied to the centroid of the global embeddings for each class. The resulting embeddings are used per class with an additional attention layer applied on the embeddings from the other classes to identify closely related classes and consider the part of the embedding orthogonal to the attention weighted average of these closely related classes. They compare the accuracy of their model vs others in the 1 shot and 5 shot setting on various datasets, including a derived dataset from CIFAR which they call Hierarchical CIFAR.     Discussion:  All reviews agree on a weak reject.     Recommendation and justification:  While the ideas appear to be on a good track, the paper itself is poorly written   as one review put it, more like notes to themselves, rather than a well written document to the ICLR audience.
All reviewers are very positive about this paper. The reviewer with the lowest score did independent experiments that show that the authors  method works well, and has had an extensive discussion with the authors that justifies a higher score. The paper is potentially very valuable to practitioners, since it shows how to compensate for a training set that is not representative of the test data.  Suggestion from the area chair to the authors: Briefly discuss the relationship between influence scores and propensity scores, which are standard in the literature on causal modeling and on sample selection bias, as in https://jmlr.csail.mit.edu/papers/volume10/bickel09a/bickel09a.pdf for example.
The provides a complexity theoretic look at GANs. The exposition is multi disciplinary, and in my personal opinion, it is an interesting look at the GANs in the context of random number generators.
This paper adapts the semi supervised DP learning methods based on voting to FL. Specifically, PATE and private kNN. The adaptation is fairly straightforward as those methods rely on averaging of votes a primitive that is a standard part of FL. The framework assumes that unlabeled data from the same distribution is available to the server, a very strong assumption. As pointed out in the reviews, the empirical evaluation has a number of major issues. For example, the comparison is with fully supervised SGD based techniques instead of a gradient based semi supervised approach. 
The reviewers are in consensus that this paper is not ready for publication: cited concerns include simple (interesting) ideas but need to be carefully analyzed empirically, contextualized (other similar studies exist), identifying convincing empirical evidences,. etc.   The AC recommends Reject.
  The paper aims at controllable generation by introducing an additional "content conditioner" block in the Transformer models. The paper further provides 4 different variants of a pre training task to train the content conditioner model.   While the proposed approach seems an incremental contribution over CTRL and PPLM, certain reviews praised the approach being novel while keeping the architecture changes minimal. Overall, reviews indicate that the overall proposed method of fine grained controlled generation with self supervision is valuable, and empirical results support its effectiveness.   All reviewers initially raised concerns regarding clarity and lack of human evaluation. However, clarity issues seem to be resolved through author/reviewer discussions and the updated revision.  R3 had important concerns regarding topic and sentiment relevance evaluations.   While the reviewer remains unconvinced after discussions with authors, after carefully reading the revised paper and discussions, I feel that the authors tried to address this point fairly  through their additional experiments and also edited their contribution statement accordingly.  Overall, at least two reviewers sounded very excited about this work and other than R3 s concerns, the general sentiment about this work was positive. Therefore, I recommend weak accept.    There are still some writing issues that I strongly encourage authors to carefully address in the future versions. Quoting from reviewer discussions:  > Differentiability of the adversarial loss. Authors just added one statement saying " Through continuous approximation.." without any more details are given, which continuous approx was used (Gumbel softmax?) and how they overcame the problem of its training instability.   > Table 6, can be misleading, authors bold the results when cocon+ is performing better than baselines (mostly in content similarity) but not the other way around topic/sentiment accuracy. The latter is arguably more important.
All reviewers agree that the paper should be rejected and there is no rebuttal.
The paper studies the problem of how to construct orthogonal convolutional layers. It is known that a convolution layer is orthogonal if and only if its filters are obtained by certain Fourier operations on an orthogonal matrix. Previous work proposes to learn this orthogonal matrix, parameterized either through Cayley transform, or the exponential of a skew symmetric matrix. This requires spectral computations with large matrices. The idea of this submission is to reduce the computational cost associated with this construction by letting this “core” matrix P be a periodic extension of a smaller orthogonal matrix P_0. Because of cancelations in the inverse DFT, this leads to sparse filters which can be implemented by dilated convolution.   The review process generated a very detailed discussion between authors and reviewers, with several important clarifications. Reviewers generally found that the paper contributes a novel construction of orthogonal convolution layers, with better efficiency at test time. Remaining concerns held by some reviewers include the limitations vis previous constructions of orthogonal convolution layers, questions about the efficacy of use of a Taylor expansion, certain minor limitations of the experiments. After detailed interaction with the authors, the reviewers converged to a decision to accept, motivated by the novelties of the construction and its advantages for test time efficiency.
This paper shows how to back propagate through a kernel between graphs that counts common random walks of infinite length between the graphs. Reviewers tend to agree that the paper is well written and the technical contributions are sound. However, there are concerns about the significance and novelty of the method relative to related work, alongside mixed experimental results. Overall that puts it as a very borderline paper. In the rebuttals, the authors argued for the significance of the contribution, but reviewers were generally unconvinced.
The reviewer concerns generally centered around the novelty of replacing the distance metric for a policy constraint. While the authors clarified many of the reviewer concerns and added some additional comparisons, in the end it was not clear why the proposed approach was interesting: while it is true that this particular distance metric has not been evaluated in prior work, and the result would have been interesting if it resulted in some clear benefits either empirically or theoretically, in the absence of clear and unambiguous benefit, it s not clear how valuable this concept really is. After discussion, the reviewers generally found the paper to not be ready for publication in its present state.
This paper proposes a GNN architecture for multi relational data to better address long range dependencies in graphs. The proposed GR GAT model is a variant of graph attention networks (GAT) with, among other modifications, vector based edge type embeddings and GRU type updates. Results are presented on AIFB, AM, and on synthetic benchmarks.  The reviewers agreed that this is an interesting contribution and that the results on the chosen synthetic benchmarks are insightful, but that experimental evaluation on real data and overall motivation of the architecture is lacking. In the rebuttal period, the authors have improved the writing and strengthened the motivation of the paper. However, given the limited amount of time, the authors were not able to sufficiently address the lack of experimental validation on real data (beyond AIFB & AM). I am inclined to agree with the reviewers that this paper needs significantly more work on the experimental evaluation, the overall presentation needs to be refined and it needs to more carefully analyse the effect of each individual architectural modification to meet the bar for acceptance. 
The authors propose a data free quantization method that can be applied post training quantization without backpropagation.  The method takes advantage of approximate Hessian information in a certain scalable approximate way. Based on the assumptions and deductions in the paper, SQuant tries to optimize constrained absolute sum of error (CASE) instead of MSE.  There are good empirical results showing the effectiveness of the method, and the paper is well written, and the method should be of broad interest.
The paper proposes a modification for adversarial training in order to improve the robustness of the algorithm by developing an annealing mechanism for PGD adversarial training. This mechanism gradually reduces the step size and increases the number of iterations of PGD maximization. One reviewer found the paper to be clear and competitive with existing work, but raised concerns of novelty and significance. Another reviewer noted the significant improvements in training times but had concerns about small scale datasets. The final reviewer liked the optimal control formulation, and requested further details. The authors provided detailed answers and responses to the reviews, although some of these concerns remain. The paper has improved over the course of the review, but due to a large number of stronger papers, was not accepted at this time.
The paper presents a novel idea with a compelling experimental study. Good paper, accept.
This work proposes to study the generalization capabilities of RL algorithms using contextual decision processes (CDPs). CDPs allows to study generalization similar to how we are used to studying generalization in supervised learning, and can separate the generalization capabilities of a learned agent wrt observation, state and action space. This proposed measure for generalization is used in an extensive study on grid world domains to evaluate existing algorithms that aim to improve generalization.  **Strengths** This manuscript is well written and the work is well motivated A novel perspective and way of measuring generalization of learned agents An empirical study that compares existing algorithms on how well they generalize in observation, state, action spaces  **Weaknesses** Some clarity issues existed (missing links to existing literature, experimental details)  empirical study is (out of necessity) limited to small scale grid worlds no deeper analysis of the results, why do algorithms perform the way they do from this novel perspective of generalization, which makes it hard to understand how one could choose an algorithm for larger scale settings which don t allow for this type of analysis  **Rebuttal** The authors updated the paper to improve the parts that were unclear, and had an extensive discussion with reviewers on the intuition of the results and converging on take aways. Unfortunately, this intuition and take aways have not been added.   **Summary** While I understand the authors wish to not speculate on intuition, I agree with the reviewers that without (experimentally supported) take aways the provided analysis is incomplete. Understanding why each algorithm achieves the performance they do wrt this novel way of measuring generalization is the only way the proposed method to measure generalization and the evaluation can be used to draw conclusions about more general problem settings. Thus, although this is a very promising direction on an important problem, the manuscript is not ready yet for publication.
The paper received mostly positive comments from experts. To summarize:  Pros:   The paper provides complex counterparts for typical architectures / optimization strategies used by real valued networks. Cons:   Although the authors include plots explaining how nonlinearities transform phase, intuition about how phase gets processed can be improved.   Improving evaluations: Wisdom et al. computes log magnitude; real valued networks may not be suited for computing real / complex numbers which have a large dynamic range, like the complex spectra. So please compare performance by estimating magnitude as in Wisdom et al.   Please add computational cost, in terms of the number of multiplies and adds, to the final version of the paper.  I am recommending that the paper be accepted based on these reviews. 
This paper studies the problem of learning a graphical model given observational and experimental data. The main novelty is the use of interventions to avoid the acyclicity constraint that plagues existing methods. Although this idea is quite standard and well known, the generality of the approach merits consideration. After the discussion, there was a consensus among the reviewers to accept this paper. Some valid concerns have been raised and we expect that the authors will take into account all of the suggestions raised by the reviewers.
This paper proposes a novel method for training neural rough differential equations, a recent model for processing very long time series data. The method involves a lower dimensional embedding of the log signature, which is obtained via pretrained autoencoder to reduce overhead. The results show significant and consistent improvements over previous methods on long time series data.  Overall, the reviewers and I all agree that this paper offers a novel and impactful contribution leading to significant improvements over previous state of the art methods for training neural rough differential equations. I recommend acceptance.
The paper studies difficulties in training deep and narrow networks. It shows that there is high probability that deep and narrow ReLU networks will converge to an erroneous state, depending on the type of training that is employed. The results add to our current understanding of the limitations of these architectures.   The main criticism is that the analysis might be very limited, being restricted to very narrow networks (of width about 10 or less) which are not very common in practice, and that the observed collapse phenomenon can be easily addressed by non symmetric initialization.   There were some issues with the proofs that were covered in the discussed between authors and reviewers. The revision is relatively extensive.   This is a borderline case. The paper receives one good rating, one negative rating, and a borderline accept rating. Although the paper contributes interesting insights to a relevant problem that clearly needs contributions in this direction, the analysis presented in the paper and its applicability in practice seems to be very restrictive at this point.  
I am somewhat of two minds from the paper. The authors show empirically that adversarial perturbation error follows power law and looks for a possible explanation. The tie in with generalization is not clear to me and makes me wonder how to evaluate the significance of the finding of the power law distribution..  On the other hand, the authors present an interesting analysis, show that the finding holds in all the cases they explored and also found that architecture search can be used to find neural networks that are more resilient to adversarial search (the last shouldn t be surprising if that was indeed the training criterion).  All in all, I think that while the paper needs a further iteration prior to publication, it already contains interesting bits that could spur very interesting discussion at the Workshop.  (Side note: There s a reference missing on page 4, first paragraph)
The paper is well written and deals with a simple yet interesting introduction of approxiThe paper is well written and deals with a simple yet interesting introduction of approxiThe paper is well written and deals with a simple yet interesting introduction of approxiThe paper is well written and deals with a simple yet interesting introduction of approxiThe paper is well written and deals with a simple yet interesting introduction of approxiThe paper is well written and deals with a simple yet interesting introduction of approxiThe paper is well written and deals with a simple yet interesting introduction of approximate Boolean logic activation functions. A number of comparison experiments showed intriguing differences for problems with potential logical structures. Authors suggest a probabilistic rational/motivation, i.e., computation in logit space, however, more theoretical investigation is critically needed to answer why they perform the way they do. There are a lot of activations in the literature, so perhaps it is not easy to make a distinct contribution in performance. Despite the large number of experiments the reviewers were not convinced on how they support authors claims and contributions. The reviewers and AC strongly encourage the authors to keep the direction and improve the paper for another conference.
The paper proposes multiplicative filter networks (GaborNet and FourierNet) as functional approximations of deepnets. The proposed networks are a sequence of multiplications linear functions of sinusoidal or Gabor filters. The authors show that in some cases the performance of proposed networks outperforms the existing deepnets using ReLu activations. This representation is notably simpler as well. Moreover, compared to classical Fourier approach, the proposed method scales to higher dimensions in practice as well. The downside of the paper is that it is not clear how to empirically use exponentially many Fourier functions. Moreover, proposed methods have more parameters, and the additional parameters are linear in size of the hidden layer.  The paper is clearly written and the authors improved the quality of the paper and added additional experiments to support their claim through the review process and I appreciate that.
Dear authors,  All reviewers pointed to severe issues with the analysis, making the paper unsuitable for publication to ICLR. Please take their comments into account should you decide to resubmit this work.
A nice paper and very close to being good.  But the focus on hyperparameter tuning of the optimisation method is really not novel, and the experimental validation is not strong enough.  With both theory and experimental just being marginal improvements, the paper is not considered quite ready yet.  Strong suggestion to improve on the weaknesses of the paper and resubmit – next time you ll have a clear acceptance.
The reviewers are unanimous in their evaluation of this paper, and I concur.
The paper proposes a new method for representation learning of time varying graphs which uses a streaming snapshot model to describe graphs on different time scales and meta learning for adaption to unseen graphs. Reviewers highlighted as strengths that the paper proposes an interesting approach for modeling temporal dynamics in graphs which of interest to the ICLR community. However, reviewers raised also concerns regarding the novelty of contributions, the empirical evaluation (also with regard to related work), as well as the clarity of presentation. In addition there was no author response. All reviewers and the AC agree therefore that the paper is not yet ready for publication at ICLR at this point.
Three reviewers suggest acceptance. Reviewers were impressed by the thoroughness of the author response. Please take reviewer comments into account in the camera ready. Congratulations!
The paper proposes an auto encoder framework IMA, a scalable model that learns the importance of modalities along with robust multimodal representations through a novel cross covariance based loss function, in an unsupervised manner. They have compared their approach to SOTA methods via multiple experiments and shown how IMA gives better performance.  The authors have addressed some of the reviewers  feedback. However, as pointed out by the reviewers, the experimental section needs better analysis of results and comparison to other methods, and the modeling section needs to be better explained and motivated. The authors have made changes in their revision, however the ICLR review process does not allow for checking the camera ready. Since we cannot accept the paper in its current form (or with small variations) and there have been many competitive submissions, we would encourage the authors to make their revisions and resubmit to other venues.
This paper proposes OpenCos for semi supervised learning that can leverage unsupervised information in open set scenarios where samples can be out of class.  They first pre train by learning an unsupervised representation using SimCLR on both the labeled and unlabeled data.  Then, they detect out of class samples in the unlabeled set based on similarity measures on the representation learned in the previous step.  The unlabeled data can now be split into in class and out of class.  OpenCos optimizes (8) which combines a semi supervised loss for in class unlabeled data and an auxiliary cross entropy loss with soft labels for the out of class samples.  Finally, they perform an auxiliary batch normalization.  The paper is easy to read and clearly structured.  It also places the work well with respect to related work.  The proposed approach makes sense; however, as pointed out by the reviewers, the novelty is marginal.  The technical innovation seems to be an extension of SimCLR and the auxiliary batch norm of Xie et al. 
Evaluating simple baselines for continuous control is important and nearest neighbor search methods are interesting. However, the reviewers think that the paper lacks citation and comparison to some prior work and evaluation on more challenging benchmarks.
The average review rating is 5.5 which means it’s somewhat borderline. One of the reviewers planned to increase the score but apparently didn’t do so formally. A subset of the main pros and cons the reviewers pointed out are:   Pros:  “Some empirical support is provided for the theory.” “ It is particularly interesting that the authors show that the second order effect of the SGD noise in the Hessian induces a power law distribution over the iterates.”  Cons:  “The escaping efficiency of the power law dynamic is only analyzed in low dimension case. ...” The author responded that Theorem 7 proves the multi dimensional case. But the AC noted that it’s very likely that escaping time is exponential in dimension (because kappa needs to be larger than d as the author noted and the det() might also be exponential in d. The author did say in the revision that the dimension should be considered as the effective dimension of the hessian, but the AC couldn’t find a formal argument about it.) “The assumptions made are somewhat strong and may not hold in some cases...”  The reviewers also had a few clarity questions which the author addressed in revisions with re organized writing. The AC weighed the pros and cons and found that the unclarity and potential exponential escaping time in the multi dimensional case outweigh the pros.     
This paper aims to address the problem of cross modal semi supervised few shot learning with noisy data, and proposed a robust cross modal semi supervised few shot learning (RCFSL) based on Bayesian deep learning. The approach combines several existing techniques for tackling a new problem in a non trivial approach. Empirical results demonstrate the effectiveness of the proposed method to some extent.   While the proposed integrated complex approach seems to be novel in the proposed unique setting, there are some major concerns from the reviewers. One concern is about the lack of clear justification on technical contributions for the proposed methodology in the complex settings. In particular, it lacks of comprehensive ablation studies for analyzing and understanding the source of gains by the proposed complex method, and the baselines in the experiments also do not look strong enough. In addition, many aspects of the paper writing and presentation are not satisfied (e.g., the math formulation in Section 2 is densely presented making it difficult to follow).   Overall, this is a borderline case, where the paper did contribute a new method for the interesting cross modal semi supervised few shot learning task, but some major concerns on the weaknesses remain at its current form. Therefore, it cannot be recommended for acceptance. Nonetheless, I hope authors can improve the paper by fully addressing these issues and hope to see it accepted in the near future.
The paper presents a new computational framework, grounded on Forward Backward SDEs theory, for the log likelihood training of Schrödinger Bridge and provides theoretical connections to score based generative models. The presentation of the results is not satisfactory (the algorithm should be clarified in several places and the notation is not accurate which raises doubts about the soundness of the method). The paper is thus very hard to read for the non experts on the subject. Furthermore, some reviewers raise concerns about the similarity of this method to other algorithms that were never cited in the paper. Finally, the empirical analysis, as of now, is limited.  In the rebuttal the authors carefully addressed lots of the comments. However paper s presentation still needs to be substantially improved (de densification of the paper would be extremely important since now the main narrative is very convoluted). The authors made several changes in the manuscript, but detailed discussion regarding training time complexity still seems to be missing (main body and the Appendix) in the new version of the manuscript, even though this was one of the main raised concerns. Overall, the manuscript requires major rewriting. Since the comments regarding the content were successfully addressed (the reviewers are satisfied with detailed answers given by the authors), the paper satisfies the conference bar and can be accepted.
All reviewers expressed consistent enthusiasm on this submission during the review process. No reviewers expressed concerns and objections to accept this submission during discussion. It is quite clear this is a strong submission and deserves accept.
The paper initially received mixed ratings, with one reviewer strongly supporting the paper given that the idea of combining unrolled algorithms and NAS is new and interesting, and one reviewer not convinced by the significance of the results. His/her main concern was the use of synthetic data only, which is not realistic. This was a legitimate concern as the performance of sparse estimation algorithms can change drastically when there is correlation in the design matrix. See for instance, the benchmarks in  F. Bach, R. Jenatton, J. Mairal and G. Obozinski. Optimization with Sparsity Inducing Penalties.   The rebuttal addresses this concern in a satisfactory manner and the area chair is happy to recommend an accept.
This paper proposes a way to transform word vectors based on a binary attribute (e.g. male/female) based on reflection, with the property that applying the reflection operator twice, the vector for a word is left unchanged.  By identifying parameterized mirror planes for each word, the proposed method can leave neutral words left unchanged.  The paper received 3 weak accepts.  There was initially one reject, but the revisions convinced the reviewer to update their score to a weak accept.  Overall, the reviewers appreciated the idea of reflection based binary word attribute transfer.   suggestions, the authors made small improvements to the writing, added missing citations, as well as additional results for another word embedding (GloVE) and another dataset (antonyms).  One of the main remaining weakness of the work, is still the small dataset.  Although somewhat alleviated by the inclusion of the antonym dataset, this is still a weakness of the paper.    The AC agrees that the paper has an nice idea and is well presented.  However, the work is limited in scope and is likely to be of limited interest to the ICLR community and would be more appreciated in the NLP community.  The authors are encouraged to improve upon the work, and resubmit to an appropriate venue.   
This paper was reviewed by 4 reviewers who scored the paper below acceptance threshold even after the rebuttal. Reviewer 4 is concerned about motivation, Reviewer 2 rightly points out that there exist numerous works that use some form of spectral layers in a deep setting on challenging datasets   something lacking in this work. Reviewer 3 is concerned about limited discussion on lie groups and the overall benefit of expm(.). Reviewer 1 reverberates the same comments regarding insufficient experiments,  comparisons and limited motivation. We encourage authors to consider all pointers given by reviewers in any future re submission.
This paper proposes an algorithm to address the disparate effect that DP has on the accuracy of minority/low frequency sub populations. Unfortunately the work does not actually guarantee or analyze the resulting privacy guarantees. In particular it may provide much worse privacy (or no privacy at all) to the minority subpopulation. The paper also calls their algorithm "fair" without using an accepted term or a careful discussion of what an algorithm needs to satisfy to be considered "fair". Using a more technical term such "reducing the accuracy disparity" would make much more sense.   
This paper presents a new approach for learning binary latent variable models using evolutionary optimization.  Pros: * A new perspective to learning binary latent variables is proposed using evolutionary algorithms. * The proposed method works well on auxiliary tasks such as zero shot denoising and inpainting.   Cons: * The proposed evolutionary optimization performs poorly on the binary VAE problem.  * It has a high computational cost that will limit its application in real world problems. * An in depth comparison with prior work on learning discrete latent variables is missing. This may include MCMC based approaches, REINFORCE based techniques, REBAR or RELAX.   This paper presents an interesting direction for learning binary latent variables using evolutionary algorithms. However, the proposed method performs poorly on the binary VAE problem which is the core problem, targetted in this paper (See likelihood values for binary VAEs in Fig. 15). The reviewers have raised concerns regarding the computational complexity of the evolutionary method in practice. They have also criticized the missing baselines for the binary VAE experiments.   The authors have argued that the proposed method excels at auxiliary problems such as zero shot image denoising and inpainting. However, these problems are not the central problem of this submission, and naturally, they have not been discussed, reviewed, and evaluated thoroughly. They can be also addressed with non binary VAEs and other forms of generative models which are not discussed in the paper.   Given these concerns, we don t believe that this submission in its current form is ready for publication at ICLR.
This paper makes an interesting contribution to the literature on algorithmic recourse. More specifically, while existing literature assumes that there is a global cost function that is applicable to all the users, this work addresses this limitation and models user specific cost functions. While the premise of this paper is interesting and novel, there are several concerns raised by the reviewers in their reviews and during the discussion: 1) While the authors allow flexibility to model user specific cost functions, they still make assumptions about the kind of cost functions. E.g., they consider three hierarchical cost sampling distributions, each of which model percentile shift, linear shift, and a mixture of these two shifts. The authors do not clearly justify why these shifts and a mixture of these shifts is reasonable. Prior work already considers lot more flexible ways of modeling cost functions (in a global fashion). For example, Rawal et. al. 2020 actually learns costs by asking users for pairwise feature comparisons. Isn t this kind of modeling allowing more flexibility than sticking to percentile/linear shifts and their mixture? 2) Several reviewers pointed out that the main paper does not clearly explain all the key contributions. While the authors have updated their draft to address some part of this concern, reviewers opine that the methods section of the paper still does not discuss the approach and the motivation for the various design choices (e.g., why a mixture of percentile and linear shifts?) clearly. 3) Reviewers also opine that some of the evaluation metrics also need more justification. For instance, Why is fraction satisfied measured at k   1 i.e, FS@1 measured and why not FS@2 or FS@3? Will the results look different for other values of k here? 4) Given that Rawal et. al. 2020 is a close predecessor of this work, it would be important to compare with that baseline to demonstrate the efficacy of the proposed approach. This comparison is missing.   Given all the above, we are unable to recommend acceptance at this time. We hope the authors find the reviewer feedback useful.
The paper proposes to compress convolutional neural networks via weight sharing across filters of each convolution layer. A fast convolution algorithm is also designed for the convolution layer with this approach. Experimental results show (i) effectiveness in CNN compression, (ii) acceleration on the tasks of image classification, object detection and neural architecture search. While the authors addressed most of reviewers  concerns, the weakness of the paper which remains is that no wall clock runtime numbers (only FLOPS) are reported   so efficiency of the approach in practice in uncertain. 
The paper received mixed reviews. The proposed ideas are reasonable and it shows that unpaired data can improve the performance of unseen video (action) classification tasks and other related tasks. The authors rightfully argue that the main contribution is the use of unpaired, multimodal data for learning a joint embedding (that generalizes to unseen actions) with positive results, but not the use of attentional pooling mechanism. Despite this, as the Reviewer3 points out, technical novelty seems minor as there are quite many papers on learning joint embedding for multimodal data. Many of these works were evaluated for fine grained image classification setting, but there is no reason that such methods cannot be used here. The revision only compares against methods published in 2017 or before. So more comprehensive evaluation would be needed to fully justify the proposed method. In addition, it seems that the proposed method has fairly marginal gain for the generalized zero shot learning setting. Overall, the paper can be viewed as an application paper on unseen action recognition tasks but the technical novelty and more rigorous comparisons against recent related work are somewhat lacking. I recommend rejection due to several concerns raised here and by the reviewers. 
The reviewers had some initial concerns about this submission. While the authors  rebuttal does a good job to address these concerns, the reviewers still have some doubts about the contribution of this paper and potential impact. In particular, it is not clear whether the performance improvements observed with the proposed algorithms is due to the ability to correct for noisy rewards or whether there are multiple other explanations for the improvement in performance. This makes it hard to predict whether the proposed algorithms will actually be useful in settings where noisy rewards or demonstration data are present.
Novelty of the proposed model is low. Experimental results are weak.
The paper proposes a set of tricks leading to a new SOTA for sampling high resolution images. It is clearly written and the presented contribution will be of high interest for practitioners.
This is a proposed method that studies learning of disentangled representations in a relatively specific setting, defined as follows: given two datasets, one unlabeled and another that has a particular factor of variation fixed, the method will disentangle the factor of variation from the others. The reviewers found the method promising, with interesting results (qual & quant).  The weaknesses of the method as discussed in the reviews and after:    the quantitative results with weak supervision are not a big improvement over beta vae like methods or mathieu et al.   a red flag of sorts to me is that it is not very clear where the gains are coming from: the authors claim to have done a fair comparison with the various baselines, but they introduce an entirely new encoder/decoder architecture that was likely (involuntarily, but still) tuned more to their method than others.   the setup as presented is somewhat artificial and less general than it could be (however, this was not a major factor in my decision). It is easy to get confused by the kind of disentagled representations that this work is aiming to get.  I think this has the potential to be a solid paper, but at this stage it s missing a number of ablation studies to truly understand what sets it apart from the previous work. At the very least, there is a number of architectural and training choices in Appendix D   like the 0.25 dropout   that require more explanation / empirical understanding and how they generalize to other datasets.  Given all of this, at this point it is hard for me to recommend acceptance of this work. I encourage the authors to take all this feedback into account, extend their work to more domains (the artistic style disentangling that they mention seems like a good idea) and provide more empirical evidence about their architectural choices and their effect on the results.
This paper proposes a way to train Gaussian variational autoencoders that does not require the computation of empirical expectations but instead approximates the decoder network by its Taylor series. Results on 3 datasets show the competitiveness of the approach.  Based on the limited novelty of the approach, three (out of 4) knowledgeable reviewers recommend rejection and I agree. Variational autoencoders are simply doing variational inference in a specific model and, as one of the reviewers has pointed out, these types of approximations have been exploited in the inference world (before the popularization of the reparameterization trick)  for many years. Methods, where we replace a term in the joint distribution with a simpler function, are known in the variational inference world as local variational approximations, see, e.g. Murphy’s book (Machine Learning: A Probabilistic Perspective, 2012, Sec. 21.8) as a reference. The community has departed from such approaches as using the re parameterization trick is unbiased, more general (e.g., not limited to Gaussian encoders) and allows for highly automated methods (no need to do derivations on a case by case basis).  Nevertheless, I encourage the authors to thoroughly explore the literature on variational inference with regards to these types of approximations. It may well be the case that, in the future, we revert back to these methods if they perform well in practice with modern architectures. For this, more comprehensive evaluations and comparisons are needed.
This is a well written paper, outlining a class of assistive algorithms. Being more or less a survey paper, it could do a better job of discussing  inverse reinforcement learning  and  collaborative inverse reinforcement learning . It could also be slightly more general: for example the human dewcision function need not be known if we model the interaction as a Bayesian game (then the human might have a latent type, which can be inferred together with the reward function). The active reward learning problem is sometimes referred to as  preference elicitation . In the end, it was not clear that the discussion in this paper had any actionable insights for future models or algorithms in this area. 
I think the model itself is not very novel, as pointed by the reviewers and the analysis is not very insightful either. However, the results themselves are interesting and quite good (on the copy task and pMnist, but not so much the other datasets presented (timit etc) where it not clear that long term dependencies would lead to better results). Since the method itself is not very novel, the onus is upon the authors to make a strong case for the merits of the paper    It would be worth exploring these architectures further to see if there are useful elements for real world tasks   more so than is demonstrated in the paper    for example showing it on tasks such as machine translation or language modelling tasks requiring long term propagation of information or even real speech recognition, not just basic TIMIT phone frame classification rate.  As a result, while I think the paper could make for an interesting contribution, in its present form, I have settled on recommending the paper for the workshop track.   As a side note, paper is related to paper 874 in that an attention model is used to look at the past. The difference is in how the past is connected to the current model. 
This paper provides a number of interesting experiments for few shot learning using the CUB and miniImagenet datasets. One of the especially intriguing experiments is the analysis of backbone depth in the architecture, as it relates to few shot performance. The strong performance of the baseline and baseline++ are quite surprising. Overall the reviewers agree that this paper raises a number of questions about current few shot learning approaches, especially how they relate to architecture and dataset characteristics.  A few minor comments:   In table 1, matching nets are mistakenly attributed to Ravi and Larochelle. Should be Vinyals et al.   The notation for cosine similarity in section 3.2 is odd. It looks like you’re computing some cosine function of two vectors which doesn’t make sense. Please clarify this.   There are a few results that were promised after the revision deadline, please be sure to include these in the final draft. 
The pros and cons of the paper can be summarized below:  Pro: * The improvements afforded by the method are significant over baselines, although these baselines are very preliminary baselines on a new dataset.  Con * There is already a significant amount of work in using grammars to guide semantic parsing or code generation, as rightfully noted by the authors, and thus the approach in the paper is not extremely novel. * Because there is no empirical comparison with these methods, the relative utility of the proposed method is not clear.  As a result, I recommend that the paper not be accepted at this time.
The paper proposes a new variational inference based continual learning algorithm with strong performance.  There was some disagreement in the reviews, with perhaps the one shared concern being the complexity of the proposed method. One reviewer brought up other potentially related work, but this was convincingly rebutted by the authors. Finally, one reviewer had an issue with the simplicity with the networks in the experiments, but the authors rightly pointed out that the architectures were simply designed to match those from the baselines.  Continual learning has been an active area for quite some time and convincingly achieving SOTA in a new way is a strong contribution, and will be of interest to the community. Progress in a field is sometimes made by iteratively simplifying an initially complex solution, and this work lays in a brick in that direction. For these reasons, I recommend acceptance.    
The submission proposes a dynamic approach to training a neural net which switches between half and full precision operations while maintaining the same classifier accuracy, resulting in a speed up in training time. Empirical results show the value of the approach, and the authors have added additional sensitivity analysis by sweeping over hyperparameters.   The reviewers were concerned about the novelty of the approach as well as the robustness of the claims that accuracy can be maintained even in the accelerated, dynamic regime. After discussion there were still concerns about the sensitivity analysis and the significance of the results.  The recommendation is to reject the paper at this time.
The paper proposes a method to change the graph structure for better robustness against adversarial attacks. The reviewers commend the authors for a clearly written paper and promising results. Several reviewers expressed concerns about experimental validation (specifically, comparison to truncated SVD and choice of baselines), complexity, and novelty. The rebuttal and follow up discussion alleviated some of the concerns, but the reviewers still have outstanding issues, therefore the AC does not recommend accepting the paper.
The paper applies proximal iteration to Q learning, which significantly improves the performance of DQN. Reviewers agreed the paper is not ready for publication, for a couple reasons. DQN is quite far from current state of the art. Improvements therefore need to be well founded to be of broad interest. If the algorithm that is being improved is not competitive, there should be more general lessons that can be extracted from how and why the improvement works. Unfortunately, the reviewers felt that there was insufficient understanding of why proximal iteration helps.
The main idea of policy as inference is not new, but it seems to be the first application of this idea to deep RL, and is somewhat well motivated.  The computational details get a bit hairy, but the good experimental results and the inclusion of ablation studies pushes this above the bar. 
This paper argues that the widely adopted graph attention networks (GAT) have a shortcoming that with the static nature of the attention mechanism, they may fail to represent certain graphs. This paper presents an alternative, GATv2, a simple variant with the same time complexity as GAT but with more expressivity, able to represent the graphs that GAT fails to. This is shown both empirically and theoretically, with various tasks on synthetic as well as standard benchmark graphs.   GATs are of high interest to the ICLR community, and this paper makes fundamental progress in how attention works in GNNs. This is one of the few papers that present both empirical and theoretical analyses, and these findings will motivate others in the community to make further advances in this field.
This paper presents a state representation learning technique aiming at extracting only state features that are relevant to solve a given task. It combines several ideas, in particular (1) keeping only features that are relevant to take actions from an information theoretic point of view, (2) model based learning, and (3) sparsity inducing constraints. Experiments on CarRacing and VizDoom show that the proposed method outperforms existing baselines.  Although authors did a thorough job trying to address reviewers  comments during the discussion period, in the end most reviewers remained unconvinced by the submission in its current state, the main remaining concerns being: * Unclear description of the methodology and informal maths * A somewhat complex optimization objective that may require tuning many hyper parameters, and whose entire relevance isn t clearly demonstrated empirically  Overall I agree with these concerns, in particular the general feeling that the theoretical part is difficult to follow, with some apparent typos / mistakes (clearly the original submission had a lot of issues, given the original reviews that required the authors to fix several points). To give a concrete example, while reading the final revision I first ran into potential issues when defining the objective $H(a_t | ...)$ on p.4: * Although the left hand side is associated to a single timestep $t$, the right hand side is a sum over all values of $t$ * The definition of $q_{\phi}$ seems weird to me, in particular the fact that it takes $o_t$ as input (through $y_{1:t}$)  > this seems like a typo (?) and otherwise I don t really understand how this defines a proper definition over states * As at least one reviewer pointed out, authors start from the mutual information $I$ but drop the entropy term $H(a_t | R_{t+1})$ by claiming it doesn t matter since the goal is to learn the ASR. However, in that objective the distribution over actions $p_{\alpha}$ seems to be learnable (through the $\alpha$ parameters), so if we try to minimize the mutual information $I$ including over $\alpha$ it would have been important to retain the entropy over actions as well.  In terms of the relevance of the results, they look pretty good but: * The proposed algorithm ends up being somewhat complex, with a lot of terms in the loss (eq. 4), and a lack of empirical validation of what actually matters. I see a single ablation study in the Appendix (Fig. 10), and possibly also the comparison to VRL (but it isn t entirely clear to me what this baseline is implementing as it lacks details). I would have appreciated a more thorough empirical analysis of how each term in eq. 4 matters. * CarRacing experiments consistently use 21 dimensions "for a fair comparison", but this dimensionality was chosen specifically for and by ASR. As a result, it doesn t really look "fair" to me: a fairer comparison would have either selected the optimal dimensionality for each method, or shown results across a range of different dimensionalities.  I also have some concerns regarding the applicability of the algorithm: 1. Relying on random actions to build a world model only works if random actions allow sufficient enough exploration of the state space. There are many situations where this isn t a realistic assumption (also alluded to by at least one reviewer). 2. Minor: in the setup of eq. 1 the reward $r_t$ doesn t directly depend on $s_t$. I m not sure to which extent this matters for the proposed algorithm, but if this is a necessary condition for it to work properly, it may cause issues in many stochastic environments.  As a result, I am recommending rejection as I believe the paper is not quite ready for publication. I would encourage the authors to try and simplify the presentation (the paper is very notation heavy and not an easy read), focusing on showing convincing theoretical and empirical justification for all components of the proposed technique.
Graph neural networks (incl. GCNs) have been shown effective on a large range of tasks. However, it has been so far hard (i.e. computationally expensive or requiring the use of heuristics) to apply them to large graphs. This paper aims to address this problem and the solution is clean and elegant. The reviewers generally find it well written and interesting. There were some concerns about the comparison to GraphSAGE (an alternative approach), but these have been addressed in a subsequent revision.  + an important problem + a simple approach + convincing results + clear and well written 
This paper forms a good contribution to the active area of adversarial training.  The main issue with the original submission was presentation quality and excessive length.  The revised version is much improved.  However, it still needs some work on the writing, in large part in the transferability section but also to clean up a large number of non native formulations like missing/extra determiners and some awkward phrasing.  It should be carefully proofread by a native English speaker if possible.  Also, the citation formatting is incorrect (frequently using \citet instead of \citep).
This paper presents a new benchmark for architecture search. Reviewers put this paper in the top tier. I encourage the authors to also cite https://openreview.net/forum?id SJx9ngStPH in their final version. 
The papers studies machine learning tasks in the presence of adversarially corrupt data (during training). In particular, it is assumed that the labels of a small constant fraction of the datapoints are arbitrarily corrupted.  The paper proposes a natural method to solve this problem and evaluates it on various datasets. As pointed out by the reviewers, the theoretical contributions of this paper are subsumed by a number of prior works (which were not initially cited). The experimental results of the paper are interesting. However, the method proposed  and evaluated is not particularly novel. In my opinion, the problems studied in this submission are important (in particular, the memory/space consideration in the context of robustness). However, this work still needs work and is not ready for publication.
The paper considers the OPE problem under the contextual bandit model with continuous action.  They studied the model of a piecewise constant value function according to the actions.   The assumption is new, though still somewhat restrictive as it requires the piecewise constant partitions to be the same for all x.  The proposed algorithm estimates the partitions, and then used it to build a doubly robust estimator with stratified importance sampling (fitting an MLP for each partition separately).   The reviewers have mixed views about the paper.  The following is the AC s evaluation based on reading the paper and consolidating the reviewers  comments and the authors  responses.  Pros:     The algorithm is new and it makes sense for the new problem setup  (though computationally intractable)   The experimental results outperform the baseline and reinforces the theory. But it s a toy example at best.  Cons:    The method is called "Q learning" but it is somewhat disappointing to see that it actually applies only to the contextual bandit model (without dynamics).  There is quite a bit of branding issues here. I suggest the authors to revise it to reflect the actual problem setup.     The estimator is assumed to be arg min, but the objective function is non convex and cannot be solved efficiently in general, e.g., (3) involves searching over all partitions... and (4) involves solving neural network partitions.  In other words, the result applies to a hypothetical minimizer that the practical solvers may or may not obtain (the authors cited Scikit Learn for the optimization algorithm and claims that the optimization problem can be solved, which is not the case ...  the SGD algorithm can be applied to solve it, but it does not necessarily find you the solution).     The theory is completely asymptotic and generic. There is no rate of convergence specified, and no dependence on the number of jumps |D_0| at all in Theorem 1.       Theorem 3 is obnoxiously sloppy. The assumptions are not made explicit (do you need Assumption 1 and 2, what is the choice of \rho? ) The notion of "minimax rate" is not defined at all.   Usually the minimax rate is the property of problem setting,  i.e., Min over all algorithms, and Max over all problems with in a family.  However, in the way the authors described the results in Theorem 3,  it says the "the minimax convergence rate of kernel based estimator is Op(n^{−1/3})."  which seems to be restricting the algorithms instead.  Such non typical choices require clear definitions and justification.    Based on what is stated, it really appears that the authors are just comparing upper bounds of the two methods.  I looked at the appendix and while there is a "lower bound analysis", the bound is not information theoretical, but rather a fixed example where an unspecified family of algorithms (I think it is a specific kernel smoothing method with a arbitrary choice of the bandwidth parameter h) will fail.   Suggestions to the authors:     Instead of a piecewise constant (and uniformly bounded) function, why not consider the total variation class, which is strictly more general and comes with the same rate?    For formalizing the lower bound, I suggest the authors to look into classical lower bounds for linear smoother, e.g., Donoho, Liu, MacGibbon (1990); which clearly illustrates that kernel smoothing type methods do not achieve the minimax rates; and that wavelets based approaches, locally adaptive regression splines, and fused lasso (You can think about the  Haar Wavelets as a basis function of piecewise linear functions ) do.   The authors can improve the paper by ensuring that the theoretical parts are clearly and rigorously presented; and perhaps to iron out the more useful finite sample analysis that depends on model parameters of interest.
The paper explores a solution for mixed precision quantization. The authors view the weights in their binary format, and suggest to prune the bits in a structured way. Namely, all weights in the same layer should have the same precision, and the bits should be pruned from the least significant to most significant. This point of view allows the authors to exploit techniques used for weight pruning, such as L1 and group lasso regularization.  Although the field of quantization and model compression/acceleration is quite mature by now and has a large body of works, this paper is novel in its approach. Although the improvements provided over SoTA results are not very large, I believe that the novelty of the approach would make this paper a welcome addition to ICLR.  There are a few issues to be dealt with pointed out by the reviewers such as confusing terminology or required clarifications, but these are minor revisions that I trust the authors will be able to add to their paper. 
This paper treats the problem of running gradient descent ascent (GDA) in min max games with a different step size for the two players. Earlier work by Jin et al. has shown that, when the ratio of the step sizes is large enough, the stable fixed points of GDA coincide with the game s strict local min max equilibria. The main contribution of this paper is an explicit characterization of a threshold value $\tau^*$ of this ratio as the maximum eigenvalue of a specific matrix that involves the second derivatives of the game s min max objective at each (strict local) equilibrium.  This paper generated a fairly intense discussion, and the reviewers showed extraordinary diligence in assessing the authors  work. Specifically, the reviewers raised a fair number of concerns concerning the initial write up of the paper, but these concerns were mostly addressed by the authors in their revision and replies. As a result, all reviewers are now in favor of acceptance.  After my own reading of both versions of the paper and the corresponding discussion, I concur with the reviewers  view and I am recommending acceptance subject to the following revisions for the final version of the paper: 1. Follow the explicit recommendations of AnonReviewer3 regarding the numerical simulations (or, failing that, remove them altogether). [The authors  phrase that "The theory we provide also does not strictly apply to using RMSprop" does not suffice in this regard] 2. Avoid vague statements like $\tau \to \infty$ in the introduction regarding the work of Jin et al. and state precisely their contributions in this context. In the current version of the paper, a version of this is done in page 4, but the introduction is painting a different picture, so this discussion should be transferred there. 3. A persisting concern is that the authors  characterization of $\tau^*$ cannot inform a practical choice of step size scaling (because the value of $\tau^*$ derived by the authors depends on quantities that cannot be known to the optimizer). Neither the reviewers nor myself were particularly convinced by the authors  reply on this point. However, this can also be seen as an "equilibrium refinement" result, i.e., for a given value of $\tau$ only certain equilibria can be stable. I believe this can be of interest to the community, even though the authors  characterization cannot directly inform the choice of $\gamma_1$ and $\gamma_2$ (or their ratio).  Modulo the above remarks (which the authors should incorporate in their paper), I am recommending acceptance.
This paper introduces the idea of cascading decision trees.  The reviewers agree that this is a potentially novel and valuable idea, but they also agree that the paper fall short in execution.  The paper would be substantially strengthened with more theoretical analysis, more discussion of why cascading decision trees are useful, and most importantly substantially more empirical evaluation, especially with more data sets and more baselines for comparison.
The paper proposes a hierarchical diversity promoting regularizer for neural networks. Experiments are shown with this regularizer applied to the last fully connected layer of the network, in addition to L2 and energy regularizers on other layers. Reviewers found the paper well motivated but had concerns on writing/readability of the paper and that it provides only marginal improvements over existing simple regularizers such as L2. I would encourage the authors to look for scenarios where the proposed regularizer can show clear improvements and resubmit to a future venue. 
The submission presents an approach to estimating physical parameters from video. The approach is sensible and is presented fairly well. The main criticism is that the approach is only demonstrated in simplistic "toy" settings. Nevertheless, the reviewers recommend (weakly) accepting the paper and the AC concurs.
The paper proposes a method to learning rate scheduling that uses information form the eigenvalues of the Hessian. It shows that this scheduler obtains the minimax optimal rate on the noisy quadratic problem; and, empirically, this scheduler demonstrates faster convergence on CIFAR 10 and ImageNet, when the number of epochs is small.  Using Hessian information in direct and indirect ways is of interest to the community, and the paper does a nice job illustrating that in a context of interest.
The paper considers a setting where the state of a (robotics) environment can be divided roughly into "context states" (such as variables under the robot s direct control) and "states of interest" (such as the state variables of an object to be manipulated), and learn skills by maximizing a lower bound on the mutual information between these two components of the state. Experimental results compare to DDPG/SAC, and show that the learned discriminator is somewhat transferable between environments.  Reviewers found the assumptions necessary on the degree of domain knowledge to be quite strong and domain specific, and that even after revision, the authors were understating the degree to which this was necessary. The paper did improve based on reviewer feedback, and while R3 was more convinced by the follow up experiments (though remarked that requiring environment variations to obtain new skills was a "significant step backward from things like [Diversity is All You Need]"), the other reviewers remained unconvinced regarding domain knowledge and in particular how it interacts with the scalability of the proposed method to complex environments/robots.  Given the reviewers  concerns regarding applicability and scalability, I recommend rejection in its present form. A future revision may be able to more convincingly demonstrate that limitations based on domain knowledge are less significant than they appear.
This work develops an approach to embed random graphs (some even with dependent edges, hence going beyond classical models such as Erdos Renyi G(n,p)) using GNNs, and uses these to develop approximation algorithms for solving NP hard scheduling problems, which typically involve some notion of minimizing weighted completion time (or equivalently, the reward incentivizes early completion, where the age of a job is a linear function of time). This is then used to schedule multiple identical robots  to solve a given set of spatially distributed tasks. The problems considered Multi Robot Reward Collection (MRRC) model vehicle routing, rideshare etc., and are well motivated.   This paper takes as motivation earlier work on “structure2vec” by Dai et al. (2016) that uses GNNs to (approximately) solve other NP hard graph problems: specifically, the random structure2vec developed here is used for an RL approach that learns near optimal solutions for the MRRC problems considered.   While the paper’s contributions were appreciated in general, its clarity, the fact that the (1 – 1/e) bound of Theorem 2 follows from classical work of Nemhauser et al. (1978), and the fact that real life examples were not considered, were considered weaknesses. The authors are encouraged to work on these aspects of the paper.  
This paper proposes a stochastic variance reduced extragradient algorithm. The reviewers had a number of concerns which I feel have been adequately addressed by the authors.  That being said, the field of optimizers is crowded and I could not be convinced that the proposed method would be used. In particular, (almost) hyperparameter free methods are usually preferred (see Adam), which is not the case here.  To be honest, this work is borderline and could have gone either way but was rated lower than other borderline submissions.
This paper proposes a neural architecture search method that uses balanced sampling of architectures from the one shot model and drops operators whose importance drops below a certain weight.  The reviewers agreed that the paper s approach is intuitive, but main points of criticism were:   Lack of good baselines   Potentially unfair comparison, not using the same training pipeline   Lack of available code and thus of reproducibility. (The authors promised code in response, which is much appreciated. If the open sourcing process has completed in time for the next version of the paper, I encourage the authors to include an anonymized version of the code in the submission to avoid this criticism.)  The reviewers appreciated the authors  rebuttal, but it did not suffice for them to change their ratings. I agree with the reviewers that this work may be a solid contribution, but that additional evaluation is needed to demonstrate this. I therefore recommend rejection and encourage resubmission to a different venue after addressing the issues pointed out by the reviewers.
This paper formulates the recommendation as a model based reinforcement learning problem. Major concerns of the paper include: paper writing needs improvement; many decisions in experimental design were not justified; lack of sufficient baselines; results not convincing. Overall, this paper cannot be published in its current form. 
Reviewers generally agree that the proposed method UMATO, a two phase optimization dimensionality reduction algorithm based on UMAP, is interesting and has potential, and that the paper is well written. However, there are several concerns with the current paper. In particular, R1 is not convinced by the performance of UMATO on real world datasets compared with previous methods such as t SNE (see the linked papers). Both R1 and R2 are concerned that given the 2 phase approach, UMATO might be much more adapted to clustered data than standard manifold embedding. They pointed out that in the Swiss  roll/S curve examples, UMATO stays very close to PCA, which is used for initialization, instead of globally unfolding the manifold as Isomap. These issues should be clarified/explored further for a better understanding and/or improvement of the current work.
All reviewers agree that the paper is well written and some of the experiments are interesting. However, the paper did not clearly highlight how this work fits in with prior research, neither did it show what the advantages of the presented homogeneous network are. The authors addressed some of these concerns in the rebuttal, but not enough to sway the reviewers. In the end all reviewers recommend rejection, and the AC sees no evidence to overturn this recommendation.
This work extends upon recent ideas to build a complete summarization system using clever attention, copying, and RL training. Reviewers like the work but have some criticisms. Particularly in terms of its originality and potential significance  noting "It is a good incremental research, but the downside of this paper is lack of innovations since most of the methods proposed in this paper are not new to us.". Still reviewers note the experimental results are of high quality performing excellent on several datasets and building "a strong summarization model." Furthermore the model is extensively tested including in "human readability and relevance assessments ".  The work itself is well written and clear.
The paper seeks to improve straight through estimators by combining them with the ideas for correcting the step direction to be closer to a natural gradient.   While some (modest) improvements are demonstrated experimentally, the paper critically lacks technical correctness and has quite some gaps when trying to derive the algorithm from the natural gradient and Rao Cramer bound. See public comments by reviewers and AC. The algorithm ends up to be a mirror descent with a mirror map, which is cheap to compute but not particularly well motivated. Moreover application of mirror descent to the activations (unlike the weights) is not well justified. The paper is rather unclear and hard to read also language wise. Please proofread _before_ submitting.
This paper proposes the use of Gaussian process regression embedded into a neural network architecture for few shot segmentation. In more detail, support and query images and support masks are fed through their encoders and their corresponding features are then used for Gaussian process regression to infer the distribution of the query mask encoding given the support set and the query images. The mean and the variance characterizing the GP predictive distribution is then fed into a CNN based decoder to make the final prediction (segmentation).  The method is evaluated on PASCAL 5^i and COCO 20^i datasets, showing the superiority of the proposed approach wrt several competitive baselines.   Overall, the reviewers found the approach of using GPs within the proposed architecture interesting and somewhat significant and novel to the few shot segmentation community. Technically, the proposed method does not develop a new algorithm and simply uses standard Gaussian process regression. The authors seemed to have addressed several concerns raised by the reviewers including the ablation study evaluating the influence of the GP module. However, the reviewers felt that there were quite a few changes/clarifications to the paper and new results that were not highlighted in the revised version, which made it difficult to provide a new assessment of the paper. Furthermore, the reviewers also thought that the authors did not provide convincing explanations in terms of the improvements from 1 shot to 5 shots, the not so good results when the model was trained with standard SGD without loss weighting and the rationale behind the success of the 5 shot setting.
The paper proposes a user interaction framework where users choose a subset of LFs from a family of LFs generated using some template (e.g. keywords for text classification).   The proposed criteria is not very surprising, but the authors present a practical and useful system that is well demonstrated both in the paper and the very careful author feedback.  These enhancements have also been incorporated in the revised version.    Apart from the literature pointed by the reviewers, here are some more papers that are related to this paper: 1. Gregory Druck, Burr Settles, Andrew McCallum: Active Learning by Labeling Features. EMNLP 2009: 81 90  2. 	Gregory Druck, Gideon S. Mann, Andrew McCallum: Learning from labeled features using generalized expectation criteria. SIGIR 2008: 595 602   3. Data Programming using Continuous and Quality Guided Labeling Functions. In AAAI, 2020.
This paper proposes a method called Dynamic Intermedium Attention Memory Network (DIAMNet) to learn the subgraph isomorphism counting for a given pattern graph P and target graph G. However, the reviewers think the experimental comparisons are insufficient. Furthermore,  the evaluation is only for synthetic dataset for which generating process is designed by the authors. If possible, evaluation on benchmark graph datasets would be convincing though creating the ground truth might be difficult for larger graphs. 
This paper presents a novel theoretical analysis for unsupervised domain adaptation based on f divergences. The reviews unanimously pointed out the interest and the quality of the theoretical part. However, some limitations in the experiments, presentation and the significance of the result have been raised. The authors provided a rebuttal that addresses some concerns. However, the reviewers agree that the experimental part still requires some extension to fully support the claim of the paper, as well as some writing improvement. The paper was evaluated to be not ready for ICLR, thus I recommend rejection. 
The paper presents a differentiable approximation of BLEU score, which can be directly optimized using SGD. The reviewers raised concerns about (1) direct evaluation of the quality of the approximation and (2) the significance of the experimental results. There is also a concern (3) regarding the significance of BLEU score in the first place, and whether BLEU is the right metric that one needs to directly optimize. The authors did not provide a response, and based on the concerns above (especially 1 2) I believe that the paper does not pass the bar for acceptance at ICLR.
I appreciate the experimental results, which includes a comparison against several baselines, however, I echo some of the concerns raised by the reviewers that the formulation is unclear and hard to follow. Moreover, the novelty over [Nachum, 2017] and [Haarnoja, 2017] seems small. Especially because [Nachum, 2017] also used expert trajectories to improve the performance in their experiments.  Detailed comment: The use of log sum exp state values is only valid for the optimal policy, so it is not clear how an on policy state value is replaced with the log sum exp state value. Also, because the equations that you derive characterize the optimal policy, I am not sure if you need importance correction at all.
Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers  comments into consideration to improve your submission for the camera ready. 
This manuscript proposes and analyzes a distillation approach to address heterogeneity in distributed learning. The main paper focuses on a relatively simple two agent kernel regression setting, and the insights developed are extended (and partially analyzed) for a multiagent setting.   There are four reviewers, all of whom agree that the method addresses an interesting and timely issue. However, reviewers are mixed on the paper score. While all reviewers agree that the setting is somewhat stylized, a subset of reviewers highlights that the results give some deep insight that might drive future analysis and implementation in the area. Other concerns raised include potential issues with the communication overhead and the simplicity of the kernel regression setting vs real world deep learning. There are initial concerns about whether the failure case is realistic, which the authors address. Extensions to the multi agent setting and a partial analysis are also addressed by the authors and partially satisfy the reviewers. Nevertheless, after reviews and discussion, the reviewers are mixed at the end of the discussion.   The area chair finds, first, that the paper is much improved, and much more applicable in the updated form than in the original version, and indeed, the insights from the simple model may be informative for practice. However, the concerns raised about the distance between theory and practice are valid. The final opinion remains borderline. Authors are encouraged to address the highlighted technical concerns in any future submission of this work. In particular, the muti0agent setting should probably be central in the discussion of this work. More ambitious empirical evaluation showing that the theory translates to practice )even if there is a gap) would also help.
The paper proposes a unified framework for point cloud upsampling, denoising, and completion through a two stage approach. It receives three reviews with three leaning to accept and one leaning to reject. Most of the reviewers like the proposed two stage approach for its simplicity and demonstrated strong performance. The reviewer recommending marginally below the acceptance threshold expresses concerns about missing comparison to neural shape implicit representation and a lack of insights on what is learned by individual layers in the network. While the meta reviewer agrees that having both would make the paper stronger, the meta reviewer feels the paper has enough merit and would like to recommend its acceptance.
The paper proposes a hybrid weighs representation method in deep networks. The authors propose to utilize the extra state in 2 bit ternary representation to encode large weight values. The idea is simple and straightforward. The main concern is on the experimental results. The use of mixed bit width for neural network quantization is not new, but the authors only compare with basic quantization method in the original submission. In the revised version of the paper, the proposed method performs significantly worse than recent quantization methods such as PACT and QIL. Moreover, writing can be improved, and parts of the paper need to be clarified.
This paper presents a method to defend neural networks from adversarial attack. The proposed generative cleaning network has a trainable quantization module which is claimed to be able to eliminate adversarial noise and recover the original image.  After the intensive interaction with authors and discussion, one expert reviewer (R3) admitted that the experimental procedure basically makes sense and increased the score to Weak Reject. Yet, R3 is still not satisfied with some details such as the number of BPDA iterations, and more importantly, concludes that the meaningful numbers reported in the paper show only small gains, making the claim of the paper less convincing. As authors seem to have less interest in providing theoretical analysis and support, this issue is critical for decision, and there was no objection from other reviewers. After carefully reading the paper myself, I decided to support the opinion and therefore would like to recommend rejection.  
The paper provides a theoretical study of what regularizations should be used in GAN training and why. The main focus is that the conditions on the discriminator that need to be enforced, to get the Lipshitz property of the corresponding function that is optimized for the generator. Quite a few theorems and propositions are provided. As noted by Reviewer3, this adds insight to well known techniques: the Reviewer1 rightfully notes that this does not lead to any practical conclusion.  Moreover, then training of GANs never goes to the optimal discriminator, that could be a weak point; rather than it proceeds in the alternating fashion, and then evolution is governed by the spectra of the local Jacobian (which is briefly mentioned). This is mentioned in future work, but it is not clear at all if the results here can be helpful (or can be generalized).  At some point of the paper it gets to "more theorems mode" which make it not so easy and motivating to read.  The theoretical results at the quantitative level are very interesting.  I have looked for a long time on Figure 1: does this support the claims? First my impression was it does not (there are better FID scores for larger learning rates). But in the end, I think it supports: the convergence for a smaller that $\gamma_0$ learning rate to the same FID indicated the convergence to the same local minima (probably). This is perfectly fine. Oscillations afterwards move us to a stochastic region, where FID oscillates. So, the theory has at least minor confirmation.   
All the reviewers are positive about the paper; R2 and R3 voted for clear accept. Overall, all the reviewers feel that evolution is comprehensive and the results are decent. There is a novel objective formulation that controls for motion diversity, disentanglement and content matching, outperforming existing methods across multiple datasets. High res videos at 1024x1024 are generated and there is cross domain video generation. Many good questions were raised by the reviewers, and they were addressed in details in the rebuttal. In particular, the question about subtle motion and short video sequences was raised (which was the concern that the AC had). The AC agrees with the reviewers that the paper warrants a publication. Please address the questions raised by the reviewers in the final version. 
The paper proposes a novel predictive model (e.g., from videos), called error encoding networks, by first learning a deterministic prediction model and then learning to minimize the residual error using latent variables. The latent variables given the sample are estimated by sampling from the prior then updating via gradient descent. The proposed method shows improved performance over the baselines. However, the qualitative results are not fully convincing, possibly because of (1) the limitation of the architecture, (2) suboptimal implementation/tuning of baselines (such as GAN and cVAE). 
The paper attempts to improve retrieval in open domain question answering systems, which is a very important problem. In this regards, the authors propose to utilize cross attention scores from a seq2seq reader models as signal for training retrieval systems. This approach overcomes typical low amount of labelled data available for retriever model. The reviewers reached a consensus that the proposed approach are interesting and novel. The proposed approach establish new state of the art performance on three QA datasets, although the improvements over previous methods are marginal. Overall, reviewers agree that the paper will be beneficial to the community and thus I recommend an acceptance to ICLR. 
This paper proposes the use of Bayesian prior upon initialization for predicting generalization performance of a neural network, and empirically shows that it can outperform flatness based measures. Understanding the underlying reasons that control generalization performance on neural networks is of great theoretical and practical importance, and reviewers find efforts in this direction valuable. However, they believe the submission in current state is not ready for publication.  Specifically, ZCFx believes the setup considered in the paper does not resemble a realistic situation, which makes claims about the Bayesian prior being a more robust predictor than flatness unsubstantiated. ZCFx appreicates authors  response and clarifications, but finds the concerns unresolved. ohft believes the paper is weak in a certain aspects, such as comparing across different architectures (including number of parameters), and comparing with SAM optimizer whose goal is to find flat minima and has shown to greatly improve the generalization performance. ohft acknowledged reading authors  response but the response did not help with changing the score of the paper. r1hF has some reservations about the novelty of the work and the limited experiments, which remained unresolved. r1hF suggests that the authors revise the paper to emphasize on the author s contribution in light of the previous work.  Based on reviewers  feedback, I suggest authors to resubmit after revising the draft to address the issues raised above.
The paper proposes a definition of the sensitivity of the output to random perturbations of the input and its link to generalization.  While both reviewers appreciated the timeliness of this research, they were taken aback by the striking similarity with the work of Novak et al. I encourage the authors to resubmit to a later conference with a lengthier analysis of the differences between the two frameworks, as they started to do in their rebuttal.
Reviewers raised various concerns and authors sent in no rebuttal. In view of the negative consensus, this paper then made a clear rejection case.
The reviewers agree that the idea of incorporating humans in the training of generative adversarial networks is interesting and worthwhile exploring. However, they felt that the paper fell short in providing strong support for their approach. The AC agrees. The authors are encouraged to strengthen their work and resubmit to a future venue.
The authors present a study on what maintains the stability of emerged communication protocols. To study this question the authors design experiments in bargaining communities of agents in 3 setups,  a) no punishment of restriction of liar agents b) allowing individual agents to refuse bargaining with  liar agents and c) introducing a global punishment system for liar agents.  Overall the reviewers agree that the design of the study is interesting, but also point that motivation and take home messages of this study are unclear. Having read the paper, I share the same opinion. The authors discuss on a very abstract level about the implications of this study for the field of AI, but this study is quite specific and clearly does not capture all the complexities or real societies. From the scale of results and study, I think it would be more valuable to draw some concrete proposals/implications about perhaps multi agent modelling or environment design in general.   All in all, this is an interesting study but some more work needs to be done around research framing. 
This paper introduces Autoregressive Diffusion Models (ARDMs), which generalises order agnostic autoregressive models and absorbing discrete diffusion.  All reviewers appreciated the paper with a few also finding it very dense. The experimental section is a bit lacking in detail. This has to some degree been answered in the discussion and should also be included in the final version of the paper.   Acceptance is recommended.
In general, the reviewers recognized the importance of the question and the innovation in the proposed algorithm, but do not seem to be super excited about the overall contribution of the paper. (One or two reviewers did not seem to respond authors  response after the AC s reminder.) The AC read the reviews and responses and observed that the main concern appears to be the empirical performance   the improvements are not as strong for larger models or if more computational time is allowed. Modern models are indeed typically large, and it would be good to discuss this point more thoroughly. If the work s focus is limited resource setting, the paper might want to state that upfront. Indeed, one reviewer is still concerned post rebuttal about a clock time comparison. Given these considerations, the AC will recommend reject for the paper but encourage the authors to resubmit to a top venue conference after revising the paper.
As far as I know, this is the first paper to combine transductive learning with few shot classification. The proposed algorithm, TPN, combines label propagation with episodic training, as well as learning an adaptive kernel bandwidth in order to determine the label propagation graph. The reviewers liked the idea, however there were concerns of novelty and clarity. I think the contributions of the paper and the strong empirical results are sufficient to merit acceptance, however the paper has not undergone a revision since September. It is therefore recommended that the authors improve the clarity based on the reviewer feedback. In particular, clarifying the details around learning \sigma_i and graph construction. It would also be useful to include the discussion of timing complexity in the final draft.
While the reviewers found the paper interesting, all the reviewers raised concerns about the fairly simple experimental settings, which makes it hard to appreciate the strengths of the proposed method. During rebuttal phase, the reviewers still felt this weakness was not sufficiently addressed.
### Description The authors note that the most recent and successful in terms of accuracy binary networks are in fact combining binary and floating point computations, in particular have residual full precision paths, with their parameters, that connect all the way to the output. Although such paths are made lightweight, they can be a bottleneck with respect to the energy consumption, memory and latency. The paper proposes a novel binary neural network model that uses only binary operations (except in the first and last layers). It is proposed to estimate the energy efficiency of binary networks more accurately, using hardware design compilers.  ### Decision Reviewers came to a consensus that the proposed BNN architecture claimed in the paper to be the main novelty, while it is indeed quite distinct from the mainstream and best performing BNNs, does not propose novel solutions with respect to the total state of the art. This is our main reason for rejection. The paper makes a great effort in steering the development of BNNs towards more energy efficient models, by carefully estimating the potential energy consumption of different models, using specialized software for design and simulation of hardware needed to run particular models. This is proposed not as a practical solution for industry but rather as a way to measure the potential efficiency of different models. While this was recognized as a great effort, some questions remained regarding fairness of comparison and possibility to reproduce the results by non experts in order that other developers could estimate and compare potential efficiency of their models. Additionally, it is unclear how power efficiency of a model with $k$ slices (BoolNet) differs from that of a $k$ bit quantized network.  ### Details First of all, I very much like the motivation for the work: to aim at a design of BNN that would be more efficient in terms of speed and energy and to measure that efficiency more precisely. I am not an expert in the hardware, however I do share the concern in this paper that full precision residual paths and blocks would incur a larger latency and higher amount of computation (more chip area, more energy,...). The fact that residual connections in BiRealNet make it necessary to read and write full precision feature maps to the global memory was non obvious to me. The current state of the art reports binary and floating point operations and largely ignores the necessary memory operations, which as authors argue are the real performance bottleneck.  The main claimed contribution of the paper is the design of a novel model. This was the main point of concert. It is indeed innovative with respect to the current trend of the best performing binary networks to make a "pure" binary network. However it is hard to agree that removing some of the components from BiRealNet or models based on it and going back to simpler architectures which were used before can be a contribution. Specifically, plain non residual BNNs were the very well known pioneering works [r1, r2]. The fact that BN in front of binarization can be simplified is trivial and well known, e.g. [r3]. Using multiple binary activations through power of two coding or uniform thresholds has been considered multiple times [r2, r6, ABCNet]. Same for group wise convolutions. I have not seen residual connections in the form of shuffle net so far, but it can also be considered as a rather standard trick. Using stride instead of max pooling or average pooling is another well known trick. Many of these solutions are in fact used in recent works, e.g. [r4] has no skip connections, combines BN with sign at the inference time and uses strided convolutions for downsampling. To summarize, the submission does not appear to propose novel modelling solutions relative to the total state of the art. Furthermore, [r3] is closely related in that they investigated the energy efficiency (however looking at the energy consumption of individual operations only) and with a similar motivation developed binary networks without 32 bit residual connections. Please see also other references pointed out by reviewers.  ### Unclear in the paper:  What is the meaning of $F$ column in Fig4a for BaseNet / Boolnet?  Why grouped convolution with input channels 256k and k groups has output of 256 channels in fig 3a.  If the outputs from each group are summed, isn t it equivalent to a full convolution 256k x 256?  What do the authors mean by 3x3 depth wise convolution?   The local adaptive shifting module is discussed inside the paragraph describing MS BConv. Is it a part of MS BConv or not?   It seems that with $k$ slices, there is $k$ times more bits per activation used and $k$ times more bits per weight used (because the channel width is multiplied by $k$). It must be therefore equivalent in terms of the power consumption to a network that uses $k$ bits per activation and weight.  Using these $k$ bits to represent uniform slices rather than powers of two slices appears inferior in terms of quantization error and accuracy. Indeed, many works have successfully quantization different models down to $4$ bits without a loss in accuracy.   ### Related work:   Rather than reviewing different methods for making networks more efficient, a deeper review of BNNs would be more helpful, in particular  looking at the works that are closer to the hardware, such as "in memory computing", "neuromorphic computing", [r5, r6].  ### Discussion  A well justified way to measure the potential energy efficiency of BNNs would be an excellent contribution that could standardize comparison and drive the development of BNNs in the energy efficient direction. Unfortunately, it does not appear at the moment that non experts in hardware and design compilers could repeat the compilation and simulation of accelerators as the authors proposed. A simplified estimation method is needed that can be used in python for any model composed of some standard blocks. The authors seem to be in a good position to propose and validate such a simplified estimator. To start with may I suggest to clarify the following questions:   Do we need power for the operation pipelines for different operation types (cache, global memory) that are not currently used?   Are the arithmetic operations implemented in hardware to optimize energy or the throughput?   Can we assume that all latencies can be masked by parallelism?   Is it a good approximation to assume that a convolution (with an efficient implementation and large enough cache) needs to read the input only once?   Can any coordinate wise transform be appended to the preceding transform on the fly, canceling a read write in between?   What is a reasonable estimate of a cost for float32 operations? It seems from the quantization literature that all such operations can be safely quantized to e.g. 8 bit representations without a loss of accuracy.  [r1] Courbariaux et al. 2016, Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1]   [r2 Hubara 2018: Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations, JMLR]   [r3 Ding et al. 2019: Regularizing Activation Distribution for Training Binarized Deep Networks]   [r4 Livochka et al. Initialization and Transfer Learning of Stochastic Binary Networks From Real Valued Ones]  [r5 Baskin et al. Streaming Architecture for Large Scale Quantized Neural Networks on an FPGA Based Dataflow Platform]  [r6 Umuroglu et al. FINN: A Framework for Fast, Scalable Binarized Neural Network Inference]
The authors propose a way to generate unseen examples in GANs by learning the difference of two distributions for which we have access. The majority of reviewers agree on the originality and practicality of the idea.
The paper has several clarity and novelty issues.
The reviewers highlighted aspects of the work that were interesting, particularly on the chosen topic of multi label output of graph neural networks. However, no reviewer was willing to champion the paper, and in aggregate all reviewers trend towards rejection.
 This paper proposes a method to capture patterns of the so called “off” neurons using a newly proposed metric. The idea is interesting and worth pursuing. However, the paper needs another round of modification to improve both writing and experiments. 
The main contribution of this paper is the training of a supervised model jointly with deep CCA for improving the representations learned in a setting where the training data is multi view.  The claimed technical contribution is modifications to deep CCA to enable it to play nicely with the minibatch gradient based training used for the supervised loss.  Pros:  This is an important problem with many applications.  Cons:  The novelty is minimal.  Some previous work has done joint training of supervised models with CCA, and some has addressed training deep CCA in a stochastic setting.  The reviewers (and I) are unconvinced that the differences from previous work are sufficient, and the paper does not carefully compare with the previous work.  The contribution to the tasks may be quite significant, however, so the paper may fit in well in an application oriented conference/journal.
This paper proposes a new theory for modified DRM and PINN for solving elliptical PDEs, and delivers valuable advances on important topics.
The pros and cons of this paper cited by the reviewers can be summarized below:  Pros: * The method proposed here is highly technically sophisticated and appropriate for the problem of program synthesis from examples * The results are convincing, demonstrating that the proposed method is able to greatly speed up search in an existing synthesis system  Cons: * The contribution in terms of machine learning or representation learning is minimal (mainly adding an LSTM to an existing system) * The overall system itself is quite complicated, which might raise the barrier of entry to other researchers who might want to follow the work, limiting impact  In our decision, the fact that the paper significantly moves forward the state of the art in this area outweighs the concerns about lack of machine learning contribution or barrier of entry.
The paper proposes a method to grow deep network architectures over the course of training. The work has been extremely well received and has clear novelty and solid experiment validation.
A deep neural network pipeline for multiview stereo is presented. After rebuttal and discussion, all reviewers learn toward accepting the paper. Reviewer3 points to good results, but is concerned that the technical aspects are somewhat straightforward, and thus the contribution in this area is limited. The AC concurs with the reviewers.
 After reading the paper, reviews and authors’ feedback. The meta reviewer agrees with the reviewers that the paper presented a very interesting idea and empirical studies.  R3 rightfully pointed out the need to clarify relation to related works, as well as the scalability issue.  Notably, because the analysis does not ensure correctness, it has limited applicability in tasks where absolution correctness are required(e.g. Dead code), but can benefit downstream tasks that do not require absolute correctness. A more thorough discussion about this perspective would strengthen the paper.   Right now the paper is borderline, the meta reviewer acknowledges the pros of the paper as mentioned in the reviews, but also thinks the paper can be further improved based on the comment. Therefore the meta reviewer decided to not accept the paper but would encourage the authors to improve the paper per comments for a future submission.  Thank you for submitting the paper to ICLR.  
This paper presents a model for question answering, where the idea is to have a collaborative model that aligns queries and sentences on a small supervised dataset and also uses semi supervised information from a weakly supervised corpus to answer open domain questions resulting in short answer spans.  The main criticism of the paper is regarding its novelty, and reviewers cite the similarities with prior work such as Chen et al. and Min et al.  There is relative consensus between the reviewers that further work using the semi supervised outlook with stronger results could strengthen the paper further.
The paper proposes an adaptive sampling mechanism for zeroth order optimization that samples perturbed points from a mixture distribution with asymptotic convergence guarantees. The reviewers raised issues regarding the clarity of presentation, potential problems with the proofs, and simplicity of the experimental setup. The authors did not provide a response. Overall, the reviewers agree that the quality of the paper is not sufficient for publishing, and therefore I recommend rejection.
The paper proposes a method on multi agent options based policy transfer where agents help each other learn by exchanging policies.  The core idea behind the paper is novel, as it addresses a new and emerging topic of social learning, and of interest to ICLR community. The authors significantly improved the paper with additional experiments and theoretical analysis during the rebuttal process, resulting in a compelling case for the significance of the method.   Unfortunately, the paper requires addressing the clarity, and a careful proofreading pass, making it unsuitable to ICLR in its current form, 
Based on current unanimous reviews, the paper is accepted.
Three reviewers have assessed this paper and they have scored it 6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera ready submission.
The reviewers overall were quite happy after the rebuttal phase, in which the authors considerably improved the presentation quality and addressed reviewer concerns, and recommended acceptance. The reviewers agreed that while the theory was short and relied on various possibly restrictive assumptions and maybe was largely an improvement in constant factors, it extended prior work (some of which was in ICLR) and was interesting and motivated the experiments which were notably faster than existing methods.
This paper introduces an algorithm for making a meta model from an ensemble of models by learning model embedding.  All reviewers appreciated the originality and potential usefulness of the paper. However, they also all think that the work is not completely ready for publication. Both the presentation and quality of the results can be improved.   There are a lot of good feedback in the discussion that can be used to make an important updated version for the next conference.
The paper presents a boosting method and uses it to train an ensemble of convnets for image classification. The paper lacks conceptual and empirical comparisons with alternative boosting and ensembling methods. In fact, it is not even clear from the experimental results whether or not the proposed method outperforms a simple baseline model that averages the predictions of T independently trained convolutional networks.
The reviewers reached a consense on that the paper is not quite ready for publication at ICRL. The main potential drawback include a) the exposition of the paper can be improved; b) it s not entirely clear that some of the assumptions (such as the threshold for the first layer, the polynomial approximation of higher layers) are meaningful , and it seems that the proof technique exploits heavily some of these assumptions and some of the key intermediate steps won t hold in practice. (see reviewer 3 s comment for more details.) The authors clarify the writing and intuitions in the response, but overall the AC decided that the paper is not quite ready for publications at the moment.  
The reviewers and authors had a productive conversation, leading to an improvement in the paper quality. The strengths of the paper highlighted by reviewers are a novel learning set up and new loss functions that seem to help in the task of protein contact prediction and protein structural similarity prediction. The reviewers characterize the work as constituting an advance in an exciting application space, as well as containing a new configuration of methods to address the problem.  Overall, it is clear the paper should be accepted, based on reviewer comments, which unanimously agreed on the quality of the work.
The paper studies non spiking Hudgkin Huxley models and shows that under few simplifying assumptions the model can be trained using conventional backpropagation to yield accuracies almost comparable to state of the art neural networks. Overall, the reviewers found the paper well written, and the idea somewhat interesting, but criticized the experimental evaluation and potential low impact and interest to the community.  While the method itself is sound, the overall assessment of the paper is somewhat below what s expected from papers accepted to ICLR, and I’m thus recommending rejection.
The authors consider local  why  or  abductive  explanations for a model and a given class, which identify a minimal subset of features such that they re sufficient to imply that the model predicts the class; and  why not  or  contrastive  explanations, which identify a minimal subset s.t. they re sufficient to imply that the model predicts a different class. The two types of explanation are related using earlier work on minimal hitting sets going back to Reiter (1987).   Reviewers were divided in their opinions. R4 was very positive but with little detail and only medium confidence, then did not participate in discussion. R2 was the only reviewer with high confidence, leaning against acceptance. The paper relies on FOL which was hard for reviewers to grapple with, and may make it challenging for readers. The presentation could be improved by clearly linking to existing work and demonstrating why the new approach is important.
This paper offers a new angle through which to study the development of comparison functions for sentence pair classification tasks by drawing on the literature on statistical relational learning. All three reviewers seemed happy to see an attempt to unify these two closely related relation learning problems. However, none of the reviewers were fully convinced that this attempt has yielded any substantial new knowledge: Many of the ideas that come out of this synthesis have already appeared in the sentence pair modeling literature (in work cited in the paper under review), and the proposed new methods do not yield substantial improvements for the tasks they re tested on.  I m happy to accept the authors  arguments that sentence to vector models have practical value, and I m not placing too much weight on the reviewer s comments about the choice to use that modeling framework. I am slightly concerned that the reviewers (especially R2) observed some overly broad statements in the paper, and I urge the authors to take those comments very seriously.  I m mostly concerned, though, about the lack of an impactful positive contribution: I d have hoped for a paper of this kind to offer a  a method with clear empirical advantages over prior work, or else a formal result which is more clearly new, and the reviewers are not convinced that this paper makes a contribution of either kind. 
The paper proposes a method for lossy image compression. Based on the encoder decoder framework, it replaces the discrete codes by continuous ones, so that the learning can be performed in an end to end way. The idea is interesting, but the motivation is based on a quantization "problem" that the authors show no evidence the competing method is actually suffering from. It is thus unclear how much does quantization in existing methods impact performance, and how much will fixing this benefit the overall system. Also, the authors may add some discussions on whether the proposed sampling of z_{c^\star} is indeed also a form of quantization.  Experimental results are not convincing. The proposed method is only compared with one method. While it works only slightly worse at low bit rate region, the gap becomes larger in higher bit rate regions. Another major concern is that the encoding time is significantly longer. Ablation study is also needed. Finally, the writing can be improved.
The paper presents a novel reinforcement learning based algorithm for contextual sequence generation. Specifically, the paper presents experimental results on the application of the gradient ARSM estimator of Yin et al. (2019) to challenging structured prediction problems (neural program synthesis and image captioning). The method consists in performing correlated Monte Carlo rollouts starting from each token in the generated sequence, and using the multiple rollouts to reduce gradient variance. Numerical experiments are presented with promising performance.   Reviewers were in agreement that this is a non trivial extension of previous work with broad potential application. Some concerns about better framing of contributions were mostly resolved during the author rebuttal phase. Therefore, the AC recommends publication. 
The authors give a characterization of stochastic mirror descent (SMD) as a conservation law (17) in terms of the Bregman divergence of the loss. The identity allows the authors to show that SMD converges to the optimal solution of a particular minimax filtering problem. In the special overparametrized linear case, when SMD is simply SGD, the result recovers a recent theorem due to Gunasekar et al. (2018). The consequences for the overparametrized nonlinear case are more speculative.  The main criticisms are around impact, however, I m inclined to think that any new insight on this problem, especially one that imports results from other areas like control, are useful to incorporate into the literature.   I will comment that the discussion of previous work is wholly inadequate. The authors essentially do not engage with previous work, and mostly make throwaway citations. This is a real pity.  I would be nice to see better scholarship.
The paper received borderline scores, before and after the rebuttal. Thus, support for paper acceptance isn t sufficiently strong. While the reviewers see merit, concerns which remain after the discussion phase, include how convincing the experimental settings and results are, and uncertainty about the motivation and practical value.
The authors improve upon existing algorithms for complete neural network verification by combining recent advances in bounding algorithms (better bounding algorithms under branching constraints and relaxations involving multiple neurons) and developing novel branching heuristics. They show the efficacy of their method on a number of rigorous experiments, outperforming SOTA solvers for neural network verification on several benchmark datasets.  All reviewers agree that the paper makes valuable contributions and minor concerns were addressed adequately during the rebuttal phase. Hence I recommend that the paper be accepted.
This paper approximates the Whittle index in restless bandits using a neural network. Finding the Whittle index is a difficult problem and all reviewers agreed on this. Nevertheless, the scores of this paper are split between 2x 4 and 2x 7, essentially along the line of whether this paper is too preliminary to be accepted. Therefore, I read the paper and propose a rejection.  The reason is that the paper lacks rigor, which was brought up by the two reviewers who suggested rejections. For instance, in the last line of Algorithm 1, it is not clear what kind of a gradient is computed. The reason is that \bar{G}_b is not a proper baseline, as it depends on the future actions of the bandit policy in any given round. I suggest that the authors look at recent papers on meta learning of bandit policies by policy gradients,  https://papers.nips.cc/paper/2020/hash/171ae1bbb81475eb96287dd78565b38b Abstract.html  https://arxiv.org/abs/2006.16507  This is the level of rigor that I would expect from this paper, to make sure that the gradients are correct.
This paper studies graphon mean field games, whereby a continuum of agents are connected by a graphon. They study a discrete time version and show existence of a Nash equilibrium (under Lipschitz conditions). Moreover they prove that it corresponds to an approximate Nash equilibrium for the game with a finite number of players, thereby validating graphon mean field games as a natural abstraction when the number of players is sufficiently large. Finally they give algorithms based on fixed point iterations (one based on discretizing the graphon index, the other based on reformulating it as a classical mean field game) for computing such an equilibrium. They give numerical experiments to validate their approach. The reviewers pointed out various writing issues or other results that would help complete the picture. Many of these were addressed and/or clarified by the authors in their revision. Overall the paper provides an appealing and relatively complete characterization of equilibria in graphon mean field games.
The paper presents a method for meta learning the loss function. The analysis mainly concerns the recently proposed TaylorGLO method on the (slightly less recent) Baikal loss. There was no consensus on this paper, but no reviewer was willing to fight for acceptance either. I found the paper not self contained, with important non standard elements undefined, starting with the Baikal loss, notations that are not defined in the main text, and a nomenclature that is also unusual with important terms such as "attractor" or "invariant" used in meanings that are non standard in optimization or machine learning.  Regarding content, most of the analyses refer to properties of the Baikal loss (not presented in the main text) that are deemed to be positive, without any theoretical support (Theorems 1 and 2). The inability to overfit is here posed as an obvious quality of a training loss. Then, a way to prevent the failure of the meta training algorithm is presented in Theorem 3. Finally, an experiment is provided, showing that the proposed meta training algorithm performs better than "vanilla" training with respect to adversarial attacks with FGSM. There is no comparison with other defense mechanisms and no analysis explains the results. Overall, although some interesting aspects may be developedin this paper, they are currently not well served by writing or the experimental evidences, so I recommend rejection. 
Based on the observation that the stochastic gradients for deep nets often stay in a low dimensional subspace, this paper proposes projected differential private SGD (DP SGD) that performs noise reduction by projecting the noisy gradients to a low dimensional subspace. Under certain assumptions, the authors provide a theoretical analysis and empirical evaluations to show that the proposed algorithm can substantially improve the accuracy of DP SGD in the high privacy regime. There is unanimous support to accept this paper after the author’s response. Thus, I recommend accept.
Since the authors have decided to withdraw this submission, it has been rejected from the conference.
the authors propose to incorporate an additional layer between the consecutive steps in LSTM by introducing a radial basis function layer (with dot product kernel and softmax) followed by a linear layer to make LSTM similar to or better at (by being more explicit) capturing DFA like transition. the motivation is relatively straightforward, but it does not really resolve the issue of whether existing formulations of RNN s cannot capture such transition. since this was not shown theoretically nor intuitively, it is important for empirical evaluations to be thorough and clearly show that the proposed approach does indeed outperform the vanilla LSTM (with peepholes) when the capacity (e.g., the number of parameters) matches. unfortunately it has been the consensus among the reviewers that more thorough comparison on more conventional benchmarks are needed to convince them of the merit of the proposed approach.
The paper points out a statistical properties of GAN samples which allows their identification as synthetic.  The paper was praised by one reviewer as well written, easy to follow, and addressing an interesting topic. Another added that the authors did an excellent job of "probing into different statistical perspectives", and the fact that they did not confine their investigation to images.  Two reviewers leveraged the criticism that various properties discovered are not surprising given the loss functions and associated metrics as well as the inductive biases of continuous valued generator networks. Tests employed were criticized as ad hoc, and reviewers felt that their generality was limited given their reduced sensitivity on certain modalities. (While Figure 10b is raised by the authors several times in the discussion, and the test statistics of samples are noted to be closer to the test data than to the random baseline, the test falsely rejects the null [p value ~  0.0] for non synthetic test data.)  I would encourage the authors to continue this line of inquiry as it is overall agreed to be an interesting topic of relevance and increasing importance, however based on the criticisms of reviewers and the content of the ensuing discussion I do not recommend acceptance at this time.
This paper presents a reinforcement learning inspired algorithm to train task specific adapters to adapt pretrained language models for downstream tasks. The paper attempts to tackle an important problem. All reviewers have concerns about whether the results are strong enough to justify claims made in the paper. I appreciate revisions that have been done by the authors during the rebuttal period. However, I believe that the paper is still below the bar for ICLR. I recommend rejecting this paper.
The reviewers appreciated that the paper was clear and well written. They also appreciated that the paper has been largely improved during the discussions. The results seem to support the claim and the experiments on Minecraft are convincing.   Yet, the reviewers had some important concerns. First the focus on RUDDER seems too strong and the method doesn t seem to be that much related to RUDDER. Presenting the work as a trajectory matching method seems more appropriate. In addition, the authors support their choice of referring to RUDDER because it comes with theoretical guarantees. But RUDDER s guarantees come from the usage of a modified LSTM while Align RUDDER doesn t use an LSTM.   The hierarchical approach was also questioned as the way to switch between different sub policies is not very well explained in the paper. Baselines wrt to the switching method could not be provided. Similarly, the structure of the Minecraft task seems to be used heavily to define the hierarchy and meta planning, so more baselines (with less structured tasks) were requested.   The method also suffers from scalability issues as the authors acknowledge that if the number of events grows, they would need to downsample the events so as to apply their method. 
This paper is borderline.  The reviewers agree that the method is novel and interesting, but have concerns about scalability and weakness to attacks with larger epsilon.  I will recommend accepting; but I think the paper would be well served by imagenet experiments, and hope the authors are able to include these for the final version
This paper combine recent ideas from capsule networks and group equivariant neural networks to form equivariant capsules, which is a great idea. The exposition is clear and the experiments provide a very interesting analysis and results. I believe this work will be very well received by the ICLR community.
The reviewers all agree that this is an interesting paper with good results. The authors  rebuttal response was very helpful. However, given the competitiveness of the submissions this year, the submission did not make it. We encourage the authors to resubmit the work including the new results obtained during the rebuttal.
The paper studied an interesting and important problem in active learning/information acquisition (AFA), and provided an RL based active learning scheme for a broad spectrum of AFA tasks, in both supervised (active classification/regression) and unsupervised (feature completion/recovery) domains. The reviewers generally find the paper well presented, and all appreciate the broad applicability of the proposed approach, which leverages reinforcement learning and a generative surrogate model to learn the acquisition/reward function of AFA. However, there are also shared concerns among several reviewers on the novelty and positioning of the proposed approach, as well as on whether the proposed experiments results well demonstrated the significance of the algorithm. Given that this is a purely empirical paper, both aspects are important to be properly addressed in a revision. 
This paper extends the Contextual Graph Markov Model, a deep unsupervised probabilistic approach. The key idea is to leverage Hierarchical Dirichlet Processes to automatically determine each layer s latent representation s size. The paper conducts experiments on graph classification tasks to show the superiority of the proposed method.  Strength * A new method is proposed. * The proposed method appears to be sound. * Experiments are conducted to demonstrate the effectiveness.  Weakness * The novelty and significance of the work are not enough. * The improvements on existing methods are not significant. * The proposed method is also not so general.     After rebuttal  Reviewer ynws, who gave the highest score, says  “I agree with the overall review of the paper by other reviewers. The proposed method is limited to the CGMM model and not generic enough to extend to other more popular graph neural networks. The improvements don t seem to be significant enough as well.”
This paper proposes an approach for learning to transfer knowledge across multiple tasks. It develops a principled approach for an important problem in meta learning (short horizon bias). Nearly all of the reviewer s concerns were addressed throughout the discussion phase. The main weakness is that the experimental settings are somewhat non standard (i.e. the Omniglot protocol in the paper is not at all standard). I would encourage the authors to mention the discrepancies from more standard protocols in the paper, to inform the reader. The results are strong nonetheless, evaluating in settings where typical meta learning algorithms would struggle. The reviewers and I all agree that the paper should be accepted, and I think it should be considered for an oral presentation.
The paper presents an analysis of the benefit of unsupervised contrastive learning for downstream classification tasks using the cross entropy loss. Building on prior work, the authors show that the contrastive loss can be bounded in terms of the cross entropy term and an “intercept” term which depends logarithmically on the number of negative samples per positive sample (for contrastive learning) rather than polynomially as in the prior work.   There are several differences between the setting here and that of the prior work by Arora et al. (2019). First, the work here focuses only on cross entropy loss and leverages the similarity of the loss structure between the contrastive loss and the cross entropy loss. Second, the assumptions here are different, e.g., boundedness of the representation. Finally, the assumption that latent classes are the same as the label classes (which is not the case in the prior work) is significantly restrictive.   The writing is poor and the presentation is not clear. Despite the title and various references to learning bounds in the abstract and the main text, there are no learning bounds in the paper. The main result is to bound the contrastive loss in terms of the cross entropy loss under the assumption that the latent classes and the label classes coincide. Authors state that getting generalization bounds is routine and, therefore, they chose not to give them — I do not see how generalization bounds follow in a straightforward manner here, and even if they do, it is important to write them for completeness.   The main contribution here is that the bounds depend logarithmically in K — the number of negative samples per positive sample — compared to sqrt{K} in the previous work. The previous bound however holds for Lipschitz losses as well, for e.g., hinge loss. So the question remains whether this improvement is only for the cross entropy loss. Regardless, K is typically small in practical applications. Even the experiments in the paper (Figure 7) suggest that the performance degrades for larger K even on simple tasks. So, the improvement is really somewhat insignificant.    The reviewers were generally positive and appreciated the paper. However, in the light of comments above (of which I am quite certain), unfortunately, I am unable to accept the paper at this point. I believe the comments above (and from the other reviewers) will help improve the overall quality of the paper. I encourage the authors to incorporate the feedback and work towards a stronger submission.
Strengths:    well written    strong results for non autoregressive NMT   a novel soft EM version of VQ VAE  Weaknesses:     as pointed out by reviewers, the improvements are mostly not due to the VQ VAE modification rather due to orthogonal (and not interesting) changes e.g., knowledge distillation. If there is a genuine contribution of VQ VAE, it is small and required extensive parameter selection     the explanations provided in the paper do not match the empirical results  Two reviewers criticize the experiments / experimental section: rigour / their discussion.  Overall, there is nothing wrong with the method but the experiments are not showing that the modification is particularly beneficial.  Given these results and also given that the method is not particularly novel (switching from EM to Soft EM in VQ VAE), it is hard for me to argue for accepting the paper.
The authors propose a framework for compressing neural network models which involves applying a weight distortion function periodically as part of training. The proposed approach is relatively simple to implement, and is shown to work for weight pruning, low rank compression and quantization, without sacrificing accuracy.  However, the reviewers had a number of concerns about the work. Broadly, the reviewers felt that the work was incremental. Further, if the proposed techniques are important to get the approach to work well in practice, then the paper would be significantly strengthened by further analyses. Finally, the reviewers noted that the paper does not consider whether the specific weight pruning strategies result in a reduction of computational resources beyond potential storage savings, which would be important if this method is to be used in practice.  Overall, the AC tends to agree with the reviewers criticisms. The authors are encouraged to address some of these issues in future revisions of the work. 
The paper proposes to define the GAN discriminator as an explicit function of a invertible generator density and a structured Gibbs distribution to tackle the problems of spurious modes and mode collapse. The resulting model is similar to R2P2, i.e. it can be seen as adding an adversarial component to R2P2, and shows competitive (but no better) performance. Reviewers agree, that these limits the novelty of the contribution, and that the paper would be improved by a more extensive empirical evaluation. 
Some reviewers seem to assign novelty to the compression and classification formulation; however, semi supervised autoencoders have been used for a long time. Taking the compression task more seriously as is done in this paper is less explored.  The paper provides some extensive experimental evaluation and was edited to make the paper more concise at the request of reviewers. One reviewer had a particularly strong positive rating, due to the quality of the presentation, experiments and discussion. I think the community would like this work and it should be accepted. 
This paper proposes a combination of three techniques to improve the learning performance of Atari games. Good performance was shown in the paper with all three techniques together applied to DQN. However, it is hard to justify the integration of these techniques. It is also not clear why the specific decisions were made when combining them. More comprehensive experiments, such as a more systematic ablation study, are required to convince the benefits of individual components. Furthermore, it seems very hard to tell whether the improvement of existing approaches, such as Ape X DQN, was from using the proposed techniques or a deeper architecture (Tables 1&2&4&5). Overall, this paper is not ready for publication. 
This paper describes a new approach to meta learning with generating new useful examples.  The reviewers liked the paper but overall felt that the paper is not ready for publication as it stands.  Rejection is recommended. 
The submission aims to improve the quality of the bootstrap when the number of samples is small.  It does so by gradient descent on the to approximate the ideal bootstrap in Wasserstein distance.  The submission combines a nice set of methodologies, and aims to address an interesting statistical problem in a principled way.  The reviewers were unanimous in their opinion that the submission falls below the threshold for acceptance to ICLR.  It was revealed in post rebuttal discussion with reviewer y4AP that they wish to retain a reject recommendation due to a lack of clarity in the methodology even after author comments.  The review details specific issues that can eventually be clarified in a revision for submission to another venue.
The paper proposes a new problem setting of predicate zero shot learning for visual relation recognition for the setting when some of the predicates are missing, and a model that is able to address it.  All reviewers agreed that the problem setting is interesting and important, but had reservations about the proposed model. In particular, the reviewers were concerned that it is too simple of a step from existing methods. One reviewer also pointed towards potential comparisons with other zero shot methods.  Following that discussion, I recommend rejection at this time but highly encourage the authors to take the feedback into account and resubmit to another venue.
The reviewers agree  that the paper is worthy of publication at ICLR, hence I recommend accept.  Regarding section 4.3 of the submission and the claim that this paper presents the first insight for existing work from a divergence minimization perspective, as pointed out by R2, I went and checked the details of RAML and they have similar insights in their equations (5) and (8). Please make this clearer in the paper. Regarding evaluation using greedy search instead of beam search, please consider using beam search for reporting test performance as this is the standard setup in sequence prediction. Please take my comments and the reviews into account an prepare the final version.
The paper proposes various improvements to Wasserstein distance based GAN training. Reviewers agree that the method produces good quality samples and are impressed by the state of the art results in several semi supervised learning benchmarks. The paper is well written and the authors have further improved the empirical analysis in the paper based on reviewer comments.
This paper proposes an interesting collaborative multi head attention (MHA) method to enable heads to share projections, which can reduce parameters and FLOPs of transformer based models without hurting performance on En De translation tasks. For pre trained language models, a tensor decomposition method is used to easily covert the original MHA to its collaborative version without retraining.   This paper receives 3 weak reject and 1 weak accept recommendations. On one hand, all the reviewers agree that the paper is well motivated and the proposed idea is interesting. On the other hand, all the reviewers also commented that the current empirical results and comparisons are weak, which are not enough to support the paper s main claim. From the current results, it is difficult to draw a conclusion that collaborative MHA is better.   Specifically, (i) From Table 2, it can be seen that the proposed method is not effective for pre trained models, i.e., even if the model size is not reduced much, the performance can be dropped significantly. (ii) More experiments, such as QA, more translation/generated tasks will make this paper more convincing. (iii) More rigorous experiments are needed to justify the practical value of the proposed method. If the authors try to emphasize that they go beyond practical realm, then probably a careful re positioning of the paper is needed, which may not be a trivial task.   The rebuttal unfortunately did not fully address the reviewers  main concerns. Therefore, the AC regrets that the paper cannot be recommended for acceptance at this time. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
Thanks for your submission to ICLR.  This paper proposes a subspace indexing model for low dimensional embedding.  The reviewers were all generally in agreement that the paper is not ready for publication.  In particular, they felt that the paper had several key weaknesses:   Relevant literature is not discussed  Relevant methods are not evaluated against in the experiments  Experiments on the whole were limited and not sufficiently convincing  The novelty of the paper is not very high  Please consider the reviewer comments carefully when preparing a future version of your paper.
The  paper proposes a new approach to multi actor RL, which ensure diversity and performance of the population of actors, by distilling the policy of the best performing agent in a soft way and maintaining some distance between the agents. The authors show improved performance over several state of the art mono actor algorithms and over several other multi actor RL algorithms.  Initially, reviewers were concerned with magnitude of the contribution/novelty, as well as some technical issues (e.g. the beta update), and relative lack of baseline comparisons.  However, after discussion the reviewers largely agree that their main concerns have been addressed.  Therefore, I recommend this paper for acceptance.
All reviewers agree that the writing is not precise. It does not help to find any novelty in the ideas, and the limited and too quickly described experiences are not convincing enough to forgive this problem. The authors chose not to oppose or comment on the detailed arguments provided by the reviewers. I agree with the reviewers in recommending the rejection of this paper. 
This paper introduces a VAE based generative model of 3D point clouds inspired by SPAIR that can do unsupervised segmentation, named SPAIR3D. The model uses both global and local latent variables to encode global scene structure as well as individual objects.  The proposed model is relatively complex, but the presentation is overall clear.   Experimental results on simple synthetic datasets look promising. However, one might argue that for these simple tasks a direct application of a simpler mixture of VAEs (such as IODINE) might be sufficient, so it would be informative to make a direct comparison between these methods and/or show results on a problem clearly out of the scope of these simpler methods (e.g. with high imbalance in the point clouds).
The paper aims to improve complex reasoning. In this regard, authors identify that acquisition of data for symbolic reasoning domains is a challenge and propose generating the data by GANs. A transformer based architecture is proposed and trained for LTL and Symbolic mathematics. Experiments show samples generated are of good quality (e.g., correct syntax). We thank the reviewers and authors for engaging in an active discussion. However, the reviewers did not find the task of such data on its own not to be particularly interesting. Also, neither the architecture nor the training algorithm is very novel. If authors could provide a complete story i.e., show the augmented data can improve the performance of neural models that compute solutions, etc., it would make the paper much stronger. Thus, unfortunately I cannot recommend an acceptance of the paper in its current form.
This work adapts the widely used DP learning algorithm to language models. Reviewers all agreed that this work tackles an important problem with clear motivation and thorough experiments, and achieved strong performance (memory reduction and effectiveness) on NLP tasks.  Thus, we recommend an acceptance.
This paper presents the use of scalable evolution strategies (S ES) in hierarchical reinforcement learning. After reviewing the paper and reading the comments from the reviewers, here are my comments:     The proposal is quite novel. It requires major improvements to clearly state how this proposal contributes in the field.   The main concern is about the experimental results. There are some flaws in the comparative results. Also, they do not support the proposal.
This paper proposes a very simple procedure to accelerate the inference time of graph structured Neural Networks, by distilling knowledge of a GNN into a node wise MLP.  Despite some concerns about the novelty of the methodology (which borrows heavily from previous KD works), reviewers generally found this empirical work well executed and providing a potentially useful baseline for large scale applications. Therefore, the AC recommends acceptance.
This paper received six reviews, consisting of three 8s two 6s and one 3. The reviewers generally felt that the proposed Electra like pretraining provided fairly significant downstream improvements. Additional ablations were provided to during the author response period and other author responses were sufficient to cause scores to rise during the discussion period. The vast majority of reviewers recommended accepting this paper and the AC also recommends acceptance.
This paper provides an important discussion about the relationship between training efficiency and label redundancy. The updates to the paper will improve the paper further. Reviewers found the paper interesting, well written, and addresses and important problem.
 This paper presents approach to improve compute and memory efficiency by freezing layers and storing latent features. The approach is simple and provide efficiency. However, there are concerns as well. One big concern is that the experiments are not on realistic settings for example real world images and the current CNN is too simple. Overall, the reviewers are split. The AC agrees with some of the reviewers that for a paper like this experiments on more realistic setting will make it significantly stronger.  
The paper proposes PassNet, which is an architecture that produces a 2D map of probability of successful completion of a soccer pass. The architecture has some similarities with UNet and has downsampling and upsampling modules with a set of skip connections between them.  The reviewers raised several issues: * Novelty compared to UNET * Lack of ablation studies * Uncertainty about what probabilities mean and issues regarding output interpretation.  The authors have tried to address these concerns in their rebuttal and provided additional experiments. They also argue that the application area (sport analytics) of the paper is novel. Even though the application area is interesting and might lead to new problems, this paper did not get enough support from reviewers to justify its acceptance.
All reviewers agree that the paper brings new knowledge in the field of locally supervised learning, and as such it should be accepted.  The authors should keep all reviewers  comments into account when preparing their camera ready version.
The reviewers uniformly suggested rejecting the current paper.  I concur and remain especially somewhat unconvinced by the authors comments on learning features.  In particular, any argument based simply on (current) performance seems rather weak.  There are methodological reasons one might want to keep features fixed, and there are a small subset of problems with well defined known useful features.  But in the long term surely we should want to be able to learn the features, and efficiently and elegantly handle the case where they are learnt continually.  I want to thank the authors for engaging.  This work has the potential to be improved and I would encourage the authors to carefully consider and incorporate the provided feedback by the reviewers into their work.
While this paper has some very interesting ideas the majority view of the reviewers and their aggregate numerical ratings are just too low to warrant acceptance.
The reviewers feel that the novelties in the model are not significant.   Furthermore, they suggest that empirical results could be improved by 1:  analyses showing how the significance network functions and directly measuring its impact 2: More reproducible experiments.  In particular, this is really an applications paper, and the experiments on the main application are not reproducible because the data is proprietary. 3: baselines that make assumptions more in line with the authors  problem setup
This paper generated significant discussion and division amongst the reviewers. On the positive side, some reviewers enjoyed both contributions, feeling the further empirical investigation of existing attacks to be interesting, and the creation of a benchmark to be very useful. On the negative side, no new positive results were proposed, criticism of previous attacks were considered to be unjust, the focus was somewhat narrow, and a benchmark could plausibly be misleading and detrimental.  Given the highly competitive nature of ICLR and the many other excellent submissions, the committee was unable to accept the paper at this time. Below are some suggestions for future submissions.  The content of the paper is generally fine, as long as the caveats and the "tone" are appropriate: we would hope to not mislead potential readers. Here are some (strong) recommendations:   Previous works were proof of concept attacks, and the authors should be careful to not frame them as being "broken"   they perhaps were not meant to be robust to these modifications.   The scope is somewhat narrow. There should be some explicitly statement and justification of the scope, and what in particular is *not* covered by the investigation.   Importantly, a single benchmark can t be a unique gold standard, for many reasons discussed by reviewers. Please state these caveats clearly and prominently in the paper and/or code release, as otherwise the presence of a benchmark could do more harm than good. In particular, Reviewer 4 brought up the following philosophical concern with a benchmark, which I believe is quite reasonable, and I reproduce verbatim. The authors should try to address this in the next version: "This kind of benchmark can push the research to a wrong direction. In my view, the point of attacks are to create an alarm for using machine learning in critical applications. Developing these benchmarks would push the competition in the direction of making existing attacks "better" (whatever "better" means in the benchmark) instead of focusing on designing defense techniques or showing the severity of attacks in other situations. This benchmark could also have a bad effect on future attacks (attacks that want to show a new threat, not the one that try to improve the performance of clean label targeted poisoning attacks on deep neural nets) to gain attention from community as they probably will not pass all the criteria of this benchmark."  As another comment (I believe mentioned by other reviewers), it would be nice if all the terms and settings were defined clearly and precisely. For a benchmarking paper, it is important that the reader can clearly understand the threat model, and what does and does not count as a valid attack.  Finally, many of the reviewers gave detailed comments and concerns. The authors should please note and discuss these concerns in future versions (or at least in a supplement or an arXiv version).
This paper proposes a method to allow models to generalize more effectively through the use of latent linear transforms.  Overall, I think this method is interesting, but both R2 and R4 were concerned with the experimental evaluation being too simplistic, and the method not being applicable to areas where a good simulator is not available. This seems like a very valid concern to me, and given the high bar for acceptance to ICLR, I would suggest that the paper is not accepted at this time. I would encourage the authors to continue with follow up experiments that better showcase the generality of the method, and re submit a more polished draft to a conference in the near future.
This work addresses the problem of detecting an adversarial attack. This is a challenging problem as the detection mechanism itself is also vulnerable to attack. The paper proposes asymmetrical adversarial training as a robust solution. This approach partitions the feature space according to the output of the robust classifier and trains an adversarial example detector per partition. The paper demonstrates improvements over state of the art detection techniques.   All three reviewers recommend acceptance of this work. Some positive points include the paper being well written with strong experimental evidence. One potential difficulty with the proposed approach is the additional computational cost associated with a per class adversarial attack detector. The authors have responded to this concern by claiming that the straightforward version of their approach is K times slower (10 in the case of 10 classes), but their integrated version is 2x slower as they only run the detector associated with the example specific class prediction. We encourage the authors to include a discussion on computational cost in the final version. In addition, there was a community comment about black box testing which will be of relevance to many in the community. The authors have already provided additional experiments to address this question as well as code to reproduce the new experiment.   Overall, the paper addresses an important problem with a two step solution of training a robust model and detecting potentially perturbed samples per class. This is a novel solution with comprehensive experiments and therefore recommend acceptance.  
although the problem of text infilling itself is interesting, all the reviewers were not certain about the extent of experiments and how they shed light on whether, how and why the proposed approach is better than existing approaches. 
This work looks at what factors can lead to the emergence of selectivity (to certain categories) in units of a neural network. While this is an intriguing area to explore, this work uses settings that are quite toy ish, making it a very hard to see how the observations could generalize to more realistic architectures or tasks. 
The paper proposes a generative model that jointly trains an implicit generative model and an explicit energy based model using Stein s method. There are concerns about technical correctness of the proofs and the authors are advised to look carefully into the points raised by the reviewers. 
This paper proposes a method for incorporating inductive biases into the model architecture of normalizing flows through a suitable probabilistic program. All reviewers agree the paper makes an interesting contribution to the growing normalizing flow literature. The paper is well written and the idea is novel. Additionally, the experimental results are promising and the additional experiments and baselines added during the rebuttal further strengthen the paper. I recommend acceptance.
The reviewers are unanimous that this is a strong submission that deserves to be accepted.
Pros:   rather novel approach to using optimistic MCTS for exploration with deterministic models   positive rewards on Pitfall  Cons:   lost of domain specific knowledge   deteministic models   lacking clarity   lacking ablations   no rebuttal  I agree with both reviewers that the paper is not good enough to be accepted.
There were both positive and negative assessments of this paper by the reviewers: It was deemed a well written paper that explores cleanly rederiving the TC VAE in the Wasserstein Autoencoder Framework and that has experiments comparing to competing approaches. However, there are two strong concerns with this paper: First, novelty appears to be strongly limited as it appears a rederivation using known approaches. Second, two reviewers were not convinced by the experimental results and do not agree with the claim that the proposed approach is better than competing methods in providing disentangled representations. I agree with this concern, in particular as assessing unsupervised disentanglement models is known to be very hard and easily leads to non informative results (see e.g. the paper cited by the authors  from Locatello et al., 2019). Overall, I recommend rejecting this paper.
This paper examines the relationship between attention and alignment in NMT. The reviewers all agreed that this is a valuable topic that is worth thinking about.  However, there were concerns both about the clarity of the paper and the framing with respect to previous work. First, it was hard for some reviewers to understand exactly what the paper was trying to do due to issues of the paper structure, etc. Second, there are a number of previous works that also examine similar concepts, and the description of how the proposed method differs seemed lacking.  Due to these issues, I cannot recommend it for acceptance in its current form.
This paper proposed to use a compressive sensing approach for neural architecture search, similar to Harmonica for hyperparameter optimization.   In the discussion, the reviewers noted that the empirical evaluation is not comparing apples to apples; the authors could not provide a fair evaluation. Code availability is not mentioned. The proof of theorem 3.2 was missing in the original submission and was only provided during the rebuttal. All reviewers gave rejecting scores, and I also recommend rejection. 
The paper focuses on semi supervised learning and presents a pseudo labeling based approach with i) mixup (Zhang et al. 2018); ii) keeping $k$ labelled examples in each minibatch.  The paper is clear and well written; it presents a simple and empirically effective idea. Reviewers appreciate the nice proof of concept on the two moons dataset, the fact that the approach is validated with different architectures. Some details would need to be clarified, e.g. about the dropout control.  A main contribution of the paper is to show that pseudo labelling plus the combination of mixup and certainty (keeping $k$ labelled examples in each minibatch) can outperform the state of the art based on consistency regularization methods, while being simpler and computationally much less demanding.   While the paper does a good job of showing that "it works", the reader however misses some discussion about "why it works". It is most interesting that the performances are not improving with $k$ (Table 1). An in depth analysis of the trade off between the uncertainty (through mix up and the entropy of the pseudo labels) and certainty, and how it impacts the performance, would be appreciated. You might consider monitoring how this trade off evolves along learning; I suspect that evolving $k$ along the epochs might make sense;  the question is to find a simple way to control online this hyper parameter.    The area chair encourages the authors to continue this very promising path of research, and dig a little bit deeper, considering the question of optimizing the trade off between certainty and uncertainty along the training trajectory.
The authors have proposed a  soft  version of VIN which is differentiable, where the cost function is trained by behavior cloning / imitation learning from expert/computer trajectories. The method is applied to a toy problem and to real historical data from mars rovers. The paper does not acknowledge nor compare against other methods, and the contribution is unclear, as is the justification for some of the aspects of the method.  Additionally it is difficult to interpret the relevance or significance of the results (45% correct).
This paper tackles the problem of using auxiliary losses to help regularize and aid the learning of a "goal" task. The approach proposes avoiding the learning of irrelevant or contradictory details from the auxiliary task at the expense of the "goal" tasks by observing cosine similarity between the auxiliary and main tasks and ignore those gradients which are too dissimilar.   To justify such a setup one must first show that such negative interference occurs in practice, warranting explicit attention. Then one must show that their algorithm effectively mitigates this interference and at the same time provides some useful signal in combination with the main learning objective.   During the review process there was a significant discussion as to whether the proposed approach sufficiently justified its need and usefulness as defined above. One major point of contention is whether to compare against the multi task literature. The authors claim that prior multi task learning literature is out of scope of this work since their goal is not to measure performance on all tasks used during learning. However, this claim does not invalidate the reviewer s request for comparison against multi task learning work. In fact, the authors *should* verify that their method outperforms state of the art multi task learning methods. Not because they too are studying performance across all tasks, but because their method which knows to prioritize one task during training should certainly outperform the learning paradigms which have no special preference to one of the tasks.   A main issue with the current draft centers around the usefulness of the proposed algorithm. First, whether the gradient co sine similarity is a necessary condition to avoid negative interference and 2) to show at least empirically that auxiliary losses do offer improved performance over optimizing the goal task alone. Based on the experiments now available the answers to these questions remains unclear and thus the paper is not yet recommended for publication.
While fusing multiple heterogeneous neural networks into a single network looks like an interesting exploration, there are many major concerns raised by the reviewers: 1) The motivation why the proposed method works is not convincing. In other words, under what conditions the proposed would work or would not work is not clear. 2) The authors failed to provide either theoretical analysis or convincing empirical studies of the proposed method. In the rebuttal, the authors did not address the critical issues raised by the reviewers. 3) There are many other detailed problems about the proposed method as well as the experimental setup.  Therefore, by considering the above concerns, this submission does not meet the standard of publication at ICLR.
This paper tackles the problem of learning off policy in the contextual bandit problem, more specifically when the available data is deficient (in the sense that it does not allow to build reasonable counterfactual estimators). To address this, the authors introduce three strategies: 1) restricting the action space; 2) imputing missing rewards when lacking data; 3) restricting the policy space to policies with "enough" data. All three approaches are analyzed (statistical and computational properties) and evaluated empirically. Restricting the policy space appears to be particularly effective in practice.  Although the problem being solved is very relevant,  it is not clear how this work is positioned with respect to approaches solving similar problems in RL. For example, Batch constrained Q learning ([1]) restricts action space, while Bootstrapping Error Accumulation ([2]) and SPIBB ([3]) restrict the policy class in batch RL. A comparison with these techniques in the contextual bandit settings, in addition to recent state of the art off policy bandit approaches (Liu et al. (2019), Xie et al. (2019)) is lacking. Moreover, given the newly added results (DR method by Tang et al. (2019)), it is not clear how the proposed approach improves over existing techniques. This should be clarified. I therefore recommend to reject this paper.  
The reviewers all agree that the problems studied in this paper are interesting, and the solutions provided are reasonable.  However qualitative and quantitative comparisons to state of the art methods are missing, and the sensing model assumed by the paper needs to be more well motivated. 
This paper proposes an approach for incremental learning of new classes using meta learning. Strengths: The framework is interesting. The reviewers agree that the paper is well written and clear. The experiments include comparisons to prior work, and the ablation studies are useful for judging the performance of the method. Weaknesses: The paper does not provide significant insights over Gidaris & Komodakis  18. Reviewer 1 was also concerned that the motivation for RBP is not entirely clear. Overall, the reviewers found that the strengths did not outweigh the weaknesses. Hence, I recommend reject. 
This manuscript makes an interesting observation: there is no reason why planning based methods like MDPs must be limited to physical or grounded environments. One can plan about more abstract textual domains. It adapts the standard methods from planning to such text domains in a fairly straightforward way. The fact that concepts from MDPs map to these problems directly is an asset: ideas could flow between these domains in the long term. While the original submission was lacking clarity and significant technical details, the authors engaged with the reviewers and resolved lingering concerns. Reviewers are unanimous that this a strong contribution.
meta score: 4  This is basically an application in which some different deep learning approaches are compared on the task of automatically identifying domain names automatically generated by malware.  The experiments are well constructed and reported.  However, the work does not have novelty beyond the application domain, and thus is not really suitable for ICLR.  Pros    good set of experiments carried out on an important task    clearly written Cons    lacks technical novelty 
For meta learning with variable shot, this paper proposes a method for adapting the learning rate by a function of the number of training examples. The functional form is theoretically derived, and the method is simple and effective. However, meta learning methods that adapt learning rates have been proposed, and the novelty is not high enough.
This paper proposes a hierarchical reinforcement learning approach that exploits affordances to better explore/prune the subtasks, and thus making the overall learning more efficient.   The idea of the paper is novel and interesting.   After the rebuttal, all the reviewers agree that the paper is a solid contribution. Therefore, I recommend acceptance of this paper.
This paper proposes a new measure to quantify the contribution of an individual neuron within a deep neural network. Interpretability and better understanding of the inner workings of neural networks are important questions, and all reviewers agree that this work is contributing an interesting approach and results.
The paper describes  a method for the link prediction problem in both directed and undirected hypergraphs.  While the problem discussed in the paper is clearly importnant and interesting, all reviewers agree that the novelty of the proposed approach is somewhat limited given the prior art.
This paper proposed a self supervised speech pre training approach, by the name of SPIRAL, to learning perturbation invariant representations in a teacher student setting.  The authors introduced a variety of techniques to improve the performance and stabilize the training.  Compared to the popular unsupervised learning model wav2vec 2.0, better WERs were reported using SPIRAL with a reduced training cost.  All reviewers considered the work solid with sufficient novelty but also raised concerns regarding the generalization under unseen real world noisy conditions and missing decoding details.  The authors responded with new Chime 3 results  and updated LM decoding results.  The new results show that, after a bug fix, SPIRAL can outperform wav2vec 2.0 when no external LM is used.    Overall the proposed approach is technically novel.  The experiments are extensive and the results are compelling. In addition, the training time can be significantly reduced compared to wav2vec 2.0. All reviewers are supportive.  So I would recommend accept.
The paper presents a novel method for learning with noisy labels based on an interesting insight into the learning dynamics of deep neural networks.   Reviewers unanimously vote for acceptance. I agree with their assessment, and it is my pleasure to recommend the paper for acceptance.   If I can draw attention to one comment, I strongly agree with R1 that the criterion in Eq. (3) is somewhat poorly motivated. I believe the paper would benefit from a clearer exposition of this part.   Please make sure to address all reviewers  remarks in the camera ready version. Thank you for submitting your work to ICLR.
This paper presents a method for unsupervised learning of disentangled representations by first training a VAE with a tangled set of latents, and then sequentially learning disentangled latent variables one at a time from the entangled initial VAE latent space. On several toy disentanglement benchmarks, the method is shown to perform competitively with previous VAE and GAN approaches.   There were several concerns from reviewers around the clarity and description of the proposed one factor at a time (OAT) training procedure. While the updated draft addressed several typos and some clarity issues, multiple reviewers continued to find the method description problematic. There were additional concerns around the viability of the method on real world datasets where the number of factors are not known, and as the authors stated the proposed method can also result in one factor of variation encoded into mulitple latent variables, which hurts on many of the disentanglement metrics.  The addition of CelebA downstream task evaluation begins to address this concern of real world data, but more rigorous experiments (including more description of how models were selected) and discussion of the limtiations of the proposed method are needed. There is also no theoretical motivation as to why the proposed intervention based factor learning algorithm should recover the ground truth factors.  Given the concerns over experimental results, clarity, and lack of theoretical motivation, I suggest rejecting this paper in the current form.
This paper proposes a WTA models for binary projection.  While there are notable partial contributions, there is disagreement among the reviewers.   I am most persuaded by the concern expressed that the experiments are not done on datasets that are large enough to be state of the art compared to other random projection investigations.
This paper proposes a method for neural architecture search in embedding space. This is an interesting idea, but its novelty is limited due to its similarity to the NAO approach. Also, the empirical evaluation is too limited; comparisons should have been performed to NAO and other contemporary NAS methods, such as DARTS.   Due the factors above, all reviewers gave rejecting scores (3,3,1). The rebuttal did not remove the main issues, resulting in the reviewers sticking to their scores. I therefore recommend rejection.
This paper proposes a theoretically sound and practically effective method to compress quantized gradients and reduce communication in distributed optimization. The method is interesting and worth publication.
This paper extends a recent approximate dynamic programming method (i.e., DP with neural networks) for a ride sharing problem. An elegant trick is proposed to obtain a more expressive function approximation without suffering a combinatorial explosion of the action space. While the idea is somewhat ad hoc in its implementation, and limited in novelty w.r.t. the ADP work that the paper builds on, the empirical performance improvement on the ride sharing problem is clear.  Initially, the reviewers also raised several clarity and presentation issues, but the authors did a good job in addressing them in their rebuttal.  The reviewers gave scores of 5,8,5. The main critique is limited novelty. During the discussion, we focused on the novelty of the approach, whether the ideas can be generalized beyond the very specific ride sharing problem, and whether the work is strong enough if viewed as an application paper. The conclusion, which my final decision is based on, is that currently, the contribution is very specific to the ride sharing problem, and it is not clear whether this idea can be extended to more general optimization problems. This means that the scope of the algorithmic approach, taken with respect to the ICLR audience, is rather narrow. On the other hand, the current presentation does not meet the bar of a strong application paper, as there is not enough novelty in the problem and data.  My advice to the authors is to broaden their investigation and evaluation. Another option would be to target a venue that is more focused on the ride sharing problem.
 pros:   Good quantitative results showing clear improvement over other model based methods in sample efficiency and computational cost (though see Reviewer 2 s concerns about the need for more experiments on computational cost).   Cool qualitative results showing discovery of BFS and DFS   Potentially novel approach (see cons)  cons:   Lack of clarity especially concerning equation (1).  Both Reviewers 1 and 3 were unsure of the rationale for this equation which lies at the heart of the method.  It looks to me like a combination of surprise and value but the motivation is not clear.  There are a number of other such places pointed out by the reviewers where model choices were made that seem ad hoc or not well motivated.   In general it s hard to understand which factors are important in driving the results you report.  As Reviewer 3 points out, more ablation studies and analysis would help here.  Providing more motivation, explanation and analysis would help the reader understand better the reasons for the performance of the model.  The results are nice and the method is intriguing.  I think this potentially a very nice paper and if you can address the above concerns but isn t quite up to the acceptance bar for ICLR this year. 
This work examines the AlphaGo Zero algorithm, a self play reinforcement learning algorithm that has been shown to learn policies with superhuman performance on 2 player perfect information games.  The main result of the paper is that the policy learned by AGZ corresponds to a Nash equilibrium, that and that the cross entropy minimization in the supervised learning inspired part of the algorithm converges to this Nash equillibrium, proves a bound on the expected returns of two policies under the and introduces a "robust MDP" view of a 2 player zero sum game played between the agent and nature.  R3 found the paper well structured and the results presented therein interesting. R2 complained of overly heavy notation and questioned the applicability of the results, as well as the utility of the robust MDP perspective (though did raise their score following revisions).  The most detailed critique came from R1, who suggested that the bound on the convergence of returns of two policies as the KL divergence between their induced distributions decreases is unsurprising, that using it to argue for AGZ s convergence to the optimal policy ignores the effects introduced by the suboptimality of the MCTS policy (while really interesting part being understanding how AGZ deals with, and whether or not it closes, this gap), and that the "robust MDP" view is less novel than the authors claim based on the known relationships between 2 player zero sum games and minimax robust control.   I find R1 s complaints, in particular with respect to "robust MDPs" (a criticism which went completely unaddressed by the authors in their rebuttal), convincing enough that I would narrowly recommend rejection at this time, while also agreeing with R3 that this is an interesting subject and that the results within could serve as the bedrock for a stronger future paper.
The paper presents an empirical study of different strategies for fine tuning a large language model for the task of generating Java Unit tests *for a specific project*.  As several reviewers pointed out, the setup itself is fairly impractical, requiring fine tuning on an individual project, thus making it applicable only to the very tail end of very large projects where the investment of doing this would make sense and where one could reasonably collect sufficient data for that project.   On top of that, the paper contributes relatively little in terms of novel techniques. This in itself would be OK if the paper presented some extremely important empirical evidence. However, reviewers also raised some important concerns with the empirical evaluation itself. For example, as reviewer 1jM4 pointed out, there is prior research explicitly showing that the BLEU score is not a good measure for code evaluation.   Overall, the meta reviewer agrees with the reviewers that this paper is below the bar for publication.
The paper considers the problem of abstention in robust classification. A number of issues were identified in the formal framework and the writing was also not up to scratch. The authors should take into regard the very many constructive suggestions made by the reviewers in preparing a revision.
This paper proposes a method for learning neural RFs with the inclusive divergence minimization problem.  Reviewers generally agree that the experiments are sufficient and convincing, and that the method is evaluated well. Results are comparable with SOTA methods for image generation. The paper is reasonably well written.  The paper is also somewhat lacking in background; most people at ICLR will not be very familiar with this learning problem. More information on the inclusive divergence minimization problem would be helpful. A major concern of reviewers is whether novelty of the method is sufficient for publication. 
This paper investigates an existing method for fitting sparse neural networks, and provides a novel proof of global convergence.  Overall, this seems like a sensible, if marginal, contribution.  However, there were serious red flags regarding the care which which the scholarship was done which make me deem the current submission unsuitable for publication.  In particular, two points raised by R4, which were not addressed even after the rebuttal:  1) "One important issue with the paper is that it blurs the distinction between prior work and the new contribution. For example, the subsection on Split Linearized Bregman Iteration in the "Methodology" section does not contain anything new compared to [1], and this is not clear enough to the reader."  2) "The newly written conclusion is still incorrect, stating again that Split LBI achieves SOTA performance on ImageNet."  I believe that R3 s high score is due to not noticing these unsupported or misleading claims.  
This submission introduces a theoretical model to explain how "in context learning" (i.e. the ability to output a correct prediction based on inputs for a task that the model was not explicitly trained on) is possible. The model uses a mixture of HMMs and shows that in context learning is a natural consequence of Bayesian inference under that model. Overall, reviewers agreed that the contribution was useful and timely, and were somewhat convinced by the theoretical arguments. However, there was some broad concern with the framing of the paper. Namely, 1) The paper claims that prompted data is OOD w.r.t. the pre training distribution. In fact, this is almost certainly not the case for many tasks and datasets. Indeed, it is highly plausible that data very similar to the example given by the paper (identifying the nationality of different celebrities) appears in the pre training dataset of large LMs. Other examples include the popular "tldr;" task format for summarization which is incredibly common on the internet, etc. 2) The paper does not sufficiently distinguish between insights gained in the toy setting considered by the theoretical model and insights that can be applied to large LMs. Most reviewers were concerned that there might not be any reason to think that the insights gained from the theoretical model would apply to large LMs. The paper, however, very much frames itself as developing insight into the behavior of large LMs.  I will recommend acceptance of this paper, but will stipulate that the above two issues should be fixed in the camera ready version. Namely, I would suggest that the authors do not refer to prompted forms of tasks/datasets as "OOD", and I would suggest that any claims about different insights are not applied to large LMs.
This work is interesting because it s aim is to push the work in intrinsic motivation towards crisp definitions, and thus reads like an algorithmic paper rather than yet another reward heuristic and system building paper. There is some nice theory here, integration with options, and clear connections to existing work.  However, the paper is not ready for publication. There were were several issues that could not be resolved in the reviewers minds (even after the author response and extensive discussion). The primary issues were: (1) There was significant confusion around the beta sensitivity figs 6,7,8 appear misleading or at least contradictory to the message of the paper. (2) The need for x,y env states. (3) The several reviewers found the decision states unintuitive and confused the quantitative analysis focus if they given the authors primary focus is transfer performance. (4) All reviewers found the experiments lacking. Overall, the results generally don t support the claims of the paper, and there are too many missing details and odd empirical choices.    Again, there was extensive discussion because all agreed this is an interesting line of work. Taking the reviewers excellent suggestions on board will almost certainly result in an excellent paper. Keep going!
This paper studies the properties of L1 regularization for deep neural network. It contains some interesting results, e.g. the stationary point of an l1 regularized layer has bounded number of non zero elements. On the other hand, the majority of reviewers has concerns on that experimental supports are weak and suggests rejection. Therefore, a final rejection is proposed.
This paper proposes an alternative loss function, the max mahalanobis center loss, that is claimed to improve adversarial robustness.   In terms of quality, the reviewers commented on the convincing experiments and theoretical results, and were happy to see the sample density analysis.   In terms of clarity, the reviewers commented that the paper is well written.   The problem of adversarial robustness is relevant to the ICLR community, and the proposed approach is a novel and significant contribution in this area.   The authors have also convincingly answered the questions of the authors and even provided new theoretical and experimental results in their final upload. 
As pointed out by some reviewers, the proposed method basically puts progressive training in the federated context. The theoretical analysis only concerns the centralized or non federated setting and thus give no insight or guidelines for progressive training in federated learning. The main advantage of saving communication mainly comes from the simple observation that less parameters are computed and communicated during each round before the full end to end stage. However, this may cause extra overhead in hyper parameter tuning including number of stage, learning rate schedules and stage wise warmup. Despite its potential effectiveness in practice, the current version of the paper falls short of the acceptance bar due to the weakness in novelty and relevant theory for federated learning.
All reviewers recommended accept after author responses. AC doesn t find any reason to overturn this consensus.
This paper proposes a selection mechanism to choose between a certified model with low clean accuracy and a naturally trained model with high accuracy, to improve the standard clean accuracy for certifiably robust models.  At a high level, the idea behind this combined system is that when the certified model cannot certify, one should avoid using it for classification, but rather should use a naturally trained model.  A state of the art naturally trained networks is used as the "core network", and a small certification network with high certifiable robustness is used as the "certification network". The major contribution is a selection network that adaptively chooses between these two networks.   Pro + The idea of using two networks adaptively is novel. The proposed selection mechanism has been shown to be able to combine the merits of both networks to obtain better natural accuracy with good certified robustness.    Con   The experiment section still has room for improvement. Specifically, the presentation of the results were not convincingly conveying the tradeoff between the clean accuracy and the certified accuracy.  After the rebuttal, the authors made some improvements that addressed many of the concerns about the clarity and reproducibility issues. However, reviewers suggest further polishing the experiment section.   Overall, I think the novelty of the paper combined with the promising results achieved outweigh the presentation issues. I would recommend accepting this paper. 
The paper proposes a combination graph neural networks and graph generation model (GraphRNN) to model the evolution of dynamic graphs for predicting the topology of next graph given a sequence of graphs.  The problem to be addressed seems interesting, but lacks strong motivation. Therefore it would be better if some important applications can be specified.    The proposed approach lacks novelty. It would be better to point out why the specific combination of two existing models is the most appropriate approach to address the task.   The experiments are not fully convincing. Bigger and comprehensive datasets (with the right motivating applications) should be used to test the effectiveness of the proposed model.   In short, the current version failed to raise excitement from readers due to the reasons above. A major revision addressing these issues could lead to a strong publication in the future. 
The authors present a neural framework for learning SAT solvers that takes the form of probabilistic inference. The whole process consists of propagation, decimation and prediction steps, so unlike other prior work like Neurosat that learns to predict sat/unsat and only through this binary signal,  this work presents a more modular approach, which is learned via energy minimization and it aims at predicting assignments (the assignments are soft which give rise to a differentiable loss). On the other hand, at test time the method returns the first soft assignment whose hard version (obtained by thresholding) satisfies the formula.  Reviewers found this to be an interesting submission, however there were some concerns regarding (among others) comparison to previous work.    Overall, this submission has generated a lot of discussion among the reviewers (also regarding how this model actually operates)  and it is currently borderline without a strong champion. Due to the concerns raised and the limited space in the conference s program, unfortunately I cannot recommend this work for acceptance. 
All Reviewers agree that the paper has a clear and solid contribution. Furthermore, all of them highlight that the paper has improved significantly after revision. Hence, my recommendation is to ACCEPT the paper. As a brief summary, I highlight below some pros and cons that arose during the review and meta review processes.  Pros:   Comparison across network architectures.   Comparison across a broad range of different data sets.   Compactness of the representation (few parameters to learn).   Authors will share code.  Cons:   Role of L2 normalization could be further discussed.
In this paper, the authors present a method that combines genetic data (using a hierarchical, graph convolution approach) with imaging data to predict schizophrenia. The reviewers raised several concerns that the authors have addressed. Some of the concerns were relevant to writing, the authors have clarified these points. Another important concern was about the baselines. The authors added other baselines. One of the baselines they added was GUIDE with random dropout, this baseline performs as well as GUIDE, but the authors argue that GUIDE leads to robust features.   I ask the authors to move the other baseline results, specifically the GUIDE with random dropout, and the relevant discussion to the main manuscript, and to consequently temper the discussion of the bayesian feature selection. Currently these additional results are only in the appendix, and not the text. Conditional on this, I recommend acceptance.
The paper uses quasi potential theory to analyze the escape behavior of SGD. Although this is a topic of interest to the ML community, the reviewers found a critical issue with the paper, which the authors admit can not be fixed during this submission. I, therefore, do not think there is a need for a longer discussion.
Two of the initial reviews of the paper were mildly positive (2 scores of 6), and one was very positive (score of 8). However, these reviews failed to notice some severe issues with the paper, which were detailed by the Area Chair in an Extra Review which was provided late. The severe issues include: clarity of exposition (undefined notation in many places) and theory (vacuous or meaningless theorems and assumptions). I apologize to the authors for not having had the chance to defend against this late review. However, the issues are indeed severe.
All reviewers agree that the paper overclaims its contributions both in the main text and in the title, and given also the limited novelty  and scope it is not suggested for publication.
Understanding the generalization behavior of deep networks is certainly an open problem. While this paper appears to develop some interesting new Fourier based methods in this direction, the analysis in its current form is currently too restrictive, with somewhat limited empirical support, to broadly appeal to the ICLR community. Please see the reviews for more details.  
An approach to make multi task learning is presented, based on the idea of assigning tasks through the concepts of cooperation and competition.   The main idea is well motivated and explained well. The experiments demonstrate that the method is promising. However, there are a few  concerns regarding fundamental aspects, such as: how are the decisions affected by the number of parameters? Could ad hoc algorithms with human in the loop provide the same benefit, when the task set is small? More importantly, identifying task groups for multi task learning is an idea presented in prior work, e.g. [1,2,3]. This important body of prior work is not discussed at all in this paper.  [1] Han and Zhang. "Learning multi level task groups in multi task learning" [2] Bonilla et al. "Multi task Gaussian process prediction" [3] Zhang and Yang. "A Survey on Multi Task Learning" 
The reviewers that provided extensive and technically well justified reviews agreed that the paper is of high quality. The authors are encouraged to make sure all concerns of these reviewers are properly addressed in the paper.
Dear Authors,  The paper was received nicely and discussed during the rebuttal period. There is consensus among the reviewers that the paper should be accepted:    The new result about query complexity of regression problem that the authors have added. Along with the result on   for (noisy) Vandemonde matrix, these make the paper lie above the accept bar.   The authors have providing satisfying clarifications during the rebuttal that convinced reviewers to increase further their scores.  The current consensus is that the paper deserves publication.  Best AC
This paper proposes an MCTS method for neural architecture search (NAS). Evaluations on NAS Bench 101 and other datasets are promising. Unfortunately, no code is provided, which is very important in NAS to overcome the reproducibility crisis.  Discussion: The authors were able to answer several questions of the reviewers. I also do not share the concern of AnonReviewer2 that MCTS hasn t been used for NAS before; in contrast, this appears to be a point in favor of the paper s novelty. However, the authors  reply concerning Bayesian optimization and the optimization of its acquisition function is strange: using the ConvNet 60K dataset with 1364 networks, it does not appear to make sense to use only 1% or even only 0.01% of the dataset size as a budget for optimizing the acquisition function. The reviewers stuck to their rating of 6,3,3.  Overall, I therefore recommend rejection. 
The field of deep learning optimization suffers from a lack of standard benchmarks, and every paper reports results on a different set of models and architectures, likely with different protocols for tuning the baselines. This paper takes the useful step of providing a single benchmark suite for neural net optimizers.   The set of benchmarks seems well designed, and covers the range of baselines with a variety of representative architectures. It seems like a useful contribution that will improve the rigor of neural net optimizer evaluation.   One reviewer had a long back and forth with the authors about whether to provide a standard protocol for hyperparameter tuning. I side with the authors on this one: it seems like a bad idea to force a one size fits all protocol here.   As a lesser point, I m a little concerned about the strength of some of the baselines. As reviewers point out, some of the baseline results are weaker than typical implementations of those methods. One explanation might be the lack of learning rate schedules, something that s critical to get reasonable performance on some of these tasks. I get that using a fixed learning rate simplifies the grid search protocol, but I m worried it will hurt the baselines enough that effective learning rate schedules and normalization issues come to dominate the comparisons.  Still, the benchmark suite seems well constructed on the whole, and will probably be useful for evaluation of neural net optimizers. I recommend acceptance.  
The paper describes a new data augmentation approach for image based RL.  The approach is both simple and effective.  It improves significantly the performance of several algorithms across a number of tasks.  The reviewers were unanimous about the benefits of the proposed technique.  This represents an important advance for RL.
This submission proposes a VAE based method for jointly inferring latent variables and data generation. The method learns from partially observed multimodal data.  Strengths:  Learning to generate from partially observed data is an important and challenging problem.  The proposed idea is novel and promising.  Weaknesses:  Some experimental protocols are not fully explained.  The experiments are not sufficiently comprehensive (comparisons to key baselines are missing).  More analysis of some surprising results is needed.  The presentation has much to improve.  The method is promising but the mentioned weaknesses were not sufficiently addressed during discussion. AC agrees with the majority recommendation to reject. 
The paper proposes a representation learning objective that makes it  amenable to planning,   The initial submission contained clear holes, such as missing related work and only containing very simplistic baselines. The authors have substantially updated the paper based on this feedback, resulting in a clear improvement.  Nevertheless, while the new version is a good step in the right direction, there is some additional work needed to fully address the reviewers  complaints. For example, the improved baselines are only evaluated in the most simple domain, while the more complex domains still only contain simplistic baselines that are destined to fail. There are also some unaddressed questions regarding the correctness of Eq. 4. Finally, the substantial rewrites have given the paper a less than polished feel.  In short, while the work is interesting, it still needs a few iterations before it s ready for publication.
The paper addresses the problem of inconsistent gradients in multi task learning, proposing ways to handle both their magnitude nd direction. Gradient directions are aligned by introducing a rotation layer between the shared backbone and task specific branches.  Reviewers appreciated the technical approach, higlighting the novelty of the rotation layers in this context. The empirical evaluations are systematic fair and insightful, and the presentation is polished. Reviewers unanimously supported accepting the paper.
The proposed method combines supervised pretraining given some expert data and further uses the supervision to regularize the Q updates to prevent the agent from exploring  nonsense  directions. There a significant problems with the paper: the approach is not novel, the assumption of large amounts of expert data is problematic, and the claim of vastly accelerated learning is not supported empirically, either in the main paper or in the additional mujoco experiments added in the appendix.
Overall the review is borderline: R2 and R4 are slightly positive and R3 is slightly negative. All the reviewers like the novel shading consistency loss proposed in the paper and, improved DIP that produces consistent image decomposition inferences, and good experimental results. However, reviewers also shared concerns about speed and the thoroughness of the evaluation, and human tolerance of shading inaccuracy. These points were addressed in details in the rebuttal, and reviewers didn’t change their initial scores.  The AC is concerned about the cut and paste neural rendering results. Because there are no cast shadows, the rendering doesn’t look realistic under the lighting conditions in the new image. It’s unclear that the proposed method would lead to a promising direction of copying and pasting contents into images for photorealistic editing. Consequentially, the paper is not ready for publication at its current form. 
The paper presents an interesting perspective on improving offline RL within BRAC framework.  Given the improvements over BRAC, the paper is well organized and easy to understand.   The overall results pique interest in comparison with more recent Offline/Batch RL papers: BRAC, BEAR, CQL.  The results in this paper bring BRAC family of methods closer to CQL with a number of practical improvements, and could have impact in practice.   However, the reviewers have slight split over the marginal value of additional machinery. There do remain some concerns:   KL divergence is not the best metric to capture OOD issues between policies.    The additional machinery in comparison to CQL may be unnecessary, at least in terms of results.    The method requires many task specific key hyper parameters, which limits the generality of the approach.  I would recommend rejection as it stands. The paper needs more careful empirical analysis that explains what methodical improvements are actually required and which ones only provide marginal bumps.  With multiple task specific hyper params, it may be tricky for these ideas to realize their potential if not clearly understood.  Further release of sufficiently documented and easy to use implementation, will probably be required for acceptance since the main argument in the paper are number of technical improvements in BRAC.
While this paper has divergent reviews, reviewer hSRE has by far the most detailed review, seems clearly the most informed on the subject, and is the least supportive.  The main issues with the paper seems to be the degree of novelty and reviewer hSRE s feeling that the results on MojuCo are unclear and not explained in the paper.  But this alone does not seem like an adequate reason for rejection and hSRE seems happy with the other aspects of the paper.  Some of hSRE s original complaints do not concern me, such as the fact that first order stationarity does not imply Pareto optimality.  I am recommending a poster.
This paper is rejected.  All of the reviewers found the empirical results strong. However, R3 and R4 pointed out concerns with the positioning of the work relative to prior work and that their approach is conceptually similar to previous work. The authors have tried to address these concerns in their rebuttal. Both reviewers appreciate the changes, but still have remaining concerns that I agree with. Based on these concerns, it is unclear if the strong empirical results are mostly due to using the NVAE architecture, rather than a methodological improvement over previous methods. The authors should work on positioning their paper in the context of prior work and the comparisons requested by R3 and R4 for a resubmission. 
This paper proposed to evaluate the robustness of CNN models on similar video frames. The authors construct two carefully labeled video databases. Based on extensive experiments, they conclude that the state of the art classification and detection models are not robust when testing on very similar video frames. While Reviewer #1 is overall positive about this work, Reviewer #2 and #3 rated weak reject with various concerns. Reviewer #2 concerns limited contribution since the results are similar to our intuition. Reviewer #3 appreciates the value of the databases, but concerns that the defined metrics make the contribution look huge. The authors and Reviewer #3 have in depth discussion on the metric, and Reviewer #3 is not convinced. Given the concerns raised by the reviewers, the ACs agree that this paper can not be accepted at its current state.
 Pros:  *  High quality evaluation across different benchmarks, plus human eval  *  The paper is well written (though one could quibble about the motivation for the method, see Cons)  Cons:  *  The approach is incremental, the main contribution is replacing marginalization or RL with G S. G S has already been studied in the context of VAEs with categorical latent variables, i.e. very similar models.  *  The main technical novelty is varying amount of added noise (i.e. downscaling Gumbel noise). In principle, the Gumbel relaxation is not needed here as exact marginalization can be done (as) effectively. Unlike the standard strategy used to make discrete r.v. tractable in complex models, samples from G S are not used in this work to weight input to the  decoder  (thus avoiding expensive marginalization) but to weight terms corresponding to reconstruction from individual latent states (in constract, e.g., to SkimRNN of Seo et al (ICLR 2018)). Presumably adding noise to softmax helps to force sharpness on the posteriors (~ argmax in previous work) and stochasticity may also help exploration.    (Given the above, "to preserve differentiability and circumvent the difficulties in training with reinforcement learning, we apply the reparameterization trick with Gumbel softmax" seems slightly misleading)   *  With contextualized embeddings, which are sense disambiguated given the context, learning discrete senses (which are anyway only coarse approximations of reality) is less practically important  Two reviewers are somewhat lukewarm (weak accept) about the paper (limited novelty), whereas one reviewer is considerably more positive. I do not believe that the reviews diverge in any factual information though.    
This paper proposes a semi supervised setting to reduce memory budget in replay based continual learning. It uses unlabeled data in the environment for replaying which requires no storage, and generates pseudo labels where unlabeled data is connected to labeled one. The method was validated on the proposed tasks.  Pros:   The semi supervised continual learning setting is novel and interesting.   The proposed approach is memory efficient, since it does not need exemplars to replay past tasks.  Cons:   The scale of experiment is small. It lacks evaluation in real world environment.   The novelty is limited, because it is a combination of existing technologies: pseudo labeling, consistency regularization, Out of Distribution (OoD) detection, and knowledge distillation.   The comparison might not be fair due to different settings.  The authors addressed the fairness and scalability with additional experiments and leave some suggestions of reviewers for future work. R3 had a concern on the error propagation of pseudo labels which I also share. The authors agreed that this is a challenge for all CL methods.  In summary, the reviews are mixed. All reviewers agree that the semi supervised continual learning setting is novel and interesting, and some have concerns on scalability and novelty of the method which I also share. So at present time I believe there is much room for the authors to improve their method and experiments before publication. 
The submission proposes a strategy for quantization of neural networks with skip connections that quantizes only the convolution paths, while leaving the skip paths at full precision.  The approach can save computation through compressing the convolution kernels, while spending more on the skip connections. Empirical results show improved performance at 2 bit quantization compared to a handful of competing methods.  Figure 5 provides some interpretation of why the method might be working in terms of "smoothness" of the loss surface (term not used in the traditional mathematical sense).  The paper seems to focus too much on selling the name "precision highway" rather than providing proper definitions of their strategy (a definition block would be a good first step), and there is little mathematical analysis of the consequences of the chosen approach. There are concerns about the novelty of the method, specifically compared to Liu et al. (2018) and Choi et al. (2018b), which propose approximately the same strategy.  Footnote 1 claims that these works were conducted in parallel with the current submission, but it is unambiguously the case that Choi et al appeared on arXiv in May, and Liu et al. appeared in ECCV 2018 and on arXiv more than 30 days before the ICLR deadline, and can fairly be considered prior work https://iclr.cc/Conferences/2019/Reviewer_Guidelines  The reviewer scores were on aggregate borderline for the ICLR acceptance threshold.  On the balance, the paper seems to fall under the threshold due to insufficient novelty and analysis of the method. 
In this paper, the authors leverage information gain in conjunction with Bayesian Neural Networks in order to to improve the robustness of Bayesian Neural Networks. However, as pointed out by reviwers, there are several mistakes in theier derivations and evaluations. Moreover, the authors failed to crrectly refer to the exisiting work proposing similar methods.
The objective of the paper is to develop a framework for solving PDES with reduced model size and for scarce observation settings. It proposes to use functional input dependent convolutions for learning spatio temporal differential operators together with a non linear numerical scheme (Picard solver). Training makes use of an adjoint formulation.  All the reviewers agree that the authors improved the initial version but opt for a reject. In its present form, the technical description is still incomplete with missing explanations. The experiments should be reinforced and the results are partly unexplained.
The paper proposes metrics for comparing explainability metrics.  Both reviewers and authors have engaged in a thorough discussion of the paper and feedback. The reviewers, although appreciating aspects of the paper, all see major issues with the paper.   All reviewers recommend reject.  
This paper provides a technique to learn multi class classifiers without multi class labels, by modeling the multi class labels as hidden variables and optimizing the likelihood of the input variables and the binary similarity labels.   The majority of reviewers voted to accept.
* Strengths  The paper proposes a novel and interesting method for detecting adversarial examples, which has the advantage of being based on general “fingerprint statistics” of a model and is not restricted to any specific threat model (in contrast to much of the work in the area which is restricted to adversarial examples in some L_p norm ball). The writing is clear and the experiments are extensive.  * Weaknesses  The experiments are thorough. However, they contain a subtle but important flaw. During discussion it was revealed that the attacks used to evaluate the method fail to reduce accuracy even at large values of epsilon where there are simple adversarial attacks that should reduce the accuracy to zero. This casts doubt on whether the attacks at small values of epsilon really are providing a good measure of the method’s robustness.  * Discussion  There was substantial disagreement about the paper, with R1 feeling that the evaluation issues were serious enough to merit rejection and R3 feeling that they were not a large issue. In discussion with me, both R1 and R3 agreed that if an attack were demonstrated to break the method, that would be grounds for rejection. They also both agreed that there probably is an attack that breaks the method. A potential key difference is that R3 thinks this might be quite difficult to find and so merits publishing the paper to motivate stronger attacks.  I ultimately agree with R1 that the evaluation issues are indeed serious. One reason for this is that there is by now a long record of adversarial defense papers posting impressive numbers that are often invalidated within a short period (often less than a month or so) of the paper being published. The “Obfuscated Gradients” paper of Athalye, Carlini, and Wagner suggests several basic sanity checks to help avoid this. One of the sanity checks (which the present paper fails) is to test that attacks work when epsilon is large. This is not an arbitrary test but gets at a key issue any given attack provides only an *upper bound* on the worst case accuracy of a method. For instance, if an attack only brings the accuracy of a method down to 80% at epsilon 1 (when we know the true accuracy should be 0%), then at epsilon 0.01 we know that the measured accuracy of the attack comes 80% from the over optimistic accuracy at epsilon 1 and at most 20% from the true accuracy at epsilon 0.01. If the measured accuracy at epsilon 1 is close to 100%, then accuracy at lower values of epsilon provides basically no information. This means that the experiments as currently performed give no information about the true accuracy of the method, which is a serious issue that the authors should address before the paper can be accepted.
The paper received mixed reviews. It proposes a variant of Siamese network objective function, which is interesting. However, it’s unclear if the performance of the unguided method is much better than other baselines (e.g., InfoGAN). The guided version of the method seems to require much domain specific knowledge and design of the feature function, which makes the paper difficult to apply to broader cases.  
The paper proposes a method to handle Mahalanobis metric learning thorough linear programming.  All reviewers were unclear on what novelty of the approach is compared to existing work.  I recommend rejection at this time, but encourage the authors to incorporate reviewers  feedback (in particular placing the work in better context and clarifying the motivations) and resubmitting elsewhere.  
The paper aims to combine Wasserstein GAN with Improved GAN framework for semi supervised learning.  The reviewers unanimously agree that:    the paper lacks novelty and such approaches have been tried before.    the approach does not make sufficient gains over the baselines and stronger baselines are missing.    the paper is not well written and experimental results are not satisfactory.
This submission describes an approach to compressing the communication in federated learning. The key idea is using a set of random samples from a prior distribution and then performing importance weighed sampling. The work performs an analysis of the privacy guarantees of this process and experimental evaluation. The main issue with this work is the authors appear to be unaware that the basic problem they pose is solved in a more comprehensive and lossless way in a recent work https://arxiv.org/abs/2102.12099 (Feldman and Talwar, ICML 2021). That work shows that any differentially private randomizer can be compressed via a simpler algorithm that performs rejection sampling using a PRG. The algorithm does not loose privacy or utility (under standard cryptographic assumptions) while guaranteeing low communication. In contrast this work loses significantly in utility and provides opaque privacy guarantees. This submission analyzes  a randomized that adds Gaussian distribution and, in particular, the compression technique in (Feldman and Talwar) applies to it. The technique proposed in this work is very similar in spirit (with prior distribution corresponding to reference distribution in the earlier work. In light of the earlier work I do not think the contributions in this submission are sufficient for publication.
The authors study an inverse reinforcement learning problem where the goal is to infer an underlying reward function from demonstration with bias.  To achieve this, the authors learn the planners and the reward functions from demonstrations. As this is in general impossible, the authors consider two special cases in which either the reward function is observed on a subset of tasks or in which the observations are assumed to be close to optimal. They propose algorithms for both cases and evaluate these in basic experiments. The problem considered is important and challenging. One issue is that in order to make progress the authors need to make strong and restrictive assumptions (e.g., assumption 3, the well suited inductive bias). It is not clear if the assumptions made are reasonable. Experimentally, it would be important to see how results change if the model for the planner changes and to evaluate what the inferred biases would be. Overall, there is consensus among the reviewers that the paper is interesting but not ready for publication. 
There was some positive consensus towards this paper, which slightly improved after the very strong author rebuttal. Reviewers, in general, appreciate the simplicity of the approach as well as its effectiveness. The most acute criticisms derived from several theoretical and technical points, similarity with [Mizadeh, 2020], and missing baseline comparisons. The author rebuttal responds to each of these points very clearly and convincingly, as well as with new experimental baseline comparisons that clearly demonstrate the effectiveness of the CPR approach. I encourage the authors to include the extensive comparison with [Mizadeh, 2020] provided in the rebuttal, especially given the similarity to the proposed approach. and to also tone down the strong claims of novelty in light of the similarities. 
This paper presents a method for transferring source information via the hidden states of recurrent networks.  The transfer happens via an attention mechanism that operates between the target and the source.  Results on two tasks are strong.  I found this paper similar in spirit to Hypernetworks (David Ha, Andrew Dai, Quoc V Le, ICLR 2016) since there too there is a dynamic weight generation for network given another network, although this method did not use an attention mechanism.  However, reviewers thought that there is merit in this paper (albeit pointed the authors to other related work) and the empirical results are solid. 
The authors tackle an interesting and important problem, developing numerical common sense. They use a crowdsourcing service to collect a dataset and use regression from word embeddings to numerical common sense.  Reviewers were concerned with the size and quality of the dataset, the quality of the prediction methods used, and the analysis of the experimental results.  Given the many concerns, I recommend rejecting the paper, but I encourage the authors to revise the paper to address the concerns and resubmit to another venue.
This paper investigates a data selection framework for domain adaptation based on reinforcement learning.  Pros: The paper presents an approach that can dynamically adjust the data selection strategy via reinforcement learning. More specifically, the RL agent gets reward by selecting a new sample that makes the source training data distribution closer to the target distribution, where the distribution comparison is based on the feature representations that will be used by the prediction classifier. While the use of RL for data selection is not entirely new, the specific method proposed by the paper is reasonably novel and interesting.  Cons: The use of RL is not clearly motivated and justified (R1,R3) and the method presented in this paper is rather hard to follow might be overly complex (R1). One fair point R1 raised is more clean cut empirical evaluation that demonstrates how RL performs clearly better than greedy optimization. The authors came back with additional analysis in Section 4.2 to address this question, but R1 feels the new analysis (e.g., Fig 3) is not clear how to interpret. A more thorough ablation study of the proposed model might have addressed the reviewer s question more clearly. In addition, all reviewers felt that baselines are not convincingly strong enough, though each reviewer pointed out somewhat different aspects of baselines. R3 is most concerned about baselines being not state of the art, and the rebuttal did not address R3 s concern well enough.  Verdict: Reject. A potentially interesting idea but 2/3 reviewers share strong concerns about the empirical results and overall clarity of the paper.
This paper offers a new method for sentence representation learning, fitting loosely into the multi view learning framework, with fairly strong results. The paper is clearly borderline, with one reviewer arguing for acceptance and another arguing for rejection. While it is a tough decision, I have to argue for rejection in this case.  There was a robust discussion and the authors revised the paper, so none of the remaining technical issues strike me as fatal. My primary concern is simply that the reviewers could not reach a consensus in favor of the paper. In particular, two reviewers expressed concerns that this paper makes too small an advance in NLP to be of interest to non NLP researchers. I think it should be possible to broaden the scope of the paper and resubmit it to another general ML venue, and (as one reviewer suggested explicitly), this paper may have a better chance at an NLP specific venue.  While neither of these factors was crucial in the decision, I d encourage the authors (i) to put more effort into comparing properly with the Subramanian and Radford baselines, and (ii) to clarify the points about the human brain. For the second point: While none of the claims about the brain are false *or misleading*, as far as I know, the authors do not make a convincing case that the claims about the brain are actually relevant to the work being done here.
This paper introduces a neural architecture search method that is geared towards yielding good uncertainty estimates for out of distribution (OOD) samples.  The reviewers found that the OOD prediction results are strong, but criticized various points, including the presentation of the OOD results, novelty as a NAS paper, missing citations to some recent papers, and a lack of baselines with simpler ensembles. The authors improved the presentation of their OOD results and provided new experiments, which causes one reviewer to increase his/her score from a weak reject to an accept. The other reviewers appreciated the rebuttal, but preferred not to change their scores from a weak reject and a reject, mostly due to lack of novelty as a NAS paper.  I also read the paper, and my personal opinion is that it would definitely be very novel to have a good neural architecture search for handling uncertainty in deep learning; it is by no means the case that "NAS for X" is not interesting just because there are now a few papers for "NAS for Y". As long as X is relevant (which uncertainty in deep learning definitely is), and NAS finds a new state of the art, I think this is great. For such an "application" paper of the NAS methodology, I do not find it necessary to introduce a novel NAS method, but just applying an existing one would be fine. The problem is more that the paper claims to introduce a new method, but that that method is too similar to existing ones, without a comparison; actually just using an existing NAS method would therefore make the contribution and the emphasis on the application domain clearer.  I have one small question to the authors about a part that I did not understand: to optimize WAIC (Eq 1), why is it not optimal to just set the parameterization \phi such that the variance is minimized, i.e., return a delta distribution p_\phi that always returns the same architecture (one with a strong prediction)? Surely, that s not what the authors want, but wouldn t that minimize WAIC? I hope the authors will clarify this in a future version.  In the private discussion of reviewers and AC, the most positive reviewer emphasized that the OOD results are strong, but admitted that the mixed sentiment is understandable since people who do not follow OOD detection could miss the importance and context of the results, and that the paper could definitely improve its messaging. The other reviewers  scores remained at 1 and 3, but the reviewers indicated that they would be positive about a future version of the paper that fixed the identified issues. My recommendation is to reject the paper and encourage the authors to continue this work and resubmit an improved version to a future venue.
The paper introduces a form of variational auto encoder for learning disentangled representations. The idea is to penalise synergistic mutual information. The introduction of concepts from synergy to the community is appreciated.   Although the approach appears interesting and forward looking in understanding complex models, at this point the paper does not convince on the theoretical nor on the experimental side. The main concepts used in the paper are developed elsewhere, the potential value of synergy is not properly examined.   The reviewers agree on a not so positive view on this paper, with ratings either ok, but not good enough, or clear rejection. There is a consensus that the paper needs more work.   
The weaknesses of the paper can briefly be summarised as follows: i) the suggested motivation is not so clear, and in addition the experimental results (by themselves questionable in the way they are obtained) do not support the main claim of the paper that "...edges are generated by aggregating the node interactions over multiple overlapping node communities, each of which represents a particular type of relation that contributes to the edges via a logical OR mechanism." In fact, the observed separation among components is not proven to be of the predicted nature. ii) empirical results are obtained using a deprecated experimental protocol. For the field to make real progress, experimental assessments should follow statistically sound protocols. Already published papers that were not following a sound protocol should not be taken as reference for future empirical assessments. The last point alone is a strong motivation for rejecting the paper.
This paper presents new non linearity function which specially affects regions of the model which are densely valued. The non linearity is simple: it retains only top k highest units from the input, while truncating the rest to zero. This also makes the models more robust to adversarial defense which depend on the gradients. The non linearity function is shown to have better adversarial robustness on CIFAR 10 and SVHN datasets. The paper also presents theoretical analysis for why the non linearity is a good function.  The authors have already incorporated major suggestions by the reviewers and the paper can make significant impact on the community. Thus, I recommend its acceptance.
The paper was discussed by the reviewers that acknowledged the rebuttal and the authors’ responses. In particular, they appreciated the fact that some of their concerns were alleviated (e.g., going beyond the single ImageNet evaluation).   More generally, while all the reviewers thought that the problem tackled by the paper was of clear interest (i.e., full end to end auto ML encompassing DA, NAS and HPO), they still expressed concerns (even after the rebuttal), in particular:  * _Clarity of the methodology_: None of the reviewers could clearly and fully understand the mathematical formulation of the joint optimization, leading to a series of questions regarding the confusing usage of the training/validation set in the experimental setup. This unfortunately made the assessment of (some aspects of) the paper speculative for the reviewers. * _Comparison with AutoHAS_: AutoHAS and DiffAutoML are obviously related methods. Even if AutoHAS has weaknesses compared to the proposed approach DiffAutoML, e.g., discretization of the continuous hyperparameters and no tuning of DA, it is still meaningful to carry out an actual comparison (possibly normalized by the different costs at play since the authors have highlighted the different memory overheads). Though the listed weaknesses of AutoHAS _should_ play in favor of DiffAutoML,  a proper experimental comparison would better support that claim.  Given those remaining concerns and the overall mixed scores, the paper is recommended for rejection. The detailed comments of the reviewers provide an actionable list of items to improve the paper for a future resubmission. 
This paper proposes an image to image translation technique which decomposes into style and content transfer using a semantic consistency loss to encourage corresponding semantics (using feature masks) before and after translation. Performance is evaluated on a set of MNIST variants as well as from simulated to real world driving imagery.   All reviewers found this paper well written with clear contribution compared to related work by focusing on the problem when one to one mappings are not available across two domains which also have multimodal content or sub style.   The main weakness as discussed by the reviewers relates to the experiments and whether or not the set provided does effectively validate the proposed approach. The authors argue their use of MNIST as a toy problem but with full control to clearly validate their approach. Their semantic segmentation experiment shows modest performance improvement. Based on the experiments as is and the relative novelty of the proposed approach, the AC recommends poster and encourages the authors to extend their analysis of the current results in a final version.
The paper proposes a semi supervised method to make deep learning more interpretable and at the same time be accurate on small datasets. The main idea is to learn dense representations from unlabelled data and then use those for building classifiers on small datasets as well as generate visual explanations. The idea is interesting, however, as one reviewer points out the presentation is poor. For instance, Table 2 is not understandable. Given the high standards of ICLR this cannot be ignored especially given the fact that the authors had the benefit of updating the paper which is a luxury for conference submissions.
This paper is very close to the decision boundary and the reviewers were split about whether it should be accepted or not. The authors updated the paper with additional experiments as request by the reviewers. The area chair acknowledges that there is some novelty that leads to (moderate) empirical gains but does not see these as sufficient to push the paper over the very competitive acceptance threshold. 
This paper presents a simple trick of taking multiple SGD steps on the same data to improve distributed processing of data and reclaim idle capacity. The underlying ideas seems interesting enough, but the reviewers had several concerns.  1. The method is a simple trick (R2). I don t think this is a good reason to reject the paper, as R3 also noted, so I think this is fine. 2. There are not clear application cases (R3). The authors have given a reasonable response to this, in indicating that this method is likely more useful for prototyping than for well developed applications. This makes sense to me, but both R3 and I felt that this was insufficiently discussed in the paper, despite seeming quite important to arguing the main point. 3. The results look magical, or too good to be true without additional analysis (R1 and R3). This concerns me the most, and I m not sure that this point has been addressed by the rebuttal. In addition, it seems that extensive hyperparameter tuning has been performed, which also somewhat goes against the idea that "this is good for prototyping". If it s good for prototyping, then ideally it should be a method where hyperparameter tuning is not very necessary. 4. The connections with theoretical understanding of SGD are not well elucidated (R1). I also agree this is a problem, but perhaps not a fatal one   very often simple heuristics prove effective, and then are analyzed later in follow up papers.  Honestly, this paper is somewhat borderline, but given the large number of good papers that have been submitted to ICLR this year, I m recommending that this not be accepted at this time, but certainly hope that the authors continue to improve the paper towards a final publication at a different venue. 
This work extends the lottery ticket hypothesis to lifelong learning and, in particular, it tackles the problem of class incremental learning. This is an important and difficult problem, and of great interest to the community. The authors considered top down and bottom up pruning strategies. The proposed approaches were validated on existing benchmarks (CIFAR10,CIFAR100, and Tiny ImageNet), reaching state of the art results, and showing that catastrophic forgetting could be alleviated. While some questions remain in terms of practical relevance, they authors showed the existence of winning tickets in the continual setting. There were concerns regarding clarity and requests for additional experiments, but all were convincingly addressed and the clarifications provided by the authors in their rebuttal further strengthened the paper.
Though the overall direction is interesting,  the reviewers are in consensus that the work is not ready for publication (better / larger scale evaluation is needed, comparison with other non autoregressive architectures should be provided, esp Transformer as there is a close relation between the methods). 
This paper provides a game based interface to have Turkers compete to analyze data for a learning task over multiple rounds. Reviewers found the work interesting and clear written, saying "the paper is easy to follow and the evaluation is meaningful." They also note that there is clear empirical benefit "the results seem to suggest that MTD provides an improvement over non HITL methods." They also like the task compared to synthetic grounding experiments. There was some concern about the methodology of the experiments but the authors provide reasonable explanations and clarification.  One final concern that I hope the readers take into account. While the reviewers were convinced by the work and did not require it, I feel like the work does not engage enough with the literature of crowd sourcing in other disciplines. While there are likely some unique aspects to ML use of crowdsourcing, there are many papers about encouraging crowd workers to produce more useful data. 
The paper proposes an interesting step in the direction of neuro symbolic reasoning. While there is no consensus among reviewers about the key novelty of the method, all acknowledge the interest of the direction. All of them also recognize that the submission improved greatly during the discussion phase: clarification of motivations, of experimental settings and results, of discussion with previous work.  However, despite those improvements, the submission is not yet ready for publication at ICLR. We encourage the authors to use the very detailed reviews and comments to improve the work. In particular, we encourage them to pay attention at three aspects:  1/ Comparison with large language models: the discussion wrt T5 is important. A key motivation for the proposed model is that it is bringing information and elements for QA (or other reasoning tasks) that purely scaling up language models can not bring. Or maybe they can bring the same kind of improvement but at a much lower computational cost. In any case, this is a very important point to justify the interest of such approach, and neuro symbolic reasoning overall, empirically.  2/ Using GPT2 (or equivalent): the discussion on using GPT 2 for generating new facts is key too. It is essential to bring this description from appendix to the core of the paper. But more discussion are expected.  For instance, what if GPT 2 generates facts that are false and lead to answering and justifying a wrong answer? In other words, how does it impact the integrity of the contextualized KG? This is an essential point that needs to be worked on more thoroughly.   3/ Overall there have been a lot of discussion to improve the motivations and the contributions. But they are not reflected in the paper necessarily. Following R2, we encourage the authors to "refocus the existing version (e.g., from vague discussion about neural symbolic models towards establishing solid comparison to the most related previous work in various sections of the submission)"    
The reviewers unanimously raised concerns on the lack of insights on why the proposed method works better than Han et al., 2020, and why WTA brings significant gains only to the proposed method and not to Han et al. I think the paper is promising but providing these insights are critical to making the work convincing to the readers. The reviewers have made excellent points to improve the paper; I d recommend the authors to incorporate them in their future submission.
All three reviewers appreciate the new method (FactorGAN) for training generative networks from incomplete observations. At the same time, the quality of the experimental results can still be improved. On balance, the paper will make a good poster.
This is a clearly written paper about integration of entity abstraction to the transformer based language modeling methods for language processing tasks that require reasoning (this is clarified by the authors later as tasks that require linger chains of reasoning) and have shown results on CLUTTR, HotpotQA, and CoQA. The reviewers seem to agree on two issues: First, it is not clear why the proposed idea does not result in a lot of improvement, except the synthetic CLUTTR. Authors provided additional experimental results on yet another dataset. Second, the paper would benefit from a detailed analysis of the experimental results, for example, why don t abstractions help on all datasets.
The proposed neural tree transduction framework is basically a combination of tree encoding and tree decoding. The tree encoding component is simply reused from previous work (TreeLSTM) whereas the decoding components is somewhat different from the previous work. They key problems (acknowledge also by at least 2 reviewers):  Pros:   generating trees input under explored direction (note that it is more general than parsing as nodes may not directly correspond to input symbols)  Cons:   no comparison with previous tree decoding work   only artificial experiments   the paper is hard too read (confusing) / mathematical notation and terminology is confusing and seems sometimes inaccurate (see R3)    
This paper was a difficult decision. Overall it seems to be a quality paper, well written and with many experiments, in particular evaluating learned representations across various tasks and datasets. The authors were also quite courteous in their replies which is appreciated. I really like the point the paper makes about video as a natural augmentation and I find that novel amid the recent NCE surge, where most papers rely critically on augmentation. R4 was also very positive about the paper overall concept.  In terms of paper weaknesses two of the reviewers voted for rejection because the paper ignores existing work on contrastive learning from videos. The authors rebuttal is that they are the first evaluating on images, not on videos. All reviewers also point out limited technical novelty, which the authors acknowledge. Finally, R1 is not very confident about the experiments.  Overall, and after calibration, the appropriate recommendation seems to be rejection.  
The paper presents an approach for learning continuous valued vector representations combining multiple input feature sets of different types, in both unsupervised and supervised settings.  The revised paper is a merger of the original submission and another ICLR submission.  This meta review takes into account all of the comments on both submissions and revisions.  The merged paper is an improvement over the two separate ones.  However, the contribution over previous work is still a bit unclear.  It still does not sufficiently compare to/discuss in the context of other recent work on combining multiple feature groups.  The experiments are also quite limited.  The idea is introduced as extremely general, but the experiments focus on a small number of specific tasks, some of them non standard.
The authors propose a low bit floating point quantization method to reduce energy and time consumption for deep learning training. Dynamic quantization and MLS tensor arithmetic are used to enhance the effectiveness of MLS. The motivation is clear and the efficient training is an important problem to address. However, the effectiveness of proposed method is not well justified and experimental results are less convincing.  In addition, the clarify of paper still needs to be further improved. 
I think we did learn something new from this paper, and I think the reviewers all seem to agree with this. The observation you make about the objective seems correct and interesting (though reviewers and ACs do sometime miss errors), but I have the following complaints that keep me from recommending acceptance:  1. The theory seems right, but in practice, all sorts of GANs with all sorts of objective functions experience "mode collapse",  so it doesn t seem like the issue you point out w/ the NS GAN objective can be the whole story.  However, we generally don t ask of a paper that it tells the whole story all in one go...  2. I do think the experiments are somewhat poorly done (compared to those for say the median paper about GANs that gets accepted to one of these conferences). Moreover, many people have made similar experimental claims to the ones that are in this paper that haven t held up on more complicated data sets, so I tend to apply more scrutiny to such claims when they re only evaluated on smaller tasks.  3. There have, as R3 points out, been a huge number of papers proposing tricks for training GANs, and some of them work really well.  What I m missing from this paper is an exploration of the relationship between your observation and those (mostly ad hoc) tricks.  Does your observation explain why those tricks are necessary?  Does it explain why some existing trick works as well as it does? If your observation is totally orthogonal to existing tricks, can you get much better performance on a challenging data set by using it? I don t feel like I got satisfactory answers to those questions.  All this being said, the paper was borderline, and I think if you dealt with some of the complaints above you would have a pretty good shot of getting a revised version accepted at another major machine learning conference.
The reviewers find the gradient compression approach novel and interesting, but they find the empirical evaluation not fully satisfactory. Some aspects of the paper have improved with the feedback from the reviewers, but because of the domain of the paper, experimental evaluation is very important. I recommend improving the experiments by incorporating the reviewers  comments.
Congratulations on getting your paper accepted to ICLR. Please make sure to incorporate the reviewers  suggestions for the final version.
The paper proposes an empirical study on the effect of various types of output layers of deep neural networks in different scenarios of continual learning. The authors draw several insights, such as ways of selecting the best output layer depending on type of scenario and a description of the different sources of performance drop (forgetting, interference, and projection drifts). The paper proposes different ways of mitigating catastrophic forgetting: a weight normalization layer, two masking strategies, and a variant of NMC using median vectors.  The paper presented a detailed experimental setup covering a large number of scenarios of continual learning: incremental, Lifelong Learning and Mixed Scenario. This was highlighted by Reviewer BTLN, and the AC agrees.   The main point of criticism for the work is the lack of novelty and the low significance of the findings. These were highlighted by all four reviewers.  Perhaps the aspect limiting significance is the fact that the feature extractors are assumed to be fixed, which is unlike most interesting settings in continual learning. This was mentioned by reviewers BTLN, uN9P, e1ZF. It is unclear whether the findings provided in this work would generalize to that setting. On that note, Reviewer e1ZF points out that not adapting the feature extractor could be the source of some inconsistencies observed. Studying this further would improve the work.  Overall, all four reviewers recommend rejecting the paper. The AC agrees with this decision and encourages the authors to consider extending the analysis to situations where the feature extractor is not fixed.
This paper offers an innovative approach to adjusting style transfer parameters. The reviewers were consistent, and all recommend acceptance.  I concur.  
Pros: + Nice demonstration of the equivalence between scaling the learning rate and increasing the batch size in SGD optimization.  Cons:   While reporting convergence as a function of number of parameter updates is consistent, the paper would be more compelling if wall clock times were given in some cases, as that will help to illustrate the utility of the approach.   The paper would be stronger if additional experimental results, which the authors appear to have at hand (based on their comments in the discussion) were included as supplemental material.   The results are not all that surprising in light of other recent papers on the subject. 
The reviewers were in agreement that the paper is below the bar for acceptance, and the authors did not provide a response to reviewer concerns.
## Description   The paper asks the question whether it is possible to accelerate training a binarized neural network from scratch to a given target accuracy [by starting with training a full precision network]. The main claimed contributions are: the idea to use *partially* pretrained networks, experimental evidence regarding the split of the training budget and measuring the speed up.  ## Review Process and Decision  All four reviewers agreed in the low rating of the paper and in the opinion that the paper is not a significant contribution. The area chair supports rejection.  ## Details  It has been already observed that pre training  in some form is needed for achieving the best accuracy:  Rastegari (2016) XNOR Net: ImageNet Classification Using Binary Convolutional Neural Networks Bulat (2019), "Improved training of binary networks for human pose estimation and image recognition" Martinez (2020): "Training Binary Neural Networks with Real to Binary Convolutions"  Alizadeh, (2019 fig. 4) notice that pre training can be viewed as a speed up, but in their setup find that training from scratch gives a better accuracy. Bulat (2019) and Martinez (2020), on the contrary do use pre training to achieve the best accuracy.  It is questionable whether the pre training in these works is partial or not. I believe the result largely depends on the pre training method used, which is not discussed in depth in the submission. More generally, some graduated optimization methods such as graduated smoothing or graduated non convexity are known to help in finding better solutions / lead to faster optimization and in fact Bulat (2019) use pre training with gradual transition from smooth activation to the sign function.  Relative to these points the technical contribution (one paragraph in the paper) is not significant. The empirical part of the contribution shows some effect, but does not indicate a breakthrough on its own. An investigation / design of pre training schemes could make it more substantial.  The empirical analysis proposed does not rule out, and in fact supports, the methodology that for the best final accuracy, the full rather than partial pre training is useful.  The gain of speed up by a factor 1.3 (diminishing to close to 1 if we are interested in the best accuracy), is of little practical interest. In particular, a slight code optimization can give a similar speed up without the complexity and hyperparameters involved in pre training. The authors write  "we are not aware of any effort to exploit binarization during the training phase" There are available public implementations that can optimize the forward pass of binary networks, in particular on GPU, while backward pass can stay in full precision. It could give a similar speed up. In particular Courbariaux (2016) provides a GPU kernel and proposes a variant of BatchNorm with bit shifts rather than multiplications, specifically used at training time. Making the emphasis on a relatively small speed up that can be obtained to train sub optimal models, in my view is not a good strategy to present this work. Rather the phenomenon that (partial) pre training helps with the goal to improve the training methods more substantially I find of higher interest.  Finally, I agree with the reviewers that the lottery ticket hypothesis (Frankle, 2020) work speaks of the speed up only hypothetically and its main (and fairly in depth) contribution is in demonstrating and investigating an interesting phenomenon about training and initialization, which I do not see relevant to this submission. 
The submission is concerned with the catastrophic forgetting problem of continual learning, and proposes a gradient based method which uses buffers of data seen previously to integrate the angles of the gradients and thereby mitigate forgetting. Empirical results are given on several benchmarks.   The reviewers were impressed with the thorough validation and strong results, but noticed that the much simpler MEGA D baseline did almost as well. Given this, they were not convinced that the proposed approach was necessary. Although the authors provided a strong rebuttal and an additional ablation, the reviewers did not feel that their concerns were met.  My recommendation is to reject the submission at this time.
The reviewers overall thought the problem was worth studying.  However, no reviewer was particularly excited about this work.  The main concern was that the new problem formulation is difficult to compare to prior work. Reviewers felt both more explanation and a deeper detailed comparison would make this a stronger paper.
The paper proposes a variant of MAML for meta learning on tasks with a hierarchical tree structure. The proposed algorithm is evaluated on synthetic datasets, and it compares favorably to MAML. The reviewers identified several significant weaknesses, including: (1) the experimental evaluation is limited, and it only includes small synthetic datasets; (2) the proposed algorithm is incremental over MAML. The reviewers agreed that the paper cannot be accepted in its current form. I recommend reject.
The authors propose a framework for relating adversarial robustness, privacy and utility and show how one can train models to simultaneously attain these properties. The paper also makes interesting connections between the DP literature and the robustness literature thereby porting over composition theorems to this new setting.  The paper makes very interesting contributions, but a few key points require some improvement: 1) The initial version of the paper relied on an approximation of the objective function in order to obtain DP guarantees. While the authors clarified how the approximation impacts model performance in the rebuttal and revision, the reviewers still had concerns about the utility privacy robustness tradeoff achieved by the algorithm.  2) The presentation of the paper seems tailored to audiences familiar with DP and is not easy for a broader audience to follow.  Despite this limitations, the paper does make significant novel contributions on an improtant problem (simultaneously achieveing privacy, robustness and utility) and could be of interest.   Overall, I consider this paper borderline and vote for rejection, but strongly encourage the authors to improve the paper wrt the above concerns and resubmit to a future venue.
The goal of the paper is to learn policies that can solve a given task while adhering to certain constraints specified via natural language. The paper closely builds upon prior work on constrained RL and passes the representation of natural language constraints by pre training an interpreter. Experiments are done in a new proposed 2D grid world benchmark. Although reviewers liked the premise, the main issue raised is that the way natural language constraints are handled is no different from the way it is done in prior work on constrained RL. The authors provided the rebuttal and addressed some of the concerns regarding paper details. However, upon discussion post rebuttal, the reviewers and AC feel that the paper does not provide clear scientific insight because the natural language part is processed separately from the policy learning part. We also believe that the paper will immensely benefit with results in more complex environments beyond the 2D grid world. Please refer to the reviews for final feedback and suggestions to strengthen the submission.
Paper explore depth wise separable convolutions for sequence to sequence models with convolutions encoders. R1 and R3 liked the paper and the results. R3 thought the presentation of the convolutional space was nice, but the experiments were hurried. Other reviewers thought the paper as a whole had dense parts and need cleaning up, but the authors seem to have only done this partially. From the reviewers comments, I m giving this a borderline accept. I would have been feeling much more comfortable with the decision if the authors had incorporated the reviewers  suggestions more thoroughly..
The paper proposes a method to prune edges in proximity graphs for faster similarity search. The method works by making the graph edges annealable and optimizing over the weights. The paper tackles an important and practically relevant problem as also acknowledged by the reviewers. However there are some concerns about empirical results, in particular about missing comparisons with tree structure based algorithms (perhaps with product quantization for high dimensional data), and about modest empirical improvement on two of the three datasets used in the paper, which leaves room for convincing empirical justification of the method. Authors are encouraged to take the reviewers  comments into account and resubmit to a future venue.   
The submission tackles the problem of data efficiency in RL by building a graph on top of the replay memory and propagate values based on this representation of states and transitions. The method is evaluated on Atari games and is shown to outperform other episodic RL methods.  The reviews were mixed initially but have been brought up by the revisions to the paper and the authors  rebuttal. In particular, there was a concern about theoretical support and the authors added a proof of convergence. They have also added additional experiments and explanations. Given the positive reviews and discussion, the recommendation is to accept this paper.
Pros: Reviewers generally agreed the paper was well written and is easy to follow. The goal of learning loss functions also seems quite promising.  Cons: There were concerns about whether credit for experimental performance was attributable to the core algorithm+functional form presented in the paper. There was also some skepticism about the specific form of the learned loss. Of greatest concern, no reviewer argued for acceptance during discussion, and one reviewer lowered their score during discussion.
The reviewers agree the paper is not ready for publication. 
This paper studies an important problem (visual relationship detection and generalization capabilities existing networks for this task). Unfortunately, all reviewers raise concerns (e.g. limited relations studied) and are largely on the fence about this paper. While this paper does not propose solutions, it does present interesting "negative results" that should get some visibility in the workshop track. 
The paper proposes adaptive optimization algorithms for federated learning that are federated versions of existing adaptive algorithms such as Adam, Adagrad, and Yogi. The paper establishes convergence guarantees for the proposed algorithms and performs an extensive experimental evaluation. Following the discussion, the reviewers were positive about the paper and felt that the author responses addressed their concerns. I recommend accept.
This paper improves the wait k based simultaneous NMT by training on an adaptive wait m policy with a controller determining the lag for sentence pair.  The controller is trained with RL to minimize the loss on a validation set. The overall model is reasonable, which is well presented. I however have the following two concerns 1. There is a clear mismatch between training/inference strategies, which raises two problems     1. The motivation:  the authors tried to explain that in discussion,  but it is not convincing enough     2. The title is misleading since there is no future information to use during inference  2. The experiments is not convincing enough in that a) the improvement over baseline is modest, and b) comparison to adaptive wait k and other strong baseline is insufficient   In conclusion I would suggest to reject this paper.  
This paper proposes a new federated learning method which uses the recently developed PAGE gradient estimator in the local updates, and provides convergence analysis for both convex and nonconvex loss functions. There are several technical questions raised by the reviewers that are not addressed by the author rebuttal. Given such technical issues and limited novelty and empirical evidence, I cannot recommend acceptance.
This work tackles an important problem of incremental learning and does so with extensive experimentation. As pointed out by two reviewers, the idea does seem novel and interesting, but the submission would require some rewriting before being potentially accepted at a venue like ICLR. I suggest focusing the paper more on the task incremental learning aspects, doing the ablation studies (and other changes) as requested by the reviewers, and having a rich appendix with details (with more discussion in the paper itself).
The paper proposes LORAS (low rank adaptive label smoothing) for training with soft targets with the goal of improving performance and calibration for neural networks. The authors derive PAC Bayesian generalization bounds for label smoothing and show that the generalization error depends on choice of the noise (smoothing) distribution. Empirical results demonstrate the effectiveness of the approach. All reviewers recommend acceptance.
This paper proposes  a general manipulation algorithm for tasks that have sparse rewards. The algorithm uses a Q attention to extract interesting pixel locations with an explicit attention module. A data augmentation method is also proposed to generalize expert demonstrations.  While the proposed method and experiments seem interesting, two out of the three reviewers had several issues regarding the clarity of the paper. The main issue is that a better ablation study needs to be performed to assess the proposed system. For instance, the reviewers question the advantages the Q attention agent brings over a standard attention module. The authors provided detailed explanations and a video showing examples of the expert demonstrations. It is not clear however if the clarity issues are completely resolved. 
The paper begins with an observation in standard trained CNNs that the correlations in the output channels are high. Building upon this the paper proposes a new "optimizer" which modifies the gradients to encourage corelations among output channels. They provide a theoretical foundation for the method, by deriving the gradient through placing a riemannian metric on the manifold of parameter tensors which encourages smoothness along the output channel dimension. Two variants (one based on a Sobolev metric) are proposed and are experiments are provided. The underlying idea and the derivation of the gradients were generally appreciated by the reviewers. However some reviewers maintained their concern regarding the effectiveness of the performed experimentation. The gains demonstrated are relatively small over the baselines and more importantly the baselines are quite far off the state of the art baselines for the particular problems. This is the primary reason for my recommendation as experiments are the only source of understanding whether the method is effective (there is little theory   mostly at an intuitive level to justify the form of the optimizer). Overall, I strongly encourage the authors to explore the idea further and strengthen the paper with stronger baselines (perhaps on larger datasets) and resubmit. 
This paper interprets pre trained masked language models (MLMs) as energy based sequence models and designs a tractable MCMC sampling algorithm based on Metropolis Hastings with proposals derived from MLMs themselves.   The strategy is simple, reasonably elegant, and fixes technical mistakes of prior work. The proposed algorithm addresses intractabilities of some naive MCMC schemes, does not require modifications to MLM training, and makes good use of MLMs themselves as proposals thus being crucially economical about resources.   We had some concerns about speed of generation and the paper s positioning with regards to existing strategies for sampling from energy based models (already during parameter estimation). While I understand that for many applications speed of generation is crucial, I think that on its own should not keep this line of research outside our best venues. And I hope steps like this one will lead to faster algorithms in the near future. I do relate to the issue of positioning, and I am glad the authors did not take it lightly. In the rebuttal phase the related work and positioning have been improved, but the authors remarked that the limited space for the camera ready was preventing them from expanding the discussion. A note to authors: it s not a bad idea to have an expanded related work section in appendix.
The paper develops steerable partial differential operator and show how it can be used to build equivariant network. Experimentation on rotated MNIST and STL10 show the merits of the proposed method. Reviewers agreed on the significance of the work and that it brings new perspective on equivariance that would be interesting to the ICLR community. Accept
This submission receives four negative reviews. The raised issues include paper organizations, presentation clarity, more experimental evaluations, the trade off between technical contribution and application configuration, and the potential impact on more general visual recognition scenarios. In the rebuttal and discussion phases, the authors do not make any response to these reviews. Overall, the AC agrees with four reviewers that the current submission does not reach the publication bar. The authors are suggested to improve the current submission based on the reviews to make further improvements.
The paper introduces a transformer like architecture to perform network inference in network games. While the reviewers acknowledge that the research direction is interesting, they raise concerns regarding the significance of the contribution in terms of methodology, particularly in light of the state of the art, and the experimental evaluation, which in their view did not support the promise of the work. The authors did not reply/follow up on the reviews during the rebuttal period. I would encourage the authors to use the reviewers  comments to revise their paper and resubmit to another conference.
This paper was quite contentious.  While reviewers appreciated the detailed response by the authors, and there is consensus that the paper addresses a relevant problem and contains interesting ideas, in the end there remain several concerns.  The paper provides a complex combination of techniques from active learning, meta learning and symbolic reasoning (via MILPs), and there are concerns about the clarity of the exposition.  For a paper claiming safety properties, there is also a lack of either formal theoretical analysis of well specified safety properties, or a compelling demonstration of its effectiveness on a real system (all experiments are carried out in simulation).
Overall this paper was discussed at length given the high variance in scores, and it was ultimately felt that the paper was a borderline paper and there was not enough enthusiasm to warrant acceptance. Several concerns in the discussion could not be resolved, in particular the bounds might not be tight, or even useful, and more explanation on the dependence of various parameters involved and assumptions involved is needed. Specifically, as pointed out by a reviewer, there is a concern about the parameter epsilon_3. It seems for natural input distributions epsilon_3 would be so small that the upper bound would scale as n^3 (given the 1/epsilon_3^2 dependence), which is then trivial since it is larger than n. The reviewers were not satisfied with the authors response regarding this.
Connecting different fields and bringing new insights to machine learning are always appreciated. But since it is challenging to do it needs to be done well. This paper falls short here.  
The authors propose a model based RL algorithm, consisting of learning a                                                            deterministic multi step reward prediction model and a vanilla CEM based MPC                                                        actor.                                                                                                                              In contrast to prior work, the model does not attempt to learn from observations                                                    nor is a value function learned.                                                                                                    The approach is tested on task from the mujoco control suit.                                                                                                                                                                                                            The paper is below acceptance threshold.                                                                                            It is a variation on previous work form Hafner et al.                                                                               Furthermore, I think the approach is fundamentally limited: All the learning                                                        derives from the immediate, dense reward signal, whereas the main challenges in RL                                                  are found in sparse reward settings that require planning over long horizons, where value                                           functions or similar methods to assign credit over long time windows are                                                            absolutely essential.
The paper proposes TOME, which extends Transformer by attending to entity mention memory. Experiments are conducted on claim verification and QA.  Reviewers generally found the paper is solid. However, the novelty appears to be limited and is mainly in the combination of existing models.
This paper studies the role of momentum in temporal difference (TD) learning algorithms, and how this can be systematically exploited to accelerate the TD type algorithms. More specifically, the authors point out that the momentum term could be quite biased, and propose a scheme to remedy this issue. However, the reviewers point out the lack of motivation about bias correction; it is unclear why bias correction is crucial to achieve acceleration. 
After discussion, all reviewers are convinced about the novelty of the proposed method, and adjusted scores to recommend acceptance. They all appreciate the attempt to attack COVID 19 using machine learning.
This work is well written and easy to follow and proposes a novel framework to utilize unlabeled output data. The authors have also given a detailed proof that the denoiser reduces the required complexity of the predictor. However, ultimately the experimental results are somewhat weak and leave doubts as to how effective the approach is. More convincing experimental results such as significant improvements on a well understood task and acknowledging that the approach is mostly useful when combined with pre training and back translation would improve the work.  Pros   Well written.   Technically novel approach to the problem of utilizing unlabeled output data.   Interesting proof on the reduced complexity requirement for the predictor.  Cons:   Experimental results are not convincing. Showing significant improvements on a well understood task would be more convincing.   The approach is only really useful when combined with pre training or back translation.
While the reviewers agree that this paper does provide a contribution, it is small and does overlap with several concurrent works. it is a bit hand engineered. The authors have provided a lengthy rebuttal, but the final reviews are not strong enough. 
The experiments are not sufficient to support the claim. The authors plan to improve it for future publication.
The paper shows the success of a relatively simple idea   fine tune a pretrained BERT Model using Variational Information Bottleneck method of Alemi to improve transfer learning in low resource scenarios.  I agree with the reviewers that novelty is low   one would like to use any applicable method for controlling overfitting when doing transfer learning, and of the suite of good candidates, VIB is an obvious one   but at the same time, I m moved by the results because of: the improvements and the success on a wide range of tasks and the surprising success of VIB over other alternatives like dropout etc, and hence I m breaking the tie in the reviews by supporting acceptance.  Its a nice trick that the community could use, if the results of the paper are an indication of its potential.
Thanks for the detailed replies to the reviewers. Their score was slightly improved, this paper is still below the bar given high competition of ICLR2020. For this reason, we decided not to accept this paper.
This paper gave a general L2O convergence theory called Learned Safeguarded KM (LSKM).  The reviewers found flaws both in theory and in experiments.  While all the reviewers have read the authors  rebuttal and gave detailed replies, they all agree to reject this paper.  I agree also.
This paper proposes a new method to perform knowledge distillation (KD) for transformer compression, where two types of contextual knowledge, namely, word relations and layer transforming relations, are considered for KD. Both pair wise and triple wise relations are modeled.   This paper receives two weak reject and two weak accept recommendations. On one hand, the reviewers appreciate that the authors have added more results into the paper to solve their concerns. On the other hand, several concerns still exist. (i) With regards to the compute performance trade off, the gains of the method does not seem too great. One reviewer feels that the authors tried to downplay the cost of their method too much. Though we care more about the inference time, the development time in practice should also not be underestimated. (ii) Compared with TinyBERT, the performance gain looks marginal on the GLUE benchmark (Table 1). (iii) It will make the paper more convincing if pre training experiments can be performed.   Overall, after reading the paper, the AC thinks that the novelty of the proposed method is somewhat limited. The AC is also hesitant about whether modeling word relations and layer transforming relations simultaneously are needed. The choices for ablation study are also not totally clear.   For example, in Figure 2, it is not clear why the authors choose SST 2 to plot the figure; in Table 5, it is unclear why SST 2, MRPC and QNLI are selected, but not others. When looking at Table 5, it is not totally convincing it is needed to model both WR and LTR, or it is needed to introduce both pair wise and triple wise relations. More careful ablation studies are needed. It also remains unclear what kind of word relations or layer transforming relations are learned.   In summary, this is a borderline paper, and the rebuttal unfortunately did not fully address the reviewers  main concerns. On balance, the AC regrets that the paper cannot be recommended for acceptance at this time. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
The paper compares transfer learning with fine tuning and joint training and then proposes a new approach (Merlin). Reviewers have pointed to the fact that Merlin works in a setting that is different from normal transfer learning settings (it assumes some target domain data is available during training). The authors acknowledge this and think it can still be a reasonable setting, but of course it makes comparisons more difficult. Overall, while there are interesting analysis and results, the paper remains borderline and more work should be done to make it a good contribution, including significantly improving the presentation to make clear the distinction in settings. I therefore recommend to reject the paper.
The paper extends the original work on flooding to individual instance level to prevent overfitting. Even though the technique is a intuitive extension, the reviewers appreciate its simplicity and effectiveness, and consider the extension necessary. Most reviewers  concerns were addressed through rebuttal.
In this paper, a data mapping method to a latent space designed for outlier detection is proposed. Outlier detection by latent space mapping has been extensively studied in the literature. Unfortunately, this paper does not fully discuss the relation of the proposed method with a large amount of existing literature and lacks novelty. 
This paper proposes Neural Oblivious Decision Ensembles, a formulation of ensembles of decision trees that is end to end differentiable and can use multi layer representation learning. The reviewers are in agreement that this is a novel and useful tool, although there was some mild concern about the extent of the improvement over other methods. Post discussion, I am recommending the paper be accepted.
This paper investigates the task of learning to synthesize tools for specific tasks (in this case, a simulated reaching task). The paper was reviewed by 3 experts and received Reject, Weak Reject, and Weak Reject opinions. The reviews are very encouraging of the topic and general approach taken by the paper   e.g. R3 commenting on the "coolness" of the problem and R1 calling it an "important problem from a cognitive perspective"   but also identify a number of concerns about baselines, novelty of proposed techniques, underwhelming performance on the task, whether experiments support the conclusions, and some missing or unclear technical details. Overall, the feeling of the reviewers is that they re "not sure what I am supposed to get out of the paper" (R3). The authors posted responses that addressed some of these issues, in particular clarifying their terminology and contribution, and clearing up some of the technical details. However, in post rebuttal discussions, the reviewers still have concerns with the claims of the papers. In light of these reviews, we are not able to recommend acceptance at this time, but I agree with reviewers that this is a "cool" task and that authors should revise and submit to another venue.
The paper proposes an interesting data dependent regularization method for orthogonal low rank embedding (OLE). Despite the novelty of the method, the reviewers and AC note that it s unclear whether the approach can extend other settings with multi class or continuous labels or other loss functions. 
This paper proposes a method for  interpretable  graph neural networks. The idea is intuitively well motivated: after training the model, discard spurious edges that are not critical to making predictions in the graph, and only retain salient edges. Experiments on synthetic and real datasets show that the proposed method is effective at dropping only the edges that are not useful for the task at hand;  while leading to  only small performance degradation.  The paper is well written. Overall, the paper brings together prior ideas in a useful way, and is well executed.
This paper explores the effects of padding in convnets used for various visual recognition tasks (classification, segmentation). This is an important and relevant design choice that is often overlooked, as noted by reviewers. However, I share the concerns of AR2 & AR4 with the evaluation. The design of the ResNet variant used for the "No Pad" baseline seems potentially fatally flawed: the bilinear upsampling used to match the feature map sizes for the residual addition results in a misalignment of the inputs with the outputs, which potentially explains the performance degradations seen throughout most experiments, as opposed to (or perhaps in addition to) the lack of positional information and border effects resulting from the NoPad scheme that is claimed as the reason for the performance drop. It is true that how to do this is an open question, as the authors argue in their added Appendix A.1, but I nonetheless share the reviewers  skepticism that the chosen approach will result in a meaningful comparison of the effect of padding and border effects. In fact, the results in Table 5 on texture recognition seem to suggest that "No Pad" approach may indeed be flawed, given that "No Pad" performs the worst, while "Reflect" padding performs best, even though both methods share the property that the network should have difficulty inferring positional information. Given the reliance on this dubious baseline throughout the results, I can t recommend the submission for acceptance in its current form. However, I still appreciate the direction of this work and hope the authors will consider resubmitting it after revising it in order to make the evaluation more convincing based on the reviewers  feedback.
This paper describes how they extend a previous phrase based neural machine translation model to incorporate external dictionaries. The reviewers mention the small scale of the experiments, and the lack of clarity in the writing, and missing discussion on computational complexity. Even though the method seems to have the potential to impact the field, the paper is currently not strong enough for publication. The authors have not engaged in the discussion at all. 
Nice start but unfortunately not ripe.  The issues remarked by the reviewers were only partly addressed, and an improved version of the paper should be submitted at a future venue.
The paper presents several interesting generalization results for Uniform LGI loss functions (a generalization of PL functions). Some of these bounds seem useful, but the overall connection with the optimization length remains unclear. This concern and other points of criticism remain present after the rebuttal phase. Other minor concerns seem fixable, but in a larger timeline compared to the camera ready one. The paper should be revised for a future venue.
The paper proposes an attention mechanism for equivariant neural networks towards the goal of attending to co occurring features. It instantiates the approach with rotation and reflection transformations, and reports results on rotated MNIST and CIFAR 10. All reviewers have found the idea of using self attention on top of equivariant feature maps technically novel and sound. There were some concerns about readability which the authors should try to address in the final version. 
The paper proposes a new approach to inductive rule prediction for knowledge graph completion. Reviewers highlighted as strengths that the paper proposes an interesting approach to an important problem that is relevant for the ICLR community. However, reviewers raised also concerns regarding model design and correctness as well as clarity of presentation (e.g., motivation, analysis, comparison to related work, evaluation). After author response and discussion, all reviewers and the AC agree that the paper is not yet ready for publication at ICLR due to the aforementioned issues.
This paper received three reviews. R1 recommends Weak Reject, and identifies a variety of concerns about the motivation, presentation, clarity and soundness of results, and experimental design (e.g. choice of metrics). In a short review, R2 recommends Weak Accept, but indicates they are not an expert in this area. R3 also recommends Weak Accept, but identifies concerns also centering around clarity and completeness of the paper as well as some specific technical questions. In their response, authors address these issues, and have a constructive back and forth conversation with R1, who remains unconvinced about significance of the empirical results and thus the conclusion of the overall paper. After the discussion period, R3 indicated that they weakly favored acceptance but agreed that the paper had significant presentation issues and would not strongly advocate for it. R1 advocated for Reject, given the concerns identified in their reviews and followup comments. Given the split decision, the AC also read the paper. While the work clearly has merit, we agree with R1 s comment that it is overall a "potentially interesting idea, but the justification and presentation/quantification of results is not good enough in the submitted paper," and feel the paper really needs a revision and another round of peer review before publication. 
Pros: + The idea of end to end training that simultaneously learns the weights and appropriate precision for those weights is very appealing.  Cons:   Experimental results are far from the state of the art, which makes the empirical evaluation unconvincing.   More justification is needed for the update of the number of bits using the sign of the gradient. 
This work demonstrates that autoregressive (AR) models for machine translation can can be competitive with their non autoregressive (NAR) counterparts in terms of practicality. This is a timely observation, given the flurry of recent work on NAR models, whose primary benefit is often cited to be fast inference.  It was argued that the results are not surprising   if this is the case, I still think this work merits acceptance because its thesis runs counter to the direction the field as a whole seems to be moving in, and the results are convincing. That said, I agree with the authors that the observation that some encoder and decoder layers are interchangeable, is not self evident (i.e. it _is_ surprising). This is of course subjective to some degree, so I am making a judgement call here. The work also has value in that it draws attention to some practices regarding evaluation in NAR machine translation literature that could be improved and made more fair (specifically regarding comparison with AR models).  There were some concerns about whether these models should be evaluated in the small batch or large batch setting. The authors have updated their manuscript in response, and it now explicitly discusses both settings. The authors have also run more experiments and added several additional results requested by reviewers to the manuscript.  All things considered, I am inclined to follow the majority and recommend acceptance.
This paper proposes a model for learning disentangled representations by assuming the slowness prior over transitions between two frames. The model is well justified theoretically, and evaluated extensively experimentally. The results are good, and all reviewers agree that this paper is among the top papers they have reviewed. For this reason, I am pleased to recommend this paper for an Oral.
Area chair is in agreement with reviewers: this is a good experiment that successfully applies specific machine learning techniques to the particular task. However, the authors have not discussed or studied the breadth of other possible methods that could also solve the given task ... besides those mentioned by the reviewers, U Nets, and variants thereof, come to mind. Without these comparisons, the novelty and significance cannot be assessed.  Authors are encouraged to study similar works, and perform a comparison among multiple possible approaches, before submission to another venue.  
This work investigates how importance sampling strategies can improve training with budgeted constraints., with a focus on the benefits from variety provided by data augmentation samples.  Initial clarification issues raised by the reviewers were taken into account such as a new title, clarification of some explanations and corrections of typos.  However, the reviewers still agree that the paper is not ready for publication for several reasons:   the comparison with the literature is still insufficient and should be better organised,   experiments too narrow to conclude general benefits from the paper as it is, since there is a single type of tasks that is studied from the same dataset family. Questions related to very small budgets, below 20% also remain open and would require a new submission.
I agree with the reviewers, and I find the careful analysis of CL approaches relying on regularization for RNN useful and insightful. I do feel that a lot of the interesting content is still in the appendix (from a quick skim and looking at the plots in the appendix) but I think something like this can potentially be unavoidable.   I do like the separation between sequence length and memory requirements. I think making observations about different types of recurrent architectures is hard, but I think the paper does a good job to raise some interesting questions.   A note that I would make (that I haven t seen raised through a quick look in the paper) is that is not clear how the Fisher Information Matrix should be computed in case of a recurrent model (which is a problem in general). E.g. a typical thing is to compute it as for a feed forward model (using the gradients coming from BPTT) which is feasible computationally, but actually that is problematic as you first sum gradients before taking their outer product rather than summing the outer products corresponding of the different terms in the gradient. I m wondering if that plays a role here as well.  Overall I think the paper does careful analysis and ablation studies and raises some interesting observation of how one should approach CL algorithms for RNN models.  
The paper discusses layer wise training of deep networks. The authors show that it s possible to achieve reasonable performance by training deep nets layer by layer, as opposed to now widely adopted end to end training. While such a training procedure is not novel, the authors argue that this is an interesting result, considering that such a training procedure is often dismissed as sub optimal and leading to inferior results. However, the results show exactly that, as the performance of the models is significantly worse than the state of the art, and it is unclear what other advantages such a training scheme can offer. The authors mention that layer wise training could be useful for theoretical understanding of deep nets, but they don’t really perform such analysis in this submission, and it’s also unclear whether conclusions of such analysis would extend to deep nets trained end to end.  In its current form, the paper is not ready for acceptance. I encourage the authors to make a more clear case for the method: either by improving results to match end to end training, or by actually demonstrating that layer wise training has certain advantages over end to end learning. 
Authors present a new multi layered capsule network architecture, implemented an EM routing procedure, and introduced "Coordinate Addition".  Capsule architectures are gaining interest because of their ability to achieve equivariance of parts, and employ a new form of pooling called "routing" (as opposed to max pooling) which groups parts that make similar predictions of the whole to which they belong, rather than relying on spatial co locality. New state of art performances are being achieved on focused datasets, for which the authors have continued the trend.  Pros:   New significant improvement to state of art performance is obtained on smallNORB, both in comparison to CNN structure as well as the most recent previous implementation of capsule network.  Cons:   Some concern arose regarding the writing of the paper and the ability to understand the material, which authors have made an effort to address.  Given the general consensus of the reviewers that this work should be accepted, the general applicability of the technology to multiple domains, and the potential impact that improvements to capsule networks may have on an early field, area chair recommends this work be accepted as a poster presentation. 
The paper considers generalization in setups in which the training sample  may be generated by a different distribution than the one genertaing the test data. This sounds much like transfer learning, and similarly sounding considerations, of a space of possible generating distributions, ways of measuring the statictical complexity of such spaces and implied error generalization results were analyzed in e.g., Jonathan Baxter s "Theoretical models of learning to learn" 1998 and S Ben David, R Schuller "Exploiting task relatedness for multiple task learning" S Ben David, RS Borbely "A notion of task relatedness yielding provable multiple task learning guarantees" Machine learning 73 (3), 273 287  The current submission does not mention these earlier works.  Furthermore, the paper suffers from mathematical sloppiness. The model uder which the generalization theorems  hold is not clearly defined. For example,  Theorem 2, Theorem 3 and Theorem 4  do not stae what are the probability spaces to which the "probaility p > 1 \delta" quantifications refer.   
The submission proposes a Kuiper statistic based loss function for survival clustering.  This loss function is applied to train a deep network.  Results are presented on a Friendster dataset.  This submission received borderline/mixed reviews.  The primary concerns were: justification of the Kuiper loss, lack of details of the experimental setup, writing style.  In the end, these concerns remain.  Of particular importance is the justification and experimental validation of the Kuiper statistic.  Although it seems a reasonable choice, from the authors  response to R3: "We now also report results for Kolmogorov Smirnov loss. Although the difference in performance between the two loss functions is not significant in the Friendster dataset, Kuiper loss has higher statistical power in distinguishing distribution tails [Tygert 2010]."  If this theoretical result from [Tygert 2010] is relevant, it should be possible to demonstrate this experimentally.  If such differences are irrelevant for the data of interest, the paper should perhaps be reframed with a better discussion of available statistics and literature (cf. Reviewer 2), and a more general presentation de emphasizing modeling choices that may have limited practical relevance. 
The authors provide an investigation into tuning learning rate schedules. The problem is certainly of great practical importance. After discussion, the reviewers felt the main idea of the paper is worth pursuing, but could use significant refinement. One reviewer suggests: " "...better treatment of the background material, clearer identification on when the weight norm behaviour happens beside norm (possibly looking also for counter examples!), rethinking section 6, and a more convincing set of experiments (for showing convincing evidence about e.g. 5.2). Regarding this last point, I want to clarify that in my review I mentioned [1] not for the grid search, but rather for the time controlled experiments. If you go with random search for selecting the hyperparameters of the learning rate adaptation methods. I personally think that a recipe to make the comparison fair enough is to choose a prior distribution (e.g. uniform/log uniform) that covers reasonable values (e.g. as used for different datasets) with mean equal/close to the known well performing ("optimal") value." Other reviewers were generally of a similar opinion. The authors are encouraged to continue with the work, taking reviewer comments into account for updated versions.
Existing PAC Bayes analysis gives generalization bounds for stochastic networks/classifiers. This paper develops a new approach to obtain generalization bounds for the original network, by generalizing noise resilience property from training data to test data.  All reviewers agree that the techniques  developed in the paper (namely Theorem 3.1) are novel and interesting.  There was disagreement between reviewers on the usefulness of the new generalization bound (Theorem 4.1) shown in this paper using the above techniques. I believe authors have sufficiently addressed these concerns in their response and updated draft. Hence, despite the concerns of R3 on limitations of this bound and its dependence on pre activation values, I agree with R2 and R4 that the techniques developed in the paper are of interest to the community and deserve publication. I suggest authors to keep comments of R3 in mind while preparing the final version. 
Three reviewers have reviewed this submission and scored it as 6/3/3. After rebuttal, the reviewers remained unconvinced. The main criticisms concerns the Jacobian  regularizaton [1] being known which makes the contributions of this submission  look diluted. Additionally, there were concerns over results (degradation) on CIFAR10 and ImageNet and other minor issues. For these reasons, this paper cannot be accepted by ICLR2020.
The paper presents an interesting treatment of transforming a block sparse fully connected neural networks to a ResNet type Convolutional Network. Equipped with recent development on approximations of function classes (Barron, Holder) via block sparse fully connected networks in the optimal rates, this enables us to show the equivalent power of ResNet Convolutional Nets.   The major weakness in this treatment lies in that the ResNet architecture for realizing the block sparse fully connected nets is unrealistic. It originates from the recent developments in approximation theory that transforming a fully connected net into a convolutional net via Toeplitz matrix (operator) factorizations. However the convolutional nets or ResNets obtained in this way is different to what have been used successfully in applications. Some special properties associated with convolutions, e.g. translation invariance and local deformation stability, are not natural in original fully connected nets and might be indirect after such a treatment.    The presentation of the paper is better polished further. Based on ratings of reviewers, the current version of the paper is on borderline lean reject.
The paper theoretically shows that the data (embedded by representations learned by GANs) are essentially the same as a high dimensional Gaussian mixture. The result is based on a recent result from random matrix theory on the covariance matrix of data, which the authors extend to a theorem on the Gram matrix of the data. The authors also provide a small experiment comparing the spectrum and principle 2D subspace of BigGAN and Gaussian mixtures, demonstrating that their theorem applies in practice.  Two of the reviews (with confident reviewers) were quite negative about the contributions of the paper, and the reviewers unfortunately did not participate in the discussion period.  Overall, the paper seems solid, but the reviews indicate that improvements are needed in the structure and presentation of the theoretical results. Given the large number of submissions at ICLR this year, the paper in its current form does not pass the quality threshold for acceptance.
This work studies parameter quantization using binary codes and proposes an encryption algorithm/architecture to compress quantized weights and achieve fractional numbers of bits per weight, and to perform decryption using XOR gates. The authors conduct experiments on datasets including ImageNet to evaluate their scheme. Much of the concern from reviewers relates to baseline comparison and details around that. Specifically, R1 believes that the submission could have a bigger impact if authors could conduct more thorough experiments, e.g. compressing more widely used and challenging architecture of ResNet 50, or trying tasks such as image detection (Mask R CNN). The authors  responded to that and mentioned their choice of the current experimental setting is to facilitate comparison with previous works (baselines), which use similar experimental settings. Nevertheless, the baseline methods could have been attempted by the authors on broader tasks, or more widely used architectures could have been investigated by authors on the baseline methods. As a result, R1 was not convinced. To ensure the paper receives the attention it deserves, I recommend considering a more thorough evaluation of the proposed method against baseline methods.
The reviewers felt that the idea of learning a posterior distribution on optimization algorithms is very novel. However, the negative flip side of this novelty was that it was not clear how the prior and likelihood were defined so that Bayes rule could be approximated. The three reviewers appeared to find the paper somewhat confusing, and while the authors  made significant changes, it would be better to resubmit for a new set of reviews of the revised paper.
This work presents new results on unsupervised machine translation using a clever combination of techniques. In terms of originality, the reviewers find that the paper over claims, and promises a breakthrough, which they do not feel is justified. However there is "more than enough new content" and "preliminary" results on a new task. The experimental quality also has some issues, there is a lack of good qualitative analysis, and reviewers felt the claims about semi supervised work had issues. Still the main number is a good start, and the authors are correct to note that there is another work with similarly promising results. Of the two works, the reviewers found the other more clearly written, and with better experimental analysis, noting that they both over claim in terms of novelty. The most promising aspect of the work, will likely be the significance of this task going forward, as there is now more interest in the use of multi lingual embeddings and nmt as a benchmark task. 
This paper was perceived as being well written, but the technical contribution was seen as being incremental and somewhat heuristic in nature. Some important prior work was not discussed and more extensive experimentation was recommended. However, the proposed approach of partitioning the graph into sub graphs and a schedule alternating between intra and inter graph partitions operations has some merit.  The AC recommends inviting this paper to the Workshop Track.
The most positive reviewers have not decided to step forward to champion the paper. Others have a negative impression which has not sufficiently changed after the answers from authors. Actually, it is acknowledge that there have been many modifications, but they are not happy enough with this situation: modifications (some significant ones) cannot always be fully checked again and even with the efforts that were made by reviewers, strong concerns remained. It has been pointed out that the direction has potential. My recommendation is based on the data that I have available.
This paper presents a novel method for class incremental learning (CIL) with the help of placebo data chosen from a free image stream. Such placebo data are unlabeled and easy to obtain in practice. To adaptively generate phase specific functions as the accurate estimation of placebos  quality for KD, this paper applies reinforcement learning based on the constraints of the CIL. The effectiveness of the method has been verified on multiple datasets, including ImageNet 1k and ImageNet Subset with both lower memory and higher accuracy than baselines. The major concern is about the novelty that the unlabeled auxiliary data is not quite new for CIL despite the minor difference in settings and methods. Moreover, the improvements over the baselines are not significant enough, which is a minor concern.
The paper introduces a method to learn rotations of a quantized embedding end to end. The proposed technique seems novel, although the technical/algorithm novelty seems to be somewhat marginal.  The empirical results are promising, although do not quite match some of the claims by the authors.  Hopefully the reviewer feedback would help in producing an even more influential paper.
Four experts reviewed the paper. All but Reviewer HSTU recommended acceptance. The authors clearly did a great job with the rebuttal, which convinced two reviewers to raise their scores above the acceptance threshold. Notably, the reviewers found the newly added experiments impressively strong. The rebuttal also addressed some clarification questions. Based on the reviewers  feedback, the decision is to recommend the paper for acceptance. As mentioned by the reviewers, some experiments and discussions in the rebuttal should be included in the paper. The authors are encouraged to make the necessary changes to the best of their ability. We congratulate the authors on the acceptance of their paper!
This work proposes a so called self supervised approach for few shot learning. The self supervision doesn t refer to the lack of use of any labels as in regular self supervised embedding learning methods (here support sets are labelled), but refer to the fact that the query set s labels aren t used in their proposed objective. Instead the query labels are predicted by a primary network, which then uses these predicted labels to predict the ground truth labels on the support set. The support set label predictions can thus be used to derive a learning signal for the model. Some results are presented that suggest the method is competitive with respect to the state of the art.  Reviewers are quite split on this work. Even the reviewers who are technically leaning toward accepting this work (rating of 6) mention concerns that are worrying, e.g. reviewer Gmcb and wTnW both mention concerns related to the fairness of the evaluation.   I too share similar concerns. First, the method in question is effectively a transductive method (as opposed to inductive, as are many of the baselines this work compares to), a distinction that the paper does not make explicit or address directly. This distinction is important, as it is well known that transductive methods have an advantage over inductive methods. I did try to look for some published transductive baselines. One is the method of Zhang et al. (2021a), which the authors do beat on mini ImageNet, but not on CUB (and in fact, the paper only reports the results from Zhang et al. on mini ImageNet, even though the original paper actually reports results on CUB, which I find odd). On CUB, for 1 shot, Zhang et al. (2021a) outperforms SPDN, while for 5 shot SPDN does only very slightly better. The paper is not clear as to whether the compared baselines are transductive or not in the cross domain experiments either.   Second, by introducing a dual network that is separate from the primal network, the proposed model effectively is increasing the capacity of their model, relative to using only a primal network. This capacity is mostly used when performing the self supervised optimization of the query labels, which would explain why this aspect of the proposed method is what yields the largest improvements. Given that capacity has a large effect on the performance of methods on few shot learning benchmarks, I m quite concerned that this is the more likely explanation for the (sometimes surprisingly large) improved performance.  That said, I don t find the paper entirely without merit. The label optimization procedure is neat, and is probably the most interesting innovation of the paper. The use of a primal dual architecture on the other hand is more incremental, e.g. relative to architectures used in semi supervised few shot learning.  Overall, at this point, given the lukewarm evaluation by reviewers and the lingering concerns (or at a minimum, lack of clarity) on the fairness of the evaluation, I m afraid I m not comfortable to recommend accepting this work as it currently stands.
In this submission, the authors presented a framework (GIANT) for self supervised learning to improve LM by leveraging graph information. Reviewers agree that the method is somewhat novel, the (partial) theoretical analysis is interesting, and the evaluations are strong. We thank the authors for doing an excellent job in rebuttal which cleared essentially all the questions reviewers initially raised.
This paper the flip side of an adversarial "attack" in that data may be perturbed to make it look like a model was performing well rather than the standard notion of adversarial attacks. The reviewers found this notion interesting and potentially worthy of investigation. However as it stands, the proposed applications and methods do not seem developed well enough as would be expected at a conference like this.
Reviewers mostly recommended to reject. Please take reviewers  comments into consideration to improve your submission should you decide to resubmit. 
The proposed approach is interesting and is differentiated enough from the recent body of work on Neural Network Granger causal modeling as it offers a mechanism for detecting signs of causality.   The authors have satisfactorily addressed the points raised in the reviews. In particular relationship with prior work and novelty of the contributions are now clearly articulated. The added discussion on the superiority of TCDF on simulated fMRI experiments is insightful. Though prediction error is only a proxy for the task at hand, the readers will appreciate the added evaluation.   The proposed approach to stability evaluation leveraging the time reversal trick is novel and particularly pertinent, and could motivate some interesting follow up work on this topic. It is also important that the authors have characterized the computational advantage of the approach. 
This paper proposes a meta learning based technique to learn how to back translate (generate a synthetic source language translation of an observed target language sentence) for the purpose of better optimising a source to target translation model.   The approach is an interesting novel angle to jointly training the translation model and the back translation component. Compared to techniques like UNMT and DualNMT, the approach offers reduced training time and a simpler formulation with fewer trainable components (and fewer hyperparameters).   During the discussion phase the authors provided additional insight, clarifications, and results that improved our perception of the paper. I would personally appreciate if the authors would update their paper with the clarifications they made to points raised by R2, R3, and R4, especially on the details about meta validation, the discussion about memory footprint, and the additional results on UNMT (and variants). 
Reviewer #1 noted that he wishes to change his review to weak accept post rebuttal, but did not change his score in the system.  Presuming his score is weak accept, then all reviewers are unanimous for acceptance.  I have reviewed the paper and find the results appear to be clear, but the magnitude of the improvement is modest.  I concur with the weak accept recommendation. 
The authors propose a new set of metrics for evaluation of generative models based on the well established precision recall framework, and an additional dimension quantifying the degree of memorization. The authors evaluated the proposed approach in several settings and compared it to a subset of the classic evaluation measures in this space. The reviewers agreed that this is an important and challenging problem relevant to the generative modeling community at large. The paper is well written and the proposed method and motivation are clearly explained.   The initial reviews were borderline, and after the discussion phase we have 2 borderline accepts, one strong accept, and one strong reject. After reading the manuscript, the rebuttal, and the discussion, I feel that the work should not be accepted on the grounds of insufficient empirical validation. Establishing a new evaluation metric is a very challenging task   one needs to demonstrate the pitfalls of existing metrics, as well as how the new metric is capturing the missing dimensions in a thorough empirical validation. While the former was somewhat shown in this work (and in many other works), the latter was not fully demonstrated. The primary reason is the use of a non standard benchmark to evaluate the utility of the proposed metrics. I agree that covering a broader set of tasks and models makes sense in general, but it shouldn’t be done at the cost of existing, well understood benchmarks. I expected to see a thorough comparison with [1], one of the most practical metrics used today which can be easily extended to all settings considered in this work (notwithstanding the drawbacks outlined in [2]). What are the additional insights? What is [1] failing to capture in practical instances? Does the rank correlation change with respect to modern models across classic datasets (beyond MNIST and CIFAR10)? This would remove confounding variables and significantly strengthen the paper.  My final assessment is that this work is borderline, but below the acceptance bar for ICLR. I strongly suggest the authors to showcase the additional improvements over methods such as [1] in practical and well understood settings commonly used to benchmark generative models (e.g. on images). The experiments suggested by the reviewers are a step in the right direction, but not sufficient.  [1] Improved Precision and Recall Metric for Assessing Generative Models. Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, Timo Aila. NeurIPS ’19  [2] Evaluating generative models using divergence frontiers.  Josip Djolonga, Mario Lučić, Marco Cuturi, Olivier Frederic Bachem, Olivier Bousquet, Sylvain Gelly. AISTATS ‘20
This paper received scores of 5,5,6,8. The reviewer giving a score of 8 stated that they would ve given a 7, but that that is not an option in the system. The other reviewer giving an acceptance scores mentioned that they would also be OK with a rejection. The details of the assessment are thus less enthusiastic than could be assumed with an overall average score of 6. I am therefore weakly recommending rejection.  The main criticisms of the reviewers are lack of novelty, lack of deeper analyses that really provide insights into why zero cost operation scoring works, and lack of the number of NAS benchmarks tested. Out of these, personally, I would not criticize a lack of novelty, since it is not trivial to put together zero cost and one shot methods and the results appear promising. However, even the most positive reviewer criticized that the work focuses on NAS Bench 201 heavily (which is particularly problematic given that NAS Bench 201 uses a fixed wiring and only allows the choice of operations; this may make the proposed method particularly applicable). During the rebuttal, the authors added NAS Bench 1shot, which is a very good step, but the proposed technique does not actually work as well there. While this may be due to the special nature of operations in the nodes rather than in the edges for NAS Bench 1shot1, for a revision, it would be good to add additional experiments on further NAS benchmarks in order to allow for a better understanding under which circumstances the proposed method works well. In particular, it would be interesting how well the method works on a quite different search space, such as the one of MobileNet.
## A Brief Summary This paper uses offline algorithms that can see the entire time series to approximate the online algorithms that can only view the past time series. The way this is done is basically, the offline algorithm is used to provide discrete class targets to train the online algorithm. The paper presents results on synthetic and historical stock market data.  ## Reviewer s1H9 **Strengths:**   Practical problem.   Novel approach.   Clear presentation. **Weaknesses:**   No other baselines.   No theoretical guarantees behind the approach.   Writing could be improved.  ## Reviewer EgW9 **Strengths:**   Clear writing.   Interesting research direction. **Weaknesses:**   The primary claim seems incorrect and unclear.    Due to the unclarity about the primary claim of this paper, it is difficult to evaluate the paper.    Lack of baselines.   The lack of discussions of the related works.  ## Reviewer gii5 **Strengths:**   Interesting and novel approach. **Weaknesses:**   Difficult to evaluate, with no empirical baselines or theoretical evidence.   The datasets used in the paper are not used in the literature before. Authors should provide experimental results on datasets from the literature as well.   The paper needs to compare against the other baselines discussed in the related works.   More ablations and analysis on the proposed algorithm is required.   Unsubstantiated claims regarding being SOTA on the task, since the paper doesn t compare against any other baselines on these datasets.   The paper can be restructured to improve the flow and clarity.  ## Reviewer zoKR **Strengths:**   Novel and interesting research topic.   Bridging classical algorithms and ML.   Clearly written.   **Weaknesses:**   Lack of motivation for the problem.   The approach only works with offline algorithms that work on time segmented data.  ## Reviewer aaFn **Strengths:**   Novel algorithm.  **Weaknesses:**   Potentially overfitting to the offline data.   Data hungry approach.   Confusion related to the occurrence moments of predicted future actions.   Section 2 is difficult to understand.  ## Key Takeaways and Thoughts Overall, I think the problem setup is very interesting. However, as pointed out by reviewers gii5 and EgW5, due to the lack of baselines, it is tough to compare the proposed algorithm against other approaches, and this paper s evaluation is challenging. I would recommend the authors include more ablations in the future version of the paper and baselines and address the other issues pointed out above by the reviewers.
The paper proposes an improved method for randomized smoothing, reducing computationally complexity compared with some previous works. The authors propose to learn score functions to denoise the randomized image prior to feeding it to a trained classification model. More specifically,  two image denoising algorithms based on score estimation are proposed to be applied regardless of noise level/type.   Strengths:   The paper shows strong quantitative results. The gap with white box smoothing is small on cifar, outperforming Salman et al. However according to the authors, the performance advantage could be mainly attributed to (1)  the use of better network architecture and (2) the multi scale training, not the major contribution of a score based denoiser.    The denoiser doesn t require access to the pre trained classifiers.   The proposed method only requires training of one score network to handle various types of noise type/level, although reviewers have raised concerns about motivation to having a method that only needs one denoiser for multiple noise levels    the computational bottleneck of randomized smoothing is the prediction time rather than training time and  using the same score function for multiple noise levels could be suboptimal.  Weaknesses:   There are some concerns about the significance of the contribution as well as novelty of the work, as the denoising + pre trained classifier architecture is already proposed. Specifically, the work can be seen as incremental to [1], although the work uses a score based image denoiser whereas [1] uses a CNN based image denoiser and this work is more efficient as it requires only one score network, while [1] trained multiple denoisers with respect to each noise levels.    Reviewers have expressed concerns on the prediction efficiency of score function based generative / denoising models.  The proposed method might exacerbate the weakness of randomized smoothing (i.e., slow prediction), especially in high dimensions.   The reviewers are curious to see the benefit of the proposed denoiser over the state of the art Gaussian denoisers (as used in [1]) under Gaussian noise setting.  Method seems to be effective for low resolution images only. The gap with white box increases on Imagenet.  [1]. Salman, Hadi, et al. "Denoised Smoothing: A Provable Defense for Pretrained Classifiers." Advances in Neural Information Processing Systems 33 (2020). 
The authors propose to use implicit policies (similar to a conditional GAN) with a GAN inspired regularizer. Theoretically, they show an equivalence between policy matching and state action visitation matching. Finally, they evaluate their approach on D4RL and showed improved performance as well as ablations.  Reviewers did not find the theoretical contribution to be significant.  While the exact form may be novel, the general result has been shown in previous work and they only use the general result as a loose motivation for their approach. All reviewers acknowledge their empirical improvements as the primary strength of the paper. While a central component of their story is joint state action regularization, Reviewer Ht1b identified that their proposed approach does not appear to directly regularize the joint state action distribution, but rather behaves more similarly to existing policy constraint methods. I agree with Reviewer Ht1b and after much back and forth discussion (both Reviewer Ht1b and myself) with the authors, I have not been persuaded otherwise.  The paper has a lot of potential   strong empirical results, but the justification and explanation of the method needs to be rewritten in light of the policy constraint regularization or a stronger argument needs to be put forth in support of joint state action regularization. I don t think this diminishes the results though, but without this substantial revision, I cannot accept the paper at this time.
The authors present a model based method for cooperative multi agent reinforcement learning and propose to use communication of future predictions (as given by a learned world model) as a way to overcome partial observability.  Overall, all reviewers found this work to be of great interest and the combination of planning + communication novel. However, all reviewers pointed that the claims that the papers makes are not fully supported by the experimental framing of the paper pointing to several shortcomings around experimental design in general and better control of appropriate baselines. The authors have since clarified several aspects in their paper and also included a new RL environment.   However, as the paper still stands does not fully provide convincing evidence of their proposal, which is however very intriguing. I would like to echo though reviewers  suggestions that the authors work a bit more on the experimental design and I really hope this work will appear at a later venue.
While the paper s topic is on a topic of interest and presents an evaluation on three synthetic datasets,  PartNet Chairs, Shop VRB Simple, CLEVR dataset, several concerns and weaknesses remain after the author response.  Main Concern and Weaknesses: * The main improvement comes from the additional supervision provided by language, which provides a strong supervision signal as the language is scripted and the parser has nearly "perfect accuracy (>99.9%) on test questions/captions", as the authors state. * Limited contribution: combination of MONet/Slot Attention with NS CL;  * Experiments limited to synthetic images with no background (relatively easy to segment) and synthetic (templated) language (easy to parse). This is especially concerning when the task is segmentation and the supervision comes from templated language, making it a strong supervision signal. * The positive impact of the objectness score on performance was not sufficiently demonstrated * Additionally, in the final discussion phase, reviewers were concerned that the with limited visual reasoning training on a subset of 25% of the data, reduces the performance [I note that I did not take this as the decisive reason for rejection as the authors did not have a chance to respond to this concern but the authors should discuss this in any future version of the paper]  The paper initially received borderline and reject scores and the authors took significant effort to address several of the comments of reviewers. While the paper was improved several of the main concerns remained and reviewers recommended reject after reading the author response and each others comments.  I agree to the concerns and recommend reject.
The paper considers the Equitable and Optimal Transport (EOT) problem which is arises in fair division of goods and multi resource allocation. The resulting problem is a linear program, which is polynomial time solvable; however, the existing polynomial time solvers either do not scale well with the dimension or are dual methods with entropic regularization for which it is unclear how to extract a primal solution. The paper shows how to extract a primal solution and also provides complexity analysis of a recently proposed projected alternating minimization method (PAM). The paper further provides a Nesterov accelerated variant of PAM.   Overall, the paper is a meaningful contribution and was considered borderline. On one side, EOT seems like an interesting problem, the paper is well presented, and the provided complexity results are technically sound. On the other hand, the reviewers felt that the EOT problem was not motivated enough, that the techniques for proving the results were mostly standard, and that the numerical experiments were insufficient. Even though the authors provided additional numerical experiments, I did not find the responses regarding motivation (particularly in the context of ML applications) and technical novelty convincing enough. The paper could have gone in either direction, but as there was ultimately no particularly strong support from any of the reviewers, I recommend rejection. The authors are advised to carefully revise the paper and resubmit.
Despite some positive points, the criticisms (and overall scores) put this paper below the bar. The reviewers raise issues of novelty, as well as problems with the experiments and argue that some claims are unsupported.
This work received borderline rates with slight preference to rejection. The main concerns range from writing, novelty to empirical evaluations. Given that no authors’ responses are submitted, we have decided to reject this work.
This paper applies and evaluates the use of Q learning for the control of microscopic collectives of Volvox algae.   While the application is indeed very cool and potentially impactful, the paper has no theoretical contribution to the field of machine learning as it consists of an empirical evaluation of an existing (and well established) algorithm.  The reviewers agree on the importance of the application, reported concerns about the current manuscript. In particular:   Reviewers QBsR and GPp7 suggested including additional comparisons to other learning algorithms   Reviewers QBsR and BtTc also suggested improving the writing  Overall, I agree with the reviewers that the current manuscript has a lot of potentials, but it could benefit from additional work.  Please carefully consider and incorporate the feedback received from the reviewers. Personally, I think that presenting a more sharp message and clearer insights would further increase the quality of exposition and help to make a stronger case for why this manuscript is relevant to the larger ML community.
This work considers the popular LQR objective but with [A,B] unknown and dynamically changing. At each time a context [C,D] is observed and it is assumed there exist a linear map Theta from [C,D] to [A,B]. The particular problem statement is novel, but is heavily influenced by other MDP settings and the also follows very closely to previous works. The algorithm seems computationally intractable (a problem shared by previous work this work builds on) and so in experiments a gross approximation is used.   Reviewers found the work very stylized and did not adequately review related work. For example, little attention is paid to switching linear systems and the recent LQR advances are relegated to a list of references with no discussion. The reviewers also questioned how the theory relates to the traditional setting of LQR regret, say, if [C,D] were identity at all times so that Theta   [A,B].   This paper received 3 reviews (a third was added late to the process) and my own opinion influenced the decision. While the problem statement is interesting, the work fails to put the paper in context with the existing work, and there are some questions of algorithm methods.  
While the reviewers agree that this is an important topic, there are numerous concerns novelty, correctness and limitations. 
This paper presents a deep reinforcement learning method that aims at ensuring resilience to observational interference. During training labels that indicate presence or absence of interference are available to the algorithm. The training objective is augmented to learn the prediction of interference that is used at test time to infer the interference label. The experimental results show superior performance in comparison to other baseline RL methods.  The main objection raised by the reviewers was on the confusing and possibly unsound causal formulation. The authors  clarifications during the discussion did not eliminate bur rather exacerbated the reviewers  doubts. I read the paper in full myself to understand whether the reviewers  confusion was justified, and whether it could be easily resolved by an improved explanation, or it is a more serious issue.  I did not succeed in clearly understanding the causal formulation nor its relevance, and also have  soundness concerns. Figure 2a does not seem to be a correct explanation of the causal mechanism. It is also not clear from this figure why z is called confounder. More generally, I was not able to reach a coherent and sound causal formulation from the authors  explanation. My conclusion is that the framing of the paper as causal inference based is not well justified. 
The reviewers found the work interesting but have concerns about the correctness of some of the claims in the paper. Also some reviewers would like to see more experiments and some have concerns about the theoretical results. Overall, I see the work promising but it requires a major revision and some improvements to pass the bar. I would recommend the authors to use the reviewers  comments and prepare the paper for future venues.
The paper proposes an unsupervised pretraining approach for 3D recognition, which is based on point cloud completion. The initial review receives a mixed rating, with two reviewers rate the paper below the bar and two above the bar. After the rebuttal, R3 changes the opinion from above the bar to a rejection recommendation. While several reviewers recognize the simplicity of the proposed method, R2 and R4 consider the proposed method a straightforward extension of known approaches for NLP and vision tasks. A lack of novelty was also pointed out as a weakness by R3 and R4. After consolidating the reviews and the rebuttal, the AC finds the weakness claims convincing and determines the paper is not ready for publication in the current form. 
The authors describes a drug design method that generates molecules by simulating adding or deleting parts of the molecules, and using graphnets to capture atom and fragment level information and construct new molecules.  Simulated annealing is used to ‘edit’ the 3D structures, and docking simulations, drug likeness and synthesizability are used to provide information back into training.  The authors compare with multiple baselines on a test set of 12 targets, including the current SOTA model, and report improved performance.  Strengths:    The proposed model outperforms other baselines in the multi objective molecules optimization benchmark.   The model doesn t rely on a data driven biological activity predictor.  Weaknesses:    The reviewers point out that the model seems to be incremental with respect to previous work.   The reviewers have concernts about the reproducibility of the work and find a lot of details lacking.  This is a borderline paper with a majority of reviewers voting for rejection. I recommend the authors to addrses the weaknesses above and resubmit to another venue.
The paper proposes a novel framework to develop useful auxiliary tasks and combine auxiliary tasks into a single coherent loss. The idea is good and the experiments are sufficient to verify the arguments. All the reviewers agree to accept the paper.
The reviewers agree that the paper is addressing an interesting problem. However, the authors analyze the effect of heterophily on GNN for node classification. The authors simplify the analysis by removing the nonlinearity in the GNN model and derive some theoretical results. However, the analysis is very specific to the simplified version of GNN, and the link to later proposed solution is also weak. Furthermore, a more significant improvement in experiments will also make the paper more convincing.
The paper proposes an transfer learning approach to reinforcement learning, where observations from a target domain are mapped to a source domain in which the algorithm was originally trained. Using unsupervised GAN models to learn this mapping from unaligned samples, the authors show that such a mapping allows the RL agent to successfully interact with the target domain without further training (apart from training the GAN models). The approach is empirically validated on modified versions of the Atari game breakout, as well as subsequent levels of Road Fighter, showing good performance on the transfer domain with a fraction of the samples that would be required for retraining the RL algorithm from scratch.   The reviewers and AC note the strong motivation for this work and emphasize that they find the idea interesting and novel. Reviewer 3 emphasizes the detailed analysis and results. Reviewer 2 notes the innovative idea to evaluate GANs in this application domain. Reviewer 1 identifies a key contribution in the thorough empirical analysis of the generalization issues that plaque current RL algorithms, as well as the comparison between different GAN models and finding their performance to be task specific.  The reviewers and AC noted several potential weaknesses: The proposed training based on images collected by an untrained agent focus the data on experience that agents would see very early on in the game, and may lead to generalization issues in more advanced parts of the game. Indeed these generalization issues are one possible explanation for the discrepancies between qualitative and quantitative results noted by reviewer 1. While the quantitative results indicate good performance on the target task, the image to image translation makes substantial errors, e.g., hallucinating blocks in breakout and erasing cars in Road Fighter. To the AC, the current paper does not provide enough insight into why the translation approach works even in cases where key elements are added or removed from the scene. The paper would benefit from a revision that thoroughly analyses such cases as well as the reason why the trained RL policy is able to generalize to them.  R1 further notes that the paper does not address the RL generalization issue, but rather presents an empirical study that shows that in specific cases it is easier to translate from a target to a source domain, than to learn a policy for the target domain. The AC shares this concern, especially given the limited error analysis and conceptual insights derived from the empirical study. There are further concerns about the experimental protocol and hyper parameter selection on the target tasks. Finally reviewer 1 questions the claim of whether data efficiency matters more than training efficiency in the proposed setting.  There is disagreement about this paper. Reviewers 2 and 3 gave high scores and positive reviews, but did not provide sufficient feedback to the concerns raised by reviewer 1, who put forward significant concerns.   The AC is particularly concerned about the experimental protocol and hyper parameter tuning directly on the test tasks. The authors counter this point by noting that "We agree that selecting configurations based on the test set is far from ideal, but we also note that this is the de facto standard in video game playing RL works, so we do not believe our work is any worse than others in the literature in this regard." The AC worries about the lack of motivation to identify a strong empirical setup to arrive at the strongest possible contribution. A key concern here is that the results seem to vary substantially by task, GAN model used, etc. and substantial tuning on the target domain seems to be required. This makes it hard to draw any generalizable conclusions. This concern can be alleviated by including additional analysis, e.g., error analysis of where a proposed approach fails, or additional experiments designed to isolate the factors that contribute to a particular performance level. However, the current paper does not go to this detail of empirical exploration. Given these concerns, I recommend not accepting the paper at the current stage.
This paper presents experiments showing that a linear mapping existing between the hidden states of RNNs trained to recognise (rather than model) formal languages, in the hope of at least partially elucidating the sort of representations this class of network architectures learns. This is important and timely work, fitting into a research programme begun by CL Giles in 92.  Despite its relatively low overall score, I am concurring with the assessment made by reviewer 1, whose expertise in the topic I am aware of and respect. But more importantly, I feel the review process has failed the authors here: reviewers 2 and 3 had as chief concern that there were issues with the clarity of some aspects of the paper. The authors made a substantial and bona fide attempt in their response to address the points of concern raised by these reviewers. This is precisely what the discussion period of ICLR is for, and one would expect that clarity issues can be successfully remedied during this period. I am disappointed to have seen little timely engagement from these reviewers, or willingness to explain why they are stick by their assessment if not revisiting it. As far as I am concerned, the authors have done an appropriate job of addressing these concerns, and given reviewer 1 s support for the paper, I am happy to add mine as well.
This paper proposed an unsupervised learning algorithm for predictive modeling. The key idea of using NCE/CPC for predictive modeling is interesting. However, major concerns were raised by reviewers on the experimental design/empirical comparisons and paper writing.  Overall, this paper cannot be published in its current form, but I think it may be dramatically improved for a future publication. 
The submission describes a method for tuning machine learning pipeline hyperparameters using transfer learning from related tuning tasks. In particular, the method uses learned meta features to construct a covariance function for a GP.  This was an extremely difficult case and could have gone either way. It was the closest case for any paper I serve as the AC for. Two of the reviewers recommended rejecting the paper and three recommended accepting, although during discussion one of the reviewers recommending accepting the paper seemed to actually be more on the reject side.  Ultimately, I have decided to recommend rejecting this submission. However, if either the clarity (especially concerning the neural network setup) or the experiments were somewhat improved I would have recommended accepting it. I view clarity as an extremely important factor when weighing whether a submission should be accepted. I concur with the reviewers on the following weaknesses of the experiments: (1) the lack of an ablation test when considering ad hoc meta features and (2) the experimental evaluation is based on mostly aggregated metrics.  I know this recommendation must be disappointing, but I encourage the authors to polish the work a bit more and resubmit it somewhere.
Summary: This paper studies a contextual bandit problem where the decision maker must communicate its intended actions (given observations of the contexts) to a controller through a constrained communication channel. The original part of the paper is that the “bandit algorithm” must encode its actions into a compressed version that then serves to the controller. In that sense, the controller must cluster the problems for the decision maker to simplify communication.  Discussion: Most reviewers appreciated that the paper is well written and proposes an original problem. The main commonly issue is that of a lack of regret analysis. The authors included an additional theoretical result giving a necessary condition for sublinear regret but the committee would still appreciate a more in depth study of the performance of the proposed algorithm, given that the condition is satisfied. One possible direction is to connect this work with the literature on "Clustering of bandits" (CoB) as raised by reviewer mXFx. The authors claim that this paper is only mildly related but the committee would kindly insist that linear rewards are just a generalization of multi armed bandits, that there is also a finite state space in CoB (finite population) and it seems possible to reduce the proposed problem to CoB under some assumptions. In that regards, it would be important that a more thorough review and comparison of that line of work is done in the main paper (note also "Latent Bandits" and related papers), even though we agree that the proposed approach is different and we appreciate its originality.  Overall, the paper is borderline, and the committee did not reach a consensus.   Decision: Reject
This paper revisits the design of positional embedding in the pre trained language models, and propose a new approach to handling the positional encoding.   Overall, the paper is well motivated. The authors have addressed most comments based on the review. The method proposed in the paper is simple and effective. Experiments are comprehensive and demonstrate the effectiveness of the proposed approaches.  
The paper formulates fluid simulation as an image to image prediction task and proposes to solve the problem using a cGAN formulation. The objective is to develop fast approximate solutions for the modeling of fluid dynamics, here Navier Stokes for incompressible flows. The images correspond to the discretization of velocity and pressure fields. Experiments are performed on a simulation for a Karman vortex street.   All the reviewers expressed concerns w.r.t. the absence of references and comparisons with closely related work in the recent but abundant literature on NN for modeling PDE dynamics, the lack of novelty and the insufficient experimental design, description and discussion.
After the rebuttal stage, three of four reviewers recommend acceptance, and one gives a borderline score but argues they lean positive. Concerns seem well addressed; the method is simple yet effective.
The reviewers raised a number of concerns including the lack of clarity of various parts of the paper, lack of explanation, incremental novelty, and insufficiently demonstrated significance of the proposed. The authors’ rebuttal addressed some of the reviewers’ concerns but not fully. Overall, I believe that the paper presents some interesting extensions for multi agent communication but in its current form the paper lacks explanations, comparisons and discussions. Hence, I cannot recommend this paper for presentation at ICLR.
The paper proposes to substitute the gradient in the second moment estimation term with the "momentumized" version, arguing that it improves both optimization and generalization. Some theoretical results are shown as well as empirical results.  The paper has been widely discussed by the reviewers and several weak points have been raised. Let me list some of the most important ones.   The theory appears to be incremental and overall very weak. The authors themselves acknowledged that this "is not a pure optimization theory paper". In details, the generalization analysis is a straightforward extension of Zhou et al. [NeurIPS 2020], while the optimization analysis inherits all the known weaknesses of previous similar analysis in deep learning optimization papers. In particular, *none* of the following is correct: the use of a regret analysis for a stochastic non convex optimization algorithm, the assumption of bounded iterates, Assumption 5, the assumption in Theorem 2 on $\alpha_t/\sqrt{v_t}$. The fact that similar mistakes were done in previous papers does not make them correct: The community should aspire at doing better not at reiterating known mistakes.   On $\epsilon$: the reviewers correctly pointed out that moving $\epsilon$ under the square root and not changing its value is not fair. The answers of the authors on this point were unconvincing.   Doubts on empirical results: it seems that not all the possible hyperparameters of the baselines were properly tuned. For example, despite being common practice, epsilon should also be tuned, see for example the experiments in Agarwal et al. 2020.  I didn t consider the discussion on AdaBelief because only marginally relevant to this paper.  Overall, the paper does not seem interesting from a theoretical point of view and its empirical comparison cannot be fully trusted for the presence of some weaknesses.
Two reviewers recommended rejection, and the last reviewer votes for acceptance. The authors provided a rebuttal, including the end to end experiment (although the AC agrees with the authors that this experiment is not crucial to the paper). The AC read the paper and the reviews. While there are clearly interesting aspects of this work, it somewhat falls short in terms of the technical contribution. Perhaps a better writing would alleviate this issue: for example, explaining the visual features is somewhat a distraction from the main point, and could be put in the end. The 3 stage training is somewhat ad hoc (or less elegant). Since there are many excellent papers submitted to ICLR this year, this paper unfortunately did not make it above the bar.
An actor critic method is introduced that explicitly aims to learn a good representation using a stochastic latent variable model. There is disagreement among the reviewers regarding the significance of this paper. Two of the three reviewers argue that several strong claims made in the paper that are not properly backed up by evidence. In particular, it is not sufficiently clear to what degree the shown performance improvement is due to the stochastic nature of the model used, one of the key points of the paper. I recommend that the authors provide more empirical evidence to back up their claims and then resubmit.
This paper studies the following hypothesis that gradient based explanations are more meaningful the more they are aligned with the tangent space of the data manifold. The reviews are negative overall. The general feeling is that the paper reads like a set of subjective observations about the meaningfulness of explanation and relationship with data manifold + tangential theory. There isn’t a coherent story.
Summary: The authors propose a method for representing a posterior over discrete latent variables in representation learning problems using a neural network. Two applications are discussed: One are certain clustering problems, in which clusters are sufficiently separated. Another is the computation of mutual information of discrete random variables. This is applied to learning image representations.  Discussion: The authors have not provided a response to the reviews.  Recommendation: Four detailed reviews unanimously recommend rejection. Main points of criticism are lack of novelty, limited and unconvincing experimental evaluation, and a poor presentation that also lacks technical detail. This work is clearly not ready for publication. 
Description of paper content:  The paper studies the problem of achieving coordination among a group of agents in a cooperative, multi agent task. Coordination graphs reduce the computational complexity of this problem by reducing the joint value function to a sum of local value functions depending on only subsets of agents. In particular, the Q function of the entire system is “expanded” up to second order in agent interactions: Q   \sum_{i \in [n]} q_i + \sum_{(i,j) \in G} q_{ij}, where the q_i is function of the i th agent’s history and current action, and q_{ij} is a function of two agents’ histories and current actions. As G does not include higher order (third and above) terms, the algorithm does not have exponential dependence on the number of agents. If G includes only a subset of pairs of agents, then the computational complexity is reduced to less than quadratic. Since the coordination problem is cooperative, the authors propose a meta agent (“coordinator”) that selects the graph G in a dynamic (state by state) fashion in order to maximize return. The optimization problems of the meta agent and the sub agents are performed by deep Q learning.  Summary of paper discussion:  The critical comment made by one reviewer was: “Going back on that trend now only to pursue the polynomial time nature of the running algorithm would in my opinion require far more diverse evaluation examples, backed by a stronger motivation highlighting real world threats of all the other MARL algorithms taking longer than polynomial time. As is, SOP CG does not contend amazingly against other MARL algorithms that chose the "NP hard? Curse of dimensionality? Fine. We ll approximate, approximate, approximate." path rather than the "Polynomial time is our topmost priority; function expressiveness can wait." path. That leads me back to the question of why pursue polynomial time at the cost of losing both the function expressiveness and the peak performance….”  Comments from Area Chair:  Looking at the experiments, the number of agents in the empirical problems is not large. For example, there are 15 agents in "Sensor." Any focus on computational complexity at this scale is hard to justify, especially with algorithms that are approximate. It seems favorable at this small scale to use function approximators that can take in all the agents  histories and actions. This obvious baseline is not included in comparisons. It is hard to justify inclusion of this paper in the conference.
The paper proposes a new way of estimating treatment effects from observational data. The text is clear and experiments support the proposed model.
While the reviewers found several interesting points about the paper, they raised several issues, which prevents me from recommending acceptance of the paper. In particular, the paper is not positioned properly in the literature, hence the novelty and the contributions are not properly clarified. The approach of the paper is reasonably simple (which would be a good thing by itself), but there seem to be natural avenues along which more complete results could be obtained, as mentioned in the reviews. Finally, the experiments should be improved (e.g., comparing with other algorithms from the literature). In summary, this is a promising work, but it requires some improvements before it can be published.
The paper extends recent value function factorization methods for the case where limited agent communication is allowed. The work is interesting and well motivated. The reviewers brought up a number of mostly minor issues, such as unclear terms and missing implementation details. As far as I can see, the reviewers have addressed these issues successfully in their updated version. Hence, my recommendation is accept.
The submission cannot be accepted as there seems to be a mistake in the proof of the main contribution (Theorem 2).
This paper studies the effectiveness of self supervised approaches by characterising how much information they can extract from a given dataset of images on a per layer basis. Based on an empirical evaluation of RotNet, BiGAN, and DeepCluster, the authors argue that the early layers of CNNs can be effectively learned from a single image coupled with strong data augmentation. Secondly, the authors also provide some empirical evidence that supervision might still necessary to learn the deeper layers (even in the presence of millions of images for self supervision).  Overall, the reviews agree that the paper is well written and timely given the growing popularity of self supervised methods. Given that most of the issues raised by the reviewers were adequately addressed in the rebuttal, I will recommend acceptance. We ask the authors to include additional experiments requested by the reviewers (they are valuable even if the conclusions are not perfectly aligned with the main message). 
The paper revisits importance sampling as an approach for combating distribution shift when training over parameterized neural networks. Contrary to recent results that suggest that importance sampling is perhaps incompatible with over parameterization, the authors find that the exponential tail of losses such as the logistic loss is the root cause. For polynomial tailed losses, authors analyze gradient descent on importance weighted polynomially tailed losses and demonstrate the advantage of importance sampling in a label shift setting.  There paper is well written and the results are sound. Overall, a good paper.
The paper investigates the average stability of kernel minimal norm interpolating predictors. The main result  establishes an upper bound on a particular notion of average stability for which it is well known that it  can be used to bound the generalization error. This upper bound holds for all interpolating predictors  from the RKHS, but it is minimized by the minimal norm predictor.   While at first glance this result looks highly interesting, a closer look reveals that the significance of the results  crucially depends on the quality of the derived upper bound. Here two reviewers raised concerns, since it is  by no means clear that even the optimized upper bound produces meaningful bounds on the generalization performance. The authors tried to address these concerns in their response and promised to update their  paper accordingly. As a result, they added a paragraph on page 8. Unfortunately, this paragraph remains extremely  vague, in particular if it comes to the more interesting case of non linear kernels. Here, the authors briefly refer to  a paper by El Karoui but no details are given. However, looking at El Karoui s paper it is anything but obvious whether  the results of that paper lead to reasonable upper bounds on the average stability for a sufficiently general class  of distributions.  As a result, I view the paper under review to be premature since it remains unclear if the observed optimality of the minimal norm solution is a real feature or just an artifact due to an upper bound that is simply too loose to make any conclusion.    
This paper is rejected.  The authors contributions are: * Propose PFPN as an expressive action policy for continuous action spaces. * Introduce a reparameterization trick for training PFPN with off policy methods. * Experiments claiming PFPN outperforms unimodal Gaussian policies and a uniform discretization scheme and that it is more sample efficient and stable across different training trials.  I and the reviewers appreciate the additions by the authors. The GMM baseline is an important addition addressing concerns from several reviewers. However, I agree with R2 s comment that "most interesting contribution of the paper is the resampling scheme. However, there is minimal evaluation of the benefit of this scheme [...] However, the added experiments with random sampling are somewhat worrying the performance improvement of the proposed re sampling scheme is quiet minor over random resampling. In the future, the authors may want to investigate the random resampling for the systems in figure 14." Without resampling, the proposed method is a location/scale state independent GMM policy. It is interesting that this outperforms the fully state dependent GMM, and the authors could investigate that further. To justify the additional complexity of the resampling step, the authors need to perform further investigation and move that to the main text.  In addition, the evaluated environments omit standard OpenAI gym environments (which the authors do have access to as evidenced by their experiments w/ DDPG on them in the Appendix). This makes evaluating baseline method performance challenging. Furthermore, the authors cite Tang & Agrawal (2018) which introduces a normalizing flow policy that outperforms GMM. It would be natural to compare to that baseline. Finally, Figurnov et al. (2018) among others shows how reparameterization gradients can be computed through GMMs. The authors should explain why this is not applicable.
One reviewer is positive, but that review is not of high quality. The other reviewers agree that this paper is interesting, but has too many limitations to be accepted by a highly competitive venue such as ICLR.
This paper focuses on understanding how the angle between two inputs change as they are propagated in a randomly initialized convolutional neural network layers. They demonstrate very different behavior in different settings and provide rigorous measure concentration results. The reviewers thought the paper is well written and easy to read with nice theoretical results. They did raise a variety of technical concerns that were mostly addressed by the authors rebuttal. My own reading of the paper is that this is a nice contribution. I therefore agree with the reviewers and recommend acceptance.
The paper considers a problem of clearly practical importance: multi label classification where the ground truth label sets are noisy, specifically they are known (or at least assumed) to be a superset of the true ground truth labels. Learning a classifier in this setting require simultaneous identification of irrelevant labels. The proposed solution is a 4 part neural architecture, wherein a multi label classifier is composed with a disambiguation or "cleanup" network, which is used as conditioning input to a conditional GAN which learns an inverse mapping, trained via an adversarial loss and also a least squares reconstruction loss ("generation loss").   Reviews were split 2 to 1 in favour of rejection, and the discussion phase did not resolve this split, as two reviewers did not revisit their assessments. R2 and R3 were concerned about the overall novelty and degeneracy of the inverse mapping problem. R1 increased their score after the rebuttal phase as they felt their concerns were addressed in comments (regarding issues surrounding the related work, the possibility of trivial solutions, and intuition for why the adversarial objective helps), but these were not addressed in the text as no updates were made.  I agree with the authors that PML is an important problem (one that receives perhaps less attention than it should from our community), and their empirical validation seems to support that their method outperforms (marginally, in many cases) methods from the literature. While the ablation study offers preliminary evidence that the inverse mapping is responsible for some of the gains, there are a lot of moving parts here and the authors haven t done a great job of motivating why this should help, or investigating why it in fact does. Based on the scores and my own reading of the paper, I d recommend rejection at this time.  My own comments for the authors: I d urge efforts to clarify the motivation for learning the inverse mapping, in particular adversarially (rather than just with the generation loss) in the text of the paper as you have in your rebuttals, and to improve the notation (the use of both D tilde and D is confusing, and the omega notation seems unnecessary). I m also not entirely clear whether the generator is stochastic or not, as the notation doesn t mention a randomly sampled latent variable (the traditional "z" here is a conditioning vector). Either way, the answer should be made more explicit.
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.    improvements to a transformer model originally designed for machine translation   application of this model to a different task: music generation   compelling generated samples and user study.  2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.    lack of clarity at times (much improved in the revised version)  3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  The main contention was novelty. Some reviewers felt that adapting an existing transformer model to music generation and achieving SOTA results and minute long music sequences was not sufficient novelty. The final decision aligns with the reviewers who felt that the novelty was sufficient.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  A consensus was not reached. The final decision is aligned with the positive reviews for the reason mentioned above. 
Although this paper tackles an important problem, all reviewers agree that it requires further work before it can be published. First, the paper would need to be polished in order to be easier to read. Stronger experiments would also be needed in order to support the claims of the paper, e.g. by considering additional datasets and proper baselines. Finally, an important concern about this paper is novelty and originality. It is not clear at this point that the contribution is substantial enough for a conference like ICLR. Addressing these points would significantly improve the paper.
This paper presents a GNN based attention mechanism and tests it on a robotic stacking task.  While all the reviewers agree that this work is novel and interesting, they also are unanimous (even after the rebuttal) in pointing to the insufficient experimental evaluation of the proposed method.  I encourage the authors to incorporate the feedback of all the reviewers.
This work presents a new theoretically motivated data augmentation technique. Reviewers agreed that the theory was interesting and has value, however raised concerns regarding the experimental evaluation which was limited to the Cifar datasets. There was some discussion over whether or not a comparison with AutoAugment would be fair, the proposed method is theoretically motivated whereas AutoAugment takes significant compute to train. I agree with the authors that if the method doesn t outperform AutoAugment on CIFAR, this would not necessarily invalidate their results. Nonetheless the work would be significantly strengthened if it included results. on additional datasets to stress test the theory. I recommend the authors add additional supporting evidence and resubmit.
Thanks for your submission to ICLR.  Reviews were fairly mixed on this paper, with two reviewers advocating for accepting the paper and two advocating for rejecting the paper.  There were some concerns raised by the reviewers, most notably novelty and some issues with the experiments.  After rebuttal, the negative reviewers maintained their scores and the positive reviewers were somewhat less enthusiastic.  In the end, the paper is quite borderline and could really go either way, but it seems that the paper could use another round of reviewing, particularly to make sure the issues raised by the reviewers are adequately addressed.  Please do keep in mind their comments when preparing a future version of the manuscript.
Both R3 and R1 argue for rejection, while R2 argues for a weak accept. Given that we have to reject borderline paper, the AC concludes with "revise and resubmit".
Thank you for submitting you paper to ICLR. The paper presents a general approach for handling inference in probabilistic graphical models that employ deep neural networks. The framework extends Jonhson et al. (2016) and Khan & Lin (2017). The reviewers are all in agreement that the paper is suitable for publication. The paper is well written and the use of examples to illustrate the applicability of the methods brings great clarity. The experiments are not the strongest suit of the paper and, although the revision has improved this aspect, I would encourage a more comprehensive evaluation of the proposed methods. Nevertheless, this is a strong paper.
The paper in its most recent version claims that deep neural networks, when very carefully regularized, outperform methods such as Gradient Boosting Trees on tabular data. This is genuinely surprising to me (in a good way), and I suppose it is as well to the community.  The paper initially received negative reviews with two key remarks that "The results are somewhat expected." (R4, R3, R2). Indeed, the original version mainly stated that very careful regularization helps on tabular data.  Naturally, the reviewers (including myself) seen then as the second key weakness that "All experiments are run on tabular data." (R4, R3).  Based on the reviews, the Authors have clarified and changed their message. I think it is well summarized by R2 "The paper has been significantly refocused and now sells itself as a way of deep neural networks being competitive versus gradient boosting methods, which are dominating the tabular heterogeneous tasks."  As R2 said and was reflected in comments by other reviewers, "[...] convinced by authors response on paper novelty, technical contribution and (after the re focusing) potential usefulness to the community".  Given the new message of the paper, a key new question surfaces. Is this indeed the first convincing demonstration that deep learning can outperform more standard methods on tabular data? R2 pointed out TabNet (see also Google Cloud offering) that already in 2019 claimed "beating GB methods for the tabular data". There is also NeurIPS work "Regularization Learning Networks: Deep Learning for Tabular Datasets"; their abstract opens with "Despite their impressive performance, Deep Neural Networks (DNNs) typically underperform Gradient Boosting Trees (GBTs) on many tabular dataset learning tasks. We propose that applying a different regularization coefficient to each weight might boost the performance of DNNs by allowing them to make more use of the more relevant inputs". The latter work did not claim to beat GBT. Regardless, the two works should be carefully discussed and compared empirically to in the new version of the work.   I am also not yet fully convinced by the added comparison to GDBT. Arguably, AutoML from the sklearn package is not the most popular way to use GDBT in practice. How would regularization cocktails compare to GDBT from XGBoost, optimized using either random search or bayesian optimization?  Based on the above, I have to recommend the rejection of the paper. The key reason is: *the new reframing of the paper is exciting but warrants a much more detailed and careful evaluation*.  I really appreciate the work the Authors have put in clarifying and changing the message of the paper. I understand this is disappointing that we won t be able to include the work in ICLR. Nevertheless, I hope that the Authors found the feedback useful, and wanted to thank the Authors for submitting the work for consideration in ICLR.
This paper presents a novel approach to producing saliency maps for interpreting deep neural networks.  In general this paper seems quite close to borderline, although on the positive side, with some low confidence reviews.  The reviewers felt that the proposed approach could be useful to the community and they seemed to feel that the qualitative results in the experiments demonstrated convincing saliency maps.  There were some concerns, however, about the quantitative experiments as the reviewers (e.g. AnonReviewer2) found that while the mean results seemed better, it wasn t clear if they were statistically significant.  Naturally, the qualitative experiments are highly subjective and there was disagreement between reviewers whether the proposed approach did indeed produce better saliency maps than existing approaches such as smoothgrad.  One reviewer indicated that they found it difficult to follow the paper and to understand the decoy concept given the writing.  During discussion AnonReviewer2 updated their score (and very thorough review) by 2 points to indicate a weak preference toward accept.  None of the reviewers argued particularly strongly for acceptance and "championed" the paper.  The low confidence, slightly above borderline reviews seem to suggest that the reviewers thought the paper was above the bar but were reluctant to argue strongly for acceptance.  The method seemed like it could be useful to them but they weren t clearly convinced that it set a new state of the art given the quantitative and qualitative empirical results.  
This work has a lot of promise; however, the author response was not sufficient to address the concerns expressed by reviewer 1, leading to an aggregate rating that is just not sufficient to justify an acceptance recommendation. The AC recommends rejection.
The paper proposes an entropy penalty related to information bottleneck to deep neural network regression problems. The reviewers had a number of questions and concerns about the paper, which the authors did not address. In light of this, the reviewers agree that the paper is not yet ready for publication. Please carefully read and address the reviewer s concerns in future iterations of this paper.
The paper proposes an approach for N D continuous convolution on unordered particle set and applies it to Lagrangian fluid simulation. All reviewers found the paper to be a novel and useful contribution towards the problem of N D continuous convolution on unordered particles. I recommend acceptance.  
The paper suggests using meta learning to tune the optimization schedule of alternative optimization problems. All of the reviewers agree that the paper is worthy of publication at ICLR. The authors have engaged with the reviewers and improved the paper since the submission. I asked the authors to address the rest of the comments in the camera ready version.
The paper contains *fresh* new ideas connecting mental models and SCMs and providing interpretations (explanations) from DAG models learned from data, including those learned by using deep learning. The usefulness of the theory is illustrated with experiments. The paper contributes some theoretical results, but the presentation has serious issues. In general, the reviewers found the paper hard to follow due to a lack of clarity in some notations, definitions, and assumptions.   The paper was discussed in depth and at length, including the reviewers, the AC, and the senior AC. After all, the gap between the current writing and what is expected from the camera ready is a bit too large, and we feel it could be a disservice to the authors and community to have the paper accepted in its current form, without passing through another round of reviews. Unfortunately, we do not have any version of "conditional acceptance."   Having said that, we feel the paper has the potential for having a significant impact, and we appreciate the novelty of the proposed approach and the connection among different fields. To avoid issues in the future, we would like to suggest the authors pay attention to the detailed feedback provided by the reviewers, including the discussion and the conversation with the AC, following the exchange on Nov/28. Some examples of points that could make the presentation clearer include 1) clarifying the contributions and providing more examples of the theoretical results, 2) making explicit that the results work for Markovian and additivity models, and 3) perhaps changing the title accordingly.
This paper proposed a strategy to train EBMs according to the length of MCMC trajectories required. The paper covers three settings with the different length of MCMC: image synthesis, adversarial defense, and density estimation. The reviewers generally find that there are interesting ideas and promising results in the paper, but the paper is not ready to publish at its current stage. The argument regarding density estimation and FID evaluation is not convincing. The proposed method is also more complicated than the baseline methods (CoopNets and PCD), and we would need a stronger argument for the added complexity.
This paper introduces sparse modeling inspired regularizations to improve deep neural network based image generators. Experimental results on both (low resolution) image synthesis and deep image prior based inverse problems are used to validate the proposed method. The majority of the reviewers were against the acceptance of the paper. As summarized by Reviewer tsoA: "There are shortcomings in the overall concept as well as its evaluation. The findings suggest that this might be a promising avenue of research, but it would need to be taken further. At present, the paper boils down too much into simply adding a simple regularizer at the end and observing that it somewhat improves some metrics in a limited number of scenarios. Due to the limitations of the evaluation, it remains unclear whether the proposed improvement carries over to state of the art models and datasets. Similarly, the promised elucidation of the purpose of the feature values never really materializes." The AC agrees with that summarization and recommends rejection.
The paper studies the problem of task specific model compression obtained from fine tuning large pre trained language models. The work follows the line of research in which model size is reduced by decomposing the matrices in the model into smaller factors. Two step approaches apply SVD and then fine tuned the model on task specific data. The present work makes the observation that after the first step (the SVD compression) the model can dramatically lose its performance, due to the mismatched optimization objectives between the low rank approximation and the target task. The work provides evidence backing this claim. The paper proposes to address this problem by weighting the importance of parameters for the factorization according to the Fisher information. Experimental evaluation shows that the proposed method can achieve better results than variants that use truncated SVD of the weight matrices.  The paper is well written and easy to read. The method is simple and effective and can be applied to in a wide range of settings. The authors provided a thorough response which clarified several points. This led Reviewer Kuwu to increase the score to 6.  All three reviewers agree that the main observation in the work is interesting and informative for researchers and practitioners working on the problem.  Reviewer jnTC points out that the paper would have been stronger if it included theoretical exploration of the reasons behind the "importance of low SVs" phenomenon.  Reviewer Kuwu and jnTC consider the results marginally novel. Reviewer Kuwu considered the significance of the reported results to be limited, and put the work marginally above the acceptance threshold. Reviewer jnTC disagrees with this view, considers and appreciates the generality of the method and the fact that it can work well even for compressed models, while improving in accuracy by a few percent over competing approaches which result in similar parameter counts. The AC agrees with Reviewer jnTC.  Overall all reviewers consider the paper borderline but recommend accepting the paper. The AC overall the topic important (reducing the footprint of language models), the method simple and well motivated. The empirical evaluation is very thorough and shows clear gains across a large number of settings.
Four reviewers evaluated your work and provided a detailed review with many suggestions. I also think that there is an interesting idea and encouraging results but there is a lack of numerical results and still some parts are still unclear and need to be polished. Consequently in its  current form, the paper can not be accepted for publication. I would advise you to carefully follow the remarks of reviewer 1 to improve  the paper.   
This paper proposed a very general idea called Atomic Compression Networks (ACNs) to construct neural networks. The idea looks simple and effective.  However, the reason why it works is not well explained.  The experiments are not sufficient enough to convince the reviewers.
All three reviewers felt that the paper was just below the acceptance threshold, with scores of 5,4,5. R1 felt there were problems in the proofs, but the authors rebuttal satisfactorily addressed this. R3 and the authors had an extended discussion with the authors, but did not revise their score from its initial value (5). R4 had concerns about the experimental evaluation, that wasn t fully addressed in the rebuttal. With no reviewers advocating acceptance, the paper will have to rejected unfortunately. 
The paper addresses learning with noisy labels, by detecting and correcting samples with noisy labels. Reviewers had concerns about the empirical evaluations, specifically about comparing to additional methods, about hyperparameter tuning, and about the improvements being vey small. There was also a concern that the analysis of the objective does not take into account explicitly the L2 regularization induced by weight decay. Based on these concerns the paper is not ready yet for publication.   
The authors present a study characterizing adversarial examples in the audio domain. They highlight the importance of temporal dependency when defining defense against adversarial attacks.  Strengths   The work presents an interesting analysis of properties of audio adversarial examples, and contrasts it with those in vision literature.   Proposes a novel defense mechanism that is based on the idea of temporal dependency.  Weaknesses   The technique identifies adversarial examples but is not able to make the correct prediction.   The reviewers raised issue around clarity, but the authors took the effort to improve the section during the revision process.   The reviewers agree that the contribution is significant and useful for the community. There are still some concerns about clarity, which the authors should consider improving in the final version. Overall, the paper received positive reviews and therefore, is recommended to be accepted to the conference.
The consensus amongst the reviewers is that the paper discusses an interesting idea and shows significant promise, but that the presentation of the initial submission was not of a publishable standard. While some of the issues were clarified during discussion, the reviewers agree that the paper lacks polish and is therefore not ready. While I think Reviewer #3 is overly strict in sticking to a 1, as it is the nature of ICLR to allow papers to be improved through the discussion, in the absence of any of the reviewers being ready to champion the paper, I cannot recommend acceptance. I however have no doubt that with further work on the presentation of what sounds like a potentially fascinating contribution to the field, the paper will stand a chance at acceptance at a future conference.
This manuscript proposes a strategy for fitting predictive models on data separated across nodes, with respect to both samples and features.  The reviewers and AC agree that the problem studied is timely and interesting, and were impressed by the size and scope of the evaluation dataset (particularly for a medical application). However, reviewers were unconvinced about the novelty and clarity of the conceptual and empirical results. On the conceptual end, the AC also suggests that the authors look into closely related work on split learning (https://splitlearning.github.io/) which has also been applied to medical data settings.
The paper explores an increasingly important questions, especially showing the attack on existing APIs. The update to the paper has also improved it, but the paper is still not yet as impactful as it could be and needs much more comprehensive analysis to correctly appreciate its benefits and role.
This paper presents a debiasing technique that modifies a model s attention mechanism by equalizing attention across social groups.  The authors show that their approach (which is perhaps the first of its kind to look at transformer based models and debiasing instead of fixed word representations) work well in debiasing across certain social group indicators while maintaining overall performance.  However, there is disagreement between reviewers in terms of acceptance of the paper (especially Reviewer 7L6Q wants the paper to be rejected and points to recent critiques such as https://aclanthology.org/2021.acl long.81.pdf that point out pitfalls with the benchmarks used in this paper).  I agree with said reviewer that a lot of these benchmarks are toy ish and finding real impact of bias in NLP models is quite elusive.  Hence, I am recommending the paper be rejected for ICLR 2022 and the suggestions below be incorporated towards a better draft for the future.
This paper proposes a deep reinforcement learning approach for solving minimax multiple TSP problem. Their main algorithmic contribution is to propose a specialized graph neural network to parameterize the policy and used a clipped idea to stabilize the training. Unfortunately, the reviewers remain to be unconvinced by the experiments after the rebuttal and the writing need to be significantly improved. Also, it would be worthwhile to study how the proposed method can generalize to other problems.  
The reviewers found it hard to understand the motivation of using both oblivious sketching and maintaining feasibility throughout the course of the algorithm, given that the ultimate running times matched those of existing work. Because there wasn t a concrete improvement over prior work, the worry is what the impact of the paper would ultimately be. There was also a concern with novelty, similarity to the work of Cohen, Lee, and Song, and a reliance on fast matrix multiplication exponents. The paper could also benefit from an improved presentation. 
Under the optimization formulation of adversarial attack, this paper proposes two methods to improve the transferability of adversarial examples, namely Nesterov Iterative Fast Gradient Sign Method (NI FGSM) and Scale Invariant attack Method (SIM). NI FGSM adapts Nesterov accelerated gradient into the iterative attacks to effectively look ahead and avoid the “missing” of the global maximum, and SIM optimizes the adversarial perturbations over the scale copies of the input images so as to avoid “overfitting” on the white box model being attacked and generate more transferable adversarial examples. Empirical results demonstrate the effectiveness of the proposed methods. The ideas are sensible, and the empirical studies were strengthened during rebuttal.
The reviewers generally agreed that the application and method are interesting and relevant, and the paper should be accepted.  I would encourage the authors to carefully go through the reviewers  suggestions and address them in the final.
meta score: 4  The paper has been extensively edited during the review process   the edits are so extensive that I think the paper requires a re review, which is not possible for ICLR 2018  Pros:    potentially interesting and novel approach to prefix encoding for character level CNN text classification    some experimental comparisons Cons:    lacks good comparison with the state of the art, which makes it difficult to determine conclusions    writing style lacks clarity.  I would recommend that the authors continue to improve the paper and submit it to a later conference. 
This paper examines learning problems where the network outputs are intended to be invariant to permutations of the network inputs.  Some past approaches for this problem setting have enforced permutation invariance by construction.  This paper takes a different approach, using a recurrent neural network that passes over the data. The paper proves the network will be permutation invariant when the internal state transition function is associative and commutative.  The paper then focuses on the commutative property by describing a regularization objective that pushes the recurrent network towards becoming commutative.  Experimental results with this regularizer show potentially better performance than DeepSet, another architecture that is designed for permutation invariance.  The subsequent discussion of the paper raised several concerns with the current version of the paper. The theoretical contributions for full permutation invariance follow quickly from the prior DeepSet results.  The paper s focus on commutative regularization in the absence of associative regularization is not compelling if the objective is really for permutation invariance.  The experimental results were limited in scope.  These results lacked error bars and an examination of the relevance of associativity. The reviewers also identified several related lines of work which could provide additional context for the results that were missing from the paper.  This paper is not ready for publication due to the multiple concerns raised by the reviewers.  The paper would become stronger by addressing these concerns, particularly the associativity of the transition function, empirical results, and related work. 
This submission proposes a graph sparsification mechanism that can be used when training GNNs.  Strengths:  The paper is easy to follow.  The proposed method is sound and effective.  Weaknesses:  The novelty is limited.  Given the limited novelty and the number of strong submissions to ICLR, this submission, while promising, does not meet the bar for acceptance.
Pros: + Clearly written paper. + Easily implemented algorithm that appears to have excellent scaling properties and can even improve on validation error in some cases. + Thorough evaluation against the state of the art.  Cons:   No theoretical guarantees for the algorithm.  This paper belongs in ICLR if there is enough space. 
This paper proposes a method to learn sentence representations that incorporates linguistic knowledge in the form of dependency trees using contrastive learning. Experiments on SentEval and probing tasks show that the proposed method underperform baseline methods.  All reviewers agree that the results are not strong enough to support the claim of the paper and have some concerns about the scalability of the implementation. They also agree that the writing of the paper can be improved (details included in their reviews below).   The authors acknowledged these concerns and mentioned that they will use them to improve the paper for future work, so I recommend rejecting this paper for ICLR.
All reviewers recommend rejection, and the authors have not provided a response. 
The paper proposes a new method for out of distribution detection by combining random network distillation (RND) and blurring (via SVD). The proposed idea is very simple but achieves strong empirical performance, outperforming baseline methods in several OOD detection benchmarks. There were many detailed questions raised by the reviewers but they got mostly resolved, and all reviewers recommend acceptance, and this AC agrees that it is an interesting and effective method worth presenting at ICLR. 
The reviewers generally agreed that the paper presents a compelling method that addresses an important problem. This paper should clearly be accepted, and I would suggest for it to be considered for an oral presentation.  I would encourage the authors to take into account the reviewers  suggestions (many of which were already addressed in the rebuttal period) and my own suggestion.  The main suggestion I would have in regard to improving the paper is to position it a bit more carefully in regard to prior work on Bayesian meta learning. This is an active research field, with quite a number of papers. There are two that are especially close to the VI method that the authors are proposing: Gordon et al. and Finn et al. (2018). For example, the graphical model in Figure 2 looks nearly identical to the ones presented in these two prior papers, as does the variational inference procedure. There is nothing wrong with that, but it would be appropriate for the authors to discuss this prior work a bit more diligently   currently the relationship to these prior works is not at all apparent from their discussion in the related work section. A more appropriate way to present this would be to begin Section 3.2 by stating that this framework follows prior work   there is nothing wrong with building on prior work, and the significant and important contribution of this paper is no way diminished by being up front about which parts are inspired by previous papers.
The paper proposes a two stage approach for anomaly detection   first train a low dimensional embedding potentially using self supervised learning methods, and then train a discriminator on top of the embedding that takes in pairs of examples and outputs a score which can be used for anomaly detection. A test example is paired with the next nearest neighbor. A common concern of the reviewers was on the claim of the paper to be a general approach for anomaly detection whereas experiments are reported only on vision datatsets. The authors have addressed this by making changes to the title and to the claims made in the paper. However R1 and R2 still have concerns about insufficient empirical evaluations, in particular lack of non vision datasets.   As the paper aims to tackle the problem where OOD examples are spread through the sphere, appearing mixed with normal examples, I think fitting a nonparametric density model (eg, using KDE) or parametric density model (eg, a mixture model) on the embeddings is a natural baseline to compare with.   I encourage the authors to strengthen the empirical section of the paper based on reviewers  comments and resubmit to a future venue. 
The paper presents a self supervised model based on a contrastive autoencoder that can make use of a small training set for upstream multi label/class tasks. Reviewers have several concerns, including the lack of comparisons and justification for the setting, as well as the potentially narrow setting. Overall, I found the paper to be borderline, the cons slightly greater than the pros, so I recommend to reject it.
 The paper proposed a new way for training models that stack the same basic block for multiple times   share the weights first and then untie the weights. Ablation study shows that the proposed algorithm has marginal improvement over the baseline. The authors also provide some theoretical justifications to how the proposed idea works. The proposed idea is straightforward and intuitive. Weight sharing has been used in previous works, and what’s new in this paper is to unshare the weights in the middle (with a heuristic rule). The hope is that by doing so, one can achieve a better tradeoff between speedup and accuracy. However the experimental supports are somehow weak and incomplete. For example, in order to show the real speedup, one should provide the full training curve (until convergence) under different settings, instead of just showing one data point (at 500K). It is very common that one can get some speedup at 500K, but the speedup totally disappears after another 500K steps.  Furthermore, the theoretical analysis is conducted in a simplified setting, and it is not very clear whether it can be used to explain what really happened during BERT training. The reviewers conducted some lengthy discussions after the author rebuttal was available. As a final consensus, we think that there are still concerns on the paper, which makes us hesitate to give an ACCEPT recommendation. 
This paper has been reviewed by four experts. Their independent evaluations were consistent, all recommended rejection. I agree with that assessment as this paper is not ready for publication at ICLR in its current form. The reviewers have provided the authors with ample constructive feedback and the authors have been encouraged to consider this feedback if they choose to continue the work on this topic.
This paper presents a method to handle class imbalance in federated learning, while accounting for data heterogeneity and privacy. The key idea is to solve a constrained optimization problem where the difference between the global and local objective values has to be less than some parameter $\epsilon$. The paper proposes a primal dual optimization algorithm called CLIMB to solve this constrained FL problem. The paper presents a theoretical analysis of the algorithm, as well as experimental results.   All the reviewers found the formulation interesting and novel and gave a positive assessment of the paper. Reviewer obo5 had some concerns about whether the optimization problem is improving fairness and getting reduced class imbalance as a side benefit or whether it is directly addressing class imbalance. After discussion with the authors, their concerns were partially addressed. Reviewer 8nbc had concerns about the assumptions and theoretical analysis. Their concerns were also mostly addressed by the authors during the discussion phase. I suggest the authors to also address Reviewer u6Lr and Reviewer Mp3G s concerns about experimental results and citing related work respectively when they revise the paper.  Overall, I recommend acceptance of the paper, and strongly encourage the authors to take the reviewers  suggestions about 1) fairness connections, 2) privacy connections, 3) theoretical analysis, 4) experimental results, and 5) prior work into account when revising it.
This paper performs visual odometry using variational information bottleneck. It assumes video and pose observations and aims to find a latent state that is maximally predictive of the pose observations, while minimizing the mutual information between the image observations and the latent state. It approximates this cost using variational inference. The paper also makes use of deterministic+stochastic latent transition models, as in Hafner et al 2019, 2020. The paper contributes generalization bounds that are based on recent work by Xu and Raginsky 2017 and Zhang et al 2018. This is useful to include, but this contribution is not the main focus of the work in my opinion, and it is not clear how tight the bounds are, especially considering that the original cost function is approximated.    Pros:  The idea of using the deterministic+stochastic transition models of Hafner et al and related works for visual odometry is very interesting and promising avenue for research.   The fact that the experiments are done on some of the major camera and IMU datasets is great.    Needs fixing:  Major: The paper mentions "Extensive ablation studies were conducted to examine the effects of (1) the deterministic component, (2) sample size and (3) extra sensors.". These are great, but I would also have expected to see a range of variations in terms of the weight gamma, including a value of zero. The appendix is vague in this regard and says " and perform a non intensive and small range grid searching." The utility of the information bottleneck idea depends heavily on the weight gamma, and I would have liked more results confirming that the method does well under a range of choices for gamma.  Major: The rotation results produced by this method on EuroC should be improved to be more competitive with okvis. As it stands, it is unclear why the method does not perform as well and the explanations offered in the paper are speculative.     Medium: The paper also mentions in the appendix: "Though 3D von Mises Fisher distribution and 4D Bingham distribution can be arguably more appropriate to model Euler angles and quaternions respectively, it is non trivial to evaluate and use them for training in practice." So, the paper represents rotations using Euler angles. The authors are encouraged to look at https://www.gilitschenski.org/igor/publication/202004 iclr deep_orientation_uncertainty_learning/  Minor: MSCKF was originally coined by Mourikis and Roumeliotis https://ieeexplore.ieee.org/document/4209642 and even though the term is used by follow up works,  it is worth adding the reference.  Finally, I would disagree with one of the reviewers that this paper needs to compare with ORBSLAM2. I think comparing with OKVIS and an MSCKF variant is sufficient for "classic" SLAM and odometry methods.    I think the paper needs one more iteration to fix these issues, even though it is very promising work.      
The paper addresses an interesting problem of clustering/link prediction/representation learning of signed graphs, where edge weights are allowed to take either positive or negative values. The paper proposed an end to end pipeline targeted at link sign prediction and the feature diffusion step. The reviewers think the proposed method is a straightforward integration of existing methods, and the convergence result is straightforward. The paper can be improved by including more novel ideas or analysis. 
Firstly, thank you authors for your thought provoking submission and discussion.  The key point of disagreement clearly is the fundamental assumption that "the result of an anomaly detection method should be invariant to any continuous invertible reparametrization f."  All reviewers found this assumption to be too strong, leading all four to recommend rejection.  I also recommend rejection at this time.  To me, it seems reasonable and practical to assume that anomalies are defined based on distance (in a fixed feature space).  So if we are allowed to deform the space, clearly this definition breaks down and the concept of an anomaly becomes empty.  Perhaps I am wrong about this, but nevertheless, the paper could do a much better job of convincing the reader that its fundamental reparametrization assumption is appropriate and of consequence in practice.
This paper presents an approach "ImpressLearn" to continual learning using the idea of task specific masks. The idea builds upon another idea   SupSup (Wortzman 2020)   which uses a backbone network shared by all the tasks and binary task specific masks. However, the number of parameters for an approach like SupSup can become excessively large when the number of tasks is very large. This paper presents a solution by having a small number of basis masks and learning a weighted combination of these basis masks to use as the task specific mask for each task. The experimental results show that ImpressLearn yields significant parameter savings as compared to SupSup.  There were several concerns shared by all the reviewers, such as (1) Limited novelty as compared to SupSup, and (2) Limited experimental evaluation and not having enough baselines. From my own reading of the paper, I largely agree with the assessment of the other reviewers.  The authors responded to the original reviews and acknowledged some of the concerns raised by the reviewers. The reviewers read the authors  response but their assessment has remained unchanged.  The basic motivation and the idea is nice but offers limited novelty (especially as compared to SupSup). If the authors could improve the experimental evaluation (more baselines, larger datasets/networks, etc), it will be a much stronger paper. However, in its current shape, I as well as the other reviewers do not think that the paper is ready for publication.
There are some interesting ideas in this paper, but I agree with reviewers that without a comparison to existing work, it is hard to place this work in its proper context. The authors make several arguments in dismissing the need for side by side comparisons, but I do not find these arguments convincing.  * First, the authors argue that there are no suitable benchmarks for them to compare, and that in particular SyGuS benchmarks would not be suitable because they are dealing with a different problem. I disagree. There are 2 tracks in SyGuS specifically for programming by example problems, one for string manipulations and one for bit vector programs. I think the string manipulation problems would be a good match for this technique.  * The authors also argue that their technique is so much more general than prior techniques that a side by side comparison would be unfair. However, their most complex benchmark, sorting, has been somewhat of a standard benchmark in the program synthesis community for about a decade now. And while a lot of recent synthesis work has focused on domain specific languages, many systems starting with Sketch and continuing with Myrth and Synquid were turning complete. Turing completeness can make a big difference if you are trying to synthesize verified code, but in the context of programming by example, turning completeness does not really present any fundamental challenges.  I am willing to believe that this technique is more scalable than existing techniques, so that while existing techniques may do better than this technique when synthesizing for small languages, this technique would surpass them when applied to a bigger language. But if that s the argument that the authors want to make I would like to see some evidence, and ideally some quantitative data as to how big a language would have to be before this technique wins out.
The reviewers agree that this is an interesting paper but it required major modifications. After rebuttal, thee paper is much improved but unfortunately not above the bar yet. We encourage the authors to iterate on this work again.
This paper proposes a simple yet effective approach for determining weight quantization bit lengths using RL. All the reviewers agree that the simplicity and performance improvements are a strong plus point. There are some concerns on applicability which have been sufficiently handled by rebuttal. AC recommends accepting the paper.
This paper interprets adaptive gradient methods as trust region methods, and then extends the trust regions to axis aligned ellipsoids determined by the approximate curvature. It s fairly natural to try to extend the algorithms in this way, but the paper doesn t show much evidence that this is actually effective. (The experiments show an improvement only in terms of iterations, which doesn t account for the computational cost or the increased batch size; there doesn t seem to be an improvement in terms of epochs.) I suspect the second order version might also lose some of the online convex optimization guarantees of the original methods, raising the question of whether the trust region interpretation really captures the benefits of the original methods. The reviewers recommend rejection (even after discussion) because they are unsatisfied with the experiments; I agree with their assessment. 
The authors propose an approach for visual navigation that leverages a semantic knowledge graph to ground and inform the policy of an RL agent. The agent uses a graphnet to learn relationships and support the navigation. The empirical protocol is sound and uses best practices, and the authors have added additional experiments during the revision period, in response to the reviewers  requests. However, there were some significant problems with the submission   there were no comparisons to other semantic navigation methods, the approach is somewhat convoluted and will not survive the test of time, and the authors did not conclusively show the value of their approach. The reviewers uniformly support the publication of this paper, but with a low confidence. 
This is certainly a boarderline paper. The reviewers agreed this paper provides a good explanation and empirical justification of why popular normalization schemes don t help in DRL. The paper then proposes a simple scheme and demonstrates how it improves learning in several domains. The main concerns are the nature of these gains and how broadly useful the new approach is. In many cases there appear to be somewhat clear wins in the middle of the learning curves, but by the end of each experiment the errorbars overlap. The most clear results are those with TD3. There are some oddities here: using half SD error bars and smoothing both underline the concern about significance.   The reviewers requested more experiments and the authors provided three more domains: two in which their method appears better. These are not widely used benchmarks and it was hard to compare the performance of the baselines with fan et al (different setup) to evaluate the claims. The paper nicely provides lots of insight and empirical wisdom in the appendix, explaining how they got the algorithms to perform well.    
The authors propose to use large batch training of neural networks, where each batch contains multiple augmentations of each sample. The experiments demonstrate that this leads to better performance compared to training with small batches. However, as noted by Reviewers 2 and 3, the experiments do not convincingly show where the improvement comes from. Considering that the described technique is very simplistic, having an extensive ablation study and comparison to the strong baselines is essential. The rebuttal didn’t address the reviewers  concerns, and they argue for rejection.
 This paper presents "Automunge" a python library for pre processing tabular data.  The authors develop a useful library that can be used by practicioners for data engineering in NNs applications.  The reviewers raised a common concern regarding the lack of focus on the actual usefulness of the librabry in improving the  performance of the models that is applied on. A common concern was the lack of performance plots compared to other alternatives.  In the response the authors have done a rather thorough job of addressing the reviewers comments and adding material in the supplementary. However, given the current presentation, the manuscript needs a considerable amount of  rewriting to incorporate the suggested changes into the main paper. As it is, I don t think ICLR is the right venue for the manuscript.  It might reach its audience better in venues like SysMl or PyCon also suggested by a reviewer.  
The paper provides a useful analysis of the role of gradient penalties and the performance of the proposed approach in semi supervised cases.
As pointed out by reviewers, the presentation needs to be improved to clarify the algorithmic and theoretical contributions.
The paper studies the problem of learning fiber distributions associated with a machine learning task, in which the goal is to predict Y, given X. One chooses a fiber space / distribution Z / D_Z, and learns a trivialization \varphi : (Y,Z)  > X. The proposed architecture first clusters the label space Y. Within each cluster i it fixes a fiber space and distribution, and then learns a mapping \Phi_i, parameterized by an invertible neural network, by minimizing the discrepancy between the generated distribution and the distribution of the training data. The paper performs experiments on the wine dataset and a dataset coming from an aerospace application, as well as synthetic data with fiber bundle structure. Since the task here is generative modeling, the paper compares to standard GAN architectures (WGAN and conditional GAN) and argues that they are not fiber learners, in the sense of this paper.   Initial reviews were split, with reviewers appreciating the novelty of the fiber learning task, while also raising questions about the paper’s relationship to conditional GANs, some points of clarity, and limitations of the experiments. After interaction in the response period, the reviewers converged to a decision to accept. The paper’s primary strength is its clear formulation — the paper provides useful language for describing conditional generative models (in particular, for discussing when a factorization of the distribution over Y and Z is appropriate), a valuable contribution to the discussion in this area.
The paper is about an approach that combines successor representation with marginalized importance sampling. Although the reviewers acknowledge that the paper has some merits (interesting idea, good discussion, extensive experimental analysis) and the authors  responses have solved most of the reviewers  issues, the paper is borderline and the reviewers did not reach a consensus about its acceptance. In particular, the reviewers feel that the contributions of this paper are not significant enough. I encourage the authors to modify their paper by taking into consideration the suggestions provided by the reviewers and try to submit it to one of the forthcoming machine learning conferences.
This paper attempts a theoretical treatment of the influence of depth in RNNs on their ability to capture dependencies in the data. All reviewers found the theoretical contribution of the paper interesting, and while there were problems raised regarding formalisation, they appear to have been adequately addressed in the revisions to the paper. The main concern in all three reviews surrounds the evaluation, and weakness thereof. The overarching point of contention seems to be that the theory relates to a particular formulation of RNNs (RAC), causing doubts that the results lift to other architectural variants which are used to obtain state of the art results on tasks such as language modelling. It seems that the paper could be significantly improved by the provision of stronger empirical results to support the theory, or a more convincing argument as to why the results should transfer from, say, RAC to LSTMs. The authors point to two papers on the matter in their response, but it is not clear this is a substitute for experimental validation. I find the paper a bit borderline because of this, and recommend redirection to the workshop.
The paper presents the combination of a model based (probabilistic program representing the physics) and model free (CNN trained with DQN) to play Flappy Bird.  The approach is interesting, but the paper is hard to follow at times, and the solution seems too specific to the Flappy Bird game. This feels more like a tech report on what was done to get this score on Flappy Bird, than a scientific paper with good comparisons on this environment (in terms of models, algorithms, approaches), and/or other environments to evaluate the method. We encourage the authors to do this additional work.
This paper proposes a model for predicting edits to trees given an edit specification that comes either from the ground truth before after state (“gold” setting, like reconstruction error of auto encoder) or from the before after state of an analogous edit. The problem setting follows mostly from Yin et al (2019).   There are several shortcomings of this paper:  1. The technical novelty of the model is somewhat limited, as it’s an assembly of components that have been used in related work. Authors insist in the discussion on the novelty of the tree edit encoder (Sec 3.2), but I think this is overstated. The related tree edit models (e.g., Tarlow et al (2019)) perform a very similar encoding *in the decoder* when training with teacher forcing. While it’s true that decoders are typically thought of as monolithic entities that generate a sequence of edits from a state, inside the teacher forced training, the models are computing a representation of a prefix of ground truth edits, which are then repeatedly used to predict next edits. AFAIU, the proposal is basically to use this hidden representation as the edit encoder.   2. The claim that the approach is more language agnostic than Dinella et al (2020) also seems shaky, as the authors admit in their response that language specific grammars need to be handled specially. E.g., I expect that the authors of Dinella et al would find it easier to extend their existing code to use a new language than to adapt this approach.  3. The submission relies too heavily on the “gold” setting (where the target output is fed as an input), and I’m skeptical of their characterization of Yin et al’s intentions when the authors say in comments, “Because of this, models that are able to reproduce the desired output effectively have a demonstrably better inductive bias that allows them to do so efficiently. This was the original motivation expressed by Yin et al. (2019).”  I don’t see this stated in the Yin et al paper. I see Yin et al. characterizing this setting as an upper bound and saying “better performance with the gold standard edit does not necessarily imply better (more generalizable) edit representation.” (Yin et al., 2019). It’s worrying that the proposed model only seems to do better in this setting, which would be very easy to game if one were aiming to directly optimize for it.   Having said this, (1) is not a standard way to think about encoding edits, (2) is debatable, and we can hope that future work does not treat improvements in the “gold” setting as a valid research goal. Further, there is another contribution around imitation learning that the reviewers appreciate. In total, reviewers did an excellent job and generally believe the paper should be accepted. I won’t go against that recommendation.
This paper studies the convergence of Adam type algorithms (two variants of AMSGrad in particular) in min max problems that satisfy a one sided "Minty variational inequality" condition.  The reviewers identified several weaknesses in the paper and the authors did not provide a rebuttal to these concerns so there was consensus to reject the paper.
This paper presents a way to generate adversarial examples for text classification.   The method is simple   finding semantically similar words and replacing them in sentences with high language model score.  The committee identifies weaknesses in this paper that resonate with the reviews below   reviewer 1 suggests that the authors should closely compare with the work of Papernot et al, and the response to that suggestion is not satisfactory.  Addressing such concerns would make the paper stronger for a future venue.
This paper is overall well written and clearly presented. The problem of ordered data clustering is relevant, and the proposed method is effective.  During the discussion, all reviewers agree with the strength of this paper and share the positive impression. The authors successfully addressed reviewers  concerns by the careful author response, which I also acknowledge. One of the reviewers raised the concern about the broader impacts, while it is also well addressed in the author response.  I therefore recommend acceptance of the paper.
The paper proposes a generative model that generates one object at a time, and uses a relational network to encode cross object relationships. Similar  object centric generation and object object relational network  is proposed in "sequential attend, infer, repeat" of Kosiorek et al. for video generation, which first appeared on arxiv on June 5th 2018 and was officially accepted in NIPS 2018 before the submission deadline for ICLR 2019. Moreover, several recent generative models have been proposed that consider object centric biases,  which the current paper references  but does not compare against, e.g.,  attend, infer, repeat  of Eslami et al., or "DRAW: A Recurrent Neural Network For Image Generation" of Gregor et al. . The CLEVR dataset considered, though it contains real images, the intrinsic image complexity is low because it features a small number of objects against table background. As a result, the novelty of the proposed work may not be sufficient in light of recent literature, despite the fact that the paper presents a reasonable and interesting approach for image generation.  
Most of the discussion centered around whether the underlying question in the literature is setup correctly in terms of its relationship to causality as the question being asked is one of an intervention. The underlying literature makes an attempt at not including things that can t be intervened on like age, but the setup of a "counterfactual" could benefit from a causal take.  Holding that aside, the paper makes progress on an established question though analysis that reveals that the Lipschitz continuity and confidence are important for causality and Stable Neighbor Search for generating counterfactuals.  The most negative reviewer in discussion writes that they re okay with the paper being accepted if the rest of the reviewers are positive. The rest of the reviewers are positive with one mentioning that the paper is well written and interesting in the discussion and that the author replies cleared up the issues about counterfactuals.
In this paper, the authors proposed a new variant of the Wasserstein autoencoder (WAE), which matches the joint distribution of data and the latent codes induced by the encoder and the joint distribution induced by the decoder in the framework of optimal transport. Because of matching the distributions that are not considered by existing autoencoders like WAEs or VAEs, I agree with the authors that the proposed method is novel to some degrees.   However, the experimental part does not support the superiority of the method well. For example, some reviewers (including me) think the results of the baselines shown in Figures 8 10 are underestimated. According to my personal experience, the WAE should perform much better on CelebA than that shown in Figure 10. The experiments in Figures 11 17 provide more reasonable results, but the advantage of the proposed method is not convincing.  Here is my suggestion: 1) Because the proposed method can achieve flexible prior, besides randomly generating data, the authors can consider adding some experiments on conditional generation, i.e., generating data from a single modality of the learned prior. I believe the proposed method will be more convincing if it can show some advantages in the conditional generation task. 2) The runtime comparison for the method and the baselines in the training phase should be discussed.  3) The short name "SWAE" is in conflict with an earlier work "Sliced Wasserstein Autoencoder", which is also called "SWAE".
The authors propose the resource constrained offline RL problem where the offline dataset contains extra features that are not available online. The goal is to use these extra features to improve performance during deployment. They propose a simple modification to TD3 BC in the continuous control setting and a simple modification to CQL in the discrete setting. They evaluate their proposed approaches on D4RL, RC D4RL (a novel dataset that they introduce for resource constrained offline RL), Atari, and a proprietary real life Ads problem.  Initial reviews identified the following concerns: * While the exact problem is novel, the idea of having access to privileged features at training time that are not available at deployment has been explored in supervised learning and online RL. The reviewers were not clear how considering the offline RL setting interacts specifically with the privileged features to produce an interesting setting. * The baseline simply trains on the limited feature set. Unsurprisingly, using the extra features can improve performance. In light of the previous point, reviewers asked for more substantial baselines, suggesting BC on the teacher and predicting the missing features as some possibilities. * The set of tasks was too limited.  The authors provided a substantial response: * Experiments on Ads data  * Experiments on Atari with CQL as the base algorithm * Additional baselines on RC D4RL HalfCheetah v2 datasets (BC on teacher and predictive) * Additional analysis  I commend the authors on the hard work they did preparing this response. It is quite substantial and does improve the paper significantly. However, reviewers and I still have a number of concerns: * The additional baselines are appreciated, however, the results are mixed. The additional baselines are a step in the right direction, but they need to be evaluated beyond a single dataset. It is hard to evaluate the results without reasonable baselines. I agree and think that even though the specific problem is novel, the idea of transfer learning is not, so it is reasonable to require that we have more extensive baselines. Furthermore, while the authors argue that their method has an edge on the more practical dataset, that is based on a very limited evaluation. Probing this further is important. * The CQL modification is quite different than the TD3+BC modification. The performance of the modification for CQL is not significantly better than CQL. What should we make of this? * For the Ads dataset, all hyperparameter settings except Transfer(0, 1) show the same performance. This seems surprising as even Transfer(0.1, 0.9) shows no difference. Finally, Transfer(0, 1) beating Transfer(1, 0) 7/10 times is not statistically significant.  At this time, the paper is not ready for publication, but the paper is moving in the right direction and I encourage the authors to submit a revised version to a future venue.
The paper proposes to extend the autoencoder loss in a deep generative model to include per latent layer loss terms.  Two variants are proposed: SAP (simple aggregation along pathway) and NAP (normalized aggregation along pathway). SAP is simply the sum of the squared norm, while NAP performs decorrelation and normalization of the magnitude.  This was viewed as novel by the reviewers, and the experiments supported the proposed approach.  In the post rebuttal phase, the inclusion of an ablation study has led to an upgrade in the reviewer recommendation.  As a result, there was a unanimous opinion that the paper is suitable for publication at ICLR.
The paper presents a prompt learning method for few shot learning in NLP.  In particular, they proposed DART, a new soft prompt tuning method, to optimize the label representations and template.   Overall, the paper is well written and well motivated. The proposed approach is interesting. The experiments were well justified and sufficient experimental analyses are provided. All reviewers support the paper.   There are a few remaining critics of the papers.     The major one is the positioning of the paper raised by the reviewers. I agree with the reviewers that it is a bit misleading to emphasize the approach requires no external architecture. Although the approach can reuse the same transformer architecture (rather than additional LSTM) so that it enjoys the beauty of simplicity, it is still required additional parameters. I would suggest better clarifying this point in the final version.     There is also a critic that the paper is related to ADAPET. However, the key ideas in this paper are sufficiently different from ADAPET. Also, ADAPET is published at EMNLP 21 after the paper is submitted, although it was in Arxiv earlier. It s fair to say this work is concurrent with ADAPET.
This paper introduces a way to measure dataset similarities. Reviewers all agree that this method is novel and interesting. A few questions initially raised by reviewers regarding models with and without likelihood, geometric exposition, and guarantees around GW, are promptly answered by authors, which raised the score to all weak accept.  
This paper proposes integrating three existing approaches to give a simple algorithm called TAIG for generating transferable adversarial examples under blackbox attacks.  In the original reviews, some strengths and weaknesses of the papers were highlighted although some of them have not reached general agreement after the discussion period.  Regarding the merits, it is generally felt that the experimental results are good and the idea of updating along the integrated gradients is new (despite a simple idea) and has some theoretical justification.  Nevertheless, even after the discussion period, some concerns still remain, including the technical novelty of the proposed method and the high computational requirements of the proposed method, among others.  We appreciate the authors for responding to the reviews by clarifying some points and providing further experimental results. The paper would be more ready for publication if all the comments and suggestions are taken into consideration to improve the paper more thoroughly.
The paper proposes a design of interpretable neural networks where each neuron is hand designed to serve a task specific role, and the network weights can be optimized via a few interactions with the environment. The reviewers acknowledged that the interpretability of neural networks is an important research direction. However, the reviewers pointed out several weaknesses in the paper, and there was a clear consensus that the work is not ready for publication. The reviewers have provided detailed and constructive feedback to the authors. We hope that the authors can incorporate this feedback when preparing future revisions of the paper.
This paper is on the theme of active reinforcement learning with a human/assistant in the loop. Under partial observability, an agent acts as per an interaction policy that gathers state/goal information from the assistant, while an operational policy assumed pre learnt in this paper executes low level actions.  The reviews acknowledge the relevance of this topic and that the paper is well structured and coherently presented overall. However, there are unanimous concerns around experimental evaluation being unconvincing, lack of strong baselines and lack of thorough coverage of related work precluding an accurate assessment of claimed contributions. As such, the paper is not in a form that can be accepted at ICLR    the authors are encouraged to revise their submission as per review feedback.
The paper proposes a new game theoretic model, Bayesian Stackelberg Markov Game (BSMG), for designing defense strategies while accounting for the defender s uncertainty over attackers  types. The paper also proposes a learning approach, Bayesian Strong Stackelberg Q learning (BSS Q), to learn the optimal policy for BSMGs. It is shown that BSS Q converges to an equilibrium asymptotically. Experimental results are provided to demonstrate the effectiveness of BSS Q in the context of web application security. Overall this is an interesting approach and an important direction of research. However, the reviewers raised several concerns, and there was a clear consensus that the paper is not yet ready for publication. The specific reasons for rejection include the following: (i) the experimental results are not presented with sufficient clarity, no statistical significance tests are performed, and the choice of baselines is weak; (ii) the contributions are not sufficiently broad, the learning process described in the paper is unclear, and the framework requires a strong assumption of knowing the attackers  distributions. I want to thank the authors for actively engaging with the reviewers during the discussion phase. The reviewers have provided detailed feedback in their reviews, and we hope that the authors can incorporate this feedback when preparing future revisions of the paper. 
This paper studies the design and analysis of contextual bandits algorithms, combining the ideas of neural network models (Zhou et al, 2020 and Zhang et al, 2020) and reward perturbations (Kveton et al, 2019, 2020); this has the computational advantage of avoiding inverting large covariance matrices, as is done in the other neural contextual bandits algorithms. Although the reviewers think that the papers need to do a better job in highlighting differences and extra challenges in the current work compared to prior works, they also acknowledge that this paper is the first that combines the above two ideas.   The reviewers also acknowledge that the additional experiments in the rebuttal period help clear the concern the reviewers have about why all regret curves look linear. However they also pointed out, that comparison with the FALCON+ algorithm (Foster et al, 2020) may be slightly unfair, as the algorithm retrains the neural network after every new iteration. Overall, the reviewers think that the pros outweight the cons, and they lean towards acceptance.
The paper addresses the task of continual learning in NLP for seq2seq style tasks. The key idea of the proposed method is to enable the network to represent syntactic and semantic knowledge separately, which allows the neural network to leverage compositionality for knowledge transfer and also solves the problem of catastrophic forgetting. The paper has been improved substantially after the reviewers  comments and also obtains good results on benchmark tasks. The only concern is that the evaluation is on artificial datasets. In future, the authors should try to include more evaluation on real datasets (however, this is also limited by availability of such datasets). As of now, I m recommending an Acceptance.
The submission presents an approach to single view 3D reconstruction. The approach is quite creative and involves predicting the weights of a network that is then applied to a point set. The presentation is good. The experimental protocol is well informed and the results are convincing. The reviewers  concerns have largely been addressed by the authors  responses and the revision. In particular, R2, who gave a "3", posted "I would now advise to raise my score (3 previously) to a be in line with the 6: Weak Accept given by the other reviewers." This means that all three reviewers recommend accepting the paper. The AC agrees.
This paper proposes an approach to unifying both full context and streaming ASR in a single end to end model.   Techniques such as weight sharing, joint training and teacher student knowledge distillation are used to improve the training.  The so called dual mode ASR is evaluated under the ContextNet and Conformer networks on Librispeech and MultiDomain datasets. The performance is good.  While the technical novelty is not overwhelmingly significant, all reviewers agree that it may have impact to the speech machine learning community as high performance streaming ASR is of great importance in real world deployment of ASR systems.  The authors have meticulously addressed the reviewers  comments and, in particular, changed the title from "universal ASR" to "dual mode ASR" as suggested by some of the reviewers.  After the rebuttal, all reviewers are supportive on accepting the paper.  
The original paper was sloppy in its use of mathematical constructs such as manifolds, made assumptions that are poorly motivated (see review #2 for details), and presented an empirical evaluation is preliminary. Based on the reviews, the authors have substantially revised the paper to try and address those issues by adding new theory, etc.  Unfortunately, it is difficult to assess whether these revisions are sufficient to address the aforementioned issues without going through a second round of "full" review. I encourage the authors to use the reviewer comments to further improve the paper, and re submit to a different venue.
This paper casts entity linking in a retrieve then read framework by first retrieving entity candidates and then finding their mentions via reading comprehension. All reviewers agree that the proposed approach is novel, well motivated, and simple yet performant. The authors have done a good job of addressing all the concerns raised, and the reviewers are unanimous in their recommendation for accepting the paper. I hope the authors will also incorporate the feedback and their responses in the final version.
The paper proposes to apply graph neural networks to predict battery state of charge. The main concern is the lack of technical novelty, since the main work is a straightforward application of existing works. The work could be better suited for a more application oriented venue.
All Reviewers and myself agree that the paper presents several major issues that require important rethinking of the research done, as well as a full rewriting of the manuscript. Hence, my recommendation is to REJECT the paper. As a brief summary, I highlight below some pros and cons that arose during the review and meta review processes.  Pros:   Code is available and clarifies parts of the approach.   Use of publicly available data sets.   Samples are provided.   Interesting problem.  Cons:   There are several problems with language and writing. Sometimes there is also incorrect terminology.   There are several problems with the description of the approach, which makes it opaque and hard to understand.   Proposed model not addressing basic limitations in the existing literature.   Insufficient evaluation.   No clear indication of successful results.   Potential lack of broad impact/interest to the ICLR community.   Unclear contribution.   Unconvincing samples.
The paper s contribution lies in using cross lingual sharing of subword representations for improving document classification.  The paper presents interesting models and results.  While the paper is good (two out of three reviewers are happy about it), I do agree with the reviewer who suggests the experimentation with relatively dissimilar languages and showing whether or not the approach works for those cases.  I am also not very happy with the author response to the reviewer.  Moreover, I think the paper could improve further if the authors presented experiments on more tasks apart from document classification.
This paper proposes to address the issue of biases and artifacts in benchmark datasets through the use of adversarial filtering. That is, removing training and test examples that a baseline model or ensemble gets wright.   The paper is borderline, and could have flipped to an accept if the target acceptance rate for the conference were a bit higher. All three reviewers ultimately voted weakly in favor of it, especially after the addition of the new out of domain generalization results. However, reviewers found it confusing in places, and R2 wasn t fully convinced that this should be applied in the settings the authors suggest. This paper raises some interesting and controversial points, but after some private discussion, there wasn t a clear consensus that publishing it as is would do more good than harm.
Strengths: Strong results on future frame video prediction using a 3D convolutional network. Use of future video prediction to jointly learn auxiliary tasks shown to to increase performance. Good ablation study.  Weaknesses: Comparisons with older action recognition methods. Some concerns about novelty, the main contribution is the E3D LSTM architecture, which R1 characterized as an LSTM with an extra gate and attention mechanism.   Contention: Authors point to novelty in 3D convolutions inside the RNN.  Consensus: All reviewers give a final score of 7  well done experiments helped address concerns around novelty. Easy to recommend acceptance given the agreement. 
The paper gives an elegant and efficient closed form solution for steering directions in the latent space of a pretrained GAN to to produce transformations in the image domain such as scaling and rotation etc,  this also extended to attribute transfer. The new method leads to "speed up, analytical transformation end points, and better disentanglement"  w.r.t to competitive methods.  All reviewers agreed on the merits of this work, and the good qualitative and quantitative results . The rebuttal addressed reviewers questions and concerns regarding the structure of the paper and its coherence.  Accept
The reviewers had raised a number of concerns which were mostly addressed during the discussion phase thanks to the additional experiments/explanations that the authors provided. However, some of the reviewers are not yet convinced about the main claims of the paper. While the paper provides a number of interesting/important/novel observations, which are obtained using an extensive set of carefully designed experiments, there are still some ambiguities about the main claims that may need further investigation/ justification.   One general issue with the current version is lack of clarity. I recommend that the authors revise the writing of the paper and make an effort to better explain/justify the main claims, ideas, and setups in the paper. Perhaps it would help to add a section early in the paper and define/review the basic concepts/definitions.  I also encourage the authors to reason about their main claims, choice of loss function (and why it is the right choice), and their experimental setups.   As an example, the authors should further investigate the impact of their proposed regularization on semantic representations.  The authors interpret the (distribution of) class information as “semantic” content/features. Indeed, when considering data with a variety of average case perturbations,  one could argue that semantic features like brightness or snow are actually uniform over classes, which might means that class selectivity could not appropriately capture the effect of those features. To evaluate semantic robustness,   it seems necessary to find a way to isolate specific semantic features in the data, i.e. by changing from snow to rain in corresponding images, rather than looking at the performance on the same brightness subset for different levels of their regularization parameter alpha.  Hence there is a need for further investigation on the effectiveness of class selectivity.   Some of the reviewers have indicated that differences in the class selectivity curves in Figures 1,2 appear marginal (e.g. at most 3% difference). Hence, additional experiments (with other data sets) could be beneficial in this regard.      
This submission proposes a method for providing visual explanations for why two images match by highlighting image regions that most contribute to similarity.  Reviewers agreed that the problem is interesting but were divided on the degree of novelty of the proposed approach.  AC shares R1’s concern that localization accuracy is not satisfactory as a quantitative measure of the quality of the explanations. In particular, it pre supposes what the explanations ought to be, i.e. that a good explanation means good localization. A small user study would be more convincing. A more convincing evaluation would also include a study of explanation of image pairs with different degrees of similarity (e.g. images that are dissimilar as well as images with the same object).  AC also shares R2’s concern about the validity of the model diagnosis application. This discussion also relies on the assumption that better localization of the whole object means a better explanation. Further, the highlighted regions in Figure 5 are very similar. Once again, a user study would help to indicate whether these results really do improve explainability.  Reviewers also had concerns about missing details and, while the authors did improve this, key details are still missing. For example, the localization method that was used was only referenced but should be described in the paper itself.  Given that several concerns remain, AC recommends rejection. 
 This paper considers the problem of training neural networks to be robust to label shifts. To do so, it proposes to use a distributionally robust optimization (DRO): instead of minimizing the expected error with respect to the empirical data distribution, the worse case expected error is minimized over a KL divergence "ball" of distributions centered at the empirical distribution with a given radius. The main contribution of the paper is an efficient algorithm for achieving this optimization, that avoids the need to project onto the uncertainty set or to sample in non standard ways from the training set. The paper provides evidence on the ImageNet data set and the ResNet 50 architecture that the proposed AdvShift algorithm outperforms reasonable baselines.  Reviewers raised concerns about the novelty of the algorithm, and claims the paper makes about the infeasibility of the sampling required by one of the competing baselines, and the need for the Lagrangian parameter used in the algorithm to be well tuned. The rebuttals addresses the concerns suitably; in particular, the novelty of the algorithm lies it it being the first DRO based solution for the label shift problem and its efficiency obtained by using KL uncertainty set and the Lagrangian formulation of the problem, which allows a closed form solution.   Due to the strong empirical and theoretical performance of the proposed AdvShift algorithm, it is recommended that this paper be accepted.
There is some positive consensus on this paper, which improved somewhat after the very detailed rebuttal comments by the authors. The use of limited amounts of OOD data is interesting and novel. There were some experimental design problems, but these were well addressed in rebuttal.  A reviewer points out that anomaly/outlier detection does not explicitly assume that there is only one class within the normal class (or in distribution data). The one class assumption is mainly made in some popular anomaly detection methods, such as one class classification based approaches for anomaly detection. The authors should take this into careful consideration when preparing a final version of this work. 
This paper introduces a model, named Crystal Diffusion Variational Autoencoder (CDVAE), that can learn to sample valid material structures. It accounts for known symmetries (SE(3), permutation) of the structure via SE(3) equivariant GNNs.  The proposed model is a complicated combination of many existing models / modeling techniques (VAEs, NCSNs, diffusion models) but it is not entirely ad hoc; the revised paper does a reasonable job in justifying the many different modeling choices made.  Existing model components, often designed in the context of molecule generation, do not account for the periodicity of the crystal s lattice structure; so this paper introduces modifications to account for this periodicity.  The paper evaluates the model on several datasets and also introduces new benchmarks that can be used for further research. The experimental results look promising but there are a few remaining clarity issues with the metrics used (cf. reviews).
The paper investigates a novel formulation of a stochastic, quasi Newton optimization strategy based on the natural idea of relaxing the secant conditions.  This is an interesting and promising idea, but unfortunately none of the reviewers recommended acceptance.  The reviewers unanimously fixated on weaknesses in the paper s technical presentation.  In particular, the reviewers expressed some dissatisfaction with many aspects, including:    Key details of the experimental evaluation were omitted (particularly concerning configuration of the baseline competitors), which is an essential aspect of reproducibility.  One consequence is that the reviewers were not confident in the veracity of the experimental comparison.    The reviewers struggled with a lack of clarity and accurate rendering of some key technical details.  An example is dissatisfaction with the non symmetry of the inverse Hessian approximation, which was not fully alleviated by the author responses.    The proposed approach does not appear to possess any intrinsic advantage over standard methods from a computational complexity perspective.  I think this is promising work, but a careful revision that strengthened the underlying technical claims appears necessary to make this a solid contribution.
Pros:    The paper proposes interesting new ideas on evaluating generative models.    Paper provides hints at interesting links between structural prediction and adversarial learning.    Authors propose a new dataset called Thin 8 to demonstrate the new ideas and argue that it is useful in general to study generative models.    The paper is well written and the authors have made a good attempt to update the paper after reviewer comments.  Cons:   The proposed ideas are high level and the paper lack deeper analysis.   Apart from demonstrating that the parametric divergences perform better than non parametric divergences are interesting, but the reviewers think that practical importance of the results are weak in comparison to previous works. With this analysis, the committee recommends this paper for workshop.
 + An interesting and original idea of embedding words into the (very low dimensional) Wasserstein space, i.e. clouds of points in a low dimensional space + As the space is low dimensional (2D), it can be directly visualized.  + I could imagine the technique to be useful in social / human science for data visualization, the visualization is more faithful to what the model is doing than t SNE plots of high dimensional embeddings + Though not the first method to embed words as densities but seemingly the first one which shows that multi modality  / multiple senses are captured (except for models which capture discrete senses) + The paper is very well written     The results are not very convincing but show that embeddings do capture word similarity (even when training the model on a small dataset)    The approach is not very scalable (hence evaluation on 17M corpus)    The method cannot be used to deal with data sparsity, though (very) interesting for visualization    This is mostly an empirical paper (i.e. an interesting application of an existing method)  The reviewers are split. One reviewer is negative as they are unclear what the technical contribution is (but seems a bit biased against empirical papers). Another two find the paper very interesting.      
Finally, all reviewers leaned towards rejection. The main concerns were missing methodological depth and questions regarding the experimental evaluation (unclear link between experimental outcomes and methodological details). The rebuttal was not perceived as being fully convincing, and finally nobody wanted to champion this paper. I think that this work has some potential, but in its present form, it does not seem to be ready for publication.
This paper analyzes a version of optimistic value iteration with generalized linear function approximation.  Under an optimistic closure assumption,  the algorithm is shown to enjoy sublinear regret.  The paper also studies error propagation through backups that do not require closed form characterization of dynamics and reward functions.  Overall, this is a solid contribution and the consensus is to accept.
The submission presents a Siamese attention operator that lowers the computational costs of attention operators for applications such as image recognition. The reviews are split. R1 posted significant concerns with the content of the submission. The concerns remain after the authors  responses and revision. One of the concerns is the apparent dual submission with "Kronecker Attention Networks". The AC agrees with these concerns and recommends rejecting the submission.
Reviewer #2 has written a nice summary of the paper which I quote below.  “The core idea is simple   which is a strength in my view   and does not require retraining the base language model, which could be important as language models become more expensive to train. However, the clarity and experiments in this paper fall short: the experimental setup has issues, the effect on perplexity is quite large but relegated to the Appendix, several claims are speculative and lacking corresponding experimental evidence, and it is unclear how the additional heuristics affect performance.  The method seems promising, but with the current experiments it is difficult to draw conclusions about how the method affects performance and which parts of it are necessary; given that this is an empirical paper, I would therefore not recommend acceptance in its current form.”  Key Strengths + Well motivated problem of considerable interest  + A relatively straightforward Bayesian solution + Proposed solution is computationally efficient compared to other competing approaches.  The paper has been thoroughly reviewed by the reviewers and as a result numerous questions has surfaced. While the authors addressed most of the questions adequately, there are still many unanswered questions. They include:   Readability issues highlighted by Reviewer #1   Reviewer #1: “"how did you measure model confidence about the toxicity label"   Reviewer #4: The perplexity gets much worse as the gedi training is introduced (i.e. Λ decreases), e.g. going from 25 to 45 on IMDb. This result is in the Appendix, and perplexity is never evaluated/reported in the other experiments.   Reviewer #4: Crucially, the GEDI training does not appear to help over just re weighting with the conditional LM ( vs. ). Could the authors comment on this result? How well does domain transfer work for less similar domains? How is perplexity affected for the models reported in Table 2?   Reviewer #4: How small can the conditional LM be? Why was medium used instead of small? What if large was used? Does the conditional LM need to be a large scale pretrained model (it would be nice to see a baseline of a simpler conditional LM)? Several heuristics are used:  weighting, nucleus filtering, keeping tokens over a threshold, repetition penalty, and rescaling the logits to positive (used in only one experiment).   How does each of these affect performance? There are no ablations, and given the small differences in some of the experiments it is unclear whether performance would actually be worse if we changed one of the heuristics. One outcome may be that the method only works for a careful balance of hyperparameters, which could be fine, but we don t have a sense of the variation.   Reviewer #2: The output in Table 6 makes me doubt how the experiments are badly controlled. The outputs from positive and negative sentiment are totally different and almost random text, meaning that the content of the generators is not controlled properly. 
The paper studies Distantly Supervised Relation Extraction(DSRE) in distributed settings. Though DSRE has been studied in Centralized setting it has not been studied in distributed platform. This  paper leverages the federated learning setup for this problem and proposes to use Lazy MIL for this purpose.  The paper  identifies the main challenge as label noise but does not attempt to characterise the severity of the problem vis a vis the centralised setup.  Though intuitive but a formal approach would have helped in understanding the importance of the derived results better.  
This paper applies spectral initialization and weight decay to neural nets with factorized layers. Although these ideas have been extensively studied in other areas, formalizing and applying them to deep neural nets is of potential interest to the community. The simulation results are nice, especially the experiments on compression methods (comparison to sparse pruning e.g. lottery tickets) and Transformers. I recommend acceptance.  
The submission considers a stochastic variant of the projective splitting algorithm, with a focus on monotone inclusion problems, and it proposes a novel separable algorithm with the ability to handle multiple constraints and non smooth regularizers.  All reviewers felt that there were merits to the submission and that the submission was borderline.  Public and non public discussion concluded that the paper would be of greater value to the community if the suggestions of the reviewers and related issues were addressed.
The paper contains useful information and shows relative improvements compared to mixup. However, some of the main claims are not substantiated enough to be fully convincing. For example, the claims that manifold mixup can prevent can manifold collision issue where the interpolation between two samples collides with a sample from other class is incorrect. The authors are encouraged to incorporate remarks of the reviewers.
All reviewers except one agreed that this paper should be accepted because of the strong author response during the rebuttal phase. Specifically the reviewers appreciated the new ablation study showing that improvements are not due to minor architectural changes, the new experiment on the number of time steps required for experiments, the agreement to change language around "neural energy minimization", the improvements to the related work, the novelty of the unrolled optimization approach, and the nice experimental results. Given this, I vote to accept. Authors: please carefully revise the manuscript based on the suggestions by the reviewers: they made many careful suggestions to improve the work and stressed that the paper should only be accepted once these changes are implemented. Once these are done the paper will be a nice addition to the conference!
The paper introduces a method for uncertainty quantification for medical applications, which quantifies both aleatoric and epistemic components.  The paper initially received three strong reject recommendations. The main limitations pointed out by reviewers relate to the limited contributions (either methodological or applicative and clinical), the lack of positioning with respect to related works, the presentation needing improvement and the lack of experimental comparison with respect to recent relevant baselines.  No rebuttal was provided. \ The AC carefully read the submission and agrees that the paper is premature for publication in the current form. Therefore, the AC recommends rejection.
There is a clear reviewer consensus to reject this paper so I am also recommending rejecting it. The paper is about an interesting and underused technique. However, ultimately the issue here is that the paper does not do a good enough job of explaining the contribution. I hope the reviews have given the authors some ideas on how to frame and sell this work better in the future.  For instance, from my own reading of the abstract, I do not understand what this paper is trying to do and why it is valuable. Phrases such as "we exploit the sparsity" do not tell me why the paper is important to read or what it accomplishes, only how it accomplishes the seemingly elided contribution. I am forced to make assumptions that might not be correct about the goals and motivation. It is certainly true that the implicit one hot representation of words most common in neural language models is not the only possibility and that random sparse vectors for words will also work reasonably well. I have even tried techniques like this myself, personally, in language modeling experiments and I believe others have as well, although I do not have a nice reference close to hand (some of the various Mikolov models use random hashing of n grams and I believe related ideas are common in the maxent LM literature and elsewhere). So when the abstract says things like "We show that guaranteeing approximately equidistant vector representations for unique discrete inputs is enough to provide the neural network model with enough information to learn" my immediate reaction is to ask why this would be surprising or why it would matter. Based on the reviews, I believe these sorts of issues affect other parts of the manuscript as well. There needs to be a sharper argument that either presents a problem and its solution or presents a scientific question and its answer. In the first case, the problem should be well motivated and in the second case the question should not yet have been adequately answered by previous work and should be non obvious. I should not have to read beyond the abstract to understand the accomplishments of this work.  Moving to the conclusion and future work section, I can see the appeal of the future work in the second paragraph, but this work has not been done. The first paragraph is about how it is possible to use random projections to represent words, which is not something I think most researchers would question. Missing is a clear demonstration of the potential advantages of doing so. 
This paper conducts experiments evaluating several different metrics for evaluating GAN based language generation models. This is a worthy pursuit, and some of the evaluation is interesting.  However as noted by Reviewer 2, there are a number of concerns with the execution of the paper: evaluation of metrics with respect to human judgement is insufficient, the diversity of the text samples is not evaluated, and there are clarity issues.  I feel that with a major re write and tighter experiments this paper could potentially become something nice, but in its current form it seems below the ICLR quality threshold. 
The paper presents a fair filter network to mitigating bias in sentence encoders by constructive learning. The approach reduces the bias in the embedding while preserves the semantic information of the original sentences.   Overall, all the reviewers agree that the paper is interesting and the experiment is convincing. Especially the proposed approach is conceptually simple and effective.   One suggestion is that the model only considers fairness metric based on the similarity between sentence embedding; however, it would be better to investigate how the "debiased embedding" helps to reduce the bias in more advanced downstream NLP applications such as coreference resolution, in which researchers demonstrate that the bias in underlying representation causing bias in the downstream model predictions. 
A variational function space prior is proposed, resulting in a variational Dirichlet posterior. After rebuttal, reviewers still had many remaining questions or concerns about the paper. For instance, rF5E outlines several concerns, many relating to factorization assumptions. Reviewer 7nPR also provides several suggestions. I will not repeat them here, but do encourage the authors to look closely at these questions and suggestions. At this particular time the paper is not strongly resonating with reviewers, but could be updated so that the value of the contributions is more obvious.
The paper introduces a procedure that uses low attention areas to de noise temporal prediction. The paper appears to focus on  temporal noise  as opposed to constant noise present in the video (it may handle shifting shadows, but not background noise).   The idea is certainly interesting, however, the experimental protocol suffers from the issues pointed out by reviewer 3:   maintaining the same protocol as prior methods to ensure a direct comparison of the results against reported scores by the sota   in the context of attention, alignment (or lack or it) is extremely important; assuming perfect alignment is not very realistic (if the alignment is perfect, one might try a simple method such as taking all readings at a point over time and considering the mode, then correcting any outliers in the off attention areas)  These specific issues were not fully addressed during the review period.  The questions raised by reviewer 2 were addressed to a satisfactory degree in the rebuttal.
This paper focuses on hate speech detection and compares several classification methods including Naive Bayes, SVM, KNN, CNN, and many others. The most valuable contribution of this work is a dataset of ~400,000 tweets from 2017 Kenyan general election, although it is unclear whether the authors plan to release the dataset in the future.  The paper is difficult to follow, uses an incorrect ICLR format, and is full of typos.  All three reviewers agree that while this paper deals with an important topic in social media analysis, it is not ready for publication in its current state. The authors did not provide a rebuttal to reviewers  concerns.  I recommend rejecting this paper for ICLR.
I thank the authors for their submission and very active participation in the author response period. The reviewers and I acknowledge the importance of designing toy environments that allow the community to systematically investigate strengths and weaknesses of RL approaches. That said, the reviewers have criticized that it is unclear whether experiments on the proposed toy MDPs would transfer to more complex standard RL benchmarks (such as Atari) [R1 & R2], and that the proposed metrics and axes of variation seem not well motivated or systematic [R1,R2,R3 & R4], thus casting doubts regarding what insights the community will be able to gain from experiments on MDP Playground. In particular, I agree with the reviewers R1 s and R4 s assessment that proposing many dimensions of variation, even if they are orthogonal, without a well formulated motivation and grounding in actual tasks the community cares about is not particularly helpful for advancing our understanding of current challenges in RL. Post rebuttal, R2 and R4 stand by their strong stance against acceptance; and R1 has increased their score as a result of the improvements of the updated paper, but they still lean towards rejection. Thus, I recommend rejection.
The authors propose to learn space aware 3D feature abstractions of the world given 2.5D input, by minimizing 3D and 2D view contrastive prediction objectives. The work builds upon Tung et al. (2019) but extends it by removing some of the limitations, making it thus more general. To do so, they learn an inverse graphics network which takes as input 2.5D video and maps to a 3D feature maps of the scene. The authors present experiments on both real and simulation datasets  and their proposed approach is tested on feature learning, 3D moving object detection, and 3D motion estimation with good performance. All reviewers agree that this is an important problem in computer vision and the papers provides a working solution. The authors have done a good job with comparisons and make a clear case about their superiority of their model (large datasets, multiple tasks). Moreover, the rebuttal period has been quite productive, with the authors incorporating reviewers  comments in the manuscript, resulting thus in a stronger submission. Based in reviewer s comment and my own assessment, I think this paper should get accepted, as the experiments are solid with good results that the CV audience of ICLR would find relevant.  
This paper introduces a conditional discrete VAE for uncertainty estimation on high dimensional data. Reviewers found the paper borderline, and two of the three reviewers stated it doesn t meet the acceptance bar due to lack of clarity in several aspects and limited technical novelty.
The paper received borderline reviews. While the reviewers acknowledged good motivation, good number of experiments and good numeral results that demonstrated the proposed method outperforms the existing state of the art, there are shared concerns: the experimental setup is not really a "low data" regime, generative models jointly trained with the multi task model only led to marginal improvements, and the prediction quality is quite low for all methods. In addition, it s unclear why the images generated by MGM have a lot of artifacts, and how the artifacts affect the performance. Overall, the reviewers were not convinced after the rebuttal.
This submission receives mixed reviews. One reviewer leans positively while two reviewers are negative. They raise several issues upon improper evaluations, insufficient experimental analysis, baseline and sota network comparisons, presentation unclarity, and technical motivations. In the rebuttal and discussion phases, the authors do not make any response to these reviews. After checking the whole submission, the AC agrees with these two reviewers that there are several drawbacks to the aspects of the technical presentation and experimental configurations. The authors shall take these suggestions into consideration and make further improvements upon the current submission.
Main content:  Blind review #3 summarizes it well:  This paper presents a technique for encoding the high level “style” of pieces of symbolic music. The music is represented as a variant of the MIDI format. The main strategy is to condition a Music Transformer architecture on this global “style embedding”.  Additionally, the Music Transformer model is also conditioned on a combination of both “style” and “melody” embeddings to try and generate music “similar” to the conditioning melody but in the style of the performance embedding.      Discussion:  The reviewers questioned the novelty. Blind review #2 wrote: "Overall, I think the paper presents an interesting application and parts of it are well written, however I have concerns with the technical presentation in parts of the paper and some of the methodology. Firstly, I think the algorithmic novelty in the paper is fairly limited. The performance conditioning vector is generated by an additional encoding transformer, compared to the Music Transformer paper (Huang et. al. 2019b). However, the limited algorithmic novelty is not the main concern. The authors also mention an internal dataset of music audio and transcriptions, which can be a major contribution to the music information retrieval (MIR) community. However it is not clear if this dataset will be publicly released or is only for internal experiments."  However, after revision, the same reviewer has upgraded the review to a weak accept, as the authors wrote "We emphasize that our goal is to provide users with more fine grained control over the outputs generated by a seq2seq language model. Despite its simplicity, our method is able to learn a global representation of style for a Transformer, which to the best of our knowledge is a novel contribution for music generation. Additionally, we can synthesize an arbitrary melody into the style of another performance, and we demonstrate the effectiveness of our results both quantitatively (metrics) and qualitatively (interpolations, samples, and user listening studies)."     Recommendation and justification:  This paper is borderline for the reasons above, and due to the large number of strong papers, is not accepted at this time. As one comment, this work might actually be more suitable for a more specialized conference like ISMIR, as its novel contribution is more to music applications than to fundamental machine learning approaches.
This was an extremely difficult case. There are many positive aspects of Graph2Seq, as detailed by all of the reviewers, however two of the reviewers have issue with the current theory, specifically the definition of k local gather and its relation to existing models. The authors  and reviewers have had a detailed and discussion on the issue, however we do not seem to have come to a resolution. I will not wade into the specifics of the argument, however, ultimately, the onus is on the authors to convince the reviewers of the merits/correctness, and in this case two reviewers had the same issue, and their concerns have not been resolved. The best advice I can give is to consider the discussion so far and why this misunderstanding occurred, so that it might lead the best version of this paper possible.
The paper proposed U net for segmentation of stagnant zones in computed tomography. Technical contribution of the paper is severely limited, and is not of the quality expected of publications in this venue. The paper is not anonymized and violates the double blind review rule. I m thus recommending rejection.
Most reviewers agree that the paper addresses a relevant problem. However, they also  believe that the paper lacks in several points : not well supported claim, sometimes clarity, incremental in term of novelty.
This work combines deep generative models (variational autoencoders, FragVAE) and multi objective evolutionary computation for molecular design. They use a multilayer perceptron as a predictor for properties. Evolutionary operations are used to explore the latent space of the generative model to produce novel competitive molecules. Experiments are executed to show the effectiveness of the proposed method with respect to Bayesian optimization based methods.  Strengths:  1   Combines multi objective evolutionary computation and deep generative modeling, which is a promising approach to tackle multi objective optimization in structured spaces.  Weaknesses:  All the reviewers agree that the paper is not yet ready for publication. They point out the following areas to improve:  1   The lack of details and clarity in the method.  2   The experimental section needs to be improved. The evaluation metrics and baselines are weak.  3   Describe better and more clearly the novelty of the proposed approach with respect to previous work in the area.
This paper investigates the properties of deep neural networks as they learn, and how they may relate to human visual learning (e.g. how learning develops across regions of the infant brain). The paper received three reviews, all of which recommended Weak Reject. The reviewers generally felt the topic of the paper was very interesting, but overall felt that the insights that the paper revealed were relatively modest, and had concerns about the connections between DNN and human learning (e.g., the extent to which DNNs are biologically plausible   including back propagation, batch normalization, random initialization, etc.   and whether this matters for the conclusions of the present study). In response to comments, the authors undertook a significant revision to try to address these points of confusion. However, the reviewers were still skeptical and chose to keep their Weak Reject scores.  The AC agrees with reviewers that investigations of the similarity   or not!   between infant and deep neural networks is extremely interesting and, as the authors acknowledge, is a high risk but potentially very high reward research direction. However, in light of the reviews with unanimous Weak Reject decisions, the AC is not able to recommend acceptance at this time. I strongly encourage authors to continue this work and submit to another venue; this would seem to be a perfect match for CogSci conference, for example. We hope the reviews below help authors to improve their manuscript for this next submission.
The authors propose a method for modeling dynamical systems that balances theoretically derived models, which may be grounded in domain knowledge but subject to overly strict assumptions, with neural networks that can pick up the slack. All reviewers were enthusiastic about this work, appreciating its balance of mathematical rigor and experimental assessment. One concern was that this paper follows on decades of related work, which was difficult to adequately summarize. However, changes made throughout discussion phase did address these concerns.
The authors proposed a meta learning framework for NAS, namely MetaD2A (Meta Dataset to Architecture), that can stochastically generate graphs (architectures) from a given set (dataset) via a dataset architecture latent space learned with amortized meta learning. Each dataset is encoded via a set encoder and the architecutres are obtained via a graph decoder. MetaD2A is trained once on a database consisting of datasets and pretrained networks and can rapidly search a neural architecture for a novel dataset. While the set encoder and graph decoder for NAS have been introduced by existing work, the main contribution of the paper is to show that the meta learning of a "dataset conditioned architecture generation" framework can enable fast generation of a good architecture without training on the target dataset. The proposed method is interesting and effective, however it requires an existing pool of good architectures for a given task, which may limit its applicability to a diverse set of real world problems. I strongly encourage the authors to include experiments on a larger pool of architectures than the NAS Bench 201 search space to show the strength of their proposed method in generating good architectures. While training MetaD2A with pairs of MetaImageNet and randomly sampled graph shows that the proposed framework can generate graphs with different types of edges, it doesn t show that it can successfully meta learn to produce better architectures for a new task from an existing pool of good architectures.   We believe that many of the reviewers comments were addressed in the rebuttal, so while the scores are low, they do not reflect neither the contribution nor the reviewers opinion well (e.g., R3, in his last post, seems to suggest that his review should be updated but it has not happened).
This work proposes a class of neural networks that can jointly perform classification and regression in the output space. The authors explore the concept of polar prototypes which are points on the hypersphere in the output space. For classification, each class is described by a single polar prototype and training is equivalent to minimizing angular distances between examples and their class prototypes. For regression, training can be performed as a polar interpolation between two prototypes. As rightly acknowledged by R3, “it is nice to see an alternative to the dominant cross entropy loss and l2 loss for deep classification and regression respectively, also the ability to tackle both” at the same time.  However, all reviewers and AC agreed that the current manuscript lacks convincing empirical evaluations that clearly show the benefits of the proposed approach. To strengthen the evaluation, (1) see R1’s concern regarding the state of the art performance on CIFAR 10;  (2) see R3’s suggestion to use more challenging datasets (e.g. ImageNet), stronger backbone networks (e.g. densenet), and also other applications (e.g. object recognition and pose estimation; face recognition and age estimation as classification and regression problems); (3) see R2’s suggestions for more baselines to be compared to. Two other requests to further strengthen the manuscript are: (1) finding alternative ways to MC or evolutionary algorithms (R2); (2) exploring class correlation in the prototype space (R2).   In the response, the authors acknowledged that their initial results were not aimed for state of the art comparison, but to show that the proposed objective is comparable to minimizing softmax cross entropy loss. The authors provide additional experiments using DenseNet as the base network and the results are still slightly inferior to state of the art performance.  The experiments using ImageNet dataset have been promised by the authors (in response to R3), but are not included in the current revision.   AC suggests in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper.  
This paper proposes a new  time varying convolutional architecture (ST GNN) for dynamic graphs. The reviewers were positive about the presentation and detailed theory, especially on the stability analysis. The shared criticism was on experimental validation synthetic datasets that the reviewers did not find appealing. The AC believes that while the lacking validation concerns are legit, there is a lack of sophisticated dynamic graph benchmarks in the community yet, so the authors did their best effort to test their method. We thus recommend to accept the paper.
This paper improves the training speed and decrease the computation cost of AdvProp, which is a method that leverages the adversarial example to improve the image recognition accuracy. The method achieves the speedup by leveraging a collection of practical heuristics, including reusing some gradient computation during training. The paper is well written, well justified with empirical supports, and can be potentially useful in many vision tasks. On the other hand, some novelty of the method is incremental, and the issues regarding empirical results and claims pointed out by the reviewers need  to be addressed in the revision.
The paper proposes a pruning approach that regularizes the gram matrix of convolutional kernels to encourage kernel orthogonality among the important filters meanwhile driving the unimportant weights towards zero. While the reviewers found the proposed method well motivated and intuitive, they believe that the proposed claims are of limited novelty and are not supported well by the experiments. Analyzing and explaining the effect of different parts of the proposed method, i.e., orthogonalization and regularization of batch normalization parameters, on the accuracy of the pruned models would significantly improve the manuscript.
The paper proposes a new method for improving generative properties of VAE model.  The reviewers unanimously agree that this paper is not ready to be published, particularly being concerned about the unclear objective and potentially misleading claims of the paper. Multiple reviewers pointed out about incorrect claims and statements without theoretical or empirical justification. The reviewers also mention that the paper does not provide new insights about VAE model as MDL interpretation of VAE it is not new.
This paper proposes a new bilinear decomposition for universal value functions.  The bilinear network has one component dependent on state and goal and another component that depends on state and action.  The experiments with the DDPG algorithm in robot simulations show that the proposed architecture improves performance data efficiency and task transfer over several baseline algorithms, including improvements on earlier bilinear decompositions.  The reviews noted several aspects of the paper could be improved, and the author response addressed several of these concerns. Multiple reviewers appreciated the insights from the experiment added in section 4.5 on a simple grid environment, which enabled a direct interpretation of the vector fields used in the method.  Several aspects of the presentation were clarified based on the reviewers comments. Additional details were also provided on the problem specification and the solution methods. During the discussion, the reviewers agreed that the revised paper presented a useful addition to the literature.  Four knowledgeable reviewers indicate to accept the paper for its contribution of an effective network architecture for a goal conditioned universal value function approximator.  The paper is therefore accepted.
This paper proposes an approach to data augmentation to train image recognition models called SaliencyMix, which involves pasting salient regions (as judged by some saliency detector) of one image into another, and mixing the two labels accordingly. Most reviewers generally agreed that the proposed approach is simple   it is easy to understand the method and its motivation, especially in the context of related augmentation approaches like CutMix   and has solid experimental results demonstrating its effectiveness.  The main objection the more negative reviewers had to the work is a perceived lack of novelty. In my view it is a new method (even if similar to prior work like CutMix), and as AR5 argues: "this approach probably starts to get to the heart of why these previous strategies work: they are probably less effective ways of doing what this paper suggests." The improvements in Table 1, columns 1 & 3 (CIFAR 10 & CIFAR 100) especially speak to this   these improvements with traditional augmentation disabled are quite substantial, even though the differences become marginal when moving to the "+" augmented versions of the dataset (as well as in ImageNet). So although the method is indeed similar to CutMix, I agree that it offers valuable insight into why these previous methods work. Besides which the results *do* show improvements over similar methods, even if the improvements are marginal.  Overall, I recommend accepting the paper as it provides useful insight into why prior methods work and proposes a new one that in practice works slightly better. Minor comments for the camera ready version:  Please revise the writing based on AR4 s good suggestions.  Highlighting a comment from AR4: > BAsNet for example, was trained on 10k images. Why not simply include these (and their mask) as part of the pretraining when considering some of the baselines ?  I recommend including discussion of this important point in the final version of the paper. The learning based approaches are effectively using additional training data. It s good that a non learning based method happens to perform best so that the results remain comparable with prior work, but this should nonetheless be discussed if the learning based approaches are to be included.  Please remove the blue text coloring (if not already planned   I m not sure if this was done as a "diff" for the response).  > Figure 3(a b) show that Montabone & Soto (2010) performs better on both the datasets and the effects are identical on CIFAR10 and ImageNet datasets  I do not see how Figure 3 shows this. Is "OpenCV Saliency" in the figure using the method from Montabone & Soto (2010)? Please clarify this by making the connection between the bar labels in the figure and the discussion in the text clear for the camera ready version of the paper.
The paper aims to address several challenges in learning neural network based optimization algorithms by increasing the #unrolled steps, increasing the #training tasks, and exploring new parameterizations for the learning optimizer. The authors demonstrated the effectiveness of applying persisted Evolution Stratergies and backdrop through over 10,000 inner loop steps can improve the performance of the learned optimizer. Empirical experiments showcased incorporating LSTM to the previous state of the art improve their training performance.   There are a lot of interesting ideas in the paper. However, packaging them together and only glance over each idea briefly unfortunately dilutes the contribution and the novelty of the work. There are still some major concerns echoed among the reviewer:  1) The proposed hierarchical optimizer seems interesting. It is one of the major contributions of the paper. But, its architecture was only briefly mentioned in Sec 3.3. Its motivation, implementation and the corresponding engineering choices remain unclear by just reading the main text. Some of the details were discussed in the appendix but it would be of great interest if authors could give some intuition on which subset of the tasks the proposed architecture gives the most improvement / failure among the 6000 tasks.  2) Training the optimizer on a diverse set of tasks is crucial for the learned optimizer to generalize. One of the paper s contributions is to further expand the task dataset from the prior work Metz et al., (2020). The authors have conducted very thorough experiments on this new dataset, which is amazing. I would argue there are even enough results for another standalone paper. However, there is surprisingly little detail on how the newly proposed dataset differs from the prior TaskSet dataset. What are the new optimization problems? How are they different from the family of tasks in TaskSet? A TSNE plot of the tasks similar to Figure 1 from Metz et al. (2020) could provide more intuition for the reader and highlight the contribution.   Overall, if the authors could provide more insight into their experiments and the proposed methods, it would help the readers greatly to see the novelty and the contribution of the paper. The current version of the paper will need additional development and non trivial modifications to be broadly appreciated by the community.   
Like the reviewers, I find this paper extremely borderline. On the one hand, it is clearly written, about a topic I find fascinating, and generally well motivated if not shockingly novel (i.e. removing some of the simplifying assumptions from Zhong et al. 2020, e.g. requiring grounding to be learned, use of real language rather than synthetically generated). On the other hand, I agree with the leitmotiv present amongst the reviews that the problem at the centre of the experimental setting is very, very simple (3 objects, 3 descriptions). I am mindful of the fact that access to computational resources is unevenly distributed, and am not expecting a paper like this to immediately scale their experiments to highly complex settings with photorealism, etc, but I can t help but feel that a more challenging task, with a deeper analysis of the problems presented by both grounding and the use of non synthetic language, would both have been highly desirable to make this paper uncontroversially worth accepting.  As a result, the decision is to not accept the paper in its present form. Work on this topic should definitely be presented at ICLR, but it s a shame this paper did not make a stronger case for itself.
The author propose a method to first learn policies for intrinsically generated goal based tasks, and then leverage the learned representations to improve the learning of a new task in a generalized policy iteration framework.  The reviewers had significant issues about clarity of writing that were largely addressed in the rebuttal.  However, there were also concerns about the magnitude of the contribution (especially if it was added anything significant to the existing literature on GPI, successor features, etc), and the simplicity (and small number of) test domains.  These concerns persisted after the rebuttal and discussion.  Thus, I recommend rejection at this time.
This paper provides a fascinating hybridization approach to incorporating programs as priors over policies which are then refined using deep RL. The reviewers were, at the end of the discussion, all in favour of acceptance (with the majority strongly in favour). An excellent paper I hope to see included in the conference.
The paper proposes a novel method that learns decompositions of an image over parts, their hierarchical structure  and their motion dynamics given temporal image pairs. The problem tackled is of great importance for unsupervised learning from videos. One downside of the paper is the simple datasets used to demonstrate the effectiveness of the method.  All reviewers though agree on it being a valuable contribution for ICLR.  In the related work section the paper mentions "...Some systems emphasize learning from pixels but without an explicitly object based representation (Fragkiadaki et al., 2016 ...". The paper you cite in fact emphasized the importance of having object centric predictive models and the generalization that comes from this design choice, thus, it may be potentially not the right citation. 
The paper received scores either side of the borderline: 6 (R1), 5 (R2), 7 (R3). R1 and R3 felt the idea to be interesting, simple and effective. R2 raised a number of concerns which the rebuttal addressed satisfactorily. Therefore the AC feels the paper can be accepted.
The main motivation of this work is to introduce robustness in federated learning, through a Wasserstein uncertainty set. The end result, however, leaves a mixed feeling: As the reviewers pointed out, the authors, perhaps for computational convenience, forgo strong duality and treat the important variable gamma as a hyperparameter, which renders large part of the work follow immediately from existing work: essentially, we simply use a different loss function in FedAvg. While there may be advantages to choose one loss over another in any specific application, this itself is not a significant contribution. The comparison against existing FL algorithms is also a bit weak: Despite of the reviewer s request, the authors did not compare to other robust FL algorithms (e.g., AFL), thus it is not clear what is the real advantage of the proposed algorithm. As a result, we believe the current draft is not ready for publication.
This paper presents a feature normalization method for CNNs by decorrelating channel wise and spatial correlation simultaneously. Overall all reviewers are positive to the acceptance and I support their opinions. The idea and implementation is relatively straightforward but well motivated and reasonable. Experiments are well organized and intensive, providing enough evidence to convince its effectiveness in terms of final accuracy and convergence speed. Also, it’s analogy to biological center surrounded structure is thought provoking. The novelty of the method seems somewhat incremental considering that there already exists a channel wise decorrelation method, but I think the findings of the paper are interesting and valuable enough for ICLR community and would like to recommend acceptance. Minor comments: I recommend authors to mention about zero component analysis (ZCA) normalization, which has been a standard input normalization method for CIFAR datasets. I guess it is quite similar to the proposed method considering 1x1 convolution. Also, comparison with other recent normalization methods (e.g., Group Norm) would be useful.  
The authors introduce a method for offline imitation learning in the presence of optimal and non optimal data. In particular, they propose to learn a discriminator that can be then further used to modify the behavior cloning loss which leads to performance improvements over baselines. The reviews mention that the idea is novel and most sections of the paper are well written and self explanatory. They do point out, however, several flaws such as the clarity of the derivation and  the thoroughness of experimental evaluation. While the paper has significantly improved during the rebuttal, its significant changes warrant another round of reviews. I encourage the authors to continue improving the paper, addressing the reviewers  feedback and resubmitting it as it has a potential to be a strong submission.
This paper proposes a multi sample variant of dropout, claiming that it accelerates training and improves generalization. CIFAR10/100, ImageNet and SVHN results are presented, along with a few ablations.  Reviewers were in agreement that the novelty of the contribution appears to be very limited, the evidence for the claims is not strong, and that the applicability of the method for achieving efficiency gains is limited to architectures that only apply dropout very late in processing, precluding applicability to models that employ dropout throughout. Importantly, comparisons to Fast Dropout (Wang 2013) seem highly relevant and are missing.  While the reviewers acknowledged some of the criticisms, virtually no arguments were offered to rebut them and no updates were made to address them. I therefore recommend rejection.
This work introduces/applies the mirror descent optimization technique to adversarial inverse reinforcement learning (AIRL). As a result, the proposed algorithm (MD AIRL) incrementally learns a parameterized reward function in an associated reward space. The two issues of standard adversarial imitation learning algorithms are 1) current "divergence" based updates may not lead to updates that better match the expert (due to geometry) 2) "divergence" based updates may suffer when only small number of demonstrations are provided. Thus the goal of this work is to (presumably)  to "robustify" the learning of reward function especially by addressing these issues. The proposed algorithm is evaluated on a bandits problem, a multi goal toy example and standard mujoco benchmark.  **Strengths** This work attempts to address the important problem of understanding and improving the updates of IRL algorithms A theoretical analysis is provided  **Weaknesses** The major concern is clarity of the manuscript. Even after updating clarity remains a concern While a lot of experiments were performed, evaluation is not entirely convincing. One reason for this that it is hard to tie the results back to the original motivation/claims of this algorithm. As one reviewer notes "it s unclear how the new algorithm affects reward functions". Furthermore, reviewers find the experimental results not entirely convincing   **Summary** After rebuttal and revision, the clarity and experimental analysis remain a concern. My recommendation is that the authors are encouraged to take the reviewers feedback and improve the manuscript. In its current form it s not quite ready yet for publication.
This paper presents the use of the Ensemble Kalman Filter (EnKF) to solve the linear quadratic Gaussian (LQG) optimal control problem. After reviewing the paper and taking into consideration of the reviewing process, here are my comments:   The related work is limited and needs more improvements to contextualize the problem and the solution.   The reinforcement learning paradigm is not really appreciated in the proposal.   The results are rather limited, so more experiments are needed to clearly validate the solution. From the above, the paper does not fulfill the standards of the ICLR. I suggest improving the paper accordingly and submitting it to a control systems venue.
This paper proposes a number of improvements to the previously published transformer based MQ forecaster model for multi horizon forecasts on time series data. They show strong empirical improvements in terms of accuracy and excess forecast variability on a large proprietary dataset, as well as four public datasets. Concerns were raised about the relatively incremental changes to the MQ forecaster model this work is based on, lack of ablations on public data and, relatedly, inability to reproduce results on the proprietary data.
the paper proposed a novel idea of  requiring users to complete a proof of work before they can read the model s prediction to prevent model extraction attacks. Reviewers were excited about the paper and ideas. Some misunderstanding raised by reviewers were sufficiently clarified by the authors in the rebuttal.
This paper shows SGD enjoys linear convergence for shallow neural networks under certain assumptions. However, reviewers reach the consensus that this paper lacks technical novelty. The meta reviewer agrees and thus decides to reject the paper.
The paper investigates adversarial examples in deep neural networks from a frequency based perspective. Their main conclusion is that adversarial examples are neither in high  or low frequency components, but instead depend on data. The topic is clearly important and the paper is overall clearly written and makes some interesting observations, backed up by empirical evidence.  However, the reviewers raised a number of critical concerns, including:   Discussion of prior work is not adequate. The paper should better explain their contribution in contrast to prior work. Specifically, the authors mention Bernhard et al. (2021) as concurrent work, although the reviewers note that the work was published 5 months before. I realize the authors most likely develop their own line of work without knowing about Bernhard et al. (2021), but I would still suggest focusing more on the differences between them. I did not take this factor into account in the final decision.   Novelty. Prior work has already shown adversarial examples are data dependent   Concerns about experimental setup (only investigate one particular attack, measure of average noise gradient not completely justified, ...)  After discussion, one reviewer downgraded their score and two others kept a more negative score. Only one reviewer was more positive with somewhat low confidence.  Overall, the paper is more on the reject side for now. Further work is needed and I strongly encourage the authors to clearly highlight the contributions of the paper in contrast to prior work. On the plus side, the work clearly has some potential and addresses an interesting topic.
This paper is intersting but has a few flaws that still need to be addressed. As one reviewer noted, "the authors seems to have simply applied the method of Andrychowicz et al. If they added some discussion and experiments clearly showing why this is a better way to improve the existing inference methods, the paper might have more impact.". Overall, this work builds on existing work, but does not really dig deep enough for answers to these questions raised by the reviewers. The committee still feels this paper will be of great value at ICLR and recommends it for a workshop paper. 
The paper considers the question of identifying bad data so that models can be trained on the subset of data that is good. This question is formulated as a utility optimization problem. The paper shows that some popular heuristics are quite bad in the framework they propose. They also propose a new algorithmic framework called DataSifter. There is empirical evaluation provided for this. Questions have been raised in the reviews about the size of the models that have been used in the empirical evaluation. The authors have responded to this by suggesting the use of proxy model techniques. There are also questions about learnability of data utility for which some responses are provided in the rebuttal.
This paper incorporates attention in the PixelCNN model and shows how to use MAML to enable few shot density estimation. The paper received mixed reviews (7,6,4). After rebuttal the first reviewer updated the score to accept. The AC shares the concern of novelty with the first reviewer. However, it is also not trivial to incorporate attention and MAML in PixelCNN, thus the AC decided to accept the paper. 
I agree with the concerns raised by the reviewers. In particular, the issues of novelty and experimental evaluation (mentioned in the revision summary) remain the major weak points of the paper. My impression is that the changes made in the revision represent a significant experimental addition to the paper, one which might merit a full pass through peer review, and one which in any event did not alter the reviewers  scores. While I think this paper has something to contribute (and the empirical results suggest the method may outperform competitors), I think it would be improved by a rewrite (and possibly a restructure) that makes the part that is your contribution much more clear. For example, in the abstract, it s only in the sentence "We show both theoretically and empirically that potential vanishing/exploding gradients problems can be mitigated by enforcing orthogonality to the shared filter bases" that we actually get to the part that is really novel about this contribution (the "enforcing orthogonality"): that would ideally be much earlier in the abstract.
The authors leverage advances in semi supervised learning and data augmentation to propose a method for active learning. The AL method is based on the principle that a model should consistently label across perturbation/augmentations of examples, and thus propose to choose samples for active learning based on how much the estimated label distribution changes based on different perturbations of a given example. The method is intuitive and the experiments provide some evidence of efficacy. However, during discussion there was a lingering question of novelty that eventually swayed the group to reject this paper. 
This paper improves calibration of neural networks by investing its connection to adversarial robustness. Two reviewers suggested acceptance, and two did rejection. As the authors and some reviewers highlighted, AC also agreed that the correlation between adversarial robustness and calibration is interesting to explore. However, as R1 pointed out, AC also thinks that the experimental results are not strong enough to meet the high standard of ICLR, e.g., Mixup often outperforms the proposed method (without further post processing) and the proposed method does not outperform the deep ensemble (although deep ensemble is expensive and both method can be combined). Due to this, AC doubts whether adversarial robustness is indeed the best way to improve calibration (it can be useful though). Hence, AC recommends rejection.
The authors propose a GAN based anomaly detection method based on simulating anomalies (low density regions of the data space) in order to train an anomaly classifier.  While the paper addresses an interesting take on an important problem, there are many concerns raised by reviewers including novelty, clarity, attribution, reproducibility, the use of exclusively proprietary data, and a multitude of textual mistakes. Overall, the paper shows promise but does not seem to be a mature and polished piece of work. As there has been no rebuttal or update to the paper I have no choice but to concur with the reviewers  initial assessments and reject.
As evident by the title the paper focuses on understanding sharpness aware minimization which is a contemporary training procedure  based on minimizing the worse case perturbation of the weights in ball. It has been observed that SAM improves the generalization and this paper aims to demystify this success. They also provide a convergence proof of SAM for non convex objectives in a simplified setting and also discuss benefits of SAM in the noisy label setting.The reviewers thought this paper was an interesting first step The reviewers raised concerns about (1) novelty of the proof technique, (2) interpretation of the analysis. The response mitigated some the concerns but did not resolve them. I concur with the reviewers. The paper has some nice insights and good potential. However, there are a few things that need to be clarified and the paper has to be substantially rewritten to reflect this and thus I do not recommend acceptance at this time.
The paper proposes a solution based on self attention RNN to addressing the missing value in spatiotemporal data.   I myself read through the paper, followed by a discussion with the reviewers. We agree that the model is reasonable, and the results are promising. However, there is still some room for improvement: 1. The self attention mechanism is not new. The specific way proposed in the paper is an interesting tweak of existing models, but not brand new per se. Most importantly, it is unclear if the proposed way is the optimal one and where the performance improvement comes from. As the reviewer suggested, more thorough empirical analysis should be performed for deeper insights of the model.   2. The datasets were adopted from existing work, but most of them do not have such complex models as the one proposed in the paper. Therefore, the suggestion for bigger datasets is valid.   Given the considerations above, we agree that while the paper has a lot of good materials, the current version is not ready yet. Addressing the issues above could lead to a good publication in the future. 
The paper explores self supervised learning on tabular data and proposes a novel augmentation method via corrupting a random subset of features. The idea is simple but effective. Experiments include 69 datasets and compare with a number of methods. The result shows its superiority. It would be inspiring more work for SSL on the tabular domain.
A meta RL algorithms that aims to improve meta policy interpretability by reducing the meta gradient variance and bias estimation. The method is evaluated on an exploration in 2d navigation and meta RL benchmarks.  Despite an important topic of research, the reviewers are unanimous that the paper is an early version and requires further work to be suitable for publishing. Specifically, the future versions of the manuscript should address the novelty by better distinguishing from the prior work, improve the evaluations, presentation of the work. 
Kernel methods are among the most flexible and powerful approaches of our times. Random features (RF) provide a recent mechanism to also make them scalable due to the associated finite (and often small) dimensional approximate feature map (in the paper referred to as linearization). The focus of the submission is the linearization of the softmax kernel (defined in (1)) while making sure that the obtained RF approximation is accurate simultaneously for the small and the large kernel values. The authors present a hybrid random feature (HRF, defined in (8)) construction parameterized by base estimators and weights, and show that specific choice of these parameters is capable of implementing the goal. Some of the HRF estimators are also accompanied by theoretical guarantees (Section 3). Their numerical efficiency is illustrated (Section 4) on synthetic examples and in the context of natural language and speech modelling, and in robotics.  Scaling up kernel methods is a fundamental task of machine learning. The authors present a nice and valuable construction in this direction which can be of both theoretical and practical interest to the community.  The submission would benefit from implementing the remarks of the reviewers to improve its clarity.
This paper proposes a type of Mixup style data augmentation that works at the batch level rather than simply between pairs of examples. Each generated example accumulates salient images from potentially many other examples while ensuring diversity across the generated examples. This is achieved through a 4 part objective with submodular and supermodular components. The paper demonstrates the method using extensive experiments, including generalization performance on CIFAR 100, Tiny ImageNet, ImageNet and GoogleCommands. It also explores weakly supervised object localization, expected calibration error, and robustness to random replacement and Gaussian noise.  Reviewer 1 thought the approach was interesting but raised some concerns with clarity, thoroughness of experiments and whether the approach was computationally prohibitive to be used in practice. I was surprised myself that a discussion on the trade off between computational expense and accuracy gain was not discussed in the submission. The authors responded to the review, adding a comparison to the BP algorithm (Narshiman and Bilmes 2005). The empirical result seems to back up the claim that the proposed algorithm finds a better solution and with less variance. It also appears to run much faster. The authors also responded to minor issues raised with respect to clarity and organization. In their response, the authors provided considerable detail with respect to running time and time complexity, and show that models trained with co mixup are practical, though they do come with a significant added cost. The authors added the requested comparisons to non mixup baselines and enhanced the ablation study. In my opinion, this is a comprehensive and satisfying response, and the paper has improved in many respects since submission.  The review from R2 was largely positive, though limited in its scope. They also expressed concerns with training time (addressed in the response to R1). Clearly the approach extends to an arbitrary (m) number of images; this was explicit in the paper/formulation and clarified by the authors. I have some concern that R2 may have skimmed the paper if they missed this point.  Reviewer 4 thought the paper was interesting and asked several clarifying questions. They expressed concern with the significance of the reported gains. Similar to R1, they asked about non mixup baselines (VAT specifically). This was addressed in the response to R1. The authors responded to the clarifying questions and addressed the issue of significance.  Like the reviewers, I think that this is an intriguing, fresh, and elegant way to perform data augmentation. I appreciate that it has been evaluated just not from the pure generalization setting, but from other angles like robustness and calibration. There are still some outstanding concerns regarding the computational effort required to use Co Mixup, so this would be nice to see in follow up work.
This paper proposes a semi supervised method for reconstructing 3D faces from images via a disentangled representation. The method builds on previous work by Tran et al (2018, 2019). While some results presented in the paper show that this method works well, all reviewers agree that the authors should have provided more experimental evidence to convincingly demonstrate the benefits of their method. The reviewers are also unconvinced by how computationally expensive this method is or by the contributions of the unlabelled data to the performance of the proposed model. Given that the authors did not address the reviewers’ concerns, and for the reasons stated above, I recommend rejecting this paper.
The submission proposes a loss surrogate for top k classification, as in the official imagenet evaluation.  The approach is well motivated, and the paper is very well organized with thorough technical proofs in the appendix, and a well presented main text.  The main results are: 1) a theoretically motivated surrogate, 2) that gives up to a couple percent improvement over cross entropy loss in the presence of label noise or smaller datasets.  It is a bit disappointing that performance is limited in the ideal case and that it does not more gracefully degrade to epsilon better than cross entropy loss.  Rather, it seems to give performance epsilon worse than cross entropy loss in an ideal case with clean labels and lots of data.  Nevertheless, it is a step in the right direction for optimizing the error measure to be used during evaluation.  The reviewers uniformly recommended acceptance.
The paper received positive recommendation from all reviewers. Accept.
The paper proposes a new computational block called "StarSaber" which is a self attention based block derived from RNN fixed point approximations.  All the 4 reviewers and the authors agree that the work is not ready for publication. While the motivation of the authors is interesting, some reviewers have raised concerns about the validity of the derivation. All the reviewers agree that authors need to clarify the difference between StarSaber and Transformer, compare with Transformer in large scale pre training experiments, compare the compute speed, do a lot of ablations to validate the design choices.  I recommend the authors to incorporate all the reviewers  comments and make a stronger submission to a future conference!
This paper proposes to use an energy based model for a multi objective molecular generation. The energy function is parameterized by relational graph convolutional network (R GCN) so that it has a permutation invariance property. The model is trained by contrastive divergence and the generation is performed by Langevin dynamics. Experiments on single and multi objective molecule generation are conducted to verify the effectiveness of the proposed framework. The paper is well written, and the experiments are comprehensive. The major shortcoming of the paper is its limited novelty, since using EBM for graph generation is a straightforward application of the existing deep EBM framework. The contribution is marginal.    During the discussion, two of the reviewers pointed out that the contribution is limited and marginal. Two reviewers pointed out that the performance gain obtained by the proposed model is marginal and not significant. One reviewer has a concern about the computational cost of MCMC. However, the authors didn’t provide a rebuttal to address the concerns raised by the reviewers. Given the fact that all the concerns from the reviewers remain, and the contribution and performance gain of the work are marginal, the AC recommends rejecting the paper.
The reviewers think the topic is important and challenging. The results are novel, and the experimental section provides a nice illustration how the joint Shapley values can be used. However, the paper can be improved by including more real world applications and experiments.
Reviewers generally agree that the main result of the paper, which generalizes the classical Wigner Eckart Theorem and provides a  basis for the space of G steerable kernels for any compact group G, is a significant result. There are also several concerns that need to be addressed. R4 notes that the use of the Dirac delta function (e.g. Theorem C.7) is informal and mathematically imprecise and needs to be fixed. R1 notes that it would be helpful to at least describe how this general formulation can be applied in machine learning.  Presentation and accessibility: the current version of the paper will be accessible to only  a small part of the machine learning audience, i.e. those already with advanced knowledge in mathematics and/or theoretical physics, in particular in representation theory. If the authors aim to make it more accessible, the writing would need to be substantially improved.
This paper investigates how to align word senses across languages. This has not been studied much as past work has primarily considered aligning word (embeddings) across languages. The paper is well written and well motivated. Unfortunately the empirical results are not very strong. The baselines are somewhat low and the gains are modest (the excuse that it is difficult to train BERT sized models in academia is acknowledged). Overall, there is not enough support for acceptance at such a competitive venue as ICLR. 
Strengths: Interesting work on using latent variables for generating long text sequences. The paper shows convincing results on perplexity, N gram based and human qualitative evaluation.  Weaknesses: More extensive comparisons with hierarchical VAEs and the approach in Serban et. al in terms of language generation quality and perplexity would have been helpful. Another point of reference for which additional comparisons were desired was: "A Hierarchical Latent Structure for Variational Conversation Modeling" by Park et al. Some additional substantive experiments were added during the discussion period.  Contention: Authors differentiated their work from Park et al. and the reviewer bringing this work up ended up upgrading their score to a 7. The other reviewers kept their scores at 5.  Consensus: The positive reviewer raised their score to a 7 through the author rebuttal and discussion period.  One negative reviewer was not responsive, but the other reviewer giving a 5 asserts that they maintain their position. The AC recommends rejection. Situating this work with respect to other prior work and properly comparing with it seems to be the contentious issue. Authors are encouraged to revise and re submit elsewhere.
This manuscript proposes and analyses can approach to address the centralized and personalized tasks in federated learning jointly. Existing work has tackled this issue by developing separate tasks. Instead, this manuscript proposes a shared architecture that aims to optimize centralized and personalized models. One observation motivating this work is that local models trained during federated learning effectively optimize local task performance. The resulting approach results well when label shifts primarily drive the client variability. Here, the centralized components are trained to optimize a balanced risk, while the local components are trained to optimize the standard empirical risk.   Reviewers agree that the manuscript is well written and appropriately addresses the timely issue posed. The main concerns are the clarity of the technical contributions and technical statements during the review. The authors respond to these concerns and have satisfied the reviewers. After discussion, most reviewers are generally strongly positive about the strength of the manuscript contributions.
The authors propose two measures of calibration that don t simply rely on the top prediction. The reviewers gave a lot of useful feedback. Unfortunately, the authors didn t respond.
This paper tackles a problem of resource allocation using reinforcement learning. An important invariant   permutation invariant   is identified as an important characteristic of this problem. Then it is shown that taking advantage of such an invariant should  dramatically improve the sample efficiency.  On behalf of the reviewers, I would like to thank the authors for addressing many concerns raised in the initial reviews. Unfortunately, a further examination revealed several other potential issues that require further clarification:  1. It seems that real data experiments do not really demonstrate whether the benefits of the approach come from multi task learning or from permutation invariance. It would make sense to run an ablation study. In particular, if the benefit is really coming from multi task learning, then the theory part of the paper becomes less relevant.  2. The metric used for finance application appear to be in adequate. It is typical in finance academic literature to look some form of risk adjusted returns. Is the MTL strategy just taking more risk? How statistically significant are the results?  Given these concerns the paper can not be accepted in its current form but we encourage authors to address these and resubmit. 
The submission proposes a machine learning approach to directly train a prediction system for whether a boolean sentence is satisfiable.  The strengths of the paper seem to be largely in proposing an architecture for SAT problems and the analysis of the generalization performance of the resulting classifier on classes of problems not directly seen during training.  Although the resulting system cannot be claimed to be a state of the art system, and it does not have a correctness guarantee like DPLL based approaches, the paper is a nice re introduction of SAT in a machine learning context using deep networks.  It may be nice to mention e.g. (W. Ruml. Adaptive Tree Search. PhD thesis, Harvard University, 2002) which applied reinforcement learning techniques to SAT problems.  The empirical validation on variable sized problems, etc. is a nice contribution showing interesting generalization properties of the proposed approach.  The reviewers were unanimous in their recommendation that the paper be accepted, and the review process attracted a number of additional comments showing the broader interest of the setting.
 I am going to recommend acceptance of this paper despite being worried about the issues raised by reviewer 1.  In particular,  1:  the best possible inception score would be obtained by copying the training dataset 2:  the highest visual quality samples would be obtained by copying the training dataset 3:  perturbations (in the hidden space of a convnet) of training data might not be perturbations in l2, and so one might not find a close nearest neighbor with an l2 search 4:  it has been demonstrated in other works that perturbations of convnet features of training data (e.g. trained as auto encoders) can make convincing "new samples"; or more generally, paths between nearby samples in the hidden space of a convnet can be convincing new samples.  These together suggest the possibility that the method presented is not necessarily doing a great job as a generative model or as a density model (it may be, we just can t tell...), but it is doing a good job at hacking the metrics (inception score, visual quality).      This is not an issue with only this paper, and I do not want to punish the authors of this papers for the failings of the field; but this work, especially because of its explicit use of training examples in the memory,  nicely exposes the deficiencies in our community s methodology for evaluating GANs and other generative models.  
The authors demonstrate that complete neural network verification methods that use limited precision arithmetic can fail to detect the possibility of attacks that exploit numerical roundoff errors. They develop techniques to insert a backdoor into networks enabling such exploitation, that remains undetected by neural network verifiers and a simple defence against this particular backdoor insertion.   The paper demonstrates an important and often ignored shortcoming of neural network verification methods, getting around which remains a significant challenge. Particularly in adversarial situations, this is a significant risk and needs to be studied carefully in further work.  All reviewers were in agreement on acceptance and concerns raised were adequately addressed in the rebuttal phase, hence I recommend acceptance. However, a few clarifications raised by the official reviewers and public comments should be addressed in the final revision: 1) Acknowledging that incomplete verification methods that rely on sound overapproximation do not suffer from this shortcoming. 2) Concerns around reproducibility of MIPVerify related experiments brought up in public comments.
This is an interesting submission, which was overall well received by the reviewers. I would recommend the authors to discuss further the vast modern litterature on efficient computation of Wasserstein distances and their minimization (see, e.g. Peyré and Cuturi 2019, and references therein)
This paper proposes a new multi agent RL algorithm, based on the PPO algorithm, that uses a mean field approximation, which results in a a permutation  invariant actor critic neural architecture. The paper includes a detailed theoretical analysis that shows that the algorithm finds a globally optimal policy at a sub linear rate of convergence, and that its sample complexity is independent of the number of agents. The paper include some experiments that validate the proposed algorithm.  The reviews of this paper are mixed. Most of the reviewers appreciate the theoretical analysis, but one reviewer does not find the theoretical justification of the mean field approximation clear. The reviewer also points out to the absence of comparisons to relevant competing algorithms. These concerns are addressed by the authors in their rebuttal. A key issue with this work is the weakness of the empirical evaluation. The proposed method is tested on only two simple tasks, and the results on the second task do not show a considerable advantage of the proposed algorithm. This paper can be strengthened by adding experiments that clearly indicate the advantage of the proposed technique.
We thank the authors for detailing their answers to the reviewers and uploading a new version of the paper with more details and experiments. While the experimental section has improved in the revision, the fact that the method proposed is an ad hoc sequence of 6 heuristic steps, not supported by theoretical justification, and that the paper is hard to follow and not rigorous in its statements, remain.  For example, the authors explain in their response that "[we] developed a new method to solve a fundamental mathematical problem". If this is the case, then one would expect the mathematical problem to be rigorously formulated, and the fact that the method solves it supported by theoretical justifications.  Regarding the problem statement (section 2.2), the authors write three equations, which however are ill defined or ambiguous. For example, equation 2.1 has an expectation. Does that mean that X is a random matrix? This is nowhere stated, and there is no expectation in the following equations. Equations 2.2 and 2.3 are about a "truncated SVD" operator, which is also not rigorously defined. Literally, the authors state in Section 2.1 that for a matrix Z, tSVD¨*(Z) USV, where S is the "diagonal matrix Z s singular values, where all except for the top r singular values are forced to be zero". But what is r? Is it a parameter of tSVD*? Or is it, as suggested in the last sentence, "the numerical rank of Z" that suggests that it depends on Z, but in that case the "numerical rank" should be defined if it is different from the rank. I take these examples to highlight that the authors should consider writing rigorous and correct equations to define the problem.  Regarding the fact that the method proposed solves the problem, the authors add in the new versions some lemmas to support the claim. Alas, these lemmas also lack rigor (and therefore correctness) in their statement and proofs. For example, in Lemma one, the statement mentions a mysterious "if the other r_{MS,i} are large enough" (what is "large enough"), mention "by expectation at least one matrix slice.." (what does the "by expectation at least" mean?), and claim as main result "at least one matrix slice [...] will enrich the matrix" (what does "enrich" mean?). Looking at the proof of Lemma 1, it is just based on a probabilistic argument that if you randomly pick enough rows or columns in a matrix, they will hit a given subset with some probability. However, the authors seem to forget that randomly selecting rows and columns is only one step in their algorithm, and that the output of "Matrix_Slicing" is obtained by subsequent steps (maximizing inner products etc...). In conclusion, the statement of Lemma 1 is not rigorous, and its proof is also vague and not correct. More generally, this lemma and the following ones are far from providing evidence that the method proposed is likely to solve the problem.  While the method may be a heuristic approach with some empirical merit, we therefore believe that the paper is currently not ready for publication.  
This paper addresses fair representation learning, with the aim of obstructing the recovery of sensitive features from the learned representation, hence enforcing the fairness of subsequent prediction tasks.  In the setting where probability density can be estimated for sensitive groups, Fair Normalizing Flows (FNF) tries to minimize the statistical distance between group wise latent representations, thereby providing theoretical fairness guarantees.  Experimental confirm the effectiveness of FNF in fairness, transferrability, and interpretability.  The paper received extensive and in depth discussion.  The rebuttal did an excellent job in clarification.  Although there are still some concerns on the theoretical properties of the optimal solution, overall the reviewers and myself find this paper interesting and worth publishing.
This paper theoretically studies the convergence of memory based continual learning with stochastic gradient descent, and suggested several methods based on adaptive learning rates.  The reviewers appreciated the novelty of the direction, and some of them thought the experimental results are promising.  However, most reviewers (3/4) were negative. I think the main reason was the paper presentation and clarity, which they found lacking (and I agree). One reviewer thought the experimental evaluation should be improved, but there might have been some misunderstanding there. Lastly, even the positive reviewer thought the results were somewhat incremental and non surprising.  I hope the authors improve their paper and re submit.
The paper focuses on the strong adversarial attack, i.e., an attack that can generate strong adversarial examples and thus can better evaluate the adversarial robustness of given deep learning models. One review gave a score of 8 while the other 3 reviewers gave negative scores. The main issue lies in the limited experiments, as a potential substitute for AA, the proposed MM should be widely tested against different defenses, just as done in the AA paper. The writing of the paper is somehow is not rigorous including many incorrect statements and unsupported claims which should be well addressed in the revision. Thus, it cannot be accepted to ICLR for its current version.
This paper presents a neural compositional model for visual question answering.  The overall idea may be exciting but the committee agrees with the evaluation of Reviewer 1:  the experimental section is a bit thin and it only evaluates against an artificial dataset for visual QA that does not really need a knowledge base.  It would have been better to evaluate on more traditional question answering settings where the answer can be retrieved from a knowledge base (WebQuestions, Free917, etc.), and then compare with state of the art on those.
This paper studies the properties of adversarial training in the large scale setting. The reviewers found the properties identified by the paper to be of interest to the ICLR community   in particular the robustness community. We encourage the authors to release their models to help jumpstart future work building on this study.
The paper formalizes the problem of gradient leakage through a Bayesian framework. They show that existing attacks can be viewed as approximations of a Bayesian optimal adversary. The empirical results show that heuristic defences are not good against stronger attacks and that the early part of the training is particularly vulnerable. There was a lively discussion in the reviews and rebuttal and the outstanding questions of the reviewers have been answered.
This work proposes to improve Mixup by using soft labels, removing the need for input mixup. The reviewers found the paper was clear and found the experiments promising. The reviewers raised concerns about the lack of experiments comparing this approach to Mixup+Label smoothing, which were addressed during the rebuttal by the authors. However, the reviewers did not find the empirical evidence strong enough given that this is mostly an empirical contribution. The authors do not necessarily need to train on the full Imagenet, but it would be beneficial to evaluate on more standard settings on the dataset considered to facilitate comparison to previous work.
This paper offers a new method for scene generation.  While there is some debate on the semantics of ‘generative’ and ‘3d’, on balance the reviewers were positive and more so after rebuttal.  I concur with their view that this paper deserves to be accepted.
The paper presents a meta algorithm for learning a posterior inference algorithm for restricted probabilistic programs. While the reviews agree that this is a very interesting research direction, they also reveal that there are several questions still open. One reviewer points out that there learning to infer should take both the time for learning+inference and the generalization to other programs into account, i.e., what happens if the program is too different from the training set? Is benefit than vanishing? Moreover, as pointed out by another review, recursion as well as while loops are not yet supported. Also, the relation to IC needs some further clarification. These issues show that the paper is not yet ready for publication at ICLR. However we would like to encourage the authors to improve the work and submit it to one of the next AI venues.
The paper proposes an FL framework that optimizes the performance of a subset of clients. Reviewers did appreciate the value of several contributions, but unfortunately consensus is that it remains below the bar, even after the discussion phase. Concerns remained on correctness issues, motivation of assumptions, and distinction of personalized vs selfish FL, even after the author feedback.  We hope the detailed feedback helps to strengthen the paper for a future occasion.
The paper proposes a training method for generative adversarial network that avoids solving a zero sum game between the generator and the critic, hence leading to more stable optimization problems. It is similar to MMD GAN, in which MMD is computed on a projected low dim space, but the projection is trained to match the density ratio between the observed and the latent space. The reviewers raised several questions. Most of them have been addressed after several rounds of discussions. Overall, they are all positive about this paper, so I recommend acceptance. I encourage the authors to incorporate those discussions in their revised paper.
The scores of the reviewers are just far to low to warrant an acceptance recommendation from the AC.
## Description The paper discovers interesting phenomena in training neural networks with binary weights:   Connection between latent weight magnitude and how important its binarized version for the network performance  training dynamics, indicating that large latent weights are identified and stabilize early on   Observation that amongst learned binary kernel, several specific patterns prevail, up to the bits who s reversal has very little effect. This is so regardless of the architecture, the layer considered or the dataset.  The paper further demonstrates how these observations may be used to compress binary neural networks below 1 bit per weight.  ## Review Process and Decision The reviewers welcomed the experimental investigation of new phenomena, but commented the overall technical quality of the work as somewhat substandard. The redundancy of consecutive affine transforms is known and not connected to binary weights investigation. The investigation itself lacks a more in depth analysis. The proposed compression results appeared not convincing to reviewers since a significant drop of accuracy occurs. The AC shares these concerns and supports rejection.  ## General Comments From my perspective, the study undertaken is methodologically „wrong“. An ad hoc training method is investigated, which is not even clearly defined in the paper (there are many „STE“ variants) and for which it is not known what it is doing, what are the real valued weights for and whether they are needed at all (as empirically argued by Helwegen et al. (2019)). As such, the investigation makes impression of poking a black box (the training method in this case). At the same time, there are more clear learning formulations, applicable in the setting of the paper (binary weights), in particular considering the stochastic relaxation: * Shayer et al. (2017): Learning Discrete Weights Using the Local Reparameterization Trick * Roth et al. (2019): Training Discrete Valued Neural Networks with Sign Activations Using Weight Distributions * Peters et al. (2018): Probabilistic binary neural networks  These methods are approximate, but at least the optimization is well posed and it is known what do the real valued weights represent (e.g. logits of binary weight probabilities). From this perspective, it can be seen that latent weights close to 0 correspond to Bernoulli weights that are almost fully random (and thus only contribute noise) and are fragile to gradient steps. Therefore the model can only perform well if it learns to be robust to their state or their state becomes more deterministic (corresponding to large latent weight). So one would actually expect to see in these models phenomena similar to the observed in the paper and not bee too much surprised or astonished by them. Furthermore, there are recent works explaining STE and its latent weights as optimizing the stochastic relaxation: * Meng et al. (2020): Training Binary Neural Networks using the Bayesian Learning Rule * Yanush et al. (2020): Reintroducing Straight Through Estimators as Principled Methods for Stochastic Binary Networks.   The authors are encouraged to make the observed phenomena more explainable by connecting to the mentioned works.  ## Further Details  *  „We show that in the context of using batch normalization after convolutional layers, adapting scaling factors with either hand crafted or learnable methods brings marginal or no accuracy gain to final model.“  From theoretical perspective, this is obvious and known to me. Practically, there could be in principle some difference due to the learning dynamics, and verifying that there is none is a useful but a weak contribution. The section devoted to this issue can be given in the appendix but is not justified in the main paper.  * „change of weight signs is crucial in the training of BWNs“  The sign determines the binary weights, so this is by definition.  * „ Firstly, the training of BWNs demonstrates the process of seeking primary binary sub networks whose weight signs are determined and fixed at the early training stage, which is akin to recent findings on the lottery ticket hypothesis for efficient learning of sparse neural networks“  In the lottery ticket hypothesis paper it is shown explicitly that the identified sparse subnetwork changes during the learning the most rather than retains its initialization state or the state in the beginning of the training. It is therefore could be of a different nature. 
This paper investigates a speech synthesis approach that directly generates raw audios from text or phoneme inputs in an end to end fashion.  The approach first maps the input texts/phonemes into a representation sequence that is aligned with the output at a lower sampling frequency by a differentiable aligner and then upsamples the representation sequence to the full audio frequency by a decoder.  A number of techniques including adversarial training and soft DTW are applied to improve the training.  The experimental results are good. There are raised concerns from the reviewers which are mostly cleared by the rebuttal of the authors.  After the rebuttal and discussion, all reviewers are supportive on accepting the paper. 
This paper analyses linear regions in ReLU networks using a new algorithm for extracting linear terms based on the data. The reviewers found the paper to be well written with sound results. While the paper itself provides only modest evidence of the algorithm’s utility (mainly in terms of highlighting some distinctions between fully connected and convolutional networks), the algorithm and the corresponding new paradigm of exploring linear terms rather than counting regions may prove useful in future analyses. Altogether, I think this paper will interest theorists focusing on ReLU networks and I recommend acceptance.
This paper presents an approach to learn the solution operator of Markovian partial differential equations (PDEs) by combining the Fourier Neural Operator (FNO) with a hyper network.  In short, the hyper network g_\theta(t) is trained to output the weights of a FNO f_w(x), which (given an initial condition) outputs the PDE solution at the time given to the hyper network. The main claimed contributions of the proposed approach (as compared to, e.g., the original FNO architecture) are that the obtained solutions improve the learning accuracy at the supervision time points and that the solutions are able to interpolate and extrapolate to arbitrary times.    The reviewers seemed to like the idea of using hyper networks for modelling continuous time FNOs. Several issues were raised by the reviewers, e.g., with respect to related work by Li et al. (2021, https://arxiv.org/abs/2106.06898), which I believe were addressed by the authors satisfactorily. However, it is still concerning that the reported performance for FNO in, e.g., 1d Burgers does not quite match those recently published  (Kovachki et al, 2021, https://arxiv.org/abs/2108.08481, Table 2).  Although the authors did report additional results using GeLU, these results are still very different to those in  Kovachki et al ( 2021) and it is unclear whether the improved performance is due to a lack of tuning the baseline FNO. I commend the authors for, as suggested by the reviewers, running more extrapolation experiments. However, I believe the reviewers also made a point about considering (training) times much longer than 1, as even the original FNO paper did this for Navier Stokes (NS) with T 50.   Overall, the paper provides modest improvement wrt FNOs, although it does extend its capabilities to interpolation and extrapolation. The paper will also benefit from providing a brief overview of FNOs.
All reviewers agreed that this is a strong paper, that the methodological contributions are both relevant and significant, and that the experimental validation is convincing. I fully share this viewpoint!
The reviewers agree that the proposed method for reducing overconfidence in ReLU networks is novel and interesting. However, the presentation of the theoretical results is too informal and imprecise to warrant acceptance without a strong accompanying experimental section, which is unfortunately lacking. I therefore cannot recommend acceptance of the paper in its current form.
The paper studies real world ML APIs  performance shifts due to API updates/retraining and proposes a framework to efficiently estimate those shifts.  The problem is very important and the presented approach definitely novel. My concern is about limited novelty of the theoretical analysis and weak experimental evaluation (just two dates, limited number of systems tested, small number of ablations). As of now the paper looks like an interesting but unfinished proposal. Looking forward to the discussion between the authors and the reviewers to address the concerns.  In the rebuttal, the authors have addressed reviewers  comments, in particular by adding additional experiments that strengthen the paper. All the reviewers recommend the paper to be accepted. It is suggested that in the camera ready version the authors will add additional details regarding the experiments, as some of the reviewers mentioned.
This paper proposes a new RNN architecture called Dynamic RNN which is based on dynamic system identification.  Reviewers questioned the expressivity of the proposed model, practical application/impact of the proposed model, and interpretability of the proposed model. Even though the authors attempted to convince the reviewers, 3 out of 4 reviewers think that this work is not ready for publication.   Specifically, R4 recommends 5 ways to strengthen the paper. I recommend the authors to incorporate this feedback and make a stronger resubmission in the future.
This paper proposes cross iteration batch normalization, which is a strategy for maintaining statistics across iterations to improve the applicability of batch normalization on small batches of data.   The reviewers pointed out some strong points but also some weak points about the paper. The paper was judged to be novel and theoretically sound, and the paper was judged to be well written.   However, there were some doubts regarding the relevance and significance of the work. Reviewers commented on being unconvinced by the utility of the approach, it being unclear when the proposed method is beneficial, and the relative small magnitude of the empirical improvement.   On the balance, the paper seems decent but not completely convincing. This means that with the current high competitiveness and selectivity of ICLR I unfortunately cannot recommend the manuscript for acceptance. 
The paper proposes GAN regularized by Determinantal Point Process to learn diverse data samples.  The reviewers and AC commonly note the critical limitation of novelty of this paper.  The authors pointed out  "To the best of our knowledge, we are the first to introduce modeling data diversity using a Point process kernel that we embed within a generative model. "  AC does not think this is convincing enough to meet the high standard of ICLR.  AC decided the paper might not be ready to publish in the current form.
The authors present an environment for semantic navigation that is based on an existing dataset, SUNCG. Datasets/environments are important for deep RL research, and the contribution of this paper is welcome. However, this paper does not offer enough novelty in terms of approach/method and its claims are somewhat misleading, so it would probably be a better fit to publish it at a workshop. 
This is an observational work with experiments for comparing iterative pruning methods.  I agree with the main concerns of all reviewers:  (a) Experimental setups are of too small scale or with easy datasets, so hard to believe they would generalize for other settings, e.g., large scale residual networks. This aspect is very important as this is an observational paper. (b) The main take home contribution/message is weak considering the high standard of ICLR.  Hence, I recommend rejection.   I would encourage the authors to consider the above concerns as it could yield a valuable contribution.
The authors propose a graph multi domain splitting framework, called GMDS, to detect anomalies in datasets with temporal information. The reviewers agree that the paper studies an important and interesting problem but they think that the paper should be improved significantly before being accepted.  In particular, the reviewers feel that the authors should provide more technical details and insights on the design of the solution proposed and the proposed method should be compared with other(even simple) baselines for the same problem.
This paper presents an interesting idea for task free continual learning, which makes use of random graphs to represent relational structures among contextual and target samples. The reviewers agreed that the technical idea is novel, the experiments are extensive and the presentation is good. The authors addressed the reviewers  concerns in the rebuttal. I recommend to accept.
This is a fairly technical paper bridging deep learning with uncertainty propagation in computations (i.e. probabilistic numerics). It is well structured, but it could benefit from further improvements in readability given that there are only very few researchers that are experts in all sub domains associated with this work. Given the above, as well as low overall confidence by the reviewers, I attempted a more thorough reading of the paper (even if not an expert myself), and I was also happy to see that the discussion clarified important points. Overall, the idea is novel, convincing and seems well executed, with good results. The technical advancements needed to make the idea work are fairly complicated and are appreciated as contributions, because they are expected to be useful in other applications too (beyond irregular sampled data) where uncertainty propagation matters.
Based on the positive reviews, I recommend acceptance. The paper analyzes when empirical risk is close to the population version, when empirical saddle points are close to the population version and empirical gradients are close to the population version.
This paper shows local convergence results for gradient descent on one hidden layer network with Gaussian inputs and sigmoid activations. Later it shows global convergence by using spectral initialization. All the reviewers agree that the results are similar to existing work in the literature with little novelty. There are also some concerns about the correctness of the statements expressed by some reviewers. 
The paper develops an instance of physics informed neural network inspired from multigrid methods for solving PDEs. The proposed framework describes the solution of a PDE problem as the sum of terms operating at different resolutions. Training is performed by an iterative optimization algorithm that alternates between the different resolution models. Experiments are performed on 1D and 2D problems.  All the reviewers agree on the originality and the potential of the proposed method. They however all consider that the current version of the work is too preliminary both in the form and in the content. The experimental contribution should be developed further with tests performed on more complex problems and complementary analyses. Some of the claims should be given more evidence or moderated. It also appeared during the discussion that the models are not well tuned, making the results inconclusive. The authors are encouraged to develop and strengthen their work.
The paper presents a method for cooperative ad hoc collaboration by learning latent representations of the teammates. The method is evaluated in three domains. All the reviewers agree that the method is novel and adds an interesting contribution to the important and difficult problem of the ad hoc collaboration, making fewer assumptions about the team and the teammates.  The next version of the paper should comment:    On the societal impact of the centralized training.   Wang et al, CoRL 2020, https://arxiv.org/abs/2003.06906, which addresses the cooperative tasks in the ad hoc teams without privileged knowledge and assumptions about the teammates.
The paper investigates the trainability and generalization of deep networks as a function of hyperparameters/architecture, while focusing on wide nets of large depth; it aims to characterize regions of hyperparameter space where networks generalize well vs where they do not; empirical observations are demonstrated to support theoretical results. However, all reviewers agree that, while the topic of the paper is important and interesting, more work is required to improve the readability and clarify the exposition to support the proposed theoretical results.  
The paper presents  CuBERT (Code Understanding BERT), which is BERT inspired pretraining/finetuning setup, for source code contextual embedding. The embedding results are tested on classification tasks to demonstrate the effectiveness of CuBERT.   This is an interesting application paper that extends existing models to source code analysis. The authors did a good job at motivating the applications, describing the proposed models and discussing the experiments. The authors also agree to share all the datasets and source code so that the experiment results can be replicated and compared with by other researchers.   One major concern is the lack of strong baselines. All reviewers are concerned about this issue. The paper could lead to a good publication in the future if the issues can be addressed. 
The paper gives an bilevel optimization view for several standard RL algorithms, and proves their asymptotic convergence with function approximation under some assumptions.  The analysis is a two time scale one, and some empirical study is included.  It s a difficult decision to make for this paper.  It clearly has a few things to be liked: (1) the bilevel view seems new in the RL literature (although the view has been implicitly used throughout the literature); (2) the paper is solid and gives rigorous, nontrivial analyses.  On the other hand, reviewers are not convinced it s ready for publication in its current stage: (1) Technical novelty, in the context of published works: extra challenges needed on top of Borkar; similarity to and differences from Dai et al.; ... (2) The practical significance is somewhat limited.  Does the analysis provide additional insight into how to improve existing approaches?  How restricted are the assumptions?  Are the online vs batch distinction from Dai et al. really important in practice? (3) What does the paper want to show in the experiments, since no new algorithms are developed?  Some claims are made based on very limited empirical evidence.  It d be much better to run algorithms on more controlled situations to show, say, the significance of two timescale updates.  Also, as those algorithms are classic Q learning and actor critic (quote the authors in responses), how well do the algorithms solve the well known divergent examples when function approximation is used? (4) Presentation needs to be improved.  Reviewers pointed out some over claims and imprecise statements.  While the author responses were helpful in clarifying some of the questions, reviewers felt that the remaining questions needed to be addressed and the changes would be large enough that another full review cycle is needed.
This paper presents a new graph neural network (GNN) architecture with attention and with applications to Boolean satisfiability.  The reviewers expressed concerns over various aspects of the paper such as a need for better ablations and an analysis of the difficulty level of the SAT problems used in evaluation.  No rebuttal was provided.
The submission proposes a novel solution for minimax optimization which has strong theoretical and empirical results as well as broad relevance for the community. The approach, Follow the Ridge, has theoretical guarantees and is compatible with preconditioning and momentum optimization strategies.  The paper is well written and the authors engaged in a lengthy discussion with the reviewers, leading to a clearer understanding of the paper for all. The reviews all recommend acceptance. 
All reviewers agree that the authors have done a great job identifying weaknesses with the current SOTA in super resolution.   However, there is also agreement that the proposed approach may be too simple to accurately capture a range of real camera distortions, and more comparisons to the SOTA are needed.   While this paper certainly has merits and opens the door for strong work in the future, there is not enough support to accept the paper in its current form.
The paper proposes a method for using multiple word embeddings in structured prediction tasks. The reviewers shared the concerns that the method seems rather specific to this use case and the empirical improvements do not justify the complexity of the approach. They also questioned the definition of the method as "architecture search" vs a particular ensembling method. Finally, I think the authors should provide more discussion of why using all the embeddings (in the sense of bias variance tradeoffs).  
The authors propose self predictive representations (predicting the agents own future latents of a forward model with data augmentation) as a means of improving the data efficiency of agents. The reviewers found the paper to be compelling (after the authors made adjustments) and pointed out that the method is likely generic and might be widely applicable. Experimental improvements in the work are significant, and the method is well explored.
This paper tackles the contextual bandit problem with general function classes and introduces a novel algorithm called regularized optimism in face of uncertainty (ROFU). Although this is an important and relevant problem, the theoretical contribution is rather weak due to the strong assumptions, which also results in a lack of consistency with the motivation and the empirical settings. Moreover, although experimental results suggest that the proposed ROFU method may have potential, the empirical contribution is unclear as the paper currently lacks a comparison with appropriate previous work. All these concerns were raised in the reviews, but unfortunately, none were addressed in the rebuttal phase.
This paper  * adheres to the Bayesian interpretation of MC dropout and applies it to Transformer based NMT, thus approximately sampling from the NMT model s posterior predictive distribution $Y_*|x_*, \mathcal D$ * as the NMT predictive distribution is over a discrete sample space, the authors compute variance of pairwise comparisons between the translation and other candidate outputs in a beam of likely translations (the authors call this BLEUVar).  Whereas the work is potentially interesting it does not seem ripe for publication. Here are some of the issues I d like to highlight:  1. OOD detection. Detection in input space seems like a natural baseline. The authors argue that OOD detection in output space takes the downstream task into consideration, but going through the conditional also makes the task considerably more difficult and computationally challenging. Though we appreciate the author s point, we don t see it as a good enough reason to discard OOD detection in input space as a serious alternative.   2. Why BDL? The motivation for Bayesian methods is clear, but BDL can at best *approximate* Bayesian reasoning, thus the question does deserve an answer. The reviewers asked for experiments that demonstrate empirically the relevance of the Bayesian formulation, for example, one reviewer suggested to compute BLEUVar in the frequentist case, and that makes perfect sense. Consider this: $q(\theta)$ likely under estimates posterior uncertainty, so let s say that $\operatorname{Var}(\theta|\mathcal D)$ is rather small, then BLEUVar as presented is in fact not capturing posterior predictive uncertainty (due to entropy of $Y_*|x_*, \mathcal D$), but rather sampling uncertainty (due to entropy of $Y_*|x_*, \theta$).   3. Unrealistic experiments: we all agreed that the experiments are weak. For example, we do not share the authors  excitement for the results around a foreign language as an example of OOD data point, we see it as an artificially simple case. We also expected more interesting cases of mixed domain data sets (for ideas, check tasks within WMT and IWSLT, as well as resources such as Opus and low resource language pairs as those in FLORES) and more generally different levels of noise (e.g., synthetic data produced by other translation engines, round trip/back translations are very typical in low resource settings).   Additional remarks/suggestions: * in my personal view, BLEUVar should *not* be based on biased statistics (beam search introduces all sorts of unknown biases); the pairwise comparison mechanism behind BLEUVar is similar to what MT researchers call minimum Bayes risk decoding (a frequentist criterion for making decisions under uncertainty). * we do believe the setting explored in this paper *is* related to confidence estimation, and even though I agree with the authors that a direct comparison is not per se needed, CE datasets could still prove useful for evaluation;  Though the paper has been appreciated for it dispenses with quality annotation, for it attempts to quantify estimation uncertainty (or epistemic, if the authors prefer) rather than sampling uncertainty (or aleatoric), and for other technical contributions (such as BLEUVar), we think this paper needs more than subtle/careful positioning, it really needs to acknowledge the relevance of certain alternatives and evaluate against them (BDL need not win every comparison, that s not so much the issue, the issue is that the current picture is too incomplete).   A final (personal) remark. I noticed the exchange regarding the suitability of the paper to an ML (vs NLP) venue. I personally do not think your submission is more or less appropriate to one or the other on the grounds of its technical content. The expert reviews attached suggest enough ideas for improvements, and I would imagine an improved version of the paper having a good chance at any major ML (or NLP) venue. 
This paper is a clear reject. The paper is very poorly written and contains zero citations. Also, the reviewers have a hard time understanding what the paper is about.
This paper extends adversarial imitation learning to an adaptive setting where environment dynamics change frequently. The authors propose a novel approach with pragmatic design choices to address the challenges that arise in this setting. Several questions and requests for clarification were addressed during the reviewing phase. The paper remains borderline after the rebuttal. Remaining concerns include the size of the algorithmic or conceptual contribution of the paper.
The paper presents a method for continual learning with a variant of VAE. The proposed approach is reasonable but technical contribution is quite incremental. The experimental results are limited to comparisons among methods with generative replay, and experimental results on more complex datasets (e.g., CIFAR 100, CUB, ImageNet) are missing. Overall, the contribution of the work in the current form seems insufficient for acceptance at ICLR.
Three experts reviewed this paper and all recommended acceptance. The reviewers liked that the work addressed a common problem in prior related work that it is hard to quantitatively evaluate slide discovery methods. Moreover, the proposed method achieves superior performance over prior arts. Based on the reviewers  feedback, the decision is to recommend the paper for acceptance. The reviewers did raise some valuable concerns, such as paper clarity, significance of the textual descriptions, that should be addressed in the final camera ready version of the paper. The authors are encouraged to make the necessary changes to the best of their ability. We congratulate the authors on the acceptance of their paper!
The authors propose a simple modification of local SGD for parallel training, starting with standard SGD and then switching to local SGD. The resulting method provides good results and makes a practical contribution. Please carefully account for reviewer comments in future revisions.
the authors propose to use volume coding to enable uniform sampling from an implicit latent space to be used together with a autoregressive language model. all the reviewers find this approach interesting, but all found that the submission would be much stronger with more thorough evaluation. in particular, i noticed that the reviewers wanted to see how the proposed ariel works in comparison to e.g. VAE on a more diverse set of benchmarks, since the choice of two datasets, one synthetic and one small, narrow domain, is somewhat limited largely due to their relative simplicity. furthermore, the reviewers were unsure whether various evaluation metrics the authors have used are exhaustive nor appropriate to demonstrate the efficacy of Ariel or to put the proposed approach correctly in the context of other approaches. i agree with the reviewers on both of these points.  i m thus recommending this manuscript be rejected, and strongly recommend the authors give a bit more thoughts on how to demonstrate the effectiveness of the proposed approach in the context of other approaches and the problem of sentence generation (which is the main problem the authors claim to tackle, as the title directly suggests.) with a better planned experiment and analysis, i believe the authors  efforts will have significant impact.
This paper introduces the problem of overlearning, which can be thought of as unintended transfer learning from a (victim) source model to a target task that the source model’s creator had not intended its model to be used for. The paper raises good points about privacy legislation limitations due to the fact that overlearning makes it impossible to foresee future uses of a given dataset.  Please incorporate the revisions suggested in the reviews to add clarity to the overlearning versus censoring confusion addressed by the reviewers.
The paper proposed locally free weight sharing strategy (CafeNet) for searching optimal network width. The proposal is a nice tradeoff between manually fixed weight sharing pattern (too small search space) and completely free weight sharing pattern (too large search space). The *originality* and *significance* are clearly above the bar. The paper is related to the general interests of deep learning research and its *applicability* deserves a spotlight presentation.  It seems the *clarity* can still be improved, so please carefully revise the paper following the reviews. BTW, I am very curious, why "locally free weight sharing strategy" goes to a short name CafeNet? I went over the paper but I didn t find the answer. Perhaps the name of the proposal should also be explained...
This paper proposes practical improvements to theoretically well founded QTRAN, which is a state of the art technique of cooperative multi agent reinforcement learning.  The improvements include new designs of loss function and action value estimator, which might be widely applicable beyond QTRAN.  However, it is not obvious if the proposed improvements actually improves the performance of QTRAN, and experimental evaluation is essential to this work.  After the discussion, there remain some major concerns about the experimental results.  In particular, the performance of baselines in the experiments is not consistent with those reported in the prior work.
The submitted paper contains interesting theoretical insights into common approaches for exploration and proposes a new way for deriving intrinsic rewards for exploration which is evaluated in several benchmark environments. While all reviewers appreciate these aspects, there are concerns about whether the paper is ready for publication. In particular, the authors’ response did not clarify all open questions and concerns (although the authors already improved the paper a lot by updating the submitted paper according to recommendations/questions of the reviewers). After discussions and author feedback, 3 knowledgable reviewers suggest (weak) rejection of the paper and 1 reviewer suggested acceptance of the paper. Considering this, I recommend to reject the paper but I would like to encourage the authors to consider the comments of the reviewers to revise their paper accordingly, as I expect the paper to then turn into a strong and impactful one.
Strengths:  The paper presents an alternative regularized training objective for supervised learning that has a reasonable theoretical justification.  It also has a simple computational formula.  Weaknesses: The experiments are minimal proofs of concept on MNIST and fashion MNIST, and the authors didn t find an example where this formulation makes a large difference.  The resulting formula is very close to existing methods.  Finally the paper is a bit dense and the intuitions we should gain from this theory aren t made clear.  Points of contention: One reviewer pointed out the close connection of the new objective to IWAE, and the authors added a discussion of the relation and showed that they re not mathematically equivalent.  However, as far as I can tell they re almost identical in purpose:  As k  > \infty in IWAE, the encoder ceases to matter.  And as M  > \infty in VDB, we take the max over all encoders.  Could the method proposed in this paper lead to an alternative to IWAE in the VAE setting?  Consensus: Consensus wasn t reached, but the "7" reviewer did not appear to have put much though into their review.
This paper presents an interesting method for creating adversarial examples using a GAN.  Reviewers are concerned that ImageNet Results, while successfully evading a classifier, do not appear to be natural images.  Furthermore, the attacks are demonstrated on fairly weak baseline classifiers that are known to be easily broken.  They attack Resnet50 (without adv training), for which Lp bounded attacks empirically seem to produce more convincing images.  For MNIST, they attack Wong and Kolter’s "certifiable" defense, which is empirically much weaker than an adversarially trained network, and also weaker than more recent certifiable baselines. 
While there was some support for the ideas presented, unfortunately this paper was on the borderline. Significant concerns were raised as to whether the setting studied was realistic, among others.
The authors introduce a method to automatically generate a learning curriculum (of goals) in a sparse reward RL setting, examining several criteria for goal setting to induce a useful curriculum.  The reviewers agreed that this was an exciting research direction but also had concerns about baseline comparisons, clarity of some technical points, hyperparameter tuning (and the effect on the strength of empirical results), and computational tractability.  After discussion, the reviewers felt most of these points were sufficiently addressed.  Thus, I recommend acceptance at this time.
This paper proposes a method for offline reinforcement learning methods with model based policy optimization where they first learn a model of the environment to learn the transition dynamics, a critic and the policy in an offline manner. They basically learn the model by training an ensemble of probabilistic dynamics models represented by neural networks that output a diagonal Gaussian distribution over the next state and reward. Then they use the covariance of the probabilistic dynamics model to get an uncertainty measure that they incorporate into the  reward when training it with the AWAC.  There were two main concerns raised by the reviewers:  1) Experiments: As pointed out by the reviewers, the experimental gains don t look very convincing. In particular, the performance of AWAC looks bad, and MB2PO doesn t give much gains on top of it. It is not clear how much better the proposed method is doing on the tasks that it does well, without any confidence intervals or variance measures provided.  2) Novelty:  This is almost a trivial combination of two existing ideas: model based policy optimization and AWAC. It is not clear how useful this particular combination is in practice, and it seems like there is not much insights gained from it.   I think better motivations, further ablations and more empirical analysis to understand the proposed model better. For example, analyzing the types of behaviors learned or how calibrated the uncertainty estimates that is incorporated into the reward is or some hyperparameter sensitivity analysis would make the paper more interesting.  As it stands right now, I am suggesting to reject this paper. I hope the authors will improve the paper for the future...  
The paper is not overly well written and motivated. A guiding thread through the paper is often missing. Comparisons with constrained BO methods would have improved the paper as well as a more explicit link to multi objective BO. It could have been interesting to evaluate the sensitivity w.r.t. the number of samples in the Monte Carlo estimate. What happens if the observations of the function are noisy? Is there a natural way to deal with this? Given that the paper is 10+ pages long, we expect a higher quality than an 8 pages paper (reviewing and submission guidelines). 
The paper presents an approach to weak supervision to address the possibly low coverage of rule based labeling functions, by assigning similar labels to similar instances (where the similarity is computed in feature space).  The reviewers main concerns were the presentation, as well as the experimental protocol and results. Several directions for improvement have been identified by the reviewers and acknowledged by the authors, but in the current state of the submission the consensus is that the paper is not ready for publication.
This paper takes inspiration from Global Workspace Theory to propose a modification for attention based network architectures. This is exemplified both in transformer models and in recurrent models (RIMs). The key idea is to replace the quadratic, pairwise communication between "specialist" units (which in transformers corresponding to the positions) by a higher order communication model which consists in a competitive, sparse writing step into a shared workspace, followed by a reading step where information is broadcasted from the global workspace to all specialists. The competitive writing step establishes a limited bandwidth channel for this communication which encourages specialization.  The reviewers agree that this is an interesting and very well written paper which unifies several existing ideas. The main contribution of this paper is in establishing a connection to GWT which may inspire future research to keep developing these ideas. The experiments on relatively small tasks (but challenging ones) provide a good proof of concept. Some concerns pointed out by some of the reviewers include a certain overstatement of the capabilities of the proposed model, as well as lack of experiments that scale up the model to larger and unstructured datasets. The authors replied with additional experiments included in the appendix, which in my opinion address these concerns convincingly.   Overall, this is a strong paper and I recommend acceptance. I encourage the authors to take into account the reviewer s suggestions in the final version. I also think that the connection to related work could be improved, as there is several related works [1, 2, 3] which asks/investigates similar questions to this paper and should probably be acknowledged:   The "shared global workspace" of this paper (Transformer + SW) is reminiscent of the Star Transformer [1], as well as other more recent works which use special units (e.g. CLS tokens) to encode "global" representations. While that work does not include the competitive component (the "bottleneck"), I think it should be acknowledged.   Variants of transformers with competition among specialists via sparsity have also been proposed, e.g. adaptively sparse transformers [2]. That framework is an alternative to top k softmax used in this paper.   Empirical studies which analyze the redundancy among specialists (in this case attention heads) and propose strategies to prune them have also been made by [3].   [1] https://arxiv.org/abs/1902.09113 [2] https://arxiv.org/abs/1909.00015 [3] https://arxiv.org/abs/1905.09418   Minor point: "Hence unlike pairwise interaction, messages passed among neural modules in the shared workspace setting also include HO interaction terms"   I believe higher order interaction happens too every two layers with pairwise interaction. Perhaps this should be clarified.
This paper proposes an amortized proximal optimization method to adapt optimization hyperparameters. Empirical results on many problems are performed.   Reviewers overall find the ideas interesting, however there still are some questions whether strong baselines are used in the experimental comparisons. The reviewers also point that the theoretical results are not useful ones since the assumptions are not satisfied in practice. One of the reviewer increased their score, but the other has maintained that the paper requires more work.  The presentation of the result is also a bit problematic; the font sizes in the figure are too small to read.  The paper contains interesting ideas, but it does not make the bar for acceptance in ICLR. Therefore I recommend a reject. I encourage the authors to resubmit this work after improving the presentation and experiments. 
The reviewers reached a consensus that the paper is not ready for publication in ICLR. (see more details in the reviews below. )
The work proposes to learn neural networks using a homotopy based continuation method. Reviewers found the idea interesting, but the manuscript poorly written, and lacking in experimental results. With no response from the authors, I recommend rejecting the paper.
Three of four reviewers rated this paper as an 8.  These positive reviewers felt that this paper provided a lot of value through extensive experimentation with MAML in the few shot setting. It was felt that the detailed analysis of the inner and outer loop of MAML provided a lot of understanding to the reader regarding the behaviour of MAML. The fourth reviewer giving a score of 3 remains concerned about high variance on some experiments. However the strength of ratings from the other reviewers make the AC more than comfortable giving an accept recommendation for this work.
The overall impression on the paper is rather positive, however, even after rebuttal, it still seem that the paper requires further work and definitely a second review round before being ready for publication. Thus, I encourage the authors to continue with the work started during the rebuttal to address the reviewers  comment, which although moved in the right direction would still benefit from further work.  Especially, I believe the experiments could be significantly improved (by for example bringing some results to the main paper). Moreover, a more thorough comparison theoretically and empirically with previous work would increase the impact of the paper. 
The authors introduce a method that improves goal conditioned supervised learning (GCSL) by iteratively re weighting the experience by a variable that correlates with the number of steps till the desired goal. The reviewers mention that the authors focus on an important problem, their method is simple and the empirical results are significant. However, they do point several flaws of the paper, the main ones being questionable theoretical claims and the clarity of the presentation. After an extensive discussion, most reviewers agree that the paper should be accepted but I do encourage the authors to take into account the comments by the reviewers for the final version of the paper and make the theory more clear.
The main strength of the paper is to provide a clear mathematical characterization of invertible neural networks. The reviewers and the AC also note potential weakness including 1) the exposition of the paper can be much improved; 2) it s unclear how these analyses can help improve the training algorithm or architecture design since these characterizations are likely not computable; 3) the novelty compared to previous work Carlsson et al. 2017 may not be enough for ICLR acceptance. These weakness are considered critical issues by the AC in the decision. 
*Summary:* Study generalization in kernel regression discussing the NTK case and experiments on finite width nets.   *Strengths:*    Mix of theoretical and empirical results in an important topic.    Advances a promising recent line of work.   *Weaknesses:*    Concerns about novelty and lack of comparison with existing works.    Concerns about insufficient contextualization of new notion of learnability.    Concerns about scope of results in relation to claims.   *Discussion:*   Reviewer gb7t (3) found their concerns about lack of novelty and comparison with prior works not sufficiently addressed in the authors’ responses. 7tiq (6) found the line of investigation promising, but also issues with presentation and found the theoretical results incremental. Mosm (5) finds that the theoretical part pertaining kernels does not offer much novelty and that the paper should have focused on the empirical study that links the NTK spectrum to generalization. q2g8 (8) confidently considers this a good paper. In their view it provides a nice theoretical analysis of generalization in the setting of kernel regression and the metric of learnability intuitive. However, they also found that the detests of the experiments are very artificial and problematic the desire of the article to extend the regime of the results to make claims about deep learning.   A the end of the discussion period, the official reviewer ratings are mixed 3,5,6,8, indicating various strengths and weaknesses (also in case of the most favorable reviews). From the reviews and discussion, I infer that the topic is worthwhile and relevant, but at the same time that the paper might not be sufficiently convincing in its current form. Therefore I lean to reject the paper. To arrive at a clear conclusion, I consulted two additional researchers.   *Additional assessment 1:*   The first additional assessment found the work  underwhelming  but admitted there is a chance they might not have fully understood the work.   *Additional assessment 2:*    The second additional assessment provided following comments: The paper s first contribution (conservation law), I didn t see it elsewhere but I think it s quite expected. The testing performance of low frequency target functions and high frequency target functions are averaged. Thus the average performance is constant which is independent on the kernel. However, in practice, kernel learning performs well because real target functions have low frequency. And the very high frequency functions are unrealistic.   About the paper s second contribution, I think the paper needs to explain how the result is different from Bordelon et al. (2020). I noticed that the method they use is different but the result seems quite similar. The paper also consider noiseless case and gives an approximation for MSE.   Also the paper should explain more about the approximation being used. For example, how much error the approximation introduce and how the approximation is different from Bordelon s approximation. I see that in the proof Φ is approximated by a matrix where each element is standard Gaussian. For me I can t understand why the approximation is reasonable.   I read through the reviews and rebuttals. I didn t see the discussion of the issue of approximation. But I think it s a major issue and the approximation is a very strong assumption. The appendix states: "we have made an approximation using the central limit theorem assuming that Φ is random with entries sampled i.i.d. from N (0, 1)". Here Φ is the matrix of eigenfunctions. Hence it is not clear how to apply the central limit theorem.   *Conclusion:*   I conclude that although the paper presents some interesting ideas on a relevant subject, it still has much room for improvement. Hence I recommend to reject this article. I encourage the authors to revise taking the above comments into consideration.
The paper provides a method to edit trained models, meaning fix mistakes in a local way so as to not ruin generalizability. The techniques provided in the paper allow for an efficient way that makes this task possible for very large models. There is an overall concensus that the problem of model editing in general is an important one and that solutions such as naive finetuning are not applicable for various reasons. In addition, the reviewers are convinced that given the need for an ML based approach for large models, this technique is superior to previous work, and mostly appreciate the novelties of the paper.  A major concern raised regards possible simpler baselines. There is a potential baseline of implementing an “engineering trick” that will simply memorize the data points where the original model was mistaken, either in their original form or as embeddings, and during inference will override its output. I tend to agree that a comparison with such a baseline would improve the paper. This being said, the discussion highlighted that this baseline has several flaws that make it clear that it cannot completely replace the method proposed here. A naive implementation of it will be “too local” and would not handle simple rephrasing of sentences. An implementation operating on the embedding space will be possible only in a subset of tasks.   To conclude, although the paper has room for some improvement (that might actually be possible towards the camera ready version), I believe that even without it the paper is in a good enough state to be published. It tackles an important problem and could lead to further advancements.
The paper presents a new dataset for multimodal QA that is deemed interesting, relevant and well executed by all reviewers. Multimodality in NLP (QA included) is an increasingly important topic and this paper provides a potentially impactful benchmark for research in it. All reviewers acknowledge that.  We hence recommend to accept this paper as a poster. We recommend the authors to further improve the draft before camera ready by using the recommendations made by the reviewers with a particular focus on an extended discussion wrt prior work on VQA and other. The paper should also add more precisions on the license(s) related to the images used in the dataset. 
This paper trains a transformer to extrapolate learning curves, and uses this in a model based RL framework to automatically tune hyperparameters. This might be a good approach, but it s hard to know because the experiments don t include direct comparisons against existing hyperparameter optimization/adaptation techniques (either the ones based on extrapolating training curves, or standard ones like BayesOpt or PBT). The presentation is also fairly informal, and it s not clear if a reader would be able to reproduce the results. Overall, I think there s significant cleanup and additional experiments needed before publication in ICLR. 
The paper studies the amount of over parameterization needed for a quadratic 2 /3 layer neural network to memorize a separable training data set with arbitrary labels. While the reviewers agree that this paper contains interesting results, the review process uncovered highly related prior work, which requires a major revision to put the current paper into perspective and generally various clarifications. The paper will benefit from a revision and resubmission to another venue, and is in its current form not ready for acceptance at ICLR 2020.
This paper proposes distributionally robust optimization (DRO) to learn robust models that minimize worst case training loss over a set of pre defined groups. They find that increased regularization is necessary for worst group performance in the overparametrized regime (something that is not needed for non robust average performance).  This is an interesting paper and I recommend acceptance. The discussion phase suggested a change in the title which slightly overstated the paper s contributions (a comment which I agree with). The authors agreed to change the title in the final version.  
The reviewers brought up many important concerns about this paper. On the positive side, the understanding of data augmentation is an important topic in deep learning, having good theoretical results is interesting here , and the experiments seem to do an okay job of backing up the theory. On the negative side, presentational issues make the paper difficult to follow and mischaracterize the results. A major issue is that some of the assumptions are hidden in the appendix and are not stated formally, and other assumptions are stated in a much weaker form, then made suddenly stronger when the theorems are stated. For example, Assumption 2 as stated holds trivially for any dataset as long as the possible data augmented versions any two different examples are disjoint (just choose the discrete metric on the images of the examples under the data augmentation function); however, in every theorem that uses A2, the distance chosen is restricted to be the L1 norm. Other Assumptions are stated strangely: for example, A1 says "i.e., for any $a_1(), a_2() \in A$, $a_1(x_1) ⫫ a_1(x_2)$ for any $x_1, x_2 \in X$ that $x_1 \ne x_2$. But what is the point of introducing $a_2$ if it s never used in the formula? And what is the meaning of the symbol ⫫? Normally, this is used for conditional independence, but there aren t any random variables in this formula ($a_1$ and $a_2$ are defined as just functions, not random functions, and $x_1$ and $x_2$ are just examples and aren t random variables either). This paper will be much stronger with these presentational issues cleared up.
The authors present a new benchmark for architecture search. Reviews were somewhat mixed, but also with mixed confidence scores. I recommend acceptance as poster   and encourage the authors to also cite https://openreview.net/forum?id HJxyZkBKDr
This paper received 4 unanimous accept (including 1 marginal accept). This well written and clear paper clarifies the relationship between transformers and a recent exciting model of the medial temporal lobe in neuroscience. There was some clarifications requested by the reviewers that were addressed during the revision. This paper will make a great computational neuroscience contribution to this year ICLR!
This paper studies RL with low switching cost under the deep RL setting. It provides new heuristics for doing so. The reviewers are worrying about whether the problem is important in practice, whether the policies obtained can be used in practice, and the theories might not be strong enough. The paper can be strengthened if better theory and more experiments are provided.  
The paper proposes a modification of the Dreamer method to account for safety constraints. There is agreement among the majority of reviewers that the paper lacks in novelty (with respect to Dreamer), in the safety analysis, and in convincing experiments. I encourage the authors to take the detailed reviews into account when improving their work.
This paper is on active deep learning in the setting where a label hierarchy is available for multiclass classification problems: a fairly natural and pervasive setting. The extension where the learner can ask for example labels as well as a series of questions to adequately descend the label hierarchy is  an interesting twist on active learning.  The paper is well written and develops several natural formulations which are then benchmarked on CIFAR10, CIFAR100, and Tiny ImageNet using a ResNet 18 architecture.  The empirical results are carefully analyzed and appear to set interesting new baselines for active learning. 
This paper shows that (under some parameter range) graph convolutional networks learns communities in the stochastic block model. The result is clean, the proof techniques rely on partitioning neurons of three types and seems applicable to more general settings. The reviewers agree that the main theorems are interesting. There are some concerns among reviewers about the presentation of the paper, but many of them seem to be already addressed in the revised version, and I would recommend the authors to continue to improve the writing. There are also some concern about experiments, but the experiments are mostly used to validate the theorems, so clarifying how they are related would suffice. Overall the paper seems to have an interesting theoretical result on GCN.
This paper proposes an approach for learning a sparsifying transform via a set of nonlinear transforms at learning time.  The presentation needs a lot of work.  The original paper was 17 pages long and very difficult to understand.  The revised paper is 12 pages long, which is still too long for the content.  The paper needs to better distinguish between the major and minor points.  It is still too difficult to judge the contribution.
This paper theoretically analyzes the use of an oracle to predict various quantities in data stream models.  Building upon Hsu et al., (2019), the overriding goal is to examine the degree to which such an oracle is can provide memory and time improvements across broad streaming regimes.  In doing so, optimal bounds are derived in conjunction with a heavy hitter oracle.  Although the rebuttal and discussion period did not lead to a consensus in the scoring of this paper, two reviewers were highly supportive.  However, the primary criticism from the lone dissenting reviewer was based on the high level presentation and motivation, and in particular, the impression that the paper read more like a STOC theory paper.  In this regard though, my belief is that the authors can easily tailor a revision to increase the accessibility to a wider ICLR audience.
The authors propose to overcome challenges in GAN training through latent optimization, i.e. updating the latent code, motivated by natural gradients. The authors show improvement over previous methods.  The work is well motivated, but in my opinion, further experiments and comparisons need to be made before the work can be ready for publication.  The authors write that "Unfortunately, SGA is expensive to scale because computing the second order derivatives with respect to all parameters is expensive" and further "Crucially, latent optimization approximates SGA using only second order derivatives with respect to the latent z and parameters of the discriminator and generator separately. The second order terms involving parameters of both the discriminator and the generator – which are extremely expensive to compute – are not used. For latent z’s with dimensions typically used in GANs (e.g., 128–256, orders of magnitude less than the number of parameters), these can be computed efficiently. In short, latent optimization efficiently couples the gradients of the discriminator and generator, as prescribed by SGA, but using the much lower dimensional latent source z which makes the adjustment scalable."  However, this is not true. Computing the Hessian vector product is not that expensive. In fact, it can be computed at a cost comparable to gradient evaluations using automatic differentiation (Pearlmutter (1994)). In frameworks such as PyTorch, this can be done efficiently using double backpropagation, so only twice the cost.  Based on the above, one of the main claims of improvement over existing methods, which is furthermore not investigated experimentally, is false.   It is unacceptable that the authors do not compare with SGA: both in terms of quality and computational cost since that is the premise of the paper. The authors also miss recent works that successfully ran methods with Hessian vector products: https://arxiv.org/abs/1905.12103 https://arxiv.org/abs/1910.05852
R3 summarizes the reasons for the decision on this paper: "The universal learning algorithm approximator result is a nice result, although I do not agree with the other reviewer that it is a  "significant contribution to the theoretical understanding of meta learning," which the authors have reinforced (although it can probably be considered a significant contribution to the theoretical understanding of MAML in particular). Expressivity of the model or algorithm is far from the main or most significant consideration in a machine learning problem, even in the standard supervised learning scenario. Questions pertaining to issues such as optimization and model selection are just as, if not more, important. These sorts of ideas are explored in the empirical part of the paper, but I did not find the actual experiments in this section to be very compelling. Still, I think the universal learning algorithm approximator result is sufficient on its own for the paper to be accepted."
The reviewers and AC all find the presented approach interesting and promising.   However, as pointed out in the reviews and as the authors recognized, the strongly convex + smooth objective setting considered is limited. Given the prevalence of non convex settings in many practical applications and the rich related literature on the analysis of SGD and variants in the non convex setting,  it would be highly desirable to (i) consider experiments on small NN architectures (since the method cannot accommodate larger architectures)  to gain some understanding of the value of the approach and (ii)  to try and extend the present analysis to the non convex case.   It would also be valuable to perform experiments illustrating the impact of theta indicated by the theory.
All the reviewers are in favor of accepting this paper, which demonstrates both theoretically and empirically the value of reward randomization in solving multi agent reinforcement learning problems. The rebuttal phase was crucial in improving the quality and evaluation of the submission. I am glad to recommend acceptance.
This paper received 4 quality reviews. The rebuttal and discussions were effective. All reviewers raised their ratings after the rebuttal. It finally received 3 ratings of 8, and 1 rating of 5. The AC concurs with the contributions made by this work and recommend acceptance.
This paper introduces an architecture that uses pooling regions and eye movements to sequentially build up an object representation.  A  confidence threshold is used to allow recognition in less time for easier images.  There was a lot of disagreement on this paper.  Those in favor argued  that it is a worthy endeavor to explore new biologically motivated architectures and foveated eye movements are an important aspect of   human vision that is worth exploring for computer vision.  Another pro was the improved robustness to some adversarial attacks.  Those arguing for not accepting the paper, argued that classification performance is not improved over SOTA and that more ablation studies  should be done to better understand the role and importance of the various aspects of the model and how they differ from other architectural designs with dilated convolutions instead of the  foveation module.   I agree that more ablation studies would be useful to better understand the role of the different model components. While I  feel that this novel sequential processing algorithm is worth publishing to increase activity in this area, I feel it would be best received after further  studies help clarify the importance of different aspects of the model. I recommend resubmission after further analysis.
The paper is a premature submission that needs significant improvement in terms of conceptual, theoretical, and empirical aspects.
The paper proposed a *novel* methodology for protecting personal data from unauthorized exploitation for training commercial models. The proposal is conceptually *intuitive* and technically *motivated*. It goes to the opposite direction of adversarial training: by adding certain error minimizing noise (rather than error maximizing noise) to the data, the model is fooled and believes there is nothing to learn from the data, and thus this can protect the data from being used for training. The paper is of not only *high quality* but also *broad interest* given the current social concerns about personal data privacy. I think its potential impact should get it a spotlight presentation.
The scores for this paper have been borderline, however the decision has been greatly facilitated by the participation of the authors and reviewers to the discussion and, more importantly, by active private discussion among reviewers and AC. Specifically, from the private discussion it seems that the reviewers find interesting ideas in this paper, but are overall are not entirely convinced about its significance, at least in the way the paper is currently positioned and motivated.   More specifically, the reviewers found the main idea of using inducing weights interesting, both technically (e.g. associated sampling scheme) but also in terms of application (sparsity). The results are insightful from a theoretical perspective. That is, the inducing weights and their treatment does seem to result in interesting and potentially useful statistical properties for the model. On the other hand, it is important to note that the high level idea of variational inducing weights, with usage of matrix normals in this setting, as well as connection to deep GPs has been studied before, as pointed out by R2 (refs [1,2]). Furthermore, even after discussions the motivation is still not entirely convincing, especially in conjunction with the experiments. Although various interesting ideas exist in the paper, both R2 and R3 in particular remain unconvinced about what is the main benefit (e.g.  pruning or runtime efficiency) stemming out of the proposed idea. Another reviewer agreed with this point in the private discussions.  Apart from overall clearer positioning of the paper, the claimed benefit would need to be supported by experiments tailored to illustrate this main point. The authors argued against some of the suggested comparisons (e.g. past pruning methods), and further discuss that there is no established experimental benchmark for the parameter efficiency of BNNs. I indeed sympathize with both of these arguments; however, I believe that if the reviewers  suggestions for extra experiments are rejected, it remains the responsibility of the authors to find a slightly different way of motivating their work and demonstrating its efficiency in some convincing, meaningful and more well defined setting with the appropriate benchmarks.   
The paper presents an algorithm to compute mixed strategy Nash equilibria for continuous action space games. While the paper has some novelty, reviewers are generally unimpressed with the assumptions made, and the quality of the writing. Reviewers were also not swayed by the responses from the authors. Additionally, it could be argued that the paper is somewhat peripheral to the topic of the conference.¨  On balance, I would recommend reject for now; the paper needs more work.
*Summary:*   Study the location of local minima for quantum generative models.   *Strengths:*    Rigorous analysis of an important question.    Clear writing with important conclusions.   *Weaknesses:*    Technical writing might not be very accessible.  *Discussion:*   Reviewers were mostly favorable about this submission. They found the topic important and the contribution significant. A main concern was that the writing might not be sufficiently self contained and the writing might not be accessible to a broad audience. Authors worked on the accessibility. In the initial review, zxWF expressed concerns about concepts, proposed methods, numerical experiments. zxWF found that the author responses carefully covered most of their comments and raised score as a consequence. zxWF still finds that some aspects could be improved, particularly in regard to experiments. F6sD found the question well motivated, the techniques impressive, and the claims important.   *Conclusion:*   Three reviewers are favorable about this work. Two of them find it good and one marginally above the acceptance threshold. I find the topic and the nature of the claims important. Considering the unanimously positive reactions from the reviewers I am recommending this article to be accepted. I ask the authors to take the comments from the reviewers carefully into account when preparing the final version of the paper.
The paper received 6, 3, 1. The main criticism is the lack of quantitative evaluation/comparison. The rebuttal did not convince the last reviewer who strongly argues for a comparison. The authors are encouraged to add additional results and resubmit to a future venue.
The paper introduces a policy gradient estimator that is based on stochastic recursive gradient estimator. It provides a sample complexity result of O(eps^{ 3/2}) trajectories for estimating the gradient with the accuracy of eps. This paper generated a lot of discussions among reviewers. The discussions were around the novelty of this work in relation to SARAH (Nguyen et al., ICML2017), SPIDER (Fang et al., NeurIPS2018) and the work of Papini et al. (ICML 2018). SARAH/SPIDER are stochastic variance reduced gradient estimators for convex/non convex problems and have been studied in the optimization literature. To bring it to the RL literature, some adjustments are needed, for example the use of importance sampling (IS) estimator. The work of Papini et al. uses IS, but does not use SARAH/SPIDEH, and it does not use step wise IS.  Overall, I believe that even though the key algorithmic components of this work have been around, it is still a valuable contribution to the RL literature. 
This paper proposes a smoothing based certification against various forms of transformations, such as  rotations, translations. The reviewers have concerns on the novelty of the work and several technical issues. The authors have made efforts to address some of issues, but the work may still significantly benefit from a throughout improvement in both presentation and technical contribution.
This paper proposes a VAE based hierarchical generative model (Latent Object Model) to model scenes with multiple objects.  The paper would benefit from a substantial revision to improve text quality and clarity. The experiments lack proper quantitative baselines and imputations; and the overall results are quite underwhelming relative to existing models.
All three reviewers are consistently negative on this paper. Thus a reject is recommended.
This work shows how activation patterns of units reminiscent of grid and border cells emerge in RNNs trained on navigation tasks. While the ICLR audience is not mainly focused on neuroscience, the findings of the paper are quite intriguing, and grid cells are sufficiently well known and "mainstream" that this may interest many people.
All four reviewers agree that the paper should be rejected in its current form, but make numerous suggestions for improving it. The main points of concern were the motivation of the proposed method, novelty and the quality of the presentation of the work. The authors did not provide a response. The AC agrees with the reviewers and recommends rejecting the paper.
 + Paper proposes simple joint deep autoencoder + classifier training where the hidden representation is split between (observed) class and (unobserved) style nodes.    Empirical evaluation is very limited, focusing on only qualitative evaluation of reconstructions and interpolations (on MNIST and EMNIST).    Unclear goal: if it is improving classifier robustness, then quantitative classifier robustness improvements should be experimentally demonstrated. If it is as a (conditional) generative model, then it should be compared to strong generative baselines (in the GVAE or GAN families). The paper currently has neither.
The reviewers are in consensus. I recommend that the authors take their recommendations into consideration in revising their manuscript.
This paper proposes a new distillation based method for using large pretrained models like BERT to produce much *smaller* fine tuned target task models.   This paper is low borderline: It has merit and meets our basic standards, but owing to capacity limitations we had to give preference to papers we see as having a higher potential impact. Reviewers had some concerns about experimental design, but those seem to have been fully resolved after discussion. Reviewers were not convinced, even after some discussion, that the method and results were sufficiently novel and effective to have a substantial impact on the state of practice in this area.
The paper presents a gradient based hyperparameter optimization method, wherein a differentiable reparameterization is proposed for various popular CNN hyperparameters including kernel size, number of channels and hidden layer size.  All reviewers have pointed out the lack of novelty (such reparameterizations are standard) and lack of convincing experiments.  The authors didn t write any rebuttal.  Overall, there is a large consensus among the reviewers that this paper is not ready for publication at ICLR.
The papers studies a novel problem and proposes an interesting algorithm. That said, the reviewers question the motivation of the paper. That is whether this method presents a viable attack on existing MT systems. The attack is not black box and MT systems often have an output length threshold beyond which the output is trimmed. Given the motivational concerns, I recommend that the paper is revised and resubmitted to other venues.
This paper proposes a novel approach, Latent Question Reformulation Network (LQR net), a multi hop and parallel attentive network designed for question answering tasks that require multi hop reasoning capabilities. Experimental results on the HotPotQA dataset achieve competitive results and outperform the top system in terms of exact match and F1 scores. However, reviewers note the limited setting of the experiments on the unrealistic, closed domain setting of this dataset and suggested experimenting with other data (such as complex WebQuesitons). Reviewers were also concerned about the scalability of the system due to the significant amount of computations. They also noted several previous studies were not included in the paper. Authors acknowledged and made changes according to these suggestions. They also included experiments only on the open domain subset of the HotPotQA in their rebuttal, unfortunately the results are not as good as before. Hence, I suggest rejecting this paper.
The paper proposes a novel post processing method technique that can mitigate the model bias, called the Ethical Module. It transforms the deep embeddings of a given model to give more representation power to the disadvantaged subgroups.   The idea of ​​resolving discrimination against a specific group through effective post processing is promising, and proposing new metrics for fairness is also a very important and relevant issue.  However, the connection between the technique proposed in this paper and the newly proposed fairness metric is not clear, so the focus of the paper is somewhat lowered. Moreover, several design choices are somewhat unclear and ad hoc. In particular, although there was a lot of improvement through the rebuttal period, it is difficult to verify the superiority of the proposed method via the experiments in the paper; Direct comparisons with existing methods for fairness is essential, and it seems necessary to consider a hyperparameter selection strategy that can be taken in a practical scenario rather than simply choosing the best performing hyperparameter for the test set.
The reviewers agree that this is an interesting and original paper that will be of interest to the ICLR community, and is likely to lead to follow up work.
This paper presents a SLAM based approach for the ALFRED benchmark. The presented method, Affordance aware Multimodal Neural SLAM has two key advantages over past works: It uses a multimodal exploration strategy and it predicts an affordance aware semantic map. It also obtains a very large performance improvement over the ALFRED benchmark. The reviewers for this paper were quite impressed by the large improvements obtained by this technique. However, there were two major concerns across the reviews: (1) Are the design choices made in this paper heavily engineered towards ALFRED ? (2) Does the work make too many assumptions about the setting (unrealistic assumptions that may not really hold in more realistic environments or the real world) ? The authors have provided a detailed response and answered many questions posed to them, but the reviewers continue to have concerns about the generalizability of the proposed method. Another point of concern pointed out by a reviewer is whether it is reasonable in a realistic setting to perform exploration with a knowledge of the downstream task. This point has not really been answered satisfactorily by the authors. My takeaway is that the method presented by the authors clearly works on ALFRED. But it contains several design choices that are largely ALFRED specific and in some cases unrealistic. This provides fewer benefits to readers looking for more general insights that can be valuable across a suite of tasks. As a result of this, and in spite of the large gains, I recommend rejecting this paper.
The paper contributes a theoretical understanding of training over parametrized deep neural networks using gradient descent with respect to square loss in the NTK regime. Besides giving guarantees on the classification accuracy using square loss, authors reveal several interesting properties in this regime including robustness and calibration.   The problem studied here is exciting and very relevant. The current version, unfortunately, has some shortcomings. For example, under a margin assumption, the authors show that the least squares solution finds something with the margin and, therefore, it yields “robustness.” There is no quantification of how “robust” is the trained model, what is the threat model, what if the noise budget is larger than the attained margin. In general, the analysis lacks any careful finer characterization or quantification of the claimed properties. Besides, as was pointed out, the setting of the neural tangent kernel regime is somewhat limited and to some extent impractical. The assumptions under which the results hold further make the setting of the paper significantly restrictive.   The writing can be improved with more emphasis on the novelty and significance of the contributions. Currently, all of the assumptions are buried in the appendix and the main paper is not even self contained. I believe the comments from the reviewers have already helped improve the quality of the paper. I encourage the authors to further incorporate the feedback and work towards a stronger submission.
The paper provides a new distance preserving embedding based on a recent result called sigma delta quantization. The authors notice that in many realistic scenarios, the input vectors are well spread and under assumptions regarding the spreadness provide a fast technique to convert the input vectors into binary vectors, possibly of lower dimension. For completeness, the authors analyze the setting where the vectors are not spread and show that by using a randomized Walsh Hadamard transform, their results still apply. The authors do not provide a completely novel approach, to quote R2 “On a technical level the results in this paper are hardly too surprising for the JL community, but it is nice to see this analysis worked out in detail”. That being said, they show that a natural idea indeed works out by providing both a theoretical analysis and experimental results. The experiments can be more thorough but do convey the point that the result indeed works and moreover is somewhat robust in that it works well even when the formal requirements do not entirely hold. There are a few issues mentioned by the reviewers that should be addressed: A clearer exposition of the guarantees and assumptions, some comparison with previous papers. However given the responses and discussions these seem minor and fixable towards a camera ready version. I recommend accepting the paper 
The paper studies the effect of manifold geometry on the complexity of the function implemented by a random ReLU network, as measured through its decomposition into linear / affine regions. In particular, it provides bounds on a surrogate for the number of such regions and the distance of a fixed point to the boundary of its region. These bounds follow from an extension of an argument of Hanin and Rolnick for Euclidean space. The bounds hold at random initialization, and are complemented with experiments in which they remain valid through training.   Initial reviews of the paper were mixed. All reviewers recognized the extension to structured / non euclidean data as an important direction, and the results as extending the argument of Hanin and Rolnick to this setting. At the same time, there were questions about the novelty, clarity, and implications of the paper. One issue concerns the implications of the results and the amount of insight they offer into the data complexity   network complexity relationship. In particular, the paper would be stronger with a more explicit accounting for the constant C_{M,\kappa} and intuitive explanations of how manifold properties such as curvature and reach affect the number of linear regions. There were also concerns regarding the statement and proof of Theorem 3, the initial version of which only held for small \epsilon. The review also raised other smaller issues regarding the paper s clarity and implications. After considering the authors feedback and revisions, reviewers retained their mixed evaluation of the paper. This appears to be a promising direction, but a paper that could benefit from further refinement.
This paper proposed using the state bisimulation metric to learn invariant representations for reinforcement learning.  The method is generic, effective, and is supported by both theoretical and experimental results.  All reviewers and I think this is a strong contribution to the area.
although the proposed method could be considered an interesting application to recently popular hypobolic space to word embeddings, it is unclear why this needs to be done so. experiments also do not support why or whether the application of hyperbolic space to word embedding is necessary.
This paper has received three positive reviews. In general, the reviewers have commented on the importance of the question related to how much selectivity is needed from units of a neural network for good classification   from both the neuroscience and ML perspectives. The reviewers also commented on the thoroughness of the experiments and the general readability of the paper. This paper should be accepted if possible.
This paper presents a hierarchical Bayesian approach to exploration in grid worlds.  The paper considers the hypothesis that humans maintain a hierarchical representation when exploring a space, where the distribution over unknown space can be modeled with a structured probabilistic program.  The paper compares the behavior of people during exploration tasks to the behavior of a Bayesian model under different distributional approximations.  The results indicate that people can behave similarly to a sophisticated Bayesian model on small grid world domains.  The reviews highlighted several concerns about the paper.  One initial concern was that the experimental domain is too simple and small compared to real world environments encountered by robots or humans.  However, this work is similar in scope to other exploration work in reinforcement learning and psychology studies, where tiny grid worlds are still commonly used to gather insight.  Thus, this concern does not reduce the potential contribution of the paper.  The reviewers raised several other concerns about the work that were largely addressed by the author response.  The remaining reviewer concerns centered on the limited strength of the evidence in the experiments, but the reviewers expect the paper will still be of interest to the broader research community.  Four reviewers indicate to accept this paper for its contribution of a study into the use of probabilistic program induction to infer possible completions of maps in small environments.  The paper is therefore accepted.
A majority of the reviewers find the paper lacks novelty and provides an insufficient discussion of the state of the art in knowledge distillation and student teacher training to warrant publication. The approach is quite narrow to the application domain and the paper does not provide novel insights on how to chose a good network. A subset of the experiments performed on an internal data set with random train test splits do not evaluate a realistic transfer setting.  
# Paper Summary  The goal of this paper is to improve generalization of fairness metrics by borrowing ideas from "mixup", which attempts to improve generalization in the non fairness setting by introducing convex combinations of training examples as virtual examples.  They adapt this idea by interpolating between protected *groups*, and adding a regularizer that forces the classifier to vary smoothly along this interpolation path. To this end, they show that, for a particular interpolation function, the (empirical) disparity in the fairness metric is upper bounded by their proposed regularizer (which depends both on the fairness metric, and the interpolation function). They consider two fairness metrics (disparate impact and equalized odds) and two interpolation functions (convex combinations in the feature space, or in a latent space).  As Reviewer 4 points out, the above is not a complete explanation for why their regularizer works: they ve only really shown that it upper bounds the empirical disparity in the fairness metric (and we could have regularized this empirical disparity directly, and indeed they do so, as a baseline, in their experiments). Presumably the intuition is that their regularizer is improving generalization by (implicitly) depending on virtual examples, but this isn t made explicit.  In a "theoretical analysis" section, they give closed form solutions using classification loss, along with L2 regularization and either (i) a regularizer penalizing the true disparity of impact or (ii) their proposed regularizer (which upper bounds the former). Both reviewer 4 and I seem to doubt if this adds much insight (the other reviewers didn t discuss this section).  They close with experiments on Adult, CelebA, and Jigsaw Toxicity, all of which show dramatic performance gains using their regularizer. However, they only compare to one external baseline (adversarial debiasing).  # Pros  1. Reviewers agreed that the paper was well written 1. The derivation of their regularizer is somewhat complex, but is described step by step, and very clearly 1. Adapting mixup to the problem of improving fairness generalization seems natural and intuitive, but this intuition is maybe given short shrift in the later sections 1. Experiments show impressive results  # Cons  1. Reviewer 1 notes that having the expected value of the classification function be equal for both protected groups does not imply fairness, since the classification function would presumably be thresholded to make hard classification decisions 1. Reviewer 4 points out that they do not actually explain why their regularizer will improve generalization better than the "usual" disparity regularizer. Instead, they only show that it upper bounds the empirical disparity in the fairness metric. Presumably, the intuition is that their mixup regularizer is doing something like adding "virtual samples" 1. I would like to see a more detailed explanation of how their regularizer is implemented, in the main text (they only say that it "can be easily optimized by computing the Jacobian of f on mixup samples") 1. Reviewers 1 and 2 would like more external baselines (there is only one at the moment, "adversarial robustness"), with reviewer 1 suggesting early stopping. The authors added a new early stopping experiment on CelebA to the appendix, but it would be nice to have this baseline included in all experiments in the main text  # Conclusion  Three of the four reviewers recommended acceptance, with the "reject" reviewer scoring it "5: weak reject". This reviewer had three main criticisms: (i) matching expected classification functions is not the same as matching classification *decisions*, (ii) fairness problems might not have a generalization problem to begin with, and (iii) the experiments don t include enough external baselines. I disagree with the second point, but agree with the other two. I think the third is the most critical, since the first could be solved in many cases by e.g. sampling instead of making hard deterministic decisions.  Overall, my opinion is that this is a borderline paper, but that it falls on the "accept" side of the boundary. The idea is intuitive, and exposition is clear, the derivation is quite interesting, and the experimental results are (aside from not having enough baselines) impressive.
This is a borderline paper. The most enthusiastic reviewer does not have much confidence in the score. The other reviewers think the paper has some value after the rebuttal, but also feel there is little technical novelty. The proposed applications of the approach are interesting.  After reading the reviews, rebuttal, and the paper, I agree that there is little technical novelty. The idea of adding node label noise to a GNN to improve GNN expressiveness dates back to (Murphy et al., 2019) and has been also explored by (Dasoulas et al., 2019), (Vignac et al., 2020), (Loukas, 2020) among others [one of which is suggested by a reviewer] (this literature is entirely missing from the paper). The paper has some novelty in proposing a regularization method for tackling the node level noise by augmenting the loss function with a denoising term. The oversmoothing justification is not properly investigated (whether the proposed solution really solves the issue in practice).  If there is space in the borderline decision boundary, this paper could be a worthwhile inclusion.  Dasoulas, G., Santos, L.D., Scaman, K. and Virmaux, A., 2019. Coloring graph neural networks for node disambiguation. arXiv preprint arXiv:1912.06058. Loukas, A., 2020. How hard is to distinguish graphs with graph neural networks?. arXiv preprint arXiv:2005.06649. Vignac, C., Loukas, A. and Frossard, P., 2020. Building powerful and equivariant graph neural networks with structural message passing. arXiv preprint arXiv:2006.15107. Murphy, R., Srinivasan, B., Rao, V. and Ribeiro, B., 2019, May. Relational pooling for graph representations. In International Conference on Machine Learning (pp. 4663 4673). PMLR.
The authors propose a framework for incorporating homogeneous linear inequality constraints on neural network activations into neural network architectures. The authors show that this enables training neural networks that are guaranteed to satisfy non trivial constraints on the neurons in a manner that is significantly more scalable than prior work, and demonstrate this experimentally on a generative modelling task.  The problem considered in the paper is certainly significant (training neural networks that are guaranteed to satisfy constraints arises in many applications) and the authors make some interesting contributions. However, the reviewers found the following issues that make it difficult to accept the paper in its present form: 1) The setting of homogeneous linear equality constraints is not well motivated and the significance of being able to impose such constraints is not clearly articulated in the paper. The authors would do well to prepare a future revision documenting use cases motivated by practical applications and add these to the paper. 2) The experimental evaluation is not sufficiently thorough: the authors evaluate their method on an artificial constraint involving a "checkerboard pattern" on MNIST. Even in this case, the training method proposed by the authors seems to suffer from some issues, and more thorough experiments need to be conducted to confirm that the training method can perform well across a variety of datasets and constraints.  Given these issues, I recommend rejection. However, I encourage the authors to revise their work on this important topic and prepare a future version including practical examples of the constraints and experiments on a variety of prediction tasks.  
This paper is concerned with combining past approximation methods to obtain a variant of Deep Recurrent GPs. While this variant is new, 2/3 reviewers make very overlapping points about this extension being obtained from a straightforward combination of previous ideas. Furthermore, R3 is not convinced that the approach is well motivated, beyond “filling the gap” in the literature.   All reviewers also pointed out that the paper is very hard to read. The authors have improved the manuscript during the rebuttal, but the AC believes that the paper is still written in an unnecessarily complicated way.   Overall the AC believes that this paper needs some more work, specifically in (a) improving its presentation (b) providing more technical insights about the methods (as suggested by R2 and R3), which could be a means of boosting the novelty. 
This paper presents an analysis of the languages that can be accepted by a counter machine, motivated by recent work that suggests that counter machines might be a good formal model from which to approach the analysis of LSTM representations.  This is one of the trickiest papers in my batch. Reviewers agree that it represents an interesting and provocative direction, and I suspect that it could yield valuable discussion at the conference. However, reviewers were not convinced that the claims made (or implied) _about LSTMs_ are motivated, given the imperfect analogy between them and counter machines. The authors promise some empirical evidence that might mitigate these concerns to some extent, but the paper has not yet been updated, so I cannot take that into account.   As a very secondary point, which is only relevant because this paper is borderline, LSTMs are no longer widely used for language tasks, so discussion about the capacity of LSTMs _for language_ seems like an imperfect fit for an machine learning conference with a fairly applied bent.
The authors appear to have largely addressed the concerns of the reviewers and commenters regarding related work and experiments. The results are strong, and this will likely be a useful contribution for the graph neural network literature.
The reviewers were split between accept (7) and borderline reject (two 5 s). All three reviewers acknowledged that the proposed approach is simple and intuitive (but this paper follows, for the most part, the concept of reservoir operation and apply it to transformers). The main criticisms were insufficient experiments (R5) and the lack of a clear conclusion (R2). I found these concerns to be valid and did not find strong reasons to overturn their recommendations. More comprehensive experiments (especially on WMT) and clear conclusions (accuracy or efficiency) would make this paper much stronger.
This work considers one shot pruning in deep neural networks. The main departure from previous work is to consider stochastic Frank Wolfe. The reported results are convincing although a number of baselines were missing from the initial submission. The authors provide a balanced account of the strengths and weaknesses of the proposed approach.  The authors adequately addressed the concerns of the reviewers. For instance they ran additional experiments to compare to missing pruning baselines. I would encourage the authors to revise the manuscript by including the missing related work, the additional clarification discussions (e.g., motivation for K sparse constraints, follow up analysis, and cost per iteration) and to include the additional experiments that were conducted (e.g., pruning with training).
Reviewers mostly recommended to reject after engaging with the authors, however since not all author answers have been acknowledged by reviewers, I am not sure if there are any remaining issues with the submission. I thus lean to recommend to reject and resubmit. Please take reviewers  comments into consideration to improve your submission should you decide to resubmit. 
This paper presents a theoretically motivated method based on homotopy continuation for transfer learning and demonstrates encouraging results on FashionMNIST and CIFAR 10. The authors draw a connection between this approach and the widely used fine tuning heuristic. Reviewers find principled approaches to transfer learning in deep neural networks an important direction, and find the contributions of this paper an encouraging step in that direction. Alongside with the reviewers, I think homotopy continuation is a great numerical tool with a lot of untapped potentials for ML applications, and I am happy to see an instantiation of this approach for transfer learning. Reviewers had some concerns about experimental evaluations (reporting test performance in addition to training), and the writing of the draft. The authors addressed these in the revised version by including test performance in the appendix and rewriting the first parts of the paper. Two out of three reviewers recommend accept. I also find the homotopy analysis interesting and alongside with majority of reviewers, recommend accept. However, please try to iterate at least once more over the writing; simply long sentences and make sure the writing and flow are, for the camera ready version.
The paper proposes a robust formulation of Deep Subspace Clustering (DSC) based on the correntropy induced metric (CIM) of the error. All three reviewers recommend rejection. Their major critiques are limited novelty, insufficient experiments and similar performance to non deep methods. The rebuttal highlights that the novelty is not DSC or CIM, but rather that the formulation does not require knowing the labels. I agree with the reviewers that the paper s novelty is very limited and didn t find the author s response compelling enough to overturn the reviewer s opinions. 
The manuscript proposes a method for addressing spurious correlations and sub population (group) shift problem by modelling intergroup interactions. Past work (GroupDRO) focuses on the worst group which is subject to failure when groups have heterogeneous levels of noise and transfer. This work focuses on the group whose gradient leads to largest decrease in average training loss over all groups. The manuscript presents insights on why the proposed method called CGD may perform better than GroupDRO by studying simple synthetic settings. The manuscript also provides empirical evaluation on seven real world datasets–which include two text and five image tasks with a mix of sub population and domain shifts.  There are several positive aspects of the manuscript, including: 1. The idea of training on the group which leads to largest overall decrease in loss is natural and interesting; 2. The synthetic examples presented in the manuscript clearly bring out the use cases of the method proposed and comparison with GroupDRO; 3. The empirical results presented lead to improved results on a variety of benchmark tasks.  There are also several major concerns, including: 1. More discussion on why the proposed method works for the chosen real world datasets by connecting them to the synthetic setups presented in the manuscript; 2. The proposed algorithm does not minimize a specific loss function; 3. The standard benchmarks are altered. For example, the CivilComments dataset is shown as a 2 group when it is originally a 8 group task (the groups being the demographics of the users) as shown in the WILDS dataset paper.  Authors clarified, among others, that the proposed approach optimizes the macro average loss function, and the standard benchmarks are not modified and the experiment setup is exactly like GroupDRO evaluation on the CivilComments WILDS dataset. Reviewers noted that the generative model has not added anything new since it is essentially the synthetic example and it just shows what every robust machine learning method is supposed to do, i.e., don t rely on e_s (group specific components) but on e_c (common components) while making predictions. It doesn t justify the procedure of choosing to focus on the group that minimizes the error for the group that decreases all other groups  errors.   The revised manuscript includes a clearer motivation, and more discussion on how the synthetic examples connect to the real world datasets. Based on that, I put an accept recommendation.
The paper observes that the number of redundant parameters is a function of the training procedure and proposes a training strategy that encourages all parameters in the model to be trained sufficiently and become useful. The method adaptively adjusts the learning rate for each individual parameter according to its sensitivity (a proxy for the parameter s contribution to the model performance). The approach encourages the use of under fitted parameters while preventing overfitting in the well fitted ones. Experimental results are presented covering a wide range of tasks and in combination with several optimisers, showing improvements in model generalization.  The paper is very well written and easy to follow (as mentioned by Reviewers NSqH, 4pzE and sSHP).   The authors provided a strong rebuttal including new experiments, like training using CNN based architectures (as requested by Reviewers sSHP and MzBV). Reviewer sSHP requested these results to be reported with STD, the AC encourages the authors to do so for the camera ready.  Reviewer MzBV points out that the paper could be improved by giving a motivation of the update rule and proving convergence. However, still recommends accepting the paper due to the novelty in the idea of not taking redundant parameters as something inevitable and devising an effective strategy to improve it. This idea was also appreciated by the other reviewers. While the AC agrees that adding these points would improve the work, it takes as valid the point made by the authors. Namely, that the intuition behind the update rule is quite clear, and many other reasonable variants were ablated (in Appendix A.4.4). Furthermore, the empirical evidence shows that the method improves generalization.  Reviewer NSqH points out that while SAGE improves the model’s generalization performance for lightly compressed models, its performance becomes more susceptible to pruning when the model is compressed heavier. While the authors responded with good points, the AC encourages them to follow the reviewer’s advice and incorporate further experiments studying this issue (e.g. other datasets).  In sum, the paper proposes a simple and effective method that is able to improve generalization of large scale models. All four reviewers recommend accepting the paper. The AC agrees and encourages the authors to incorporate the requests mentioned above.
All reviewers agree the paper does not meet the acceptance bar, and an authors  rebuttal is not available. Therefore, I recommend rejection.
The paper presents a method to make CNN focus more on structure rather than texture by constraining a random set of neurons per feature map to have constant activation.  The paper has limited novelty and unclear analysis of the experimental results, for instance plots of accuracy vs strength of adversarial perturbation should be produced. Tables are not readable and results tend to be cluttered and confusing.  Some comparisons seem to be cherry picked as pointed out by some reviewers. Although the approach seems to be well received by the reviewers they all shared similar concerns about having a stronger motivation and better validation of the approach (that is not amount of comparisons but the right comparisons that would clear doubts and make the work directly comparable to others). I strongly encourage the authors to perform a deeper analysis and to clearly work on hypothesis and validation of their work. In my opinion, although the reviewers think different, the experiments are not sufficient to validate the strong claim of the paper.
The paper is about nonlinear system identification in an EM style learning framework. The idea is to use nonlinear programming for the E step (finding a MAP estimate) and then refine the model parameters. In flavor, this approach is similar to the work by Roweis and Ghahramani.   However, this paper does not offer any new insights whatsoever and the (very short) methods section arrives at proposing to compute the maximum a posteriori estimate (eq. 5). While the motivation for this given in the paper is a bit hard to understand it is of course a very well known and useful estimator. Besides the maximum likelihood estimator this is one of the most commonly used point estimators, see any textbook on statistical signal processing. There has been quite a bit of work in the signal processing community over the last 10 years, and a good overview can be found here: https://web.stanford.edu/~boyd/papers/pdf/rt_cvx_sig_proc.pdf This should give evidence that this is indeed a standard way of solving the problem and it does work really well. Given that we have so fast and good optimizers these days it is common to solve Kalman filtering/smoothing problems via this optimization problem. The paper does not contain any analysis at all. The experiments do of course show that the method works (when there is low noise). Again, we know very well that the MAP estimate is a decent estimator for unimodal problems. The MAP estimator can also be made to work well for noisy situations.  As for the comments that the sequential Monte Carlo methods do not work in higher dimensions that is indeed true. However, there are now algorithms that work in much higher dimensions than those considered by the authors of this paper, e.g. https://ieeexplore.ieee.org/document/8752074 which also contains an up to date survey on the topic. Furthermore, when it comes to particle smoothing there are also much more efficient smoothers than 10 years ago. The area of particle smoothing has also evolved rapidly over the past years.   Summary: The paper makes use of the well known MAP estimator for learning nonlinear dynamical systems (states and parameters). This is by now a standard technique in signal processing. There are several throw away comments on SMC that are not valid and that are not grounded in the intense research of that field over the past decade. 
This paper proposes an efficient algorithm to obtain a node embedding based on its local PageRank scores. The proposed approach uses a hashing technique and a local partition approach to make the method more efficient and effective. However, the paper has significant drawback and can be further improved in the following aspects:  1. The experimental evaluation is weak and does not allow us to draw meaningful conclusions about the proposed algorithm.  2. The proposed algorithm does not show significant performance improvement on the link prediction task.
The reviewers agree that this paper provides a sensible mechanism for producing word embeddings that exploit correlating features in the data (e.g. texts written by the same author), but point to other work doing the same thing. The lack of direct comparison in the experimental section is troublesome, although it is entirely possible the authors  were not aware of related work. Unfortunately, the lack of an author response to the reviews makes it hard to see the argument in defense of this paper, and I must recommend rejection.
The paper proposes several subsampling policies to achieve a clear reduction in the size of augmented data while maintaining the accuracy of using a standard data augmentation method. The paper in general is clearly written and easy to follow, and provides sufficiently convincing experimental results to support the claim. After reading the authors  response and revision, the reviewers have reached a general consensus that the paper is above the acceptance bar. 
This paper marries the idea of Gaussian word embeddings and order embeddings, by imposing order among probabilistic word embeddings. Two reviewers vote for acceptance, and one finds the novelty of the paper incremental. The reviewer stuck to this view even after rebuttal, however, acknowledges the improvement in results. The AC read the paper, and agrees that the novelty is somewhat limited, however, the idea is still quite interesting, and the results are promising. The AC was missing more experiments on other tasks originally presented by Vendrov et al. Overall, this paper is slightly over the bar.
The idea of having two policies with opposing strategies, one aiming to maximize a notion of surprise whereas the other tries to minimize it, is an interesting one. However, even after the author rebuttal, all reviewers have lingering concerns about the evaluation protocol. In addition, there are remaining questions about the bonuses used; there are concerns that these only work for very specific domains. For these reasons, I m recommending rejection. I encourage the authors to carefully read the concerns of the reviewers about evaluation and consider using a different evaluation protocol for a future version of this work.
The authors have proposed a language+vision  dual  attention architecture, trained in a multitask setting across SGN and EQA in vizDoom, to allow for knowledge grounding. The paper is interesting to read. The complex architecture is very clearly described and motivated, and the knowledge grounding problem is ambitious and relevant. However, the actual proposed solution does not make a novel contribution and the reviewers were unconvinced that the approach would be at all scalable to natural language or more complex tasks. In addition, the question was raised as to whether the  knowledge grounding  claims by the authors are actually much more shallow associations of color and shape that are beneficial in cluttered environments. This is a borderline case, but the AC agrees that the paper falls a bit short of its goals.
This paper presents an approach for modular multi task learning. All the reviewers believe the goals are appealing and the idea is reasonable. However, R2 and R4 raise concerns with respect to novelty. There are also strong concerns regarding experiments. The concerns vary from reproducibility to small improvements and right baselines. The rebuttal fails to provide any new experiments or handle the reviewer concerns. All reviewers and AC agree that paper is not yet ready for publication.
The paper is proposing a test time adaptation method without modifying the training. The proposed idea is simple and effective, adapting the normalization layers using the entropy of the model predictions as a loss function. The paper presents an extensive empirical study. Paper received unanimously accept scores. It also has potential to be impactful as it is easy to apply without any strong assumption/requirement. A clear accept!
This paper introduced a concept called ReLU stability to motivate regularization and enable fast verification. Most of the analysis was presented empirically on two simple datasets and with low performing models. I feel theoretical analysis and more comprehensive and realistic empirical studies would make the paper stronger. In general, the contribution of this paper is original and interesting.  
This paper shows that images synthesized to match adversarially robust representations are similar to original images to humans when viewed  peripherally.  This was not true for adversarially non robust representations.  Additionally the adversarially robust representations were similar to the texform model image from a model of human peripheral vision.   Reviewers increased their score a lot during the rebuttal period as   the authors provided more details on the experiments and agreed to  tone down some of the claims (especially the strong claim that the robust representations capture peripheral computation similar to current SOA texture peripheral vision models).  As well stated by  reviewer s6dV, two representations with the same null space are not necessarily the same.    With reviewer scores of 8 across the board, reviewers agree that this is interesting work that should be presented at the conference.  I agree.
The reviewers raised a number of concerns including the appropriateness of the chosen application and the terms in which social dilemmas have been discussed, the lack of explanations and discussions, missing references, and the extent of the evaluation studies. The authors’ rebuttal addressed some of the reviewers’ concerns but not fully. Overall, I believe that the work is interesting and may be useful to the community (though to a small extent., in my opinion). However, the paper would benefit from additional explanations, experiments and discussions pointed in quite some detail by the reviewers. AS is, the paper is below the acceptance threshold for presentation at ICLR.
The paper proposes a GAN framework for dynamic point cloud superresolution. It does not need scene flow supervision for training and has an interesting adaptive upsampling mechanism. Results are shown on several datasets and are reasonably convincing. Overall, all the reviewers are slightly positive about the work. After the rebuttal, all the five reviewers converged to a marginally above the threshold recommendation. The meta reviewer agreed with their assessment and would like to recommend accepting the paper.
The authors rely on the recent convex formulation of neural network training to establish an interesting correspondence between whitening of input data and batch normalization (after and before relu). While convex formulation is not scalable to realistic architectures, it is a great insight that would add value to our understanding of the batch normalization tool.
This paper proposes a method for 4 bit quantized training of NNs (forward and backward), obtaining SOTA 4 bit training quantization, motivated by an analysis of rounding schemes (an important aspect) in quantized training. The main concerns from the reviewers were that the approach was not practical (both a general concern, and of specific note here since the word is used in the title and motivation of the work), due to lack of compatibility with (current) general purpose hardware, and lack of suitability of the approach for specialized hardware, so it is unclear what the actual use case is for the approach. The authors argued that (1) this is not a problem on some hardware and (2) that past works have not been held to this standard. I did not find the authors to provide a strong argument during the discussion period to address these concerns.
This paper proposes an approach to pruning units in a deep neural network while training is in progress. The idea is to (1) use a specific "scoring function" (the absolute valued Taylor expansion of the loss) to identify the best units to prune, (2) computing the mean activations of the units to be pruned on a small sample of training data, (3) adding the mean activations multiplied by the outgoing weights into the biases of the next layer s units, and (4) removing the pruned units from the network. Extensive experiments show that this approach to pruning does less immediate damage than the more common zero replacement approach, that this advantage remains (but is much smaller) after fine tuning, and that the importance of units tends not to change much during training. The reviewers liked the quality of the writing and the extensive experimentation, but even after discussion and revision had concerns about the limited novelty of the approach, the fact that the proposed approach is incompatible with batch normalization (which severely limits the range of architectures to which the method may be applied), and were concerned that the proposed method has limited impact after fine tuning.
Most of the reviewers think this paper is clearly a valuable addition to ICLR based on the convincing theoretical analysis and extensive experimental results. Please refer to reviewers s review for more detailed discussions of the pros and cons of the paper.
The paper introduces a benchmark suite providing a series of synthetic distributions and metrics for the evaluation of generative models. While providing such a tool kit is interesting and helpful and it extends existing approaches for evaluating generative models on simple distributions, it seems not to allow for very different additional conclusions or insights.This limits the paper s significance. Adding more problems and metrics to the benchmark suite would make it more convincing.
 pros:   novel idea for multi step QA which rewrites the query in embedding space   good comparison with related work   reasonable evaluation and improved results  cons:  There were concerns about missing training details, insufficient evaluation, and presentation.  These have been largely addressed in revision and I am recommending acceptance.
The proposed paper presents low rank compression method for DNNs. This topic has been around for a while, so the contribution is limited. Lebedev et. al paper in ICLR 2015 used CP factorization to compress neural networks for Imagenet classification; in 2019, the idea has to be really novel in order to be presented on CIFAR datasets. The latency is not analyzed.  So, I agree with reviewers.
This paper proposes improvements to WaveNet by showing that increasing connectivity provides superior models to increasing network size. The reviewers found both the mathematical treatment of the topic and the experiments to be of higher quality that most papers they reviewed, and were unanimous in recommending it for acceptance in the conference. I see no reason not to give it my strongest recommendation as well.
This work propose a compression aware training (CAT) method to allows efficient compression of  feature maps during inference. I read the paper myself. The proposed method is quite straightforward and looks incremental compared with existing approaches based on entropy regularization.   
The paper studies the robustness of deep learning against label noise on MNIST, CIFAR 10 and ImageNet. But the generalization of the claim "deep learning is robust to massive label noise" is still questionable due to the limited noise types investigated. The paper presents some tricks to improve learning with high label noise (batch size and learning rate), which is not novel enough. 
All the reviewers agree that this paper made a solid contribution of understanding the algorithmic regularization of SGD noise  (in particular the label noise for regression) after reaching zero loss. The framework is novel and has the potential to extend to other settings.
Two trust region constrained optimization for policy gradient RL, where the second trust region is based on a virtual policy built from a memory buffer and using an attention mechanism to combine prior policies. The reviewers agree that the paper is well written, the idea is novel, and the paper is extensively evaluated. The authors are commended for running the additional baselines during the rebuttal period.  However, the paper still contains some shortcomings, specifically, the results are somewhat inconclusive even after the rebuttal. While it is not expected that the method wins across the board, it is important to provide an analysis of the limitations of the method. When is the algorithm appropriate to use, and when is it not?   To make the paper stronger, in the next version of the paper should:   move the theory in the main text (Appendix C).   provide the analysis of the algorithm and its limitations.
With positive unlabeled learning the paper targets an interesting problem and proposes a new GAN based method to tackle it. All reviewers however agree that the write up and the motivation behind the method could be made more clear and that novelty compared to other GAN based methods is limited. Also the experimental analysis does not show a strong clear performance advantage over existing models. 
This paper studies the influence of recommender systems on users  preferences. The authors propose a method for estimating preference shifts, evaluating their desirability, and avoiding such shifts (when needed).  After the initial review and discussion period, a fourth reviewer with significant recsys experience and a very good knowledge of this sub area was invited to provide an additional review of the paper. This is reviewer vNt7. Their review was positive overall but did highlight some limitations and potential ways to improve the paper s grounding in the recsys literature.  Overall, the main strengths of this paper were that it studies an interesting and practically motivated question. The reviewers also found the proposed solution reasonable.   The main limitations are twofold. One, the results use a single set of simulation assumptions. Showing similar results under different simulation assumptions would be helpful to better understand the robustness and potential limitations of the approach. Two, there is a certain disconnect with the simulation literature. See comments from reviewers vNt7 and kWQ2 (although I found your reply to Virtual Taobao convincing).  Overall and given the final reviewer recommendations (three marginally above and one marginally below), this is a very borderline paper. However, the consensus view of the committee is that it would benefit from additional work before publication.  I am sorry that I cannot recommend acceptance at this stage. I do believe that some of the suggestions from the reviewers highlighted above (more diverse simulation, better grounding in current recsys simulation literature and in the field) will be useful in preparing the next version of this work.
Dear authors,  All reviewers agreed that, while the problem considered was of interest, the theoretical result presented in this work was of too limited scope to be of interest for the ICLR audience.  Based on their comments, you might want to consider a more theoretically oriented venue for such a submission.
The paper proposes a rotationally equivariant transformer architecture for predicting molecular properties. The proposed architecture demonstrates good computational efficiency and good results on three benchmarks.  All four reviewers recommend acceptance (two weak, two strong), citing the novelty of the architecture, the good computational efficiency of the model and the good empirical results as the main strengths of the paper. The reviewers expressed minor criticisms and recommendations for improvement, some of which were addressed by the authors during the reviewing process, which led to an increase in scores.  Overall, this is a nice contribution of machine learning to science, and I m happy to recommend acceptance to ICLR.
The paper introduces an AutoML method for irregular multivariate time series.   The method automates the selection of the configuration as well as the hyperparameter optimization depending on the task. A Bayesian approach handles the network structure search while VAEs + attention is used to learn  representations from irregularly sampled data. There is an additional contribution: anomaly detection via a sample energy function from a GMM on time windows.  While there is some novelty in the proposed approach, mostly in the way in which existing techniques are combined, the paper also has some limitations:   running the framework over the set of possible models is computationally intensive; in their response, the authors indicate the search space can be constrained, however, doing so would also decrease the performance; in AutoML, added complexity cannot be avoided, but there is no notion of how much longer it takes to find suitable models compared to taking off the shelf methods.   although the paper is geared towards irregularly sampled time series, there are no experiments where the data is naturally irregularly samples; artificially introduced patterns are no substitute for this; (PhysioNet, as suggested by one of the reviewers or MIMIC III both have this type of data and are frequently used in benchmarks)   AutoML is presented as a general framework, but mostly handles clustering and anomaly detection;  unclear of how useful it would be for forecasting or regression; classification realists are shown in Appendix F against simple baselines (GRU D is not considered, for instance) and even so AutoML does not achieve state of the art results in half of the cases
This paper presents an approach for machine learning to fix programming errors via edits to abstract syntax trees. The main contributions are a pretraining scheme based on masking out subtrees and some minor architectural modifications compared to previous work. Reviewers found the paper to contain a significant amount of work, but there are some questions about significance relative to previous work that framed the problem similarly, and about experimental methodology. Authors did a great deal of work in the rebuttal to address many of the experimental methodology questions, but this also introduced substantial unreviewed changes to the model, the pretraining approach, and the experiments. In total, the remaining concerns about significance and the substantial changes lead us to recommend that this paper be revised and resubmitted to the next conference.
This article provides an analysis of feedforward neural network with iid Gaussian weights and biases in the infinite width limit. The paper  complements earlier work on this topic by taking a function space approach, considering neural networks as infinite dimensional random elements on the input space. This is a well written and rigorous theoretical paper. Although, as noted by a reviewer, there are no direct practical implications, the result is interesting in itself, highly relevant to the ICLR audience, and likely to lead to further exploration of the connections between Gaussian processes and neural networks.   There were a few questions regarding the proofs that have been answered satisfactorily by the authors.   I recommend acceptance. 
In this paper, the stopping condition of Bayesian Optimization (BO) is discussed. This problem is very important when BO is applied to the Hyper parameter optimization (HPO) task. All the reviewers agree that the proposed approach based on high probability confidence bound on the regret is interesting and reasonable.  An important issue raised by a reviewer is that many existing BO works discussed how to achieve efficiency and saving budget in BO although they did not explicitly mention the stopping condition. Due to the lack of discussion regarding the relationship with these highly related studies, we have to conclude that the paper cannot be accepted in its current form.
This paper studies a stochastic approximation framework for multi agent consensus algorithms driven by Markovian noise in the spirit of the classical paper of Kushner & Yin. The authors  main result is that   modulo a series of assumptions, some conceptual, some technical   the generated sequence of play reaches a consensus, and they also estimate the rate of this convergence.   Even though the paper s premise is interesting, the reviewers identified several weaknesses in the paper, and the reviewers that raised them where not convinced by the authors  replies (especially regarding the relative lack of numerical evidence to demonstrate the claims that are not supported by the theory, such as the role of Assumption 6). After my own reading of the paper and the discussion with the reviewers during the rebuttal phase, I concur that this version of the paper does not clear the bar for acceptance   but, at the same time, I would encourage the authors to submit a suitably revised version at the next opportunity.
The paper is exceptionally well summarized by Reviewer QC5G which is difficult to improve up on. I will save the readers the effort of reading more text (without adding more substance). The reviewers unanimously rated this paper highly. The discussion has been robust,  enlightening and also has improved the revised paper.
The paper presents a new neural program synthesis architecture, SAPS, which seems to produce accuracy improvements in some synthesis tasks. The reviewer consensus, even after discussion with the authors, was that the paper is not acceptable at the conference. Two concerns emerge during discussion, even considering the authors efforts to improve the paper. First, the system seems to have many "moving parts", but there is a lack of rigorous ablation studies to demonstrate which components of the system (or combination thereof) make significant contributions to the results. I agree with this assessment: it is not sufficient to demonstrate increased scores, even if the experimental protocol and clear and sound (more on this later), but there must be some evidence as to why this increase happens, both in the discussion and in the empirical segment of the paper, by conducting a thorough ablation study. Second, all reviewers had issues with proper and fair comparison with prior work, with the consensus being that the model is not adequately compared to convincing benchmarks in the paper.  The results of the paper sound like there is something promising going on, but the need for a clear presentation of what is the driving factor behind any improvement is not only a superficial stylistic requirement, but a key tenet of proper scholarship. This is one front on which the paper fails to make a successful case for the work and methods it describes, and unfortunately is not ready for publication at this time (despite having a cool title).
The reviewers unanimously appreciated the quality of the experiments. The main point raised was about the related work by Wang et al. but that was addressed by the authors in the rebuttal. I thus encourage the authors to make sure that discussion is reflected in the final version of their work.
This paper proposes a method for improving exploration by implementing intrinsic rewards based on optical flow prediction error. The approach was evaluated on several Atari games, Super Mario, and VizDoom.  There are several strengths to this work, including the fact that it comes with open source code, and several reviewers agree it’s an interesting approach. R1 thought it was well written and quite easy to follow. I also commend the authors for being so responsive with comments and for adding the new experiments that were asked for.  The main issue that reviewers pointed out, and which I am also concerned about, is how these particular games were chosen. R3 points out that these 5 Atari games are not known for being hard exploration games. Authors did conduct further experiments on 6 Atari games suggested by the reviewer, but the results didn’t show significant improvement over baselines.  I appreciate the authors’ argument that every method has “its niche”, but the environments chosen must still be properly motivated. I would have preferred to see results on all Atari games, along with detailed and quantitative analysis into why FICM fails on specific tasks. For instance, they state in the rebuttal that “The selection criteria of our environments is determined by the relevance of motions of the foreground and background components (including the controllable agent and the uncontrollable objects) to the performance (i.e., obtainable scores) of the agent.” But it doesn’t seem like this was assessed in any quantitative way.  Without this understanding, it’d be difficult for an outsider to know which tasks are appropriate to use with this approach. I urge the authors to focus on expanding and quantifying the work they depict in Figure 8, which, although it begins to illuminate why FICM works for some games and not others, is still only a qualitative snapshot of 2 games. I still think this is a very interesting approach and look forward to future versions of this paper.
 Pros:   A useful and well structured dataset which will be of use to the community   Well written and clear (though see Reviewer 2 s comment concerning the clarity of the model description section)   Good methodology  Cons:   There is a question about why a new dataset is needed rather than a combination of previous datasets and also why these datasets couldn t be harvested from school texts directly.  Presumably it would ve been a lot more work but please address the issue in your rebuttal.   Evaluation: Reviewer 3 is concerned that the evaluation should perhaps have included more mathematics specific models (a couple of which are mentioned in the text).  On the other hand, Reviewer 2 is concerned that the specific choices (e.g. "thinking steps") made for the general models are non standard in seq 2 seq models.  I haven t heard about the thinking step approach but perhaps it s out there somewhere. It would be helpful generally to have more discussion about the reasoning involved in these decisions.  I think this is a useful contribution to the community, well written and thoughtfully constructed.  I am tentatively accepting this paper with the understanding that you will engage directly with the reviewers to address their concerns about the evaluation section.  Please in particular use the rebuttal period to focus on the clarity of the model description and the motivation for the particular models chosen.  Also consider adding additional experiments to allay the concerns of the reviewers.
While many work in the literature (PlaNet (2018), Dreamer (2020), SimPLe (2019), etc.) learn world models to perform well on a particular task at hand, the motivation behind this work is that dynamics models benefit if they are task agnostic, hence would be able to perform a wider range of tasks, as opposed to just doing one task really well. In order to do this, they propose to learn a latent representation that models inverse dynamics of the system / environment rather than capturing information about the task specific rewards, and incorporate a planning for solving specific tasks in which they can measure performance.  To show broad applicability of their method, the authors tested their approach on Atari and DM Control Suite (from pixels), and also simple grid worlds to illustrate the concepts, and demonstrated strong performance over SOTA model free algorithms (even the ones that do not have open source implementations). Reviewers and myself agree that the paper is well written, easy to follow, and the approach is well motivated.  After the review period, the authors have done work to improve the draft, particularly including ablation studies with and without planning, addition comparisons, and improved visualizations, after taking in the comments and feedback from the reviewers after the initial reviews, which satisfied some of the reviewers. One reviewer asked for a real robotic task, but I feel that while it will help the paper, many existing works focus purely on DM control from pixels, and this work has performed experiments on both DM Control and Atari, two reasonably different domains, and IMO makes up for the lack of real world robotics experiment. That being said, a discussion on how the proposed method would work in a real robotic task, as suggested by R4 would be good to have.  I believe the work in its current state is ready for acceptance for ICLR 2021, and should be a fine contribution to the visual model based RL works. I m excited to see this work presented to the community, and I m going to recommend acceptance (Poster).
The article studies the role of the activation function in learning of 2 layer overparaemtrized networks, presenting results on the minimum eigenvalues of the Gram matrix that appears in this type of analysis and which controls the rate of convergence. The article makes numerous observations contributing to the development of principles for the design of activation functions and a better understanding of an active area of investigation as is convergence in overparametrized nets. The reviewers were generally positive about this article. 
The manuscript presents an approach for identifying sources of uncertainty in object classification tasks by disentangling representations in latent spaces.  Three reviewers agreed that the manuscript is not ready for publication.  Some of the concerns are conceptual flaws, weak evaluation protocol, and an incorrect interpretation of experiment results.  There is no author response.  
This paper presents an empirical study which shows that pruning FBNets with larger capacity results in a model with higher accuracy than one searched via neural architecture search. The below are pros and cons of the paper mentioned by the reviewers:  Pros   The observation that optimized architectures such as FBNets can benefit from pruning is interesting.   The paper is well written and easy to follow.  Cons   It is trivially known that training larger model and then pruning it will yield a better performing model, than training a smaller model from scratch.     The authors do not propose a novel pruning technique for optimized CNN architectures, and use existing pruning techniques for all experiments.    The experimental validation is only done with FBNets on ImageNet, and it does not show when pruning starts to break down.   All reviewers unanimously voted for rejection, especially since the main “findings” of this paper that compact architectures can be further pruned down for improved accuracy/efficiency tradeoff, and that pruning a larger compact model results in models that outperform smaller models trained from scratch, have been already shown in many of the previous works on neural pruning. In fact, compact networks such as MobileNets and EfficientNets are the standard architectures for measuring the effectiveness of pruning techniques, and thus the contribution of this work reduces down to showing that the same results can be obtained with FBNets. This could be of interest to some practitioners, but is definitely not sufficient to warrant publication.
Main content: Proposes combining flexible activation functions   Discussion: reviewer 1: main issue is unfamiliar with stock dataset, and CIFAR dataset has a bad baseline. reviewer 2: main issue is around baselines and writing.  reviewer 3: main issue is paper does not compare with NAS.  Recommendation: All 3 reviewers vote reject. Paper can be improved with stronger baselines and experiments. I recommend Reject.
The topic of the paper is the use of partial information decomposition (PID) for the analysis of interactions in latent representations.  All reviewers ended up appreciating the paper after a good extensive discussion with the authors. The numerical investigation is somewhat on the short side. One reviewer asks for more ablation studies and one reviewer asks for more investigation on real datasets to show the advantage of the method.  The paper is borderline. The theoretical development is fine. But one could argue that the paper could benefit from some more work on the experiments. However, the main points of the method is in place and further validation of the method can be left for future contributions.
The authors develop novel adaptive adversarial attacks for 3D Point Cloud Classification tasks. They show that many existing defenses are broken by develop a novel pooling operation, DeepSym, and demonstrate that using this they can achieve significant improvements in adversarial robustness of 3D Point Cloud Classification.   All reviewers agreed that the paper makes interesting contributions and that the setting of 3D point cloud classification is interesting from a security perspective. The shared concern of the reviewers was around novelty. This was addressed in the rebuttal to some extent, but there remained some lingering questions that made this paper borderline and unfortunately, the program committee had to decide to reject it.  I would urge the authors to revise their manuscript to clarify the novelty relative to prior work (particular those that use similar pooling operations) and resubmit to a future venue. 
The paper proposed a new kind of activation function called matrix activation function that can be learnt jointly with the weights and biases. The paper got 2 strong rejects and 3 rejects. The major challenges include unclear motivation, limited novelty, incomplete related work, weak experiments, and poor paper writing. The author rebuttals did not convince the reviewers. The AC also read through the paper and agreed that the paper is below the bar of ICLR. In particular, the authors neglected a large literature of learning activation functions in the original version,   (two more examples:  [*] Xiaojie Jin, Chunyan Xu, Jiashi Feng, Yunchao Wei, Junjun Xiong, Shuicheng Yan: Deep Learning with S Shaped Rectified Linear Activation Units. AAAI 2016: 1737 1743.  [#] Yan Yang, Jian Sun, Huibin Li, Zongben Xu: ADMM Net: A Deep Learning Approach for Compressive Sensing MRI. NIPS 2017. )  making them unable to compare with existing learnable activation functions thoroughly in the revised version in order to justify the necessity of using matrix activation functions. So the AC recommended rejection.
The paper proposes a new framework to express and analyze embedding methods based on the stable coloring problem. Reviewers highlighted as strengths that the paper provides an interesting perspective for understanding one of the central approaches in NLP, graph learning, and other fields   and as such could inspire promising research directions. However, reviewers raised concerns regarding the significance of contributions (theoretical insights and analysis, relation to prior work, missing empirical evaluation etc.) as well as the clarity of presentation (also with regard to correctness and scope). All reviewers and the AC agree that the paper is not yet ready for publication at ICLR and would require an additional revision to address the aforementioned issues.
There wasn t enough enthusiasm to push this paper over the bar, based on no reviewer championing the paper (the one score above 6 was consulted and thought this was a fair assessment). The reviewers appreciated the contributions of the paper but felt that in terms of technical depth, there was a lot of overlap with prior work, and the statements of the results themselves were good but not exciting enough to convince the reviewers. Some suggestions for further improvement that came up were to try to extend this to update time for low rank approximation, which was an application that other work that built off of Cohen et al did, see, e.g., https://arxiv.org/abs/1805.03765 . Regarding presentation, it would be great if in a re submission the authors handle the presentation concerns of some of the reviewers regarding the experiments.
The article is concerned with depth width tradeoffs in the representation of functions with neural networks. The article presents connections between expressivity of neural networks and dynamical systems, and obtains lower bounds on the width to represent periodic functions as a function of the depth. These are relevant advances and new perspectives for the theoretical study of neural networks. The reviewers were very positive about this article. The authors  responses also addressed comments from the initial reviews. 
The paper predicts properties of quantum states through RNNs.  The idea is nice, but the results are very limited and require more work.  It seems to be more suited for a conference focussing on quantum ML even when the authors have an ML background.  All reviewers agree on a rejection, and their arguments are solid.  The authors offered no rebuttal.
This paper proposes a few shot learning method that uses Fisher information matrix based task affinity. The experimental results show that the proposed method achieved better performance than existing methods. This paper is well written. The newly proposed task affinity score is interesting. The experimental results and theoretical analysis support the effectiveness of the proposed method. The authors are encouraged to address the reviewers  concerns in the paper.  Although the distance between task representations is symmetric in neural processes, they do not use the symmetric distance for meta learning. They input the task representations into the neural network, so the output can be asymmetric.
This paper analyses a recurrent neural network model trained to perform a simple maze task, and reports that the network exhibits multiple hallmarks of neural selectivity reported in neurophysiological recordings from the hippocampus— in particular, they find place cells which also are tuned to task relevant locations, cells which anticipate possible future paths, and a high proportion of neurons tuned to task variables.    The reviewers appreciated the interesting empirical analysis, and the demonstration that multiple such features could arise in the same neural network— to the best of my knowledge, this had not been demonstrated explicitly before. However, there were also multiple concerns, which lead to this paper beeing discussed extensively and controversially. In particular, it is not clear which features arise from which learning objective, for example, for place cells to arise, do we  just need sensory prediction, or do we need q learning? In addition, there were some points in which the tightness of the analogy between model and biology is questionable— in particular, this refers to the comprising between hippocampal recordings and the evaluation of the network.  Finally, it is also clear that  some of these observations reported in the paper are, indeed, empirical observations rather than explanations. Because of these shortcomings, there was no consensus and strong support from the reviewers for acceptance of the paper.  After extensive discussion between both the reviewers, the AC and the program chair, the final decision was to not accept the paper. We do hope that the reviews will help you in improving the study and its presentation. It clearly has potential to be a valuable contribution to the literature.  
This paper presents a simple yet effective approach to improve self supervised contrastive approaches like MoCo. There are concerns with respect to novelty/simplicity and low improvements over MoCov2. AC believes that simplicity is good and while gains might not be as huge, they still show usefulness of new loss. It might also provide insights for future papers on self supervised learning. Overall, the sentiment is that paper is above the bar.
I agree with the reviewers that this paper has serious limitations in the experimental evaluation.
This paper presents a deep learning method that aims to address the curse of dimensionality problem of conventional convolutional neural networks (CNNs) by representing data and kernels with unconstrained ‘mixtures’ of Gaussians and exploiting the analytical form of the convolution of multidimensional Gaussian mixtures. Since the number of mixture components rapidly increases from layer to layer (after convolution) and common activation functions such as ReLU do not preserve the Gaussian Mixtures (GM), the paper proposes a fitting stage that fits a GM to the output of the transfer function and uses a heuristic to reduce the number of mixture components. Experiments are presented on MNIST (2d) and ModelNet10 (3D), which show competitive performance compared to other approaches such as classic CNNs, PointNet and PontNet++ methods.  There is somewhat an overall consensus on the novelty of the proposed approach and its potential to pave the way for further research. There were, however, several issues raised by the reviewers in terms of clarity, memory footprint and computational cost that limits the applicability of the method to more complex datasets. While the authors expanded on the dense fitting in their comments and in the revised version of the paper, it still remains unclear the role of the negative weights, as the dense fitting stage seems to constrain all the weights to be positive. In terms of memory footprint, the authors refer to the theoretical footprint and their implementation does not match this. Finally, it is acknowledged by the authors that the computational cost is a limitation that hinders the method from achieving competitive performance in more complex tasks.
The paper conducts a series of empirical studies to evaluate the robustness of smoothed attribution methods. Although the reviewers think this is an important direction, there are several concerns about the experimental settings, such as the sample size and the models to be tested. Also, one of the main finding that Lp based smoothing methods are non robust to non Lp norm perturbations is well known and is not that surprising.
In the context of constructing negotiation dialogue strategies/policies, the authors explore the use of graph attention networks (GATs) for determining the sequence of negotiation dialogue acts   specifically leading to a (1) hierarchical dialogue encoder via pooled BERT + GRU encoding  > (2) GAT over dialogue strategies/acts (many technical details around graph usage)  > (3) GRU decoder. While a relatively straightforward replacement relative to similar architectures with other  structural  encoders, they provide a sound end to end training strategy that is shown to perform well on the buyer seller negotiation task via CraigslistBargain dataset where they demonstrate SoTA performance.    Pros   + Studying the pragmatics component of negotiation dialogue strategies has received recent interest and this seems a good milepost that demonstrates mainstream methodological approaches for this task (i.e., this is a good baseline for future innovations) + The paper is well written in that it is easy to understand intuitively while having sufficient detail to understand the details. + The empirical results appear promising and meet the standard within this sub community   showing improvements with automatic and human evaluation.    Cons      This builds on existing datasets, which are known to have undesirable properties (e.g., automatic evaluation, small number of dialogue datasets, use of explicit dialogues acts, etc.) While it still meets the standards of this sub community, it still isn t a completely convincing task.   While the use of GATs is novel in this setting and they get it to work within the overall architecture, this is something that many people are likely trying at this time   so there isn t an exciting  disruptive  step here.   The empirical results, while satisfactory from a quantitative perspective, even in reading the Appendices, it isn t clear that these are significantly better from a planning perspective or if it is just  pattern recognition  gains.  Evaluating along the requested dimensions:   Quality: The underlying method is fairly straightforward and the authors incorporate up to date GAT related methods to get this to work in this setting. The empirical results are sound if predicated on the general quality in this sub community where you have the standard machine translation evaluation problem for meaning vs. lexical closeness. To mitigate, they use BERTScore and human evaluation   which is at the higher end of what can be reasonably expected.   Clarity: The paper is written clearly overall, especially if considering the appendices where there is significant detail. Related to empirical evaluation, it isn t easy to intuitively interpret the results, but this is again par for the course. Additionally, I believe the authors did a good job responding to reviewer concerns.   Originality: While all of the reviewers agreed that the approach was novel in this setting, one of the reviewers explicitly pointed out that using GATs in negotiation dialogues isn t that exciting   and I mostly agree. I view this as something that somebody would have done and will serve as a good baseline; although I think this sub field is going to need more datasets to continue progressing.   Significance: As stated above, it is a good baseline that I think many are likely thinking of (as the TOD community has been doing this for a bit now). However, it is done well.  Honestly, I agree with the reviewers that this is a somewhat borderline paper   mostly due to it being a fairly  obvious  idea and the nature of the subfield making it not entirely clear if the improvements are due to knowing the target performance while training or due to the methodological advance. Personally, I am convinced, but it isn t totally clear. That being said, it is a well written paper and I think the reviewer issues were sufficiently addressed. Thus, I would prefer to see it accepted as I think it will be a strong methodological baseline for this problem (which hopefully will accumulate more convincing datasets and standard evaluation). 
After reading the reviews and the rebuttal I unfortunately feel the paper is not ready to be accepted.   The reasoning for this decision is as follows: * the empirical evaluation is somewhat weak in its current format, and even adding experiments going from BlockStacks to MNIST would have improved the results, or potentially other synthetically generated data. Or playing with which relation is used during the transfer phase. Something to give a bit more weight to the empirical section and help it connect better with the theoretical one  * But maybe more importantly (and to some extend this is true for the formalism introduced as well), I think there needs to be a bit more context. After reading the reviews, I went and read the paper, and for example in results provided, it is not clearly explained what is the relationship between the proposed method and some of the baselines. I noticed that the related work section ended up in the appendix, which is fine, to the extent the main text can connect to the literature a bit. But while I agree that the introduction of the method seems good and clear, and this is a hard and important problem that lacks a proper framework and the proposal in the paper is quite interesting. It is also important to understand its relation to other frameworks, and to explain clearly what it tries to fix in other proposal. And to interpret the result, maybe justifying or providing some intuition of why the proposed model performs better. I think this is very crucial particularly for a topic that is still in a growing phase, which makes it harder to judge. I know in the appendix, the author mention domain adaptation which is also something that jumps in mind when looking at this architecture. However this point is not discussed or mentioned as much in the main paper.  In current form, while the paper reads well, one is left to trying to understand whether these results are significant. I think the work is definitely very interesting and I hope the authors will resubmit it with modification. I just feel in the current format it will not have the impact it should, because of a preserved weak experimental section and not a clear grounding in the literature, making readers unsure of the significance of the work.
This paper studies multivariate time series forecasting by making relational inference in a latent space. It attempts to address the important issue of reducing the computational complexity of the inferred graph. This motivation is well articulated.  Despite its merits, concerns have been raised regarding the relatively weak evaluation without using datasets involving more many nodes to demonstrate the scalability of the proposed method, which is a major selling point of the paper. As such, while the motivation of the work is clear, its experimental evaluation is not thorough enough to demonstrate the scalability of the proposed method.  The authors made the remark in their response that they are not aware of any public time series dataset of this size (which is not agreed by another reviewer who pointed out that some much larger datasets were used in other papers). Note that it is not uncommon in other work to use synthetic datasets to evaluate the scalability as well as other properties of the proposed methods.  Moreover, clarity of the presentation also has room for improvement.  The paper has potential for publication in a top venue if the comments and suggestions are incorporated to revise the paper.
The paper is about the use of autoregressive dynamics models in the context of offline model based reinforcement learning. After reading the authors  responses and the other reviews, the reviewers agree that this paper has several strengths (well written, easy to follow, the approach is novel and simple to implement, the empirical evaluation is well executed and the results are reproducible) and it deserves acceptance. The authors need to update their manuscript by keeping into considerations all the suggestions provided by the reviewers (clarifications and additional empirical comparisons).
The paper studies the features extracted by the pre trained language model and how fine tuning makes use of these features. The paper is well motivated by two lines of research in the NLP area   1) probing approaches for understanding the features extracted in the pre training model, 2) model behavior analysis that shows models take shortcuts for making predictions. The paper provides a comprehensive study to bridge the gaps between these two lines of discussion.   All the reviewers agree the paper has strong merits and concerns have been addressed.   
Initially this paper received mixed reviews. After reading the author response, R1 and and R3 recommend acceptance.  R2, who recommended rejecting the paper, did not participate in discussions, did not respond to author explanations, did not respond to AC emails, and did not submit a final recommendation. This AC does not agree with the concerns raised by R2 (e.g. I don t find this model to be unprincipled).  The concerns raised by R1 and R3 were important (especially e.g. comparisons to NMS) and the authors have done a good job adding the required experiments and providing explanations.  Please update the manuscript incorporating all feedback received here, including comparisons reported to the concurrent ICLR submission on counting. 
All reviewers agree that this paper is not ready for publication. In addition to the technical comments, the authors should pay attention to the comments by Reviewer 3 about the naivete of the motivation provided for the work. Filter bubbles (to the extent that they really exist; there is controversy about this) have multifactorial origins. 
The reviewers appreciated the paper s applied neural net approach to the problem of designing features for 2SLS regression for IV analysis as an alternative to sieve approaches. The paper would make a good contribution to ICLR. While the paper does not focus on theory   learning data driven features appears to be mostly heuristic   it should still be grounded in a sound approach to the IV problem, and the reviewers recommend various important technical clarifications regarding the foundations of IV models; the authors should implement these suggestions very carefully and correctly in future versions. For example, even if the structural models are well specified in that Eq. (5) holds for some parameters, since the dependence is non linear on parameters, it is not clear when we should expect this to be identifying of (theta_X,theta_Z) (these are in fact not identifiable in general) and when we should expect the proposed method to be consistent.
This work proposes a modification of a GNN architecture by feeding random node features to bootstrap the message propagation. This enables the discriminability of automorphic node pairs with a lightweight, simple change. Experiments are reported showing improvements over baselines.  Reviewers had mixed impressions of this work. On one hand, they found the proposed model principled and with strong empirical performance. On the other hand, they perceived a general lack of novelty and a somewhat misleading theoretical analysis. After careful review, the AC ultimately believes that this work does require an extra iteration that further solidifies the contributions and aligns the theoretical analysis with the empirical performance. In particular, the use of random initialization is folklore in the GNN literature, especially with regards to spectral methods (e.g. power iterations are typically initialized using a random vector, and these constitute the simplest forms of linear GNNs). The authors are encouraged to address these comparisons with further detail, as well as the excellent feedback given by the reviewers. 
The paper combines logical reasoning and statistical methods to improve knowledge graph completion. Rules are mined from the KG using AMIE and recursive backward steps are taken, using the mined rules, to determine if a fact is true. The reviewers agree that the paper can be improved by explaining more details of the method to make it more easy to understand.  
The paper proposes a method for decentralized learning of cooperative games by maximizing the mutual information between the agents. The paper is novel and interesting and well evaluated.  Prior to the rebuttal, most of the reviewers saw presentation as the biggest weakness. Specifically, it was not clear what InfoPG refers to, and how it is related to the mutual information. During the rebuttal the authors cleaned up the misunderstandings around the presentation and provided a detailed analysis in the Appendix.  While the author responses provided helpful clarification and analysis, the authors should revise the paper holistically to remove unnecessary terminology and connections, and bring the analysis in the main text.
The paper proposes a new scoring function for OOD detection based on calculating the total deviation of the pairwise feature correlations. This is an important problem that is of general interest in our community.  Reviewer 2 found the paper to be clear, provided a set of weaknesses relating to lack of explanations of performance and more careful ablations, along with a set of strategies to address them. Reviewer 1 recognized the importance of being useful for pretrained networks but also raised questions of explanation and theoretical motivations. Reviewer 3 was extremely supportive, used the authors  code to highlight the difference between far from distribution behaviour versus near distribution OOD examples. The authors provided detailed responses to all points raised and provided additional eidence. There was  no convergence of the review recommendations.  The review added much more clarity to the paper and it is no a better paper. The paper demonstrates all the features of a good paper, but unfortunately didn t yet reach the level for acceptance for the next conference. 
The presented method uses mode connectivity to help illustrate the surfaces of parameter space between various selections of models (either through changes of parameters, learning methods, or epochs), and canonical correlation analysis (CCA) to visualize the similarity of model layers across two different selected models.  These analyses are then used to study 3 forms of learning heuristics: stochastic gradient descent with restart (SGDR), warmup, and distillation.   Reviews tend to be leaning toward acceptance.   Pros: + R1: Well written + R1: Papers that analyze learning strategies are generally informative to the larger community. These experiments haven t been previously performed. + R1: Thorough experiments + R3: Results brought into context of prior hypotheses  Cons:   R3: Batch normalization not studied, but authors have added experiments in response.   R3 & R2: Practical implications not clear, but authors have added a discussion.  
This paper aims to improve the efficiency of adversarial training. Specifically, by analyzing the differences between the adversarial perturbations generated by FGSM RS and the adversarial perturbations generated by PGD, this paper proposes a new single step attacker I PGD (which imitates PGD by creating diverse adversarial perturbations) to accelerate adversarial training. Empirical results are provided on CIFAR 10 and Tiny ImageNet to support the effectiveness of the proposed method.  Overall, the reviewers think it is an interesting paper, but are severely concerned about some statements. During the discussion period, the authors actively clarify these points. However, the Reviewer bLbt is not fully convinced and believes 1) the approximation in Eq. (6) is incorrect and 2) the proposed method is loosely motivated by imitating the behavior of PGD. The authors fail to further follow up on this discussion. The Reviewer bLbt and the Reviewer Dz2K are also concerned that the proposed I PGD AT only yields margin improvements over Fast Adversarial Training. In addition, as suggested by the reviewer AAHj, given this work focuses on developing efficient adversarial training, it is important to include results on larger scale datasets like ImageNet.  I encourage the authors to incorporate all the reviewers  comments and make a stronger submission next time.
PROS:  1. Interesting and clearly useful idea 2. The paper is clearly written. 3. This work doesn t seem that original from an algorithmic point of view since Reed & De Freitas (2015) and Cai et. al (2017) among others have considered using execution traces. However the application to program repair is novel (as far as I know). 4. This work can be very useful for an educational platform though a limitation is the need for adding instrumentation print statements by hand.  CONS:  1. The paper has some clarity issues which the authors have promised to fix.   
The paper introduces a linear projection method, inspired by ANOVA, for finding a supervised low dimensional embedding.  A positive aspect is that the method is straightforward, and it is even slightly surprising that in the family of linear models, there still was an uncovered "niche".  The paper was considered useful for the purpose studied in the paper, single cell RNA seq data analysis. But to claim broader usefulness, more evidence should be presented.  One particular detail which was brought up by all reviewers was the PCA preprocessing. For ICA it is a sensible choice, as linear ICA is essentially "just" a rotation of the PCA components. But the justification is not as good for a supervised method. PCA may be necessary in practice, but may lose important category relevant information.  The paper still needs a significant revision before publication.  Even though the method is straightforward method, a lot of time and discussion was required for expert reviewers to understand it. 
The paper describes an architecture search method which optimises multiple objectives using a genetic algorithm. All reviewers agree on rejection due to limited novelty compared to the prior art; while the results are solid, they are not ground breaking to justify acceptance based on results alone. 
# Quality: The technical contribution of the paper seems reasonable and there were only minor points being highlighted by the reviewers.  # Clarity: The paper would benefit from being more polished. During the rebuttal, the authors suggested that several reviewers misunderstood the paper. This alone should encourage the authors to improve clarity.  # Originality: Several reviewers presented concerns about the claims of the authors and the existence of connections to existing literature. Nonetheless, the proposed approach seems novel to the best of the reviewers and my knowledge.  # Significance of this work:  The topic of the manuscript is relevant and impactful. However, several reviewers suggested to include additional baselines in the experiments to validate the goodness of the proposed approach.  # Overall: The paper presents an interesting idea, with a high potential impact. Despite the interesting topic and some interesting insights, all the reviewers agree that the manuscript is not ready for publication just yet. I want to encourage the authors to keep improve it and resubmit it at the next conference.
 * Strengths  This paper applies deep learning to the domain of cybersecurity, which is non traditional relative to more common domains such as vision and speech. I see this as a strength. Additionally, the paper curates a dataset that may be of broader interest.  * Weaknesses  While the empirical results are good, there appears to be limited conceptual novelty. However, this is fine for a paper that is providing a new task in an interesting application domain.  * Discussion  Some reviewers were concerned about whether the dataset is a substantial contribution, as it is created based on existing publicly available data. However, these concerns were addressed by the author responses and all reviewers now agree with accepting the paper.
Most of the reviewers agree that this paper presents an interesting idea. Practically implementing a BNN that gains real world speedup is challenging, and as past work [1] showed, the bottleneck could shift into other layers(besides the accumulation). The paper would benefit from a thorough discussion about the practical impact in implementing the proposed method and relation to past work.   The meta reviewer decided to accept the paper given the positive aspects, and encourages the author to further improve the paper per review comments.  Thank you for submitting the paper to ICLR.   [1] Riptide: Fast End to End Binarized Neural Networks 
This work targets an important problem: susceptibility of ML models to adversarial perturbations that make them completely misclassify an input, as opposed to "just" fail to get the right fine grained class while getting the correct coarse grained one. This natural question did not receive enough attention so far, so having this work look into it is a definite plus.  However, as the reviewers point out, this study has a number of issues in terms of the methodology of the experiments. For example, it is unclear whether the proposed (natural) variant of training the robust model is particularly beneficial for the stated goal. As such, it seems that the paper is not ready for publication and the authors are strongly advised to revise the article and submit it again.
The paper proposes a way to tackle oversmoothing in Graph Neural Networks. The authors do a good job of motivating their approach, which is straightforward and works well. The paper is well written and the experiments are informative and well carried out. Therefore, I recommend acceptance. Please make suree thee final version reflects the discussion during the rebuttal.
This work proposes a new approximation method for softmax layers with large number of classes. The idea is to use a sparse two layer mixture of experts. This approach successfully reduces the computation requires on the PTB and Wiki 2 datasets which have up to 32k classes. However, the reviewers argue that the work lacks relevant baselines such as D softmax and adaptive softmax. The authors argue that they focus on training and not inference and should do worse, but this should be substantiated in the paper by actual experimental results.
The reviewers generally agreed that the novelty of the work was very limited. This is not necessarily a deal breaker for a largely applied contribution, but for an applied paper, the evaluation of the actual application on edge devices is not present. So if the main contribution is the application, and there is no evaluation of this application, then it does not seem like the paper is really complete. As such, I cannot recommend it for acceptance.
The paper proposes an algorithm to defend against black box attacks. All the reviewers think the current experiments are not convincing enough, and the method seems to have some issues (e.g., not scalable). 
This paper proposes an interesting approach for learning to decide whether a query graph is isomorphic to a subgraph within the target graph.  The approach has a number of interesting aspects from the machine learning perspective, e.g. the anchored graphs and the order embeddings.  Empirical results show promise in ablation studies and against a few baselines.  However this paper also has a number of issues as pointed out by the reviewers, placing it right on the borderline.  Most notably the clarity of the presentation could be improved as it seems to confuse a few reviewers at various points.  Another thing that I’d like to highlight is that the way to convert the pairwise scores f(z_q, z_u) into the final decision about G_T and G_Q seems worthy of a longer discussion.  Is a simple average across all pairs the best we can do?  I imagine if the query graph is small but the target graph is large then even if the G_Q does match a subgraph of G_T the average score can be quite low.  Overall I do like the ideas proposed in this paper, but also recognize that the paper can benefit from more improvement, so I’d like to recommend rejection but encourage the authors to submit again in the next round.
In this paper, the authors proposed a general framework, which uses an explicit function as an adjustment to the actual learning rate, and presented a more adaptive specific form Ada+. Based on this framework, they analyzed various behaviors brought by different types of the function. Empirical experiments on benchmarks demonstrate better performance than some baseline algorithms. The main concern of this paper is: (1) lack of justification or interpretation for the proposed framework; (2) the performance of the proposed algorithm is on a par with Padam; (3) missing comparison with some other baselines on more benchmark datasets. Plus, the authors did not submit response.  I agree with the reviewers’ evaluation.
Originality: The paper can be developed into a very nice contribution, if the value of the newly introduced optimization variance is evaluated more thoroughly (e.g., through simple theory, or through more rigorous experiments).  Main pros:   One of the early works studying epoch wise double descent   Optimization variance is an interesting concept and might very well be useful for finding good early stopping points.  Main cons:   Findings about epoch wise double descent remain inconclusive   optimization variance is not sufficiently evaluated to judge its usefulness: theoretical justification for why is this an important quantity and when does it arise naturally is something missing at this point.  Overall: there was a consensus that the paper focuses and provides an interesting story, with new ideas; however, paper s conclusions are not strongly supported by experiments; more experiments are needed to make arguments conclusive. 
The authors propose a deep multi agent RL framework to compute equilibria in a economics problem. Several reviewers raised issues with the presentation, as well as issues with evaluating the impact of the work, partly because the novelty of the approach is made insufficiently clear. While the authors have resolved some of the confusions arising from the presentation in their rebuttal, resulting in 2 out of the 4 reviewers to increase their score, the concerns regarding novelty mostly remain. For these reasons, I don’t think this work is ready for publication at ICLR at the moment and recommend rejection.
Authors present a method to disentangle epistemic from aleatoric uncertainty for avoiding the noisy TV problem during self driven exploration. This is an important area where we need more ideas and experiments. The authors present a biologically inspired approach and through experiments. Although it doesn t present the state of the art exploration in well known RL environments, I acknowledge that new solutions to problems that were previously intractable often would face such an issue. The prediction to discriminate neuroscientific modulations that directly encode epistemic and aleatoric uncertainty is bold but not very specific. Unfortunately, as the reviewers noted, the manuscript in the current form doesn t quite meet the bar yet. I suggest comparing methods for directly estimating uncertainty. I also suggest adding discussion on the estimation bias for the epistemic uncertainty for the proposed method. I strongly encourage the authosr to continue this interesting line of work.
The paper presents a hierarchical version of NMF for the CP decomposition of tensors.  The idea is similar to Chinocki etal 2007 and extends Gao etal 2019, and in Chinocki was presented for the standard linear formulation with regularisation terms.  The extension here doesn t use the standard ALS algorithm but rather presents a neural network analogue, though the functions are still linear, its just that back prop etc. are used for the computation.  The authors point out their formulation is a more flexible representation and optimisation (in response to AnonReviewer5), and thus represents an improvement.  While this is an interesting implementation, in NNs, the model is still fairly simple.    Moreover the experimental results are restricted to a few data sets.  There are literally hundreds of NMF variants in publication and many different evaluations are done.  The experimental work here, while showcasing the work, is not extensive.  For instance, more empirical comparisons should have been made against prior hierarchical NMF on a battery of data.  So this is good, publishable work, and the authors have repaired many of the issues raised by the reviewers.  The work, however, is borderline in empirical work and the contribution is not strong.
This paper presents a Bayesian GAN approach designed for a federated learning setting. In contrast to recent Bayesian GAN approaches that use Gaussian priors or iteratively updated priors on GAN parameters, this paper proposes a more complex prior motivated by expectation propagation, dubbed as EP GAN, and uses this formulation to construct a federated GAN. The paper claims that this prior better captures the multimodal distribution structure of the non iid heterogeneous data across the different clients.  The paper looks at an interesting problem, i.e., federated training of GANs, which is indeed a problem that has received a lot of interest lately. The paper received mixed reviews. The reviewers raised several concerns, some of which included (1) weak baselines, (2) not considering what happens when we switch to more advanced GAN models, (3) performance of the approach when the number of clients is large, and (4) lack of clarity in the presentation.   The authors responded to some of these concerns and it is commendable that they reported some additional results during the discussion phase. However, after an extensive discussion among the reviewers and between reviewers and authors, and after my own reading of the manuscript, concerns still lingers over many of the above mentioned points. Another concern is the overly complex nature of the approach as compared to other recent federated GAN approaches which raises the question as to whether the actual improvements warrant the complexity of the proposed approach. From the report experiments, the improvements appear to be rather slim.  Considering these aspects, unfortunately, the paper in its current shape does not seem ready for acceptance. The authors are advised to consider the feedback from the reviewers which will strengthen the submission for a future submission.
The authors introduce an approach for designing pseudo labels in semi supervised segmentation. The approach combines the idea a refining pseudo labels with self attention grad CAM (SGC) and a calibrated prediction fusion, and consistency training by enforcing pseudo labels to be robust to strongly augmented data.   The reviewers overall like idea and point out the good level of performance obtained by the method in the challenging semi supervised context. However, they also point out the limited novelty of the approach, and the need for a better positioning with respect to related works. After rebuttal, reviewers were satisfied with authors  answers and paper modifications, and all recommend acceptance. \ The AC considers that the submission is a nice combination of existing techniques and likes the simplicity of the one stage approach, which reaches good performances. Therefore, the AC recommends acceptance.
The main criticisms were around novelty: that the analysis is rather standard. Given that all the reviewers agreed the paper is well written, I m inclined to think the paper will be a useful contribution to the literature. The authors also highlight the analysis of the discretization, which seems to be missed by the most critical reviewer. I would suggest to the reviewers that they use the criticisms to rework the paper s introduction, to better explain which parts of the work are novel and which parts are standard. I would also suggest that standard background be moved to the appendix so that it is there for the nonexpert, while making the body of the work more focused on the novel aspects.
The reviewers all raise critical issues with regard to both description and equations, and indicate that figures are not helpful. This is even after the revision. In response to KMX2, the authors suggested they will post additional experiments, but did not return to indicate that. This seems critical to answer empirical concerns about generality of the approach. We recommend to address these issues, if the authors decide to resubmit.   The meta review and recommendation discount the review of RC7c. Unfortunately, the AC and other reviewers, weren t able to engage RC7c in the discussion and the review was extremely short.
I would like to thank the authors for the their time and effort on this work. The paper is proposing an activation function that combines RELU like piecewise activation functions and a primitive attention mechanism. Then, they show that their proposed method works better in transfer settings.  I think the approach authors taking here is more akin to a gating mechanism rather than an attention. So I would recommend the authors to change the name perhaps Gated Rectified Linear Units. The paper is interesting, but I agree with AnonReviewer4, that the experiments are not very convincing focusing on small scaled experiments in the supervised learning setting only. I would recommend the authors to compare their approach against other results from the literature as well. As it is right now it is not clear how significant the results in this paper are. I don t think the transfer and meta learning experiments are very well motivated in this paper. I would recommend the authors to better motivate those results.  After considering my suggestions above and the comments from the reviewers I would recommend the authors to consider resubmitting to another conference.   
The authors propose to approximate the kernel matrix used in the Sinkhorn algorithm by a combination of sparse + low rank approximation. To do so, the authors propose to compute a low rank approximation of a sparsified (thresholded below a certain value to be 0) kernel matrix using Nyström, and then correct it by adding back the true entries at non sparse entries, after removing those obtained from the approximation. This results in a matrix whose application then results in sparse + low rank.  The first version of the paper contained mostly experimental evidence, which was deemed a bit short by some reviewers.The authors have added theoretical material on the way. Although I believe these are worthy additions, as AC, I do not feel comfortable accepting the paper as of now, because I believe these additions were not properly reviewed. I understand this must be disappointing for the authors, who have sprinted to add new content during the rebuttal phase, but I hope they agree that the rebuttal process is not here to handle entirely new sections, but rather to improve existing parts. In particular, that section should be reviewed by authors knowledgeable on low rank kernel factorization, something I did not see in the pool of reviewers. I also believe the paper still has a few shortcomings. Taken together, I therefore recommend a re submission.  ideas to improve the paper    the authors claim to use Nyström on a sparsified matrix (see eq. 4). The sparsified kernel is no longer positive definite. I would like the authors to comment on this. I understand Nyström could be used naively without any psd ness guarantees, but I think a heads up is needed.There are, furthermore, several local/global factorizations of kernel matrices available out there (e.g. MEKA, https://www.jmlr.org/papers/v18/15 025.html), the main difference here being that the product by such approximation must be guaranteed to be positive for it to work in the Sinkhorn algorithm. I would expect that bounds in expectation to break down sometimes, and therefore result in "catastrophic" failures (i.e. nan s). I think that an algorithm that claims to improve or replace another one, and which has such blind spots, needs such additional experiments (I have read the Limitations section in Appendix B, something more precise would improve the paper). I understand these were not part of the original Nyström paper for Sinkhorn, but since this is an increment over that previous work, therefore lacking a bit its originality, more knowledge needs to be contributed.    For instance, since the authors write an entire paragraph on this (Appendix B), I am surprised that there is not direct mention to the fact that a sparse sinkhorn may simply *not* converge, because it may not satisfy the fully indecomposable property required of matrices for Sinkhorn s algorithm to converge.     i dont think that users have the various identities (14,15) in mind when they think about "backpropagating" through Sinkhorn. What is typically needed is to compute the differentiable properties of the regularized OT matric and/or of the regularized OT cost w.r.t. *point locations* (i.e. x_i). The statement "LCN Sinkhorn works flawlessly with automatic backpropagation" is misleading in the sense that it ignores that problem altogether. Since so many extensions of OT today relay on that differentiability, the section, as it is written now, is problematic.    several methods claim to be faster of more efficient than Sinkhorn to solve OT. Either these methods display faster theoretical convergence (e.g. by using acceleration) or display faster practical convergence (e.g. heavy ball variants) using synthetic, controlled datasets. Using synthetic data helps exhibit highlight relevant regimes for regularization parameters, including those where LSE Sinkhorn may converge but LCN does not work, or vice versa. I understand that the authors  wanted to use real data, but it would be great to clarify whether that setup was used because LCN works better there (in which case this becomes more of a paper at the intersection of OT and word embeddings) or because this happened to be the first and only example the authors thought of.
The paper looks at the worst class adversarial error for multi class classification problems. The question is given a certain level of adversarial error on average, is it possible that some classes have adversarial error significantly worse than average? And if so, is this a problem? I agree with the authors that there are applications where such an imbalance could be problematic; other than the examples provided by the authors I can also think of this being important from a point of view of fairness, depending on what exactly the class labels represent. The reviewers have raised the question of low accuracies reported in the empirical results compared to the state of the art on those datasets for adversarial learning. I share these concerns   especially it s worth understanding whether more accurate models also have such an imbalance, or whether this imbalance is a result of incomplete training or models that are not representationally powerful enough. While I agree with the authors that  state of the art  results  are not required for ICLR submissions, especially those making conceptual contriubtions, in this case I think further experiments may be needed in addition to addressing the other questions raised in the reviews. The authors acknowledge that they have made significant revisions in response to the reviews, but I think that would require a fresh review cycle.
This paper considers the question of how to quantize deep neural networks, for processors operating on low precision integers.  The authors propose a methodology and have evaluated it thoroughly. The reviewers all agree that this question is important in practice, though there was disagreement about how novel a contribution this paper is specifically, and on its clarity. The clarity questions were resolved on rebuttal, so I lean to accepting the paper.
This paper proposes a GAN based method to recover images from a noisy version of it. The paper builds upon existing works on AmbientGAN and CS GAN. By combining the two approaches, the work finds a new method that performs better than existing approaches.  The paper clearly has new interesting ideas which have been executed well. Two of the reviewers have voted in favour of acceptance, with one of the reviewer providing an extensive and detailed review. The third reviewer however has some doubts which were not resolved completely after the rebuttal.  Upon reading the work myself, I am convinced that this will be interesting to the community. However, I will recommend the authors to take the comments of Reviewer 2 into account and do whatever it takes to resolve issues pointed by the reviewer.  During the review process, another related work was found to be very similar to the approach discussed in this work. This work should be cited in the paper, as a prior work that the authors were unaware of.  https://arxiv.org/abs/1812.04744 Please also discuss any new insights this work offers on top of this existing work.  Given that the above suggestions are taken into account, I recommend to accept this paper. 
This paper proposes a flexible environment for studying never ending learning. During the discussion period, all reviewers found the paper to be borderline.  Pros:   we don t have good lifelong or never ending RL environments, and this paper seems to provide one   includes a number of interesting features such as multiple input modalities, non episodic interactions, flexible task definitions  Cons:   procedurally generated, toy environment   unclear if the environment reflects the characteristics of real world NEL problems  In the balance, I think the environments add value to the RL community, and being presented at ICLR would increase its visibility.
Contributions of this type are very important for the community. There is a great deal of confusion among practitioners about how to pick optimizers. Perhaps worse, there is confusion among optimization researchers about how to demonstrate the effectiveness of their novel algorithms on deep learning tasks. I applaud this paper as one of the best attempts to make sense of this confusion.  Unfortunately, I am recommending that it is rejected. This was an extremely difficult decision. This paper was very thoroughly discussed by reviewers, both with the authors and after the feedback phase. I agree with R4 that this paper is exemplary in terms of its breadth of optimizer choices. I also agree with R3 that this paper s choices regarding hyperparameter search spaces and seed fixing significantly diminish the contribution of the paper at hand. The key issue that persuaded my decision centered on whether the paper s evidence supported its conclusions.  The two key conclusions that I want to highlight are:  1. *evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer*  2. *different optimizers exhibit a surprisingly similar performance distribution compared to a single method that is re tuned or simply re run with different random seeds*  These conclusions can only be supported if optimizers are well tuned. Based on R3 s remarks and a quick reading of the paper, I am concerned that the use of fixed search spaces means that these optimizers cannot be considered well tuned. This concern splits into two sub concerns.  1. I appreciate the author s desire to encode "no prior knowledge about well working hyperparameter values". Unfortunately, I don t think this is realistic or possible. The learning rate range used in this paper did not include 1e100 for good reasons, all of which depend on the prior knowledge of our community. This isn t just a glib concern, the apparently neutral search spaces may bias the conclusions towards well known methods whose hyperparameters are well understood.  2. I am also skeptical of the choice to use the same range for hyperparameters with "similar naming". The reason is that these hyperparameters *may have been misnamed by the inventors* and may, in fact, play very different roles in the dynamics of optimization.  Top line conclusions have a way of becoming memes in our community. Therefore, it is critical that conclusions, as stated, are actually supported by the experimental design and the empirical evidence. Unfortunately, I am not confident that this is is the case for the paper at hand.  It is clear that this paper represents a heroic effort by the authors. I am aware of the challenges involved in getting this type of paper published and of the urgent need for them. I hope that the authors address the concerns that I expressed and the concerns of the reviewers in a future submission.
This paper proposes to combine Depthwise separable convolutions developed for 2d grids with recent graph convolutional architectures. The resulting architecture can be seen as learning both node and edge features, the latter encoding node similarities with learnt weights. Reviewers agreed that this is an interesting line of work, but that further work is needed in both the presentation and the experimental front before publication. In particular, the paper should also compare against recent models (such as the MPNN from Gilmer et al) that also propose edge feature learning. THerefore, the AC recommends rejection at this time. 
The reviewers are in consensus. I recommend that the authors take their recommendations into consideration in revising their manuscript.
This paper is on the problem of adversarial example generation in the setting where the predictor is only accessible via function evaluations with no gradients available. The associated problem can be cast as a blackbox optimization problem wherein finite difference and related gradient estimation techniques can be used. This setting appears to be pervasive. The reviewers agree that the paper is well written and the proposed bandit optimization based algorithm provides a nice framework in which to integrate priors, resulting in impressive empirical improvements. 
Well written paper on a novel application of the local reprarametrisation trick to learn networks with discrete weights. The approach achieves state of the art results.  Note: I apreciate that the authors added a comparison to the Gumbel softmax continuous relaxation approach during the review period, following the suggestion of a reviewer. This additional comparison strengthens the paper.
This work proposes a non decreasing quantile functional form for distributional RL, and secondly propose using the distributional error as a means of exploration. The experimental results are very exciting. The paper, however, needs further work before acceptance: the reviewers raised concerns about Theorem 1: a full proof is not included (nor written convincingly during discussion), and while several encouraging experiments were added during the discussion to the paper addressing the reviewers concerns, they fell short (understandably, given the time available).  Thus on this basis, I recommend rejection at this time, but think it likely that with these adjustments the paper will be accepted in future.
The reviewers highlight that several of significant claims of the paper are not backed up by experiments, and the experiments themselves lack sufficient detail, therefore, at this stage, I recommend rejection. I suggest the authors address the questions and comments they have received before considering whether they might resubmit or not.
This paper characterizes a particular kind of fragility in the image classification ability of deep networks: minimal image regions which are classified correctly, but for which neighboring regions shifted by one row or column of pixels are classified incorrectly. Comparisons are made to human vision. All three reviewers recommend acceptance. AnonReviewer1 places the paper marginally above threshold, due to limited originality over Ullman et al. 2016, and concerns about overall significance. 
This paper addresses the challenging application of denoising FIB SEM images. State of the art results are reported on a real and a noisy simulated dataset. Unfortunately, this paper failed to convince the reviewers and received 4 negative ratings. The paper misses critical comparisons against baselines and appears rather limited in scope. The authors failed to provide adequate answers to some of the reviewers  points.
This paper proposes a hybrid LSTM Transformer method to use pretrained Transformers like BERT that have a fixed maximum sequence lengths on texts longer than that limit.  The consensus of the reviewers is that the results aren t sufficient to justify the primary claims of the paper, and that—in addition—the missing details and ablations cast doubt on the reliability of those results. This is an interesting research direction, but substantial further experimental work would be needed to turn this into something that s ready for publication at a top venue.
The reviewers in general found the paper approachable, well written and clear.  They noted that the empirical observation of mode collapse in active learning was an interesting insight.  However, all the reviewers had concerns with novelty, particularly in light of Lakshminarayanan et al. who also train ensembles to get a measure of uncertainty.  An interesting addition to the paper might be some theoretical insight about what the model corresponds to when one ensembles multiple models from MC Dropout.  One reviewer noted that it s not clear that the ensemble is capturing the desired posterior.  As a note, I don t believe there is agreement in the community that MC dropout is state of the art in terms of capturing uncertainty for deep neural networks, as argued in the author response (and the abstract).  To the contrary, I believe a variety of papers have improved over the results from that work (e.g. see experiments in Multiplicative Normalizing Flows from over a year ago).
This paper proposes an approach to type inference in dynamically typed languages using graph neural networks. The reviewers (and the area chair) love this novel and useful application of GNNs to a practical problem, the presentation, the results. Clear accept.
All the reviewers found the paper to contain an interesting idea with insightful experiments. The rebuttal further improved confidence of the reviewers. The paper is accepted.
The authors provide a convolutional neural network for predicting the satisfiability of SAT instances. The idea is interesting, and the main novelty in the paper is the use of convolutions in the architecture and a procedure to predict a witness when the formula is satisfiable. However, there are concerns about the suitability of convolutions for this problem because of the permutation invariance of SAT. Empirically, the resulting models are accurate (correctly predicting sat/unsat 90 99% of the time) while taking less time than some existing solvers. However, as pointed out by the reviewers, the empirical results are not sufficient to demonstrate the effectiveness of the approach. I want to thank the authors for the great work they did to address the concerns of the reviewers. The paper significantly improved over the reviewing period, and while it is not yet ready for publication, I want to encourage the authors to keep pushing the idea to further and improve the experimental results. 
The reviewers and authors have had a significant and healthy discussion around this manuscript. The reviewers remain concerned about the some of the central claims in this manuscript. While they have appreciated the clear communication and willingness of the authors to clarify most of their concerns, this central issue unites the reviewers in maintaining their desire to see a more significant revision of this work before publication. I recommend that the authors take the reviewers  recommendations in improving the presentation and comparison of their ideas.
The initial reviews for this paper were somewhat diverging, however the paper did not receive any significant negative criticism to push it towards below the acceptance threshold. The reviewers have found some minor issues about the paper. Following the reviewer recommendations, the meta reviewer recommends acceptance.
Initially, some reviewers have raised several points of criticism regarding certain aspects of the model whose novelty/significance was a bit unclear. After the rebuttal and the discussion phase, however, everyone agreed that most of these concerns could be addressed in a convincing way, and finally all reviewers were in favor of this paper. After carefully going over all the reviews, the rebuttal and the discussions, I fully agree with the reviewers and came to the conclusion that this paper indeed contains some interesting, novel and relevant contributions.
This paper proposes an approach to semi supervised few shot learning. In a discussion after the rebuttal phase, the reviewers were somewhat split on this paper, appreciating the advantages of the algorithm such as increased robustness to distractors and the ability to adapt with additional iterations, but were concerned that the contributions over Ren et al were not significant. Overall, the contributions of this paper don t quite warrant publication at ICLR.
This work proposes to improve mixed precision training by adaptively scaling the loss based on statistics from previous activations to minimize underflow during training. However, the method is designed rather heuristically and can be improved with stronger theoretical support and improved representation of the paper.  
The paper proposes the replacement of the softmax layer in a neural network with one parametrized by a kernel. The kernel itself is learned during training from the space of radial basis kernels. The resulting models are compared against identical networks with softmax, linear kernels, second order pooling and kervolutions on several datasets, encompassing vision and NLP tasks.  First, the reviewers raised questions about the novelty of the work. Theorem 4.3, based on which the method is derived, has existed in the literature and seems to be related to the uniqueness of the power series expansions for kernels. There is novelty in using this theoretical result to write an approximation of a positive definite kernel in a way which can be learned. Specifically, it is written as a finite weighted sum of existing kernels, where the coefficients are learned. Reviewer pWF3 posed a valid question about the quality of the approximation, to which the authors responded with an equally valid, and comprehensive, appendix on the error bounds of the approximation. Still, it is worth tempering the statement that the search is  exhaustive  over the space of radial kernels or that the kernel is optimal (instead, the search appears over a large class of radial kernels, and the kernel is approximately optimal with an extremely low distance from the actual optimum). Along the same lines of rephrasing claims, reviewer WDU4 also pointed out several statements and claims which were not entirely accurate, which the authors then proceeded to resolve, resulting in notable changes from the initial version of the paper. Specifically, there was mention of a "non parametric kernelized classifier". This has been fixed, but it did seem to have initially confused other reviewers, who suggested related work that, it turns out, are not necessarily suitable contenders. The changes made definitely improved the paper, and resolved most of the reviewer s concerns. Nevertheless, the appendix added comparing the method to non parametric models could be improved. For instance the authors stated "Wilson et al. use Gaussian RBF and spectral mixture kernels. Our method has the capability to automatically learn any positive definite radial kernel. Note that Gaussian RBF and spectral kernels are all radial kernels."   is there any intuition, or proof, of a case when the method introduced here learns a network + classifier that the method by Wilson et al. cannot learn? Or for which deep kernel learning requires considerably more resources? (DKL has been optimized and made considerably faster since the initial paper in 2016).  https://proceedings.neurips.cc/paper/2016/hash/bcc0d400288793e8bdcd7c19a8ac0c2b Abstract.html Also, while the present work is backed by 4.3, DKL also has a theoretical grounding. https://www.jmlr.org/papers/volume20/17 621/17 621.pdf  There was some discussion on the exhaustiveness of the experiments, and it was concluded that the datasets are sufficient, while the reviewers were not in agreement as to whether the authors considered sufficient contenders. A comparison against DKL, at least, appears to be warranted.  Overall, the paper brings a contribution in terms of improving the performance of backbones with limited expressiveness through the use of a kernel parametrized classifier, learned by optimizing an approximation of a formulation that spans the entire space of radial basis kernels. The paper was updated considerably during the reviewer process, to its betterment, however, an experimental comparison against deep learning with non parametric kernelized classifiers is still missing.
In this work, the authors focus on the high dimensional regime in which both the dataset size and the number of features tend to infinity. They analyze the performance of a simple regression model trained on the random features and revealed several interesting and important observations.  Unfortunately, the reviewers could not reach a consensus as to whether this paper had sufficient novelty to merit acceptance at this time. Incorporating their feedback would move the paper closer towards the acceptance threshold.
Transformer models have proven to be quite successful when applied to a variety of ML tasks such as NLP.  However, the computational and memory requirements can at times be prohibitive, such as when dealing with long sequences.  This paper proposes locality sensitive hashing to reduce the sequence length complexity, as well as reversible residual layers to reduce storage requirements.  Experimental results confirm that the performance of Transformer models can be preserved even with these new efficiencies in place, and hence, this paper will likely have significant impact within the community.    Some relatively minor points notwithstanding, all reviewers voted for acceptance which is my recommendation as well.  Note that this paper was also vetted by several detailed external commenters.  In all cases the authors provided reasonable feedback, and the final revision of the work will surely be even stronger.
Following a strong consensus across the reviewers, the paper is recommended for rejection. They have all acknowledged some weaknesses of the paper, for instance  * Inadequate reference to prior work * Unsatisfactory level of polishing * Too limited evaluation, with more comparisons to baselines required * The proposed approach ("Dijkstra algorithm") is not enough justified and motivated  * Clarity (missing definitions of key components).  This list, together with the detailed comments of the reviewers, highlight opportunities to improve the manuscript for a future resubmission. 
This paper introduces a realism metric for generated covariates and then leverage this metric to produce a novel method of interpolating between two real covariates. The reviewers found the method novel and were satisfied with the response form the authors to their concerns. However, Reviewer 4 did have reservations about the response to his/her points 3 and 4. Moreover, in the discussion period it was decided that while the method was well justified by intuition and theory, the empirical evaluation—which is the what matters at the end of the day—was unconvincing.  
This paper tries to explain why Adam is better than sgd for training attention model. In specific, it first provides some empirical and theoretical evidence that a heavy tailed distribution of the noise in stochastic gradients is the cause of SGD s worse performance. Then the authors studied a clipped variant of SGD that circumvents this issue, and revisited Adam through the lens of clipping. Overall, this paper conveys some interesting ideas. On the other hand, the theorems proved in this paper do not provide additional insight besides the intuition and the experiments are weak (hyperparameters are not carefully tuned). So even after author response, it still does not gather sufficient support from the reviewers. This is a borderline paper, and due to a rather limited number of papers the conference can accept, I encourage the authors to improve this paper and resubmit it to future conference. 
The paper describes an RL technique to learn how to branch in discrete optimization.  This advances the state of the art in comparison to previous imitation learning techniques.  However, the reviewers and a public reader raised concerns about the validity of the experiments due to several inconsistencies and differences with previous work that might suggest some cherry picking.  This is too bad since the reviewers really liked the work, but it is important to make sure that the experimental evaluation is done fairly.  I read the paper and I share the concerns regarding the experimental methodology.  Hence the experimental evaluation needs to be revised before publication.
This paper proposes a new loss function that can be used in place of the standard maximum likelihood objective in training NMT models. This leads to a small improvement in training MT systems.  There were some concerns about the paper though: one was that the method itself seemed somewhat heuristic without a clear mathematical explanation. The second was that the baselines seemed relatively dated, although one reviewer noted that this seemed like a bit of a lesser concern. Finally, the improvements afforded were relatively small.  Given the high number of good papers submitted to ICLR this year, it seems that this one falls short of the acceptance threshold.
This paper introduces a technique to generate L0 adversarial examples in a black box manner. The reviews are largely positive, with the reviewers especially commenting on the paper being well written and clearly explaining the method. The main drawbacks raised by the reviewers is that the method is not clearly compared to some prior work, but in the rebuttal the authors provide many of these numbers. On the whole this is a useful and interesting attack that would be worth accepting.
This paper presents a theoretical justification for the Adam optimizer in terms of decoupling the signs and magnitudes of the gradients. The overall analysis seems reasonable, though there s been much back and forth with the reviewers about particular claims and assumptions. Overall, the contributions don t feel quite substantial enough for an ICLR publication. The interpretation in terms of signs is interesting, but it s very similar to the motivation for RMSprop, of which Adam is an extension. The performance result on diagonally dominant noisy quadratics is interesting, but it feels unsurprising that a diagonal curvature approximation would work well in this setting. I don t recommend acceptance at this point, though these ideas could potentially be developed further into a strong submission. 
The paper describes N Bref, a new tool for decompilation of stripped binaries. Compared to previous tools for neural based decompilation, this tool is based on two new ideas: a) to separate the generation of data declarations from the generation of the code itself, and b) the use of more sophisticated network architectures. These network architectures, however, all come from prior work, so the contribution in that regard is only their application to this particular problem.   The authors addressed many of complaints raised by reviewers, particularly with regards to presentation and explanations, but I think the most substantial concerns remain.   The most substantial concern is novelty. The technique is built on a combination of existing models, and its only original idea seems to be to treat the generation of data declarations and the code itself as separate tasks to be handled by independently trained networks.   In terms of results, the paper shows some quantitative improvements over prior work, although it is not so clear that those improvements matter. The quality improvement is measured in terms of AST differences, but it is not clear how often those AST differences translate into semantic differences. More importantly, the tool is restricted to un optimized binaries, which significantly limits its applicability for any real world application. Prior work by Katz et al. is evaluated against optimized binaries, as are other types of lifting such as the Helium project by Mendis et al [1]. Given the prevasiveness of optimization in deployed code, a tool that cannot handle it has virtually no applicability.  I think some significant technical novelty could make up for the lack of evaluation against optimized binaries. Alternatively, strong results on optimized binaries would justify publication even if the technique is built from existing building blocks, but as it stands, I think the paper is too incremental to merit acceptance.   [1]  Charith Mendis, Jeffrey Bosboom, Kevin Wu, Shoaib Kamil, Jonathan Ragan Kelley, Sylvain Paris, Qin Zhao, Saman P. Amarasinghe: Helium: lifting high performance stencil kernels from stripped x86 binaries to halide DSL code. PLDI 2015: 391 402 
Existing works mostly focus on model compression for the classification task. This paper aims for an efficient recommendation system that can well balance the model compression and model accuracy, which therefore brings in new challenges and opportunities. The authors propose to unify the model compression and feature embedding compression and develop an effective and reasonable solution.  The concerns raised by the reviewers have been well fixed and all reviewers agree on the paper s contribution.  The paper is therefore recommended for acceptance. 
The authors design a GAN based text to speech synthesis model that performs competitively with state of the art synthesizers.  The reviewers and I agree that this appears to be the first really successful effort at GAN based synthesis.  Additional positives are that the model is designed to be highly parallelisable, and that the authors also propose several automatic measures of performance in addition to reporting human mean opinion scores.  The automatic measures correlate well (though far from perfectly) with human judgments, and in any case are a nice contribution to the area of evaluation of generative models.  It would be even more convincing if the authors presented human A/B forced choice test results (in addition to the mean opinion scores), which are often included in speech synthesis evaluation, but this is a minor quibble.
I had a little bit of difficulty with my recommendation here, but in the end I don t feel confident in recommending this paper for acceptance, with my concerns largely boiling down to the lack of clear description of the overall motivation.  Standard adversarial attacks are meant to be *imperceptible* changes that do not change the underlying semantics of the input to the human eye. In other words, the goal of the current work, generating "semantically meaningful" perturbations goes against the standard definition of adversarial attacks. This left me with two questions:  1. Under the definition of semantic adversarial attacks, what is to prevent someone from swapping out the current image with an entirely different image? From what I saw in the evaluation measures utilized in the paper, such a method would be judged as having performed a successful attack, and given no constraints there is nothing stopping this.  2. In what situation would such an attack method would be practically useful?  Even the reviewers who reviewed the paper favorably were not able to provide answers to these questions, and I was not able to resolve this from my reading of the paper as well. I do understand that there is a challenge on this by Google. In my opinion, even this contest is somewhat ill defined, but it also features extensive human evaluation to evaluate the validity of the perturbations, which is not featured in the experimental evaluation here.  While I think this work is potentially interesting, it seems that there are too many open questions that are not resolved yet to recommend acceptance at this time, but I would encourage the authors to tighten up the argumentation/evaluation in this regard and revise the paper to be better accordingly!
This paper develops a new large language model trained on 25TB of (simplified) HTML text data. The HTML tags provide valuable information about the document structure. The training adapted the BART denoising objectives (to inject noisy size hint to control generation length during training). The paper also studies various prompting methods for the model. The model achieves state of the art performance on zero shot summarization and several text classification tasks. Reviewers have found the motivation of pretraining with structured text convincing, and the results are good.
This paper investigates some variants of the double Q learning algorithm and develops theoretical guarantees. In particular, it focuses on how to reduce the correlation between the two trajectories employed in the double Q learning strategy, in the hope of rigorously addressing the overestimation bias issue that arises due to the max operator in Q learning. However, the reviewers point out that the proofs are hard to parse (and often hand waving with important details omitted). The experimental results are also not convincing enough.     
The paper proposes a new dataset for studying knowledge grounded conversations, that would be very useful in advancing this field. In addition to the details of the dataset and its collection, the paper also includes a framework for advancing the research in this area, that includes evaluation methods and baselines with a relatively new approach. The proposed approach for dialogue generation however is a simple extension of previous work by (Zhang et al) to user transformers, hence is not very interesting. The proposed approach is also not compared to many previous studies in the experimental results. One of the reviewers highlighted the weakness of the human evaluation performed in the paper. Moving on, it would be useful if further approaches are considered and included in the task evaluation.   A poster presentation of the work would enable participants to ask detailed questions about the proposed dataset and evaluation, and hence may be more appropriate. 
This paper presents a new phenomenon referred to as the "local elasticity of neural networks". The main argument is that the SGD update for nonlinear network at a local input x does not change the predictions at a different input x  (see Fig. 2). This is then connected to similarity using nearest neighbor and kernel methods. An algorithm is also presented.  The reviewers find the paper intriguing and believe that this could be interesting for the community. After the rebuttal period, one of the reviewers increased their score.  I do agree with the view of the reviewers, although I found that the paper s presentation can be improved. For example, Fig. 1 is not clear at all, and the related work section basically talks about many existing works but does not discuss why they are related to this work and how this work add value to this existing works. I found Fig. 2 very clear and informative. I hope that the authors could further improve the presentation. This should help in improving the impact of the paper.  With the reviewers score, I recommend to accept this paper, and encourage the authors to improve the presentation of the paper.
The paper presents a timely method for intuitive physics simulations that expand on the HTRN model, and tested in several physicals systems with rigid and deformable objects as well as other results later in the review.   Reviewer 3 was positive about the paper, and suggested improving the exposition to make it more self contained. Reviewer 1 raised questions about the complexity of tasks and a concerns of limited advancement provided by the paper. Reviewer 2, had a similar concerns about limited clarity as to how the changes contribute to the results, and missing baselines. The authors provided detailed responses in all cases, providing some additional results with various other videos. After discussion and reviewing the additional results, the role of the stochastic elements of the model and its contributions to performance remained and the reviewers chose not to adjust their ratings.  The paper is interesting, timely and addresses important questions, but questions remain. We hope the review has provided useful information for their ongoing research. 
The authors propose a dataset and a method for the task of SpokenQA. The dataset is generated by using Google TTS to generate audio segments corresponding to the CoQA dataset and then using an ASR system to generate (noisy) transcripts of these speech segments. The authors then propose a method which uses a combination of various known techniques.   Pros:   A good first attempt at creating an interesting dataset for a useful task  Cons:   Lack of clarity in writing   Use of original clean text as input to the model which beats the purpose (in a natural setting such clean text will not be available)  All reviewers have appreciated the effort and attempt at creating a new dataset for this task. However, they have also pointed out that paper is not very clearly written and some important issues (use of clean text as input to the model) need to be adequately addressed before the paper is ready for acceptance. 
This paper studies the problem of kernel similarity matching using Hebbian neural networks. Specifically, the authors propose to compute the approximate feature map for the kernel using the least square loss function, Legendre transformation, and Hebbian parameter update rules.  Reviewers generally agree that the proposed method is interesting. However, there are major issues with the current manuscript, both theoretically and empirically. Theoretically, there is no guarantee for the convergence of the method. In fact, the non convergence of the approximation error, when the dimensionality (number of features) increases, indicates that the proposed method is not consistent, in contrast with other methods such as Kernel PCA or Nystrom approximation based methods. As observed by the authors, this may be related to the unstable convergence of the stochastic gradient descent ascent optimization procedure. For consistency, the approximation error needs to approach zero as the number of features becomes large.  Overall, empirical results compared with existing methods are not  satisfactory. As the authors themselves point out in their discussion, "this method does not provide the same sorts of theoretically guarantees or empirically observed robustness of sampling based methods".  The authors are encouraged to take  reviewers  comments and suggestions into account to improve their current work.
Main content:  Blind review #2 summarizes it well:  The authors provide a method to modify GRFs to be used for classification. The idea is simple and easy to get through, the writing is clean. The method boils down to using a latent variable that acts as a "pseudo regressor" that is passed through a sigmoid for classification. The authors then discuss learning and inference in the proposed model, and propose two different variants that differ on scalability and a bit on performance as well. The idea of using the \xi transformation for the lower bound of the sigmoid was interesting to me   since I have not seen it before, its possible its commonly used in the field and hopefully the other reviewers can talk more about the novelty here. The empirical results are very promising, which is the main reason I vote for weak acceptance. I think the paper has value, albeit I would say its a bit weak on novelty, and I am not 100% convinced about the this conference being the right fit for this paper. The authors augment MRFs for classification and evaluate and present the results well.      Discussion:  As blind review #1 points out:  Even from the experiments (including the new traffic one), it is unclear how much better the method is either because we don t know if the improvements are statistically significant and that in many of the results, unstructured models like RF or logistic regression are very competitive casting some doubt on whether these datasets were well suited for structured prediction.     This paper is a desk reject as review #2 s points out that anonymity was broken by the inclusion of a code link that reveals the authorship, which is true as a simple search on the GitHub user "andrijaster" immediately brings us to https://arxiv.org/pdf/1902.00045.pdf which is a draft of this submission showing all author names.
The paper proposes an approach that allows online finetuning of an offline RL policy by adaptively changing a BC regularization term.  Even after discussions with the authors, the reviewers had several concerns. First, the paper seems to be limited in novelty as the "REDQ+AdaptiveBC seems incremental on top of TD3+BC". Second, there were concerns that the adaptive regularization term was insufficient as a contribution given its heuristic nature.  Given the consensus among reviewers of this paper, I recommend rejecting this paper.
This paper presents a reinforcement learning algorithm to target variable in every time step.  Although the paper proposes an important problem in many real world applications, there were various major criticisms raised by reviewers.  Most importantly, technical novelty is not well motivated or justified.  There is also a significant lack of a specific description of the proposed method, discussion of computational complexity, clarity and presentation, and evaluation metrics, which decreased the enthusiasm of the reviewers.
A paper that studies two tasks: machine translation and image translation. The authors propose a new multi agent dual learning technique that takes advantage of the symmetry of the problem. The empirical gains over a competitive baseline are quite solid. The reviewers consistently liked the paper but have in some cases fairly low confidence in their assessment.
I thank the authors for their submission and very active participation in the author response period. I want to start by stating that I rank the paper higher as is currently reflected in the average score of the reviewers. The reasons for this are that a) R2 and R3, while responding to the author s rebuttal, do not seem to have updated their score or indicated that they want to keep their initial assessment of the paper   in particular, R2 has acknowledged that additional experiments by the authors were useful and results on KeyCorridorS4/S5R3 are nice, and b) I disagree with R2 s sentiment that MiniGrid is not a suitable testbed   it is by now an established benchmark for evaluating RL exploration and representation learning methods (see list of publications on https://github.com/maximecb/gym minigrid). However, despite my more positive stance on the paper, I fully agree with R1 and R2 that a comparison to EC is needed in order to shed light into which factors of EC SimCLR actually led to improvements in comparison to RIDE. I therefore recommend rejection, but I strongly encourage the authors to take the feedback from the reviewers and work on a revised submission to the next venue.
I am recommending rejection for this paper for the following reasons:  I agree that the main claim is an obvious consequence of the structure of the GAN generator and the prior. I m also not sure why the authors restricted their analysis to GANs, but that s not a super important point to me.  More important is that the experimental validation of the ensembling idea is way below the bar for this conference.  Moreover, I know the authors touched on this a bit, but all of these modern GAN variants that work on imagenet are implicitly ensembling anyway through e.g. the conditioning input and the structure of the special batch norm.  Finally, the authors didn t respond to the reviews. 
The paper proposes a simple modification to Laplace approximation  to improve the quality of uncertainty estimates in neural networks.  The key idea is to add “uncertainty units” which do not affect the predictions but change the Hessian of the loss landscape, thereby improving the quality of uncertainty estimates. The “uncertainty units” are themselves trained by minimizing a non Bayesian objective that minimizes variance on in distribution data and maximizes variance on known out of distribution data. Unlike previous work on outlier exposure and prior networks, the known out of distribution data is used only post hoc.  While the idea is interesting and intriguing, the reviewers felt that the current version of the paper falls a bit short of the acceptance threshold (see detailed comments by R3 and R2’s concerns about Bayesian justification for this idea). I encourage the authors to revise and resubmit to a different venue. 
I recommend acceptance. The two positive reviews point out the theoretical contributions. The authors have responded extensively to the negative review and I see no serious flaw as claimed by the negative review.
Even though reviewers found some responses by the authors satisfactory, several concerns regarding the paper still remain. The authors are strongly encouraged to:  1) Explore how dataset size impacts accuracy. 2) Reason about annotation costs via empirical experiments. 3) Including benchmark datasets in experimental evaluations.
The reviewers highlight a lack of technical content and poor writing. They all agree on rejection. There was no author rebuttal or pointer to a new version. 
The novelty of the paper are: + introduces a new Hopfield network with continuous states, hence can be learned end to end differentiation and back propagation. + derives efficient update rules + reveals a connection between the update rules and transformers + illustrate how the network can be used as a layer in deep neural network that can perform different functions  The presentation was clear enough for the reviewers to understand and appreciate the novelty, although there were a few points of confusion. I would recommend the authors to address several suggestions that came up in the discussions including:   additional analysis to highlight when and how the networks is able to outperform other competing models   intuitions about the proofs for the theorems (okay to leave the detailed derivation in the appendix)   
This paper introduces a new graph neural network architecture designed to learn to solve Circuit SAT problems, a fundamental problem in computer science. The key innovation is the ability to to use the DAG structure as an input, as opposed to typical undirected (factor graph style) representations of SAT problems. The reviewers appreciated the novelty of the approach as well as the empirical results provided that demonstrate the effectiveness of the approach.  Writing is clear. While the comparison with NeuroSAT is interesting and useful, there is no comparison with existing SAT solvers which are not based on learning methods. So it is not clear how big the gap with state of the art is. Overall, I recommend acceptance, as the results are promising and this could inspire other researchers working on neural symbolic approaches to search and optimization problems.
Adversarial training is usually done on the image space by directly optimizing the pixels. This paper suggests the adversarial training over intermediate feature spaces in the neural network. The idea is very simple. The authors have done extensive experiments to justify its performance. But the performance gain though this idea seems to be marginal. Further, the layer to conduct the adversarial training can be optimized within the framework, which aligns with the general autoML idea. The new version L ALFA has been well introduced, but unfortunately, the practical result can be very straightforward, that is just to select the final layer. The more important ALFA hyperparameters that would most benefit from automatic tuning are not sufficiently treated.  There have been extensive discussions between the authors and the reviewers. After incorporating the reviewers  comments, the paper will have a good chance to be accepted at another venue.   
The paper presents a variant of sliced wasserstein distance , where the slicing operation is performed with a neural network. The resulting distance is studied and experiments on synthetic data and as cost in generative modeling are performed.  While the idea of the paper is not that novel, the work is overall well executed. Reviewers agreed that the paper is borderline weak accept. Accept as a poster.
This paper proposes a new dataset called ComPhy to evaluate the ability of models to infer physical properties of objects and to reason about their interactions given these physical properties. The paper also presents an oracle model (named oracle because it requires gold property labels at training time) that is modular and carefully hand designed, but shows considerable improvement over a series of baselines. The reviewers for this submission had several concerns including: (a) [VByS] "concerns are about the complexity that the proposed method can handle"\ (b) [VByS] "the method is only demonstrated on a simple synthetic dataset"\ (c) [8BUA] "I am struggling to see any direct application"\ (d) [8BUA] "choosing 4 videos as reference"   why use ref videos, why use 4\ (e) [8BUA] "Baselines showing results with ground truth object properties should be reported"\ (f) [3cQE] "no innovation in the type or structure of questions asked"\ (g) [3cQE] "neither the CPL framework nor the implementation of any module is novel"\ (h) [DJEq] "The only difference is that this paper infers hidden properties instead of collisions"\ (i) [DJEq] "The dataset is not comprehensive enough"   only 2 properties and simplistic and synthetic videos\  The authors have provided detailed responses to these concerns and I discuss these below.  The authors have addressed (c),(d) and (e) well in their rebuttal.  I don t think (a) is concerning. The proposed model is not expected to solve the dataset entirely inspite of having access to gold properties at training time. As the authors mention, this indicates the complexity of the task at hand.  The authors also address (f) well. I dont think there is any need for innovation in the structure of questions asked. QA is merely a mechanism to probe the model, and using CLVERER style questions seems appropriate.  I disagree with the sentiment behind (g). The proposed oracle model clearly inherits modules from past works and assembles them to suit the needs of the dataset. It is this assembly that differentiates it from past works. This is true of most papers in our field, including ones that are widely acknowledged to be important papers. The underlying modules in proposed networks are rarely novel, but their assembly can lead to improvements on benchmarks. Furthermore, the oracle model, isnt the central contribution of this work. The dataset is, and hence, the requirement for novelty is reduced. The oracle is meant to serve as a guideline to show what one may achieve given gold labels at training, and it serves that purpose well.  Re (h), my takeaway is that inferring properties based on their dynamics and without any link to their appearance is an important step, and past datasets do not exhibit this characteristic. And thus, in spite of being a limited differentiation from CLEVERER, I think this is interesting.  Re: (b) and (i) I do agree with some aspects of these, with the reviewers. I think its still valuable to have a dataset with synthetic videos, given that models today are unable to solve this dataset. Moving to more realistic videos is a next step. However, as the reviewer [DJEq] points out, it would be desirable to add more physical properties and add more complex scene elements like ramps. That would have added a lot more diversity to the dataset   visually, with regards to physical properties and with regards to the types of reasoning required.  Having said that, I believe that the dataset in its present form is still valuable to the community, and hence I recommend acceptance. I think adding more physical properties and scene elements will have made this a much stronger submission.
The paper efficiently computes quantities, such as variance estimates of the gradient or various Hessian approximations, jointly with the gradient, and the paper also provides a software package for this. All reviewers agree that this is a very good paper and should be accepted.
The paper presents a technique of training robust classification models that uses the input distribution within each class to achieve high accuracy and robustness against adversarial perturbations.   Strengths:    The resulting model offers good robustness guarantees for a wide range of norm bounded perturbations    The authors put a lot of care into the robustness evaluation  Weaknesses:     Some of the "shortcomings" attributed to the previous work seem confusing, as the reported vulnerability corresponds to threat models that the previous work did not made claims about  Overall, this looks like a valuable and interesting contribution. 
The paper proposes to compute local representations on device, which are then shared between clients using an alignment mechanism. Reviewers did appreciate the value of the topic and several contributions, but unfortunately consensus is that it remains below the bar, even after the discussion phase. Concerns remained on privacy and motivational positioning with FL, and lack of simpler baselines, even after the author feedback.  We hope the detailed feedback helps to strengthen the paper for a future occasion.
The paper proposes an interesting take on Graph Matching by posing the problem as learning the Topology through Graph Convolutional Networks.  There is consensus that the methods proposed are new but the impact is not clear. One major point against the paper seems to be that Code is yet to be released.
All reviewers suggest rejection. Beyond that, the more knowledgable two have consistent questions about the motivation for using the CCKL objective. As such, the exposition of this paper, and justification of the work could use improvement, so that experienced reviewers understand the contributions of the paper.
This paper proposes a new selection paradigm for selecting the optimal architecture in neural architecture search (NAS), in particular for methods that involve a one shot model and that deploy gradient based methods for the search. Basically, the paper focuses on examining the max selection very closely and found the magnitude of architecture weights are misleading. Instead, the paper proposes much more intuitive finalization step, pick the operator that has the largest drop in validation if the edge is removed. All reviewers agreed that the idea is interesting, the paper is well written, and the results found in the paper are interesting. In addition, author response satisfactorily addressed most of the points raised by the reviewers, and most of them increased their original score. Therefore, I recommend acceptance.
This meta review is written after considering the reviews, the authors’ responses, the discussion, and the paper itself.  The paper proposes a system for learning disentangled object centric 3D based representations of scenes and shows that the proposed model works well on several tasks, including few shot classification and VQA.   The reviewers point out that the direction is important (R1, R3), the model is sensible (R2), and the reported results are good (R1, R4); on the downside, they note that the system is complicated (R1), the considered datasets are relatively simplistic (R1, R3, R4), some ablations are missing (R2, R3), and comparisons with baselines are not necessarily convincing (R2, R3). The authors did a good job of addressing the concerns in the rebuttal, by reporting additional ablation results, baselines, and experiments on the realistic Replica dataset.  All in all, I recommend acceptance. The direction of the work is important and complex, the experimental evaluation presented in the paper is extensive, and the results are good relative to relevant baselines. On the downside, the proposed system and the paper are somewhat complicated and overwhelming, which may limit the benefit for the readers. I hope the authors will take this into account in the future. 
The reviewers all appreciated the importance of the topic: understanding the local geometry of loss surfaces of large models is viewed as critical to understand generalization and design better optimization methods.  However, reviewers also pointed out the strength of the assumptions and the limitations of the empirical study. Despite the claim that these assumptions are weaker than those made in prior work, this did not convince the reviewers that the conclusion could be applied to common loss landscapes.  I encourage the authors to address the points made by the reviewers and submit an updated version to a later conference.
The paper claims that one of the most common (and obvious) pruning methods in the literature today (global magnitude pruning) is "overlooked" and "seen as a mediocre baseline by the community." As an active member of the pruning research community myself, I can attest that this is simply not true. I am in strong agreement with reviewer MHY2 and   after reading the discussion around that review and the paper itself in detail   I confidently recommend rejection.  Magnitude pruning itself dates back decades, at least to the work of Janowski (Pruning vs. Clipping in Neural Networks, 1988). The paper is correct that *global* magnitude pruning (in which all weights are compared in a layer agnostic manner) was largely ignored in favor of layer wise magnitude pruning (i.e., pruning all layers by the same amount) in much of the work that popularized magnitude pruning (e.g., Han et al., 2015). However, global magnitude pruning has become much more popular since that time. In work establishing the lottery ticket hypothesis, Frankle and Carbin (The Lottery Ticket Hypothesis) use it in certain cases and   later   in all cases (Frankle et al., Linear Mode Connectivity and the Lottery Ticket Hypothesis). In the past several years, global pruning in general has become the de facto way to use all new pruning heuristics (e.g., SNIP: Single Shot Network Pruning based on Connection Sensitivity; Picking Winning Tickets Before Training by Preserving Gradient Flow; Pruning Neural Networks without Any Data by Iteratively Conserving Synaptic Flow). Moreover, other papers have specifically advocated that global magnitude pruning is state of the art within recent years at this very conference: Comparing Rewinding and Fine Tuning in Neural Network Pruning (Renda et al., ICLR 2020 oral): "We propose a pruning algorithm...that matches state of the art tradeoffs between Accuracy and Parameter Efficiency across networks and datasets:...globally prune the 20% of weights with the lowest magnitudes." (This paper does not cite Renda et al. despite the fact that it is a prominent paper that directly contradicts the purported problem that the paper relies on to support the significance of the findings.)  In short, in the pruning literature, the idea that global pruning, magnitude pruning, or global magnitude pruning is overlooked or is not recognized as a strong baseline is simply preposterous. The reason that global magnitude pruning has "largely been ignored in recent years, generally being relegated to the position of a baseline for comparison" is because it is a simple technique whose efficacy has long been known and established   exactly what a good "baseline for comparison" should be.  The paper has narrowed its claims somewhat during the discussion and revision period, advocating for a one shot global magnitude pruning strategy that "does not require any complex pruning frameworks like RL or sparsification schedules [or]...iterative procedure." To do so, however, the proposed method replaces each of these "complex" hyperparameters with another set: whether or not to use a minimum threshold (MT) and where to set it. Even if the approach isn t iterative, the hyperparameter search necessary to set it almost certainly is, and it is unclear whether searching for the MT value is any more efficient than the other approaches. The costs of this hyperparameter search need to be measured. And iterative pruning s costs can often be mitigated by making pruning gradual, something the paper considers superficially in the revisions.  Finally, as reviewer MHY2 observes, one of the primary reason papers *don t* use global magnitude pruning is that, although it leads to higher sparsities than layerwise magnitude pruning, it also often leads to higher FLOP counts. Although FLOP counts are a terrible indicator of real world speedup, they are a much higher fidelity indicator than parameter count, which neglects the fact that   in convolutional networks   a small number of parameters can lead to vastly more FLOPs if they operate on larger activation maps (i.e., before the activation maps have been downsampled). In the revisions, the paper gives a token nod (and a superficial dismissal) to this fact in Sections 4.3 and 6, but the paper needs to fully acknowledge this point by measuring and discussing its consequences. "Look[ing] at this in future work" is not enough.  Due to these many concerns, I strongly recommend rejection.
This paper extends the DiCE estimator with a better control variate baseline for variance reduction.  The reviewers all think the paper is fairly clear and well written. However, as the reviews and discussion indicates,  there are several critical issues, including lack of explanation of the choice of baseline, the lack more realistic experiments and a few misleading assertions.  We encourage the authors to rewrite the paper to address these criticism. We believe this work will make a successful submission with proper modification in the future.  
The paper provides theoretical bounds for imitation learning with rewards (algorithm from Wang et al. (2019)). The bounds/proofs are highly novel and a very interesting contribution to the community, even though they are a lot more conservative than what is observed in practice. All reviewers agree on this point. It is laudable that the authors also additionally provide an experimental evaluation. After the revision and the discussion, quite a few of the reviewers are still not 100% convinced about them, on the one hand as they would have liked to see more tasks, and on the other hand due to concerns about the reward relaxation (i.e., doesn t match the assumptions in the theorems any longer) which is required for experiments on standard benchmarks. In the final answer the authors provide evidence that there is no big discrepancy, which is good enough (given that there don t seem to be any alternatives to get around this issue, except removing the experimental section altogether, which would be undesirable). Please clearly point out those limitations of the experiments in the paper and also incorporate this evidence.
This paper proposes a differentiable trust region based on closed form projects for deep reinforcement learning. The update is derived for three types of trust regions: KL divergence, Wasserstein L2 distance, and Frobenius norm, applied to PPO and PAPI, and shown to perform comparably to the original algorithms.  While empirically the proposed solutions does not bring clear benefits in terms of performance, as correctly acknowledged by the authors, it is rigorously derived and carefully described, bringing valuable insights and new tools to the deep RL toolbox. The authors improved the initial submission substantially based on the reviews during the discussion period, and the reviewers generally agree that the work is of sufficient quality that merits publication. To improve the paper and its impact, I would recommend applying the method to also off policy algorithms for completeness. Overall, I recommend accepting this submission.
The paper proposes a set based neural subsampling model that selects both features and instances using a two stage model. The motivation is to allow for scaling to large datasets by first subsampling using an initial stage that does not model second order interactions (which would require work quadratic in the dataset size to model), and then following up with a second more sophisticated pass that includes second order interactions. Its results show an empirical improvement over previous methods in the case of very small subsample sizes.  The responses from the reviewers in discussion were varied—and often off base for all reviewers—and as a result I took an even more deep look at this paper than usual. I think the variance in reviewer response is a symptom of the fact that the paper is somewhat confusingly written, and sometimes has parts that give the opposite impression to what the authors intend. For example, the author response says "we emphasize again that our method is a subsampling method. This is very different from core set selection methods" but then Figures 11 and 12 explicitly label the subset produced by the algorithm a "coreset." There is general confusion as to what exactly is being subsampled (features or instances) and even what datasets were being evaluated on—it s not that the information is not there, but rather that it s easy to miss while reading the paper. We can see this happening where most of the reviewers were misunderstanding or making factual errors about the paper, and I can see how this happened by reading the paper.  The reviewers also shared some concerns about the baselines, and indeed some things about the baseline comparisons are confusing: for example, Figure 4 seems to report DPS having below 70% accuracy on MNIST while subsampling to a size of 25, but the original DPS paper (Huijben et al, ICLR 2020) in their Figure 2 reports a percent error of 6.6% (at Pixels removed: 96.8%, which I believe corresponds to keeping 25 pixels as 28*28*(1 0.968)   25). This does seem to back up the reviewer s speculation that "the baselines are most likely unfairly weaker." The presentation of the results should make this sort of thing more clear (if the setup isn t the same as DPS s MNIST setup, how does it differ? if the setup is the same, as seems to be the case the way the paper is presently written, why are the result accuracies so different from what is published in the DPS paper?).  The reviewer comment about theory is not one I count against the submission. Although it is certainly true that this paper would be greatly strengthened by some theoretical backing, it is also part of a line of work that eschews theory—so we cannot reasonably disqualify it for doing so.  To sum: although the technical contributions of this paper do seem to be significant, I expect that if the paper is published as presently written, it will confuse ICLR readers just as it has confused our reviewers. This leads me to lean against accepting this paper at this time.
This paper introduces an adversarial approach to enforcing a Lipschitz constraint on neural networks. The idea is intuitively appealing, and the paper is clear and well written. It s not clear from the experiments if this method outperforms competing approaches, but it is at least comparable, which means this is at the very least another useful tool in the toolbox. There was a lot of back and forth with the reviewers, mostly over the experiments and some other minor points. The reviewers feel like their concerns have all been addressed, and now agree on acceptance. 
Overall, the paper is missing a couple of ingredients that would put it over the bar for acceptance:    I am mystified by statements such as "RL2 no longer gets the best final performance." from one revision to another, as I have lower confidence in the results now.    More importantly, the paper is missing comparisons of the proposed methods on *already existing* benchmarks. I agree with Reviewer 1 that a paper that only compares on benchmarks introduced in the very same submission is not as strong as it could be.  In general, the idea seems interesting and compelling enough (at least on the Krazy World & maze environments) that I can recommend inviting to the workshop track.
The paper proposes a simple and general technique based on the information bottleneck to constrain the information flow in the discriminator of adversarial models. It helps to train by maintaining informative gradients. While the information bottleneck is not novel, its application in adversarial learning to my knowledge is, and the empirical evaluation demonstrates impressive performance on a broad range of applications. Therefore, the paper should clearly be accepted. 
This paper formulates a method for training deep networks to produce high resolution semantic segmentation output using only low resolution ground truth labels. Reviewers agree that this is a useful contribution, but with the limitation that joint distribution between low  and high resolution labels must be known. Experimental results are convincing. The technique introduced by the paper could be applicable to many semantic segmentation problems and is likely to be of general interest. 
The paper presents "deep deducing", which means learning the state action value function of 2 player games from a payoff table, and using the value function by maximizing over the (actionable) inputs at test time.  The paper lacks clarity overall. The method does not contain any new model nor algorithm. The experiments are too weak (easy environments, few/no comparisons) to support the claims.  The paper is not ready for publication at this time.
The paper describes an adversarial training approach that, in addition to the commonly used robustness loss, requires the network to extract similar representation distributions for clean and attacked data. The proposed method is inspired by domain adaptation approaches that require a model to extract domain invariant/agnostic features from two domains. Although the experimental results are solid and technically sound, the novelty of the methodology is not enough, as the domain classifier and the gradient reversal layer are the same with those methods in domain adaptation such as "unsupervised domain adaptation by backpropagation". On the other hand, more recent SOTA methods are missing and only smaller scale datasets are used for evaluation. During the discussions, the major concerns from three reviewers are novelty.   I totally agree that the simplicity of the method should be a virtue. However, the idea of domain invariant representation learning is already established well, and its application to adversarial training is quite intuitive to the community. Also, the similar methodology already exists in domain adaptation. According to the top tier conference culture in the ML community, what most valuable is the novelty and insight, not the performance. In the end, I think that this paper may not be ready for publication at ICLR, but the next version must be a strong paper.
The paper discusses a simple but apparently effective clustering technique to improve exploration. There are no theoretical results, hence the reader relies fully on the experiments to evaluate the method. Unfortunately, an in dept analysis of the results is missing making it hard to properly evaluate the strength and weaknesses. Furthermore, the authors have not provided any rebuttal to the reviewers  concerns.
The paper uses several types of information to predict one specific lab test response for patients. The predictions are made by combining and tailoring mainly existing techniques.  The reviewers raised a number of concerns, and the authors clarified many of them and provided additional results. In particular the following issues were discussed: Comparing to state of the art methods and methods having the same information available, specifics of empirical evaluations and of methodological novelty, choice of the particular data sets, and justifiability of conclusions.  The main remaining weakness is the limited novelty, which should not be interpreted as the contributions of the paper being trivial.  In contrast, the solid engineering work done by the authors in this paper will be valuable in developing clinical decision support tools, and the authors are encouraged to incorporate the new results and feedback in future work and submissions.
This paper proposes using a neural network to learn an approximate solution for desired boundary conditions to accelerate the semiconductor device simulation. The work shows that speed up simulation is increased significantly. However, the major concern about this work is the limited contribution to the machine learning community as exposed by the reviewers. 
The authors present an algorithm for label noise correction when the label error is a function of the input features.  Strengths   Well motivated problem and a well written paper.  Weaknesses   The reviewers raised concerns about theoretical guarantees on generalization; it is not clear why energy based auto encoder / contrastive divergence would be a good measure of label accuracy especially when the feature distribution has high variance, and when there are not enough clean examples to model this distribution correctly.   Evaluations are all on toy like tasks with small training sets, which makes it harder to gauge how well the techniques work for real world tasks.   It’s not clear how well the algorithm can be extended to multi class problems. The authors suggested 1 vs all, but have no experiments or results to support the claim.  The authors tried to address some of the concerns raised by the reviewers in the rebuttal, e.g., how to address unavailability of correctly labeled data to train an auto encoder. But other concerns remain. Therefore, the recommendation is to reject the paper. 
although some may find the proposed approach as incremental over e.g. gu et al. (2018) and kiela et al. (2018), i believe the authors  clear motivation, formulation, experimentation and analysis are solid enough to warrant the presentation at the conference. the relative simplicity and successful empirical result show that the proposed approach could be one of the standard toolkits in deep learning for multilingual processing.   J Gu, H Hassan, J Devlin, VOK Li. Universal Neural Machine Translation for Extremely Low Resource Languages. NAACL 2018. D Kiela, C Wang, K Cho. Context Attentive Embeddings for Improved Sentence Representations. EMNLP 2018.
The reviewers have issues with the lack of enough experimental results as well as with novelty of the solution proposed. I recommend rejection.
The paper identifies an interesting problem in sigmoid deep nets, addressed diffferently by batchnorm, and proposes a different simple fix. It shows empirically that constraining neuron s weights to sum to zero improves training of a 100 layers sigmoid MLP. The work is currenlty limited in its theoretical contribution, and regarding the showcased practical interest of the method compared to batchnorm (it s not appplicable to RELUs and shows positive effect on optimization but not generalization).  
The authors introduce an RL algorithm / architecture for partially observable                                                       environments.                                                                                                                       At the heart of it is a filtering algorithm based on a differentiable version of                                                    sequential Monte Carlo inference.                                                                                                   The inferred particles are fed into a policy head and the whole architecture is                                                     trained by RL.                                                                                                                      The proposed methods was evaluated on multiple environments and ablations                                                           establish that all moving parts are necessary for the observed performance.                                                                                                                                                                                             All reviewers agree that this is an interesting contribution for addressing the                                                     important problem of acting in POMDPs.                                                                                                                                                                                                                                  I think this paper is well above acceptance threshold. However, I have a few points that I                                          would quibble with:                                                                                                                 1) I don t see how the proposed trampling is fully differentiable; as far as I                                                      understand it, no credit is assigned to the discrete decision which particle to                                                     reuse. Adding a uniform component to the resampling distribution does not                                                           make it fully differentiable, see eg [Filtering Variational Objectives. Maddison                                                    et al]. I think the authors might use a form of straight through gradient approximation.                                            2) Just stating that unsupervised losses might incentivise the filter to learn                                                      the wrong things, and just going back to plain RL loss is not in itself a novel                                                     contribution; in extremely sparse reward settings, this will not be                                                                 satisfactory.           
All reviewers find the proposed data augmentation approach simple, interesting and effective. They agree that paper does a good job exploring this idea with number of experiments. However the paper also suffers from some drawbacks, and reviewers raise questions about some of the conclusions of the paper   in particular how to designate an augmentation as either negative or positive is not clear apriori to training. While I agree with this criticism, I believe the paper overall explores an interesting direction and provides a good set of experiments than can be built on in  future works, and I suggest acceptance. I encourage authors to address all the reviewers concerns as per the feedback in the final version.
The paper received mixed reviews. While AnonReviewer1 and AnonReviewer2 liked the idea of jointly learning global local representations, the other reviewers were concerned about the technical novelty. Reviewers also raised various questions about the experiments and ablation studies. AC found that the rebuttal well addressed the reviewers  questions about the experiments, but it failed to elaborate on the "why" of combining global and local self supervised representations. AC agreed with AnonReviewer3 and AnonReviewer4 s concerns on technical novelty. Considering the reviews, we regret that the paper cannot be recommended for acceptance at this time.  The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
This paper proposed two algorithms for curriculum learning, one based on the the knowledge of a good solution (e.g. a local minima or a solution found by SGD) and another one proposed for natural image datasets based on entropy and standard deviation over pixels.   Reviewers seem to like the ideas behind the proposed algorithms and their simplicity. However, there are several major concerns that are shared among reviewers: 1  One of the algorithms needs knowledge of a good solution (e.g. a local minima or a solution found by SGD) which makes it impractical and the other one doesn t use any information about the mapping between input and the label. 2  Discussing previous work on curriculum learning, explaining how proposed algorithms are different than previous work and empirical comparison to other curriculum learning methods are lacking or need a significant improvement. 3  The experiment section needs improvement both in terms of experimental methodology and having more tasks/datasets.  Reviewers have done a great job at pointing to specific areas that need improvement. I hope authors would use reviewers  comments to improve their work.  Given the above major concerns, I recommend rejecting this paper.  
This paper aims to disentangle semantics and syntax inside of popular contextualized word embedding models. They use the model to generate sentences which are structurally similar but semantically different.   This paper generated a lot of discussion. The reviewers do like the method for generating structurally similar sentences, and the triplet loss.  They felt the evaluation methods were clever.  However, one reviewer raised several issues.  First, they thought the idea of syntax had not been well defined. They also thought the evaluation did not support the claims.  The reviewer also argued very hard for the need to compare performance to SOTA models.  The authors argued that beating SOTA is not the goal of their work, rather it is to understand what SOTA models are doing.  The reviewers also argue that nearest neighbors is not a good method for evaluating the syntactic information in the representations.    I hope all of the comments of the reviewers will help improve the paper as it is revised for a future submission.
The authors develop a framework for improving robustness certificates obtained by randomly smoothed classifiers in settings with multiple outputs (segmentation or node classification), by combining local robustness certificates obtained for individual classifiers. They validate their results empirically and demonstrate gains from their approach.  The reviewers were mostly in agreement that the authors make a novel and interesting contribution. However, there were a lot of technical concerns raised by reviewers that, while addressed during the discussion phase, would require a substantial revision of the paper to address adequately. Overall, I feel the paper is borderline but recommend rejection and encourage the authors to incorporate feedback from the reviewers and submit to a future venue.
This paper proposes to analyze the generalization error of deep learning models and GANs using the Lipschitz coefficient of the model.  There was significant discrepancies in the evaluation of the paper among reviewers. While all reviewers acknowledged the interesting theoretical approach to understand generalization and the relevance to ICLR of the problem, they disagreed about the readiness level of the paper. Some concerns were expressed in terms of clarity (and the AC agrees with these), but most importantly, reviewer wKt9 pointed an important flaw in the current analysis that was not properly responded to by the reviewers (see below). In discussion, other reviewers were also concerned by this flaw, and so the AC decided to recommend a major revision of the paper taking the reviewers comments in consideration.  ## Important flaw in the paper analysis (from wKt9)  Basically, Theorem 1 assumes that a loss $f(h,x)$ is $L$ Lipschitz w.r.t. input $x$ in some compact set of diameter $B$ for any $h$. The author shows that the: $\sup_{h \in H} |E_{P} f(h,X)   E_{\hat{P}} f(h,X)|$ is upper bounded by $L B + C \sqrt{\text{stuff}/m}$.  The concern of wKt9 is that the LHS is upper bounded *trivially* and deterministically by the tighter $L B$ [see proof sketch next] for any distribution $P$ and $\hat{P}$ just because of the compactness of the input set and that $f$ is $L$ Lipschitz; one does not even need to include the number of samples $m$ in the analysis (thanks to the very strong assumption on $f$). The reviewer also was concerned that later (Theorem 3), the authors study ways that we can make $L$ exponentially small (which is interesting), but this has both the issues that: 1) it tells you nothing about the absolute performance of your network, as this only bounds the variation between any two distributions (indeed including the empirical and true distribution; but the fact that it also contains all distributions should indicate how loose this bound is!), and so perhaps the best empirical error one can obtained is still big 2) the current version of Theorem 3 uses a loose bound with a dependence on $m$ which was not even needed (as per the result above).  While it s true that empirically one can observe small empirical error, and thus combining this with a small Lipschitz constant would indicate good absolute performance; but the current presentation of the theory is rendered quite problematic by the above refinement, and should be corrected in a revision.  ### Proof sketch: For simplicity, I ll prove it for $P$ being a discrete distribution and $\hat{P}$ being the empirical; but I m pretty sure you can extend it to continuous distributions as well.  Note that we have $|f(h,x)   f(h,x )| \leq L B$ for all $x, x $ in the compact set of diameter $B$ and for all $h$.  Now $$E_{P} f(h,X)   E_{\hat{P}} f(h,X)   \sum_j \pi_j f(h, x_j )   \frac{1}{m} \sum_i f(h,x_i)$$  For each $x_i$, associate several $x_j$ s so that the total sum of their probabilities is $1/m$ (split some $\pi_j$ in multiple pieces if necessary)   we can augment the index set for these new pieces, to obtain new probabilities $\pi_j $ and call $I_i$ the set of associated indices to $x_i$. We have $\sum_{j \in I_i} \pi _j   1/m$  We thus have: $$E_{P} f(h,X)   E_{\hat{P}} f(h,X)   \sum_i \sum_{j \in I_i} \pi _j \left[ f(h, x _j)   f(h,x_i) \right]$$  Thus: $$|E_{P} f(h,X)   E_{\hat{P}} f(h,X)| \leq  \sum_i \sum_{j \in I_i} \pi _j \left| f(h, x _j)   f(h,x_i) \right| \leq L B$$  This is true for any $h$, so this is also true for the $\sup$, *deterministically*! QED
This paper proposes a general regularization algorithm which builds on the dropout idea. This is a very significant topic. The overall motivation is good, but the specific design choices are less well motivated over, for example, ad hoc choices. Some concerns remain after the post rebuttal discussion with the reviewers: the improvement is incremental in terms of concepts and methodology, the clarity needs to be improved and the experiments are somehow weak. In summary, the main idea and research direction is interesting, but the attempted generality of the algorithm and the significance of the area call for a more clear and convincing presentation. 
This submission proposes to combine the CutMix data augmentation of Yun et al 2019 with the standard consistency loss of  and the  structured consistency loss of Liu et al 2019 and applies the resulting approach to the Cityscapes dataset.  The reviewers were unanimous that the paper is not suitable for publication at ICLR due to a lack of novelty in the method.  No rebuttal was provided.
The paper presents tackles the problem of finding strategies that are   unlike Nash which is safe   both safe (non exploitable, to some extent) and able to exploit the opponent. The proposed solution is a convex combination of exploitation and safety that is efficient to compute. Overall, the paper is borderline. Given that the objective and its analysis are not especially surprising, a lot rides on the thoroughness of the empirical results, which could be improved.
The paper received negative and borderline reviews. The reviewers have raised several concerns about the novelty of the approach and the lack of convincing experiments. The rebuttal only partially addresses these concerns. Overall, the area chair agrees with the reviewer s assessment and follows their recommendation.
This work introduces a simple and effective method for ensemble distillation. The method is a simple extension of earlier “prior networks”: it differs in which, instead of fitting a single network to mimic a distribution produced by the ensemble, this work suggests to use multi head (one head per individual ensemble member) in order to better capture the ensemble diversity. This paper experimentally shows that multi head architecture performs well on MNIST and CIFAR 10 (they added CIFAR 100 in the revised version) in terms of accuracy and uncertainty.  While the method is effective and the experiments on CIFAR 100 (a harder task) improved the paper, the reviewers (myself included) pointed out in the discussion phase that the limited novelty remains a major weakness. The proposed method seems like a trivial extension of the prior work, and does not provide much additional insight. To remedy this shortcoming, I suggest the authors provide extensive experimental supports including various datasets and ablation studies.    Another concern mentioned in the discussion is the fact that these small improvements are in spite of the fact that the proposed method ends up using many more parameters than the baselines. Including and comparing different model sizes in a full fledged experimental evaluation would better convey the trade offs of the proposed approach. 
This paper proposes a new method to answering queries using incomplete knowledge bases. The approach relies on learning embeddings of the vertices of the knowledge graph. The reviewers unanimously found that the method was well motivated and found the method convincingly outperforms previous work.
Heterophily is known to degrade the performance of graph neural networks. This paper explores whether, for graph convolutional networks (GCNs), this is a general phenomenon, or if there are some circumstances under which a GCN can still perform well in a heterophilous setting. This paper characterizes one such setting under a contextual stochastic block model (CSBM) distribution with two classes (generalized in the appendix to multiple classes). The main takeaway is that there are indeed scenarios where a GCN can be expected to perform well, even under heterophilic neighborhoods.  There are limitations, and the reviewers have been fairly thorough in pointing these out: the analysis is specific to GCNs under CSBM, and there are a number of assumptions on the node label/feature/neighborhood distributions. The non linear operations in the GCN have also been dropped. Even still, the reviewers were generally satisfied that the experiments backed up the claims in this specific scenario.  There is still quite a bit more to do in order to make this a more general result. Essentially, this paper shows that heterophily is not always a problem. One reviewer has stated that it is not always considered a problem anyway, but at least this paper outlines a specific scenario in which this is theoretically true. However, there is still a large space of “bad” heterophily, and this paper leaves open what these are, and how to deal with them. It is also possible that there are other “good” scenarios as well that are unexplored.  Still, in the narrow scope under which the analysis lies, the paper is clear and accomplishes what it sets out to do. I would encourage the authors to ensure that the paper incorporates the suggestions of the reviewers, particularly with regard to scope, to ensure that the paper is properly grounded in its claims.  All reviewers leaned towards the side of acceptance, except one who did not engage in post review discussion. After reading over their review, and the subsequent response, I am satisfied that their concerns have been adequately addressed.
The paper aims to provide a framework for learning non linear feature mappings such that are invariant to environments. The critical concern raised by the reviewers is their assumption: that causal features of the label are conditionally independent given the label. But in any DAG, conditioning on a common child (here, the label) renders the parents dependent. Their assumption thus is not going to hold other than on a measure zero set of parameters.
This paper proposes a framework of image restoration by searching for a MAP in a trained GAN subject to a degradation constraint. Experiments on MNIST show good performance in restoring the images under different types of degradation.  The main problem as pointed out by R1 and R3 is that there has been rich literature of image restoration methods and also several recent works that also utilized GAN, but the authors failed to make comparison any of those baselines in the experiments. Additional experiments on natural images would provide more convincing evidence for the proposed algorithm.  The authors argue that the restoration tasks in the experiments are too difficult for TV to work. It would be great to provide actual experiments to verify the claim.
The authors address the important and understudied problem of tuning of unsupervised models, in particular variational models for learning disentangled representations.  They propose an unsupervised measure for model selection that correlates well with performance on multiple tasks.  After significant fruitful discussion with the reviewers and resulting revisions, many reviewer concerns have been addressed.  There are some remaining concerns that there may still be a gap in the theoretical basis for the application of the proposed measure to some models, that for different downstream tasks the best model selection criteria may vary, and that the method might be too cumbersome and not quite reliable enough for practitioners to use it broadly.  All of that being said, the reviewers (and I) agree that the approach is sufficiently interesting, and the empirical results sufficiently convincing, to make the paper a good contribution and hopefully motivation for additional methods addressing this problem.
This paper proposes a new graph Hierarchy representation (HAG) which eliminates the redundancy during the aggregation stage and improves computation efficiency. It achieves good speedup and also provide theoretical analysis. There has been several concerns from the reviewers; authors  response addressed them partially. Despite this, due to the large number of strong papers, we cannot accept the paper at this time. We encourage the authors to further improve the work for a future version.     
The paper proposes an interesting way of prioritizing samples in replay that is compatible with many RL methods. It is evaluated experimentally on different tasks and with different RL algorithms.  The reviewers highly appreciated the revised paper and the detailed replies and discussions. While this iteration improved the paper substantially, it is still not ready for publication in its current form. In particular:   The paper is still not self contained enough   The reviewers are still not convinced about the statistical significance   More tasks should be added   PER needs to be added as a baseline The authors promised those changes for the final version, but those are so substantive that the paper will need to go thorough another complete review cycle. Hence, we d like to encourage the authors to re submit at a different venue.  P.S.: Careful with double blind submissions, acknowledgements should not be included.
The authors proposed to train an energy based model with a hierachical variational approximations. The entropy can be tricky in hierarchical variational approximations.  The authors suggest using the auxillary samples to guide an importance samples to compute the gradient of the entropy. They evaluate their approach on a slew of models. The idea is straightfoward and could potentially be applied to other hierarchical variational models out side of the energy based model setting.  The authors were responsive and clarified many agressive questions. I d ask the authors to clean up two things    Equation 8 would be easier to follow if it kept the expectation from   equation 6 thereby making z_0 feel like it materialize out of thin   air     A more detailed discusion of when the proposal is good and what could   be missed out	when relying on the generating z to center the proposal
The authors propose to use attention over past time steps to try and solve the gradient flow problem in learning recurrent neural networks. Attention is performed over a subset of past states by a hueristic that boils to selecting best time steps.  I agree with the authors that they offer a lot of comparisons, but like the reviewers, I am inclined to find the experiments not very convincing of the arguments they are attempting to make.  The model that they propose has similarities to seq2seq in that they use attention to pass more information in the forward pass; in a sense this is a seq2seq model with the same encoder and decoder, and there are parallels to self attention. The model also has similarities to clockwork RNNs and other skip connection methods.. However, the experiments offered to not tease out these effects. It is unsurprising that a fixed size neural network is unable to do a long copy task perfectly, but an attention model can. What would have been more interesting would have been to explore if other RNN models could have done so. The experiments on pMNIST aren t really compelling as the baselines are far from SOTA (example: https://arxiv.org/pdf/1606.01305.pdf report 0.041 error rate (95.9% test acc) with LSTMs and regularization).  Text8 also shows worse results in full BPTT on LSTM.  If BPTT is consistently better than this method, it defeats the argument that gradient explosion and forgetting over long sequences is a problem for RNNs (one of the motivations offered for this attention model). 
Solid, but not novel enough to merit publication.  The reviewers agree on rejection, and despite authors  adaptation, the paper requires more work and broader experimentation for publication.
This paper proposes a new solution to the problem of domain generalization where the label distribution may differ across domains. The authors argue that prior work which ignores this observation suffers from an accuracy vs invariance trade off while their work does not.   The main contribution of the work is to 1) consider the case of different label distributions across domains and 2) to propose a regularizer extension to Xie 2017 to handle this.   There was disagreement between the reviewers on whether or not this contribution is significant enough to warrant publication. Two reviewers expressed concern of whether 1) naturally occurring data sources suffer substantially from this label distribution mismatch and 2) whether label distribution mismatch in practice results in significant performance loss for existing domain generalization techniques. Based on the experiments and discussions available now the answer to the above two points remains unclear. These key questions should be clarified and further justified before publication.
This paper investigates the training dynamics of simple neural attention mechanisms, in a controlled setting with clear (but rather strict) assumptions. Some reviewers expressed caution about the applicability of the assumptions in practice, but nevertheless there is agreement that the results deepen our understanding and enrich our toolkit for reasoning about attention. In support of this, in the discussion period, it was emphasized that the work uses different techniques than most current work in this direction. I am therefore confident that the paper will be useful, and recommend acceptance.  I strongly encourage the authors to improve the clarity of the work and thorough citation, as suggested by the reviewers.
This paper introduces a very simple but effective method for the neural architecture search problem. The key idea of the method is a particular continuous relaxation of the architecture representation to enable gradient descent like differentiable optimization. Results are quite good. Source code is also available. A concern of the approach is the (possibly large) integrality gap between the continuous solution and the discretized architecture. The solution provided in the paper is a heuristic without guarantees.  Overall, this is a good paper. I recommend acceptance.
This paper provides an extensive investigation of the robustness of Deep Deterministic Policy Gradient algorithm.  Papers providing extensive and qualitative empirical studies, illustrative benchmark domains, identification of problems with existing methods, and new insights can be immensely valuable, and this paper is certainly in this direction, if not quite there yet.   The vast majority of this paper investigates one deep learning algorithm in designed domain. There is some theory but it s relegated to the appendix. There are a few issues with this approach: (1) there is no concrete evidence that this is a general issue beyond the provided example (more on that below). (2) Even in the designed domain the problem is extremely rare. (3) The study and perhaps even the issue is only shown for one particular architecture (with a whole host of unspecified meta parameter details). Why not just use SAC it works? DDPG has other issues, why is it of interest to study and fix this particular architecture? The motivation that it is the first and most popular algorithm is not well developed enough to be convincing. (4) There is really no reasoning to suggest that the particular 1D is representative or interesting in general.  The authors including Mujoco results to address #1. But the error bars overlap, its completely unclear if the baseline was tuned at all this is very problematic as the domains were variants created by the authors. If DDPG was not tuned for the variant then the plots are not representative. In general, there are basically no implementation details (how parameters were tested, how experiments were conducted)or general methodological details given in the paper. Given the evidence provided in this paper its difficult to claim this is a general and important issue.   I encourage the authors to look at John Langfords hard exploration tasks, and broaden their view of this work general learning mechanisms. 
The paper presents a multi agent RL algorithm where the rewards of the other agents are only known up to some accuracy. The setting is somewhat restrictive, in the sense that the transition is assumed to be known. It would perhaps have been more interesting for the paper to also consider unknown transitions, so as to bring it closer to work in single agent reinforcement learning. It also seems to not be making a very good job of linking the related work to the contribution of this paper (even after looking at the appendix).    Authors briefly say in the introduction  "Alternative frameworks improve robustness, e.g., to changes in environment dynamics, observation or action spaces (Pinto et al., 2017; Li et al., 2019; Tessler et al., 2019), but do not address reality gaps due to reward function mismatches, as they use inappropriate metrics on the space of adversarial perturbations"  Authors should try and better explain the differences with those papers. Do  they consider changes in dynamics rather than the reward? It appears that the former is more general than the latter. Couldn t the authors compare with them with an appropriate experiment?  It is also hard to see how this exactly connects with a reality gap. What is the  training  environment? What is the  testing  environment? This is simply a robust optimisation algorithm applied to multi agent games with partially unknown reward functions.   In addition the experiments themselves are not explained clearly.   On the plus side, I think the algorithmic details and experimental are interesting. If there was a better explanation and discussion/comparison with related work, then it would have been a good paper. Authors are encouraged to make a stronger effort to compare with other methods both in terms of the algorithm and experimentally.
In this paper, the authors change the loss function of NNs to reduce the separability of the different classes in one of the hidden layers. The rationale for this assumption that the trained network will be more robust against white box model inversion attack. The reviewers all concur that the paper had some merit, but that the paper is not well presented and believe the paper is not ready to be presented at ICLR.  Also, the separability issue is not totally explained, because a reduced L2 norm might not be the whole story that explains why a white box model inversion would rely on for leaking information. This might need to be proof further and a couple of experiments in which there is still leakage of information shows the additional robustness from the new penalty. 
The paper provides the theoretical justification for the "label trick" (using labels in graph based semisupervised learning tasks). The authors performed a thorough evaluation of their analysis, which constitutes an experimental contribution. The authors provided a rebuttal that the AC finds to have reasonably addressed the reviewers  concerns. We recommend acceptance.
This paper extends prototypical networks to few shot 1 way classification. The idea is to introduce a null class to compare against with a null prototype. The reviewers found the idea sound and interesting. However, the response was mixed because the reviewers were not convinced of the significance of the improvements. Furthermore, there were questions raised about the motivation that were not sufficiently addressed in the rebuttal. Batch normalization layers will not necessarily lead to zero mean if the trainable offset is not disabled. The authors did not clarify whether they disable this offset. I encourage the authors to resubmit after addressing the issues raised by the reviewers.
This paper received overall positive scores. One reviewer (R3) recommended clear reject.   All reviewers agree that the paper introduces a novel idea and its effectiveness is supported by the experimental results. There are concerns about clarity of presentation and certain missing analyses, which have been addressed by the authors in the rebuttal. Thus the ACs recommend acceptance.  
This manuscript proposes a new algorithm for instance wise feature selection. To this end, the selection is achieved by combining three neural networks trained via an actor critic methodology. The manuscript highlight that beyond prior work, this strategy enables the selection of a different number of features for each example. Encouraging results are provided on simulated data in comparison to related work, and on real data.  The reviewers and AC note issues with the evaluation of the proposed method. In particular, the evaluation of computer vision and natural language processing datasets may have further highlighted the performance of the proposed method. Further, while technically innovative, the approach is closely related to prior work (L2X)   limiting the novelty.   The paper presents a promising new algorithm for training generative adversarial networks. The mathematical foundation for the method is novel and thoroughly motivated, the theoretical results are non trivial and correct, and the experimental evaluation shows a substantial improvement over the state of the art.
The authors address the problem of learning environment invariant representations in the case where environments are observed sequentially. This is done by using a variational Bayesian and bilevel framework.  The paper is borderline, with two reviewers (R2 and R3) favoring slightly acceptance and two reviewrs (R4 and R1) favoring rejection.  R4 points out that the current experiments do not do a good job of reflecting a continual learning setup and that simple modifications on existing IRM based methods could outperform the method proposed by the authors. The authors are encouraged to take into account the reviewer s suggestions to improve the paper.  R1 argued initially that the proposed solution is not learning at all since it has errors very close to random guessing. While the authors have improved their method in the revision, the results are still close to random guessing, which questions the practical usefulness of the proposed approach. Also, in the revision, the authors managed to obtain better results when their method is combined with Environment Inference for Invariant Learning (EIIL), but these results are secondary and not the main part of the paper.  The authors should improve the work taking into account the reviewrs  comments.
The paper is well motivated and tackles a hard and long standing problem with seq2seq models: diversity and controllability. The authors propose simple architecture for controllable text summarization. They use multiple decoders controlled by a gating mechanism which can be learnt or controlled manually. They control mostly the abstractiveness and specificity properties of the model.  Pros + the proposed approach is somewhat novel (several earlier work have proposed multiple decoder models to control the generation   as pointed by the reviewer team)  + the proposed modifications are motivated well, the approach is simple and easy to understand.  + the paper is well written and easy to read. + the authors made an effort to address most of the reviewers comments even added human evaluation scores (which was asked by reviewers) + It seems a highly flexible way of enriching existing models in a simple way for additional control behavior in output summary generation of documents.  Cons + During discussions, reviewers have circled around the novelty and continued to raise concerns about the weaknesses of benchmarks and comparison to related work and the fact that the proposed model has more parameters is potential advantage over other models that might contribute to the performance gains. Thus, the paper could be made stronger with further evaluations that could possibly make it stand out.
This paper proposed a regularization term to control the bit width and encourage the DNN weights moving to the quantization intervals. The paper is well written and the idea of using the sinusoidal period as a continuous representation is novel. However, the theoretical analysis provided are not consistent with the proposed method.  As for the experimental results, the proposed method incurs significant degradation as compared to the baseline, and comparison with recent quantization methods is lacking.
I recommend this paper to be accepted. All reviewers are in agreement that this paper is above the bar.
The paper provides a solid and thorough analysis to the two basic methods of fine tuning, linear probing (LP) and fine tuning (FT). The authors provide an important and highly interesting observation about the performance of both in and out of domain (OOD) setting. They validate the known phenomena that FT outperforms LP in the in domain (ID) setting, but demonstrate that when tested on OOD data, LP is in fact more performant and back this observation with a theoretical and empirical analysis. The remedy provided is also a known, yet slightly less popular technique of setting the final layer (LP) first, then finte tuning (FT LP). The authors provide thorough experiments showing that this technique enjoys the best of both worlds, meaning ID and OOD. I found it worth noting that during the rebuttal period the authors provided experiments on additional larger scale datasets and models and the results of the paper carried over to these new setting. The reviews agree that the analysis provided is both interesting and novel. Even though the paper does not provide a new technique, there is a consensus that the understanding it gives on known techniques is a welcome addition to ICLR.
This paper provides a normal map inspired implicit surface representation involving a smooth surface whose high frequency detail comes from normal displacements.  Reviewers were impressed with the results and theoretical discussion in the paper.  The AC agrees.  The authors were responsive to reviewer feedback and addressed some questions about parameter choice during the rebuttal phase, including new experiments/discussion in the supplementary document.  Note the response to reviewer WHEF notes that the authors will be releasing data/code; the AC strongly hopes the authors are true to their word in that regard.  The AC chose to disregard some comments from reviewer G54X regarding tests with noise, as this method appears to be tuned to computer graphics applications; the level of empirical work here aligns with past work in the area.  Of course the authors are encouraged to include some tests responding to the reviewer comments in the camera ready.  The AC also found the score from reviewer WHEF to be somewhat uncalibrated with the tone of their review, but of course their assessment is quite positive nonetheless.   One small comment:  The abstract appears a bit strangely on the OpenReview site because of line breaks; if possible, please remove the line breaks.  Another small comment:  The "spectral shape representation" phrase used a bit in the discussion below might not be advisable, as this phrase typically refers to the intrinsic spectrum of a shape (e.g. Laplace Beltrami analysis)
This paper presents a toolbox for the exploration of layerwise parallel deep neural networks. The reviewers were consistent in their analysis of this paper: it provided an interesting class of models which warranted further investigation, and that the toolbox would be useful to those who are interested in exploring further. However, there was a lack of convincing examples, and also some concern that Theano (no longer maintained) was the only supported backend. The authors responded to say that they had subsequently incorporated TensorFlow support, they were not able to provide any more examples due to several reasons: “time, pending IP concerns, open technical details, sufficient presentation quality, page restriction.” I agree with the consensus reached by the reviewers.
The paper introduces a simple and interesting method that adaptively smoothes the labels of augmented data based on a distance to the “clean” training data. The reviewers have raised concerns about limited novelty, minor improvement over baselines, and insufficient experiments. The author’s response was not sufficient to eliminate these concerns. The AC agrees with the reviewers that the paper does not pass the acceptance bar of ICLR.
The authors have taken inspiration from recent publications that demonstrate transfer learning over sequential RL tasks and have proposed a method that trains individual learners from experts using layerwise connections, gradually forcing the features to distill into the student with a hard coded annealing of coeffiecients. The authors have done thorough experiments and the value of the approach seems clear, especially compared against progressive nets and pathnets. The paper is well written and interesting, and the approach is novel. The reviewers have discussed the paper in detail and agree, with the AC, that it should be accepted.
This paper introduces a method to determine which precision to use for the weights, as well as a quantisation method using hysteresis to improve performance with low precision weights, including 4 bits. Reviewers tend to agree that the two points presented are useful and can have a large impact on the field. Generally, reviewers pointed out that motivations, notations and experimental studies could be improved. This has been partly addressed by the authors. I recommend to accept this paper for ICLR 2022.
This paper proposes to use LOO to characterize the generalization error of neural networks via the connection between NN and kernel learning. The reviewers find the new results interesting. The meta reviewer agrees and thus recommend acceptance.
The reviewers agree that the paper introduces an interesting approach for estimating Shaley values in real run time. The effectiveness of the method is well demonstrated across different tasks/datasets.
This work proposes capsule networks with deformable capsules for tackling object detection. All reviewers agreed that object detection is an important problem that is interesting to the ICLR community. Reviewers also agree that the proposed approach is novel and interesting, and in particular they mention that proposing an efficient capsule network for detection is non trivial. On the less positive side, during the discussion phase all reviewers had concerns about the weak experimental validation, particularly missing ablation studies to analyse the effectiveness of their contributions. At the end, all reviewers felt that, while this is a promising direction of research for object detection, the experimentation should be improved.
The paper proposes a variant of recurrent neural networks based on Long Short Term Memory. Unlike the standard LSTM, the proposed mass conserving LSTM subtracts the output hidden state of the LSTM from its current cell state, thus preserving the "mass" stored in the cell states at each step. A left stochastic recurrent weight matrix is also used to conserve the "mass" across the time steps. Empirical experiments demonstrated the effectiveness of the proposed MC LSTM on a range of datasets such as addition & arithmetic tasks, traffic forecast, and rainfall modeling models.   Several issues were clarified during the rebuttal period in a way that satisfied the reviewers. However, some concerns still remain unanswered:  1) This is an empirical paper that proposes a modified LSTM that brings forward a few different ideas: L1 norm, stochastic transition matrices, and subtracting the output hidden states. An ablation study is a MUST in such an applied work. It has been pointed out by other reviewers that there are many prior references on LSTMs variants. It would greatly strengthen the paper by considering more diverse baselines. There is no experiment nor discussion on how much each modification helps wrt the final accuracy. Thus it remains unclear how the results can generalize to other problems.  2) Although the results seem convincing across various datasets that mass conservation seems to help, the datasets are non standard benchmarks in the machine learning conferences thus there is a lack of competitive prior baselines. As the proposed LSTM has a different number of parameters compared to the standard LSTM, is it fair to compare the different architectures under the same number of neurons? What happens if we compare the architectures with the same number of parameters? And how well does the model scale as we vary the hidden size? It would be helpful to keep the contributions into perspective by using standard RNN benchmark datasets such as Penn TreeBank or Wiki 8.  Overall, the basic idea seems interesting, but the lack of ablation studies significantly hurt the contribution and the positioning of the paper. Given the current submission, the paper needs further development, and non trivial modifications, to be broadly appreciated by the machine learning community.  
The reviewers have various reservations. While the paper has interesting suggestions, it is slightly incremental and the results are not sufficiently compared to other techniques. We not that one reviewer revised his opinion 
The reviewers have reached consensus that while the paper is interesting, it could use more time.  We urge the authors to continue their investigations.
The work presents a new way to generate images from sounds. The reviewers found the problem ill defined, the method not well motivated and the results not compelling. There are a number of missing references and things to compare to, which the authors should change in a follow up.
Reviewers are in a consensus and recommended to reject after engaging with the authors. Please take reviewers  comments into consideration to improve your submission should you decide to resubmit. 
The paper presents an interesting technique for constrained policy optimization, which is applicable to existing RL algorithms such as TRPO and PPO. All of the reviewers agree that the paper is above the bar and the authors have improved the exposition during the review process. I encourage the authors to address all of the comments in the final version.
This paper explores prototype vs linear classifiers for few shot learning. It has been found that pre training a classifier network, followed by training a linear head can produce competitive results to meta trained prototypical networks. A natural question therefore is whether one can directly derive prototypical classifiers from pre trained classifiers. Naively applying this idea doesn’t work well in practice though, and this paper performs a theoretical investigation to determine why. The theory is a generalization of Cao et al., 2020, that doesn’t require assumptions on the class conditional distributions. The theory suggests that the variance of the norm of the feature vectors plays a role, so the paper explores a few feature transformations to reduce this. It demonstrates on a few benchmark datasets that transforming the feature vectors can indeed allow us to create prototypical classifiers from pre trained networks. As a minor quibble, the paper twice refers to “direction of the norm of class mean vectors”   This should just be the direction of the class mean vectors, right? Norm is not a direction, it’s a magnitude.  During the discussion phase, a number of questions arose, mainly around the clarity of the presentation and a request for a few additional baselines (e.g., L2 normalization combined with LDA/V N). These points were resolved by the authors. The main outstanding issue is whether there is enough novelty/significance in the paper to merit acceptance, and on that point, the reviewers felt this is borderline. On the one hand, the theory is more general and does directly point to aspects of the feature space that can yield better generalization results. On the other hand, tricks like L2 normalization are already known, and the utility of prototype classifiers over linear classifiers like SVMs is unclear.  After careful consideration, further discussion with the reviewers, as well as the program committee, it was generally agreed that this paper does not quite meet the bar in terms of the novelty or significance of its contribution. The authors mentioned time and space benefits relative to fine tuned classifiers in their response, and I think one way to improve the paper would be to demonstrate this advantage in a real world application or challenging scenario.
The paper received 3, 3, 6. All reviewers agree that the method is technically interesting. The main concern shared by the reviewers are the experiments which are somewhat underwhelming. The AC believes that this is a solid technical paper that needs a little bit more work. The authors are encouraged to strengthen their evaluation and resubmit to a future conference.
The reviewers find the per difficult to read. Reviewers also had concerns regarding the correctness of various claims in the paper. The paper was also found lacking in experimental analysis, as it only tested on relatively small datasets, and only no a CNN architecture. Overall, the paper appears to be lacking in quality and clarity, and questionable in correctness and originality.
After reading the author s response, all the reviwers still think that this paper is a simple extension of gradient masking, and can not provide the robustness in neural networks.
This paper is a fair effort, making some headway on a problem of practical importance.  There was some discussion of scoping and whether the contribution was Machine Learning y enough.  I m kind of ambivalent on that particular question: I think the general rule is that the further out of scope the paper seems, the better the results need to be for people to overlook it.  I think in this case, unfortunately, even the two most positive reviewers did not evince enough excitement about this paper for it to get accepted in light of the scoping concerns.  Given the various constraints involved, I don t think I can recommend acceptance.  In order to get it accepted into a future conference I would recommend either: a) Submit to a more Software Engineering focused venue b) Really shore up the evaluation such that the reviewers sympathetic to this kind of paper will find it unimpeachable and score it more generously.
This paper proposes a new approach to training networks with low precision called Block Minifloat. The reviewers found the paper well written and found that the empirical results were sufficient. In particular, they found the hardware implementation was a strong contribution. Furthermore, the rebuttal properly addressed the comments of the reviewer.
This paper proposes a method for regularizing image classifiers by encouraging their hidden activations to conform to a PDE.  This is a reasonable idea, and the authors clearly improved the paper a lot in response to the reviews.  However, the main tasks of MNIST and SVHN classification seem way too easy, and the baselines all need to be tuned to be as fast as possible for a given accuracy, if that s the relevant metric.  I agree with the reviewers that this line of work is promising but that the current paper is not sufficiently illuminating or well executed to meet ICLR standards.
The paper studies semantic type detection.  The problem is of practical significance to  i  tabular data.  However, in its current form, there are concerns about  the scope of novelty and technical significance.
This paper presents a meta learning method where the standard ReLU activation units are replaced by the stochastic LWTA units to learn data driven sparse representation.  Most of reviewers like the idea of embedding the stochastic LWTA into a MAML framework. Initial assessment pointed out the lack of clarity in various places in the paper. Authors did a good job in the author’s rebuttal period, to clarify the paper. Experiments demonstrated the competitive performance over existing meta learning methods. Two of reviewers raised their overall score to 5 (from 3). However, all reviewers have concerns in the incremental technical novelty and feel that presentation should be improved to make the paper more clear and friendly to readers. While the idea is interesting, which is backed up by experiments, the paper is not ready for the publication at the current stage. I encourage to resubmit the paper after addressing these concerns.
All reviewers give acceptance scores.  One reviewer also commented that they would like to increase their score from 6 to 7 (which isn t possible in the system). I encourage the authors to add the substantial new results generated during the rebuttal into the paper.
Dear authors,  The reviewers all appreciated your goal of improving dimensionality reduction techniques. This is a field which does not enjoy the popularity it once did but remains nonetheless important.  They also appreciated the novel loss and the use of triplets.to get the global structure.  However, the paper lacks some guidance. In particular, it oscillates between showing qualitative results (robustness to outliers, "nice" visualizations) and quantitative ones (running time, classification performance). I agree with the reviewers that the quantitative ones should have used the same preprocessing for t SNE and TriMap (either PCA or no PCA), regardless of the current implementation in software tools.  Given that the quantitative results are not that impressive, may I suggest focusing on the qualitative ones for a resubmission? The robustness of the emeddings to the addition or removal of a few points is definitely interesting and worth further investigation, optionally with a corresponding metric.
It appears that the reviewers have reached a consensus that the paper is not ready for publication at ICLR.
This paper presents an interesting model which at the time of submission was still quite confusingly described to the reviewers. A lot of improvements have been made for which I applaud the authors. However, at this point, the original 20 babi tasks are not quite that exciting and several other models are able to fully solve them as well. I would encourage the authors to tackle harder datasets that require reasoning or multitask settings that expand beyond babi.  
Pros:   an original idea: learn an additional inverse policy (that minimizes reward) to help find actions that should be avoided.  Cons:   not clearly presented   conclusions are not not validated    empirical evidence is weak   no rebuttal  The three reviewers reached consensus that the paper should be rejected in its current form, but make numerous suggestions for improving it for a future submission. 
The authors have proposed an architecture that incorporates a VIN with a DNC to combine low level planning with high level memory based optimization, resulting in a single policy for navigation and other similar problems that is trained end to end with sparse rewards. The reviews are mixed, but the authors did allay the concerns of the most negative reviewer by adding a comparison to traditional motion planning (A*) algorithms. 
The paper introduces drop out probabilities which are adaptive to the similarity of model parameters between clients. The reviewers liked the idea, however missed several aspects, such as a convergence analysis or at least discussion, as well as an analysis of additional cost of the adaptive step, and finally several concerns on the strength of the experimental setup and benchmarks.  Unfortunately consensus among the reviewers is that it remains below the bar even after the discussion phase.  We hope the detailed feedback helps to strengthen the paper for a future occasion.
With scores of 5, 4 and 3 the paper is just too far away from the threshold for acceptance.
This work investigates the choice of a  baseline  for attribution methods. Such a choice is important and can heavily influence the outcome of any analysis that involves attribution methods. The work proposes doing (1) one vs one attribution in a sort of contrastive fashion (2) generating baselines using StarGAN.  The reviewers have brought out a number of valid concerns about this work:  1. One vs one attribution appears to be novel, and distinctive enough from the more prevalent "one vs all" formulations. I am perhaps more optimistic than the reviewers that such a formulation is in fact useful, but I can see where the hesitancy can come from. 2. It s not clear that the evaluation shows that the proposed method is in fact superior to the others. All the reviewers touched upon this one way or another. 3. Somewhat simplistic datasets used for evaluation (noted that there are CIFAR10 results in the rebuttal).  This was more borderline than the scores would indicate. I thank the authors for the extensive replies and extra experiments. I encourage them to incorporate more of the feedback and resubmit to the next suitable conference. I do believe that doing experiments on ImagetNet (like previous work does, such as IG) would be quite worthwhile and convincing. I suspect the computational expense could be mitigated by re using pretrained networks, of which there are many available for ImageNet specifically.
This paper proposes a technique for training embodied agents to play Visual Hide and Seek where a prey must navigate in a simulated environment in order to avoid capture from a predator. The model is trained to play this game from scratch without any prior knowledge of its visual world, and experiments and visualizations show that a representation of other agents automatically emerges in the learned representation. Results suggest that, although agent weaknesses make the learning problem more challenging, they also cause useful features to emerge in the representation.  While reviewers found the paper explores an interesting direction, concerns were raised that many claims are unjustified. For example, in the discussion phase a reviewer asked how can one infer "hider learns to first turn away from the seeker then run away" from a single transition frequency? Or, the rebuttal mentions "The agent with visibility reward does not get the chance to learn features of self visibility because of the limited speed hence the model received samples with significantly less variation of its self visibility, which makes learning to discriminate self visibility difficult". What is the justification for this? There could be more details in the paper and I d also like to know if these findings were reached purely by looking at the histograms or by combining visual analysis with the histograms.  I suggest authors address these concerns and provide quantitative results for all of the claims in an improved iteration of this paper. 
This paper develops a new mechanism SubMix that provides next token prediction under a variation of the differential privacy constraint. There is disagreement among the reviewers when assessing the quality of this work. Even though the study of private predictions in large language models is new, the reviewers raised several issues in the proposed approach. First, the formulation of partition level DP created confusion about the privacy guarantees provided by the mechanism. Given the similarity to PATE, it might be useful to articulate if there is any difference between the privacy guarantee in this paper and the one of PATE. Second, the authors might want to further clarify the reason for having two sub parts, which has also created some confusion. Even after reading the author s response and the updated revision, the AC still could not understand the relevant privacy argument. In summary, the paper may require further clarification and revision before it is ready for publication.
 The authors propose a pretraining strategy learning inductive biases in transformers for deduction, induction, and abduction.  Further, the claims and results seem to indicate that such pretraining is more successful in transformers which provide a more malleable architecture for learning inductive (structural) biases.  There are open questions that remain, specifically surrounding disentangling high performance from structural bias learning (i.e. is pretraining doing what we think it is) and whether datasets are the "correct" mechanism for imparting such biases/knowledge.
The paper can also improved thorough a more thorough evaluation. 
This paper proposes use of a novel generative modelling approach, over both sequences and structure of proteins, to co design the CDR region of antibodies so achieve good binding/neutralization. The reviewers are in agreement that the problem is one of importance, and that the technical and empirical contributions are strong. There are concerns over the relevance of evaluating the method by using a predictive model as ground truth. Still, the overall contributions remain.
This paper presents a method for conditional generations for GANs. The reviewers note the lack of novelty, or the lack of a theoretical or empirical motivation for the novel bits. They point out flaws in the correctness of the paper, and limited experimental evaluation.  The reviewers agree to reject the paper. Unfortunately the authors did not answer the reviewers. I therefore recommend to reject the paper for this conference, and I strongly suggest that the authors address the reviewers concerns if they are to submit this paper again in a future venue.
This paper conducts a comparison between a small set of models (4 in total) for unsupervised learning. Specifically, the authors focus on comparing Bayesian Confidence Propagating Neural Networks (BCPNN), Restricted Boltzmann Machines (RBM), a recent model by Krotov & Hopfield (2019) (KH), and auto encoders (AE). The authors compare trained weight distributions, receptive field structures, and linear classification on MNIST using the learned representations. The first two comparisons are essentially qualitative comparisons, while on classification accuracy, the authors report similar accuracy levels across the models.  This paper received mixed reviews. Reviewers 4 and 5 felt it did not contribute enough for acceptance, while Reviewers 2 & 3 were more positive. However, as noted by a few of the reviewers, this paper does not appear to achieve much, and provides very limited analysis and experiments on the models. It isn t introducing any new models, nor does it make any clear distinctions between the models examined that would help the field to decide which directions to pursue.  The experiments add little insight into the differences between the models that could be used to inform new work. Thus, the contribution provided here is very limited.   Moreover, the motivations in this paper are confused. In general, it is important for researchers at the intersection of neuroscience and machine learning to decide what their goal is when building and or comparing models. Specifically, is the goal: (1) finding a model that may potentially explain how the brain works, or (2) finding better machine learning tools?  If the goal is (1), the performance on benchmarks is less important. However, clear links to experimental data, such that experimental predictions may be possible, are very important. That s not to say that a model must be perfectly biologically realistic to be worthwhile, but it must have sufficient grounding in biology to be informative for neuroscience. However, in this manuscript, as was noted by Reviewer 4, the links to biology are tenuous. The principal claim for biological relevance for all the models considered seems to be that the update rules are local. But, this is a loose connection at best. There are many more models of unsupervised learning with far more physiological relevance that are not considered here (see e.g. Olshausen & Field, 1996, Nature; Zylberberg et al. 2011, PLoS Computational Biology; George et al., 2020, bioRxiv: https://doi.org/10.1101/2020.09.09.290601). It is true that some of these models use non local information, but given the emerging evidence that locality is not actually even a strict property in real synaptic plasticity (see e.g. Gerstner et al., 2018, Frontiers in Neural Circuits; Williams & Holtmaat, 2018, Neuron; Banerjee et al., 2020, Nature), an obsession with rules that only use pre  and post synaptic activity is not even clearly a desiderata for neuroscience.  If the goal is (2), then performance on benchmarks, and some comparison to the SotA, is absolutely critical. Yet, this paper does none of this. Indeed, the performance achieved with the four models considered here is, as noted by Reviewer 4, very poor. In contrast, there have been numerous advances in unsupervised (or "self supervised") learning in ML in recent years (e.g. Contrastive Predictive Coding, SimCLR, Bootstrap Your Own Latent, etc.), all of which achieve far better results than the four models considered here. Thus, the models being compared here cannot inform machine learning, as they do not appear to provide any technical advances. Of course, some models may combine goals (1) & (2), e.g. seeking increased physiological relevance while also achieving decent benchmark performance (see e.g. Sacramento et al., 2018, NeurIPS), but that is not really the situation faced here, as the models considered have little biological plausibility (as noted above) and achieve poor performance at the same time.  Altogether, given these considerations, although this paper received mixed reviews, it is clearly not appropriate for acceptance at ICLR in the Area Chair s opinion.
This paper shows that the double descent phenomenon of ridgeless regression appears under considerably general settings of the input distributions by showing a lower bound of the excess risk. The analysis covers various types of input distributions including deterministic and random feature maps and its asymptotic sharpness is also shown.  One reviewer raised a concern about its novelty compared with existing work, but the authors properly clarified the novelty in the rebuttal and updated version of the manuscript. Although there were some other minor concerns, the reviewers all agree that this paper gives a valuable theoretical result supporting universality of double descent phenomenon. I also concur with this assessment. I think this paper is a solid theoretical paper giving an informative result as a piece of researches in double descent. Thus, I would recommend acceptance of this paper.
Although there was some initial disagreement on this paper, the majority of reviewers agree that this work is not ready for publication and can be improved in various manners. After the discussion phase there is also serious concern that the experiments need more work (statistically), to verify if they hold up. More comparisons with baselines are required as well. The paper could also be better put in context with the SOTA and related work. The paper does contain interesting ideas and the authors are encouraged to deepen the work and resubmit to another major ML venue.
This paper is not suitable for publication at ICLR. The paper contains a useful message, that neural networks are not a silver bullet, and are especially not well suited to deductive problems. However, as several reviewers pointed out, the claims of the paper are undermined by the fact that it ignores a lot of relevant work on using neural networks in the context of logic reasoning. Reviewer 2 provides a particularly useful list of relevant works on the topic. 
In this paper, the authors propose a new type of (missing not at random) model they call the MCM (mixed confounded missingness). The authors further discuss that given their model, naive imputation strategies do not work, and a model tailored imputation strategy is needed.  The reviewers did not receive the paper favorably, with main complaints centering around: (a) outlining novelty compared to existing approaches to missing data, (b) whether imputation is a good strategy for dealing with missing data, and (c) whether the paper s results are actually sound.  Here s my perspective on these worries.  The paper aims to deal with missing data in a causal inference context (in other words, the target of inference is a causal effect, and our data happens to have entries missing not at random).  Further, the paper aims to work within a graphical modeling formalism for missing data models.  Finally, the paper points out that imputation is to be done with care if data is missing not at random (a point both myself, and reviewers agreed with).  Areas of improvement in the paper, in my mind, would be: (i) better literature review and putting authors  work in context of prior work, (ii) being clear about identification, and (iii) discussion of estimation strategies (not just imputation).  Dealing with missing data (in particular right censoring, but also more general types of missingness) in causal inference is a very old problem, with an established literature in statistics and public health.  In fact, methods for dealing with both causal inference and missing data together are a part of standard graduate curriculum in epidemiology and biostatistics in many Universities.  (i) Literature review and context.  Some papers the authors may find helpful to review:  James M. Robins, Andrea Rotnitzky, Daniel O. Scharfstein.  Sensitivity Analysis for Selection bias and unmeasured Confounding in missing Data and Causal inference models.  Part of the The IMA Volumes in Mathematics and its Applications book series (IMA, volume 116).  This paper discusses lots of relevant things, but in particular sensitivity analysis methods to violations of MAR in settings the authors worry about.  James M. Robins. Non response models for the analysis of non monotone non ignorable missing data. Statistics in Medicine, 16:21–37, 1997.  This paper is an early example of an MNAR model that may be represented by a directed acyclic graph.  Karthika Mohan, Judea Pearl, and Jin Tian. Graphical models for inference with missing data. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 1277–1285. Curran Associates, Inc., 2013.  Ilya Shpitser, Karthika Mohan, Judea Pearl.  Missing data as a causal and probabilistic problem.  In Proceedings of the Thirty First Conference on Uncertainty in Artificial Intelligence (UAI 15), pp. 802 811, AUAI Press, 2015.  Rohit Bhattacharya, Razieh Nabi and Ilya Shpitser. “Full Law Identification In Graphical Models Of Missing Data: Completeness Results.” In Proceedings of the Thirty Seventh International Conference on Machine Learning (ICML 20), pp. 7153 7163, 2020.  These papers deal with general models of missing data using graphs.  Since the authors use graphical models as well, I urge them to put their contribution in context with this prior work.  (ii) Identification.  The authors should clearly discuss whether treatment effects are identified under their model, and if so, by what function.  If this function is not closed form (which can happen in missing data), this should be discussed as well.  This should be contrasted with other missing data work that derives identification under MNAR, particularly using graphs.  (iii) Estimation.  The authors chose to use imputation.  Imputation is a sampling approach to inference in missing data.  Others include maximum likelihood or Bayesian methods (via EM), or semi parametric inference via influence functions.  If the authors chose to concentrate on imputation, specifically, they should explain why (as other methods have noted advantaged, e.g. statistical efficiency, quantification of uncertainty, etc.).  Cautioning against naive imputation is a fine thing to do, but everyone working on missing data problems already knows naive imputation does not work for MNAR data.  Please do not oversell your contributions.  Saying things like: "MCM being the first formalisation of a missingness mechanism when there are treatments at play." is neither true, nor helpful for the peer review process.  With all that said, the MCM model has the potential to be an interesting MNAR model, and placed in proper context of existing work, could be a very interesting addition to the missing data literature.  However, the draft needs a bit more work before it is ready for publication.
While much of generative modeling is tasked with the goal of generating content within the data distribution, the motivation of this work is to examine whether ML techniques can generate creative content. This work has 2 core contributions:  1) Two new datasets of creative sketches: birds and creatures, that have part annotations (size ~ 10K samples for each set). The way the datasets are structured with the body part annotations will facilitate the creativity aspect of the approach later described in the paper.  2) This paper propose a GAN model that is part based, which they call DoodlerGAN. It is inspired partly by the human s creative process of sequential drawing. Here, the trained model determines the appropriate order of parts to generate, which makes the model well suited for human in the loop interactive interfaces in creative applications where it can make suggestions based on user drawn partial sketches.  They show that the proposed model, trained on the part annotated datasets, are able to generate unseen compositions of birds and creatures with novel body part configurations for creative sketches. They conduct human evaluation and also quantitative metrics to show the superiority of their approach (for human preference, and also FID score).  Many reviewers, including R1 and myself observe that the datasets provided, along with the parts based labeling and modeling approach are a clear advantage over existing datasets and methodology. With ever growing importance of generative models used in real world applications, including the creative industry, I believe this paper provides a much needed fresh take on creative ways of using our generative models besides making them larger, or achieving better log likelihood scores. Many reviewers, including R3, would think that this work is indeed a "Delightful, well written paper! I have concerns about its fit here." I strongly believe such works in fact definitely *do* belong at ICLR, and I think this work has the potential to get researchers in the generative modeling community to rethink what they are really optimizing for.  I believe this paper will be a great addition to ICLR2021, and I look forward to see their presentation to the community to spark more creativity in our research endeavors. For this reason, I m strongly recommending an acceptance (Poster).
This paper performs an empirical comparison of similarity based attribution methods, which aim to "explain" model predictions via training samples. To this end, the authors propose a handful of metrics intended to measure the acceptability of such methods. While one reviewer took issue with the proposed criteria, the general consensus amongst reviewers is that this provides at least a start for measuring and comparing instance attribution methods.   In sum, this is a worthwhile contribution to the interpretability literature that provides measures for comparing and contrasting explanation by training example methods. 
The authors propose a flexible variational posterior approximation, relaxing unrealistic factorization and strong parametric constraints that are standard. There was a mixed reception from reviewers. Overall, the paper is on the borderline. The presentation and empirical investigation could be changed so that the nice contributions in the paper are more easily recognized. Indeed, after rebuttal several reviewers still felt like their concerns were not fully addressed. One reviewer was concerned about the evaluation metrics, and wanted to see Stein discrepancy instead of ESS, and did not feel the ESS was sufficiently motivated (as described in updated comments). Another reviewer felt the uncertainty of the predictive distribution was sufficiently well evaluated. Another reviewer generally satisfied by the response. The decision could go either way, but the paper would probably be more widely appreciated by a significant revision, carefully taking into account the questions of the reviewers. The authors are encouraged to accommodate reviewer questions in future versions of the paper.
PROS: 1. All the reviewers thought that the work was interesting and showed promise 2. The paper is relatively well written  CONS: 1. Limited experimental evaluation (just MNIST)  The reviewers were all really on the fence about this but in the end felt that while the idea was a good one and the authors were responsive in their rebuttal, the experimental evaluation needed more work. 
I think this is a very solid and good work in the topic of "Practical Massive Parallel MCTS."   I think it will be good to open up perspectives among ICLR s audience going beyond just Deep Learning and Machine Learning. I also noted a lot of positive comments during the evaluation and discussion period.  Still, it was a borderline case and not an easy decision (primarily because of the concerns raised by R3 towards the end of the discussion period). In the end the program committee decided that the paper does meet the bar.  We think that the work is interesting and original, though not without weaknesses. 
### Summary  The paper demonstrates the applicability of pruning to tabular datasets, which aren t typically explored in the literature on pruning. The work identifies that yes, pruning can indeed be applied to this domain with some success.   ### Discussion  #### Strengths    An unconventional domain that, nonetheless, should be studied.   #### Weaknesses  The empirical setup does not include comparisons to baselines or ablations (e.g., different importance metrics).  ### Decision  I recommend Reject. Reviewer k3Jq provides a precise and constructive set of criticisms that if solved would make for an interesting and significant piece of work.
This paper introduces a method for making synchronous SGD more resistant to failed or slow workers. The idea seems plausible, but as the reviewers point out, the novelty and the experimental validation are somewhat limited. For a contribution such as this, it would be good to see some experiments on a wider range of tasks, and experiments with real rather than simulated workloads. I don t think this work is ready for publication at ICLR. 
This paper focuses on studying the double descent phenomenon in a one layer neural network training in an asymptotic regime where various dimensions go to infinity together with fixed ratios. The authors provide precise asymptotic characterization of the risk and use it to study various phenomena. In particular they characterize the role of various scales of the initialization and their effects. The reviewers all agree that this is an interesting paper with nice contributions. I concur with this assessment.  I think this is a solid paper with very precise and concise theory. I recommend acceptance.
This work presents a proposal for increasing the compositionality of emergent languages that uses a measure of topographic similarity as an auxiliary loss function on the communication game. The authors find that in certain cases this loss indeed results in increased generalization but overall the authors do not find a strong relation between high weights for weighting loss and and generalization. All reviewers agree that the of compositionality is very important and the idea of explicitly optimizing for compositionality (through the topographic similarity metric) is also novel. At the same time a number of concerns are raised by reviewers:   a) B3Jo and WEY2 raise the point that more evidence is required to establish the robustness of the current findings, e.g., by controlling whether topsim is merely inducing a regularization behaviour and providing more confidence on the current presented results (e.g., Figure 2 currently provides a somewhat perplexed pictured as the additional loss doesn t seem to improve across the board).  b) The relation between compositionality and generalization is not a new one and it is not clear what exactly the current paper is adding on this discussion and, as zB6g and  B3Jo point, this makes it seem rather incremental.  c) the paper is currently somewhat hard to follow with numerous results reported in a somewhat raw format, little to no examples and important details being presented only in Appendix (e.g., the loss function is only given in p12 and the actual format of L_{C} is never provided)  As such, I cannot recommend acceptance at this time but, given the importance of the topic, I sincerely hope the authors will work on incorporating reviewers  feedback for a later resubmission.
The paper tackles the problem of generalizing to a new environment by learning a small set up anchor policies (even just 2 for the final approach) which span a sub space that can be searched efficiently in a new environment. The discussion and additional experiments managed to convince most reviewers that the method indeed works as the authors had hypothesized (especially regarding functional diversity). At the moment the analysis is mainly based on empirical observations, it would be good to also have a thorough theoretical analysis of the method.
Paper shows that the question of linear deep networks having spurious local minima under benign conditions on the loss function can be reduced to the two layer case. This paper is motivated by and builds upon works that are proven for specific cases. Reviewers found the techniques used to prove the result not very novel in light of existing techniques. Novelty of technique is of particular importance to this area because these results have little practical value in linear networks on their own; the goal is to extend these techniques to the more interesting non linear case. 
The paper proposes a new distributed training method for graph convolutional networks, using subgraph approximation. The reviewers raised multiple challenges, such as novelty, validity of experiments, and some technical issues. The authors did not respond to the reviewers  comments. The AC agreed with the reviewers that the paper, in the current form, is not ready for publication.
This paper proposes an MCTS approach to goal conditioned planning, where the search generates high level sequences of subgoals for low level policies. This top level planner is basically a search based implementation of SSST for potential gain in computational requirements with the help from the advanced search techniques behind PUCT that combines MCTS and prior information.   Reviewers generally agreed that this is an interesting and novel approach to planning and reinforcement learning. However, reviewers generally expressed that the experiments fall short to convince readers that this technique has greater impact and potential for a wider range of applications, other than GridWorld like environments. Authors are encouraged to provide a more extensive set of experiments, adding more variety to the domains and ablation studies such as the impact of incorrect prior on the overall search performance.  
The paper proposes a novel method for embedding sequences of states and actions into a latent representation that enables efficient estimation of empowerment for an RL system. They use empowerment as intrinsic reward for safe exploration. While the reviewers agree that this paper has promise, they also agree that it is not quite ready for publication in its current state. In particular, the paper is lacking a theoretical justification for the proposed approach, the definition of empowerment used by the authors raised questions, and the manuscript would benefit from more clear and detailed description of the method. For these reasons I recommend rejection.
This paper focuses on disentangled representation learning from multi view data, which is an interesting and hot topic. However, there are several papers published in the last couple of years (especially in NeurIPS2020 and ECCV2020) solving very similar problems with closely related contributions to this paper. The contributions of this paper compared to all recent works in this space is unclear. Contributions and benefits of individual components in the method are not investigated. Although the method is designed for multi view settings, the authors run experiments on simple settings with only two views. The experiments seem quite limited and do not show the method s capabilities. The rebuttal does not properly address the reviewers  concerns either.   The paper received four reviews with three recommending below acceptance threshold (rejection) and one above the acceptance threshold (although this one was the least confident scoring). Given all the above shortcomings and reviewer recommendations I do not recommend acceptance of the paper.   
This paper looks at a formulation of online multi objective optimization problem.  All reviewers agree on the score, 6, which is quite rare but is not really informative; none of them are very excited about the paper, but they all find it interesting.  I have read it as well myself. The paper is rather clear and well written. I have three majors concerns. 1) I am not fully convinced by the objective R_{MOD} as it reduces to the dynamic regret in the single objective problem, as the later cannot be minimized unless we make strong stationarity assumption. This is obviously the case here (see Assumption 2). Then the choice of parameters would depend on some "stationarity" quantity (V_T). I am not really enthusiastic about this either. 2) The analysis is rather classical once the problem is reduced to a single objective, so the analysis is not really breathtaking. Yet I admit that I quite enjoyed reading about this reduction, the idea is quite neat.  3) Multi objective online optimization has already been considered in online learning, but the related works did not really mention it. For instance, Blackwell approachability is such an example [1,2,3] (yet I am not sure that it can cover the Pareto front idea). It would be interesting to see how those approach compares (notably, the online mirror descent has been widely studied in that case).  All in all, I do understand the reviewers, and this paper is certainly borderline, but I do not think it reaches the acceptance bar yet. As a consequence, I would rather recommend rejection this year.   [1] J. Abernethy, P. Bartlett, D. Hazan. Proceedings of the 24th Annual Conference on Learning Theory, PMLR 19:27 46, 2011. [2] V. Perchet. Approachability, regret and calibration: Implications and equivalences, Journal of Dynamics & Games,181 254, 2014. [3] A. Rakhlin, K. Sridharan, and A. Tewari. Online learning: Beyond regret. Proceedings of the 24th Annual Conference on Learning Theory, PMLR, 19:559–594, 2011.
This paper studies the decision boundaries of a certain class of neural networks (piecewise linear, non linear activation functions) using tropical geometry, a subfield of algebraic geometry that leverages piece wise linear structures.  Building on earlier work, such piecewise linear networks are shown to be represented as a tropical rational function. This characterisation is used to explain different phenomena of neural network training, such as the  lottery ticket hypothesis , network pruning, and adversarial attacks.  This paper received mixed reviews, owing to its very specialized area. Whereas R1 championed the submission  for its technical novelty, the other reviewers felt the current exposition is too inaccessible and some application areas are not properly addressed. The AC shares these concerns, recommends rejection and strongly encourages the authors to address the reviewers concerns in the next iteration.  
The authors present a combination of few shot learning with one class classification model of problems. The authors use the existing MAML algorithm and build upon it to present a learning algorithm for the problem. As pointed out by the reviewers, the technical contributions of the paper are quite minimal and after the author response period the reviewers have not changed their minds. However, the authors have significantly changed the paper from its initial submission and as of now it needs to be reviewed again. I recommend authors to resubmit their paper to another conference. As of now, I recommend rejection.
With reviewer scores of (7, 7, 9, 7), and with only one low confidence score (R5 s score of 7 with confidence of 2) it is obvious that the paper should be accepted.  
While I understand and have empathy with the authors  viewpoint of their work and novelty, this unfortunately has not reached reviewers  hearts in the way that they intended. There has been no strong support for acceptance, as the questions about the amount of novelty piled up. Some interactions between authors and reviewers happened. It is clear that the authors made an effort to show the differences with respect to other tree related models and algorithms, as well as to highlight the strengths of the approach which was claimed to be too similar and simplistic. I believe the interactions helped with improving the first viewpoint of reviewers, however this improvement has not been enough, as reviewers did not significantly changed their stances. This is a short process and indeed it is not easy to change first impressions. If anything to add is that I hope that the impressions can be used to give a new presentation to the work that will enhance the work and its view by others.
The paper proposes and studies a task where the goal is to classify an image that has been intentionally degraded to reduce information content.  All the reviewers found the comparison of human and machine performance interesting and valuable. However the reviewers expressed concerns and noted the following weaknesses: the presented results are not convincing to support our understanding of the differences between human and machine perception (R1), using entropy to quantify the distortion is not well motivated and has been addressed before (R1), lack of empirical evidence (R2).  AC suggests, in its current state the manuscript is not ready for a publication. We hope the detailed reviews are useful for improving and revising the paper. 
The paper received mixed reviews: R (R3), WA (R2), A (R1). AC has read the reviews, rebuttal and paper. AC is concerned about the short planning horizon, which seems like a major issue: (i) as R1 notes, most MPC algorithms use much longer horizons as they find it helps performance and (ii) the claim of the approach to be able to pick the planning horizon is moot if its dynamic range is small.  Overall, the paper is very borderline. The idea is interesting but without addressing longer horizons, the contribution is limited. Under guidance from the PCs, the AC feels that the paper just falls below the acceptance threshold and thus cannot be accepted unfortunately. The work is definitely interesting however and should be revised for a future submission.   
The paper provides a transfer learning approach to HPO. It builds and improves upon existing methods of zero shot HPO where the high level idea is to use the outcomes of hyper parameters on an offline collection of datasets in order to speed up HPO on a new dataset. On the plus side, the methods provided seem to be novel, and the results seem to be promising. The main issue is the writing and clarity of the paper, making it hard to be certain of the good qualities of the paper. Aggregating the reviews, the details are too spread out between the appendix and main body, the techniques require more motivation behind them, and important details of the experiment are somewhat vague. The authors provided a modified version which is definitely a step in the right direction, however, it does not seem to be enough. I think this is a solid paper based on a promising idea. However, given the almost unanimous agreement about that crucial gap in clarity even after the modified version was uploaded, I recommend rejecting the paper. 
This paper developed an accelerated gradient flow in the space of probability measures. Unfortunately, the reviewers think the practical usefulness of the proposed approach is not sufficiently supported by realistic experiments, and the clarity of the paper need to be significantly improved. The authors  rebuttal resolved some of the confusion the reviewers had, but we believe further substantial improvement will make this work a much stronger contribution. 
The authors provide a convexification for the GAN training via integral probability metrics induced by two layer neural networks. The exposition relies on the convexification tools recently proposed by the Pilanci et al., and provides interesting insights to follow up in the future.
This paper proposes an attack method to improve the transferability of adversarial examples under black box attack settings.  Despite the simplicity of the proposed idea, reviewers and AC commonly think that the paper is far from being ready to publish in various aspects: (a) the presentation/writing quality, (b) in depth analysis and (c) experimental results.  Hence, I recommend rejection.
I recommend acceptance. This paper presents an interesting "in between" of work on lottery tickets and work on supermasks, and I think it is sufficiently novel to merit acceptance even if the significance of the results will need to be left to the judgment of future researchers. The reviewers seem broadly in favor of acceptance, and I defer to their judgment as a proxy for that signal.  For a quick bit of context, work on "supermasks" (Zhou et al., 2019) has shown that randomly initialized networks contain subnetworks that can reach high accuracy without training the weights themselves. That is to say, within randomly initialized networks are high accuracy subnetworks. This work is interesting in its own right and has had a number of interesting implications for the theoretical community. This work derives from work on the lottery ticket hypothesis (LTH; Frankle & Carbin 2019), which shows that randomly initialized networks contain subnetworks that can train to full accuracy on their own. The key distinctions between these two kinds of work are (1) the LTH trains the subnetworks, while supermask work does not and (2) the LTH work requires that the subnetworks train to full accuracy, while work on supermasks obtain high (but not full) accuracy in many cases. No one approach is "better" than the other; they simply showcase different properties of neural networks.  As far as I understand, this paper creates space for an "in between:" high accuracy subnetworks are created by finding subnetworks at random initialization and flipping the signs of some of the weights to improve accuracy. This is a limited modification to the subnetworks that falls short of actually training them (LTH work) but is more than leaving them at their random initializations (supermask work). Doing so appears to produce subnetworks that perform better than in supermask work but with a lighter weight procedure than LTH work. The procedure for accomplishing this feat is different than either approach (using SynFlow to find the subnetwork and a binary neural network training scheme to find the signs), and there is probably significant room for improvement in this new algorithmic space (just as there was for both LTH and supermasks).  This is novel and interesting, and I defer to the reviewers who find it worthy of acceptance. I have reservations about the eventual significance of the work, but that determination will be made by future researchers.
This paper proposed a joint learning approach which combines item based representations and interest based representations to improve recommender systems. Overall the scores are negative among all the reviewers. The reviewers acknowledge that the proposed approach provides a simple yet effective way to improve the existing item based representations. However, all the reviewers pointed out concerns around the motivation and limited novelty (the proposed approach mostly combines a few existing approaches together without careful examination/exploration in the experiments). Furthermore, the baselines considered in the paper are on the relatively weak side. The authors didn t provide any response. Therefore, I vote for reject.
Granger Causality is a beautiful operational definition of causality, that reduces causal modeling to the past to future predictive strength. The combination of classical granger causality with deep learning is very well motivated as a research problem. As such the continuation of the effort in this paper is strongly encouraged. However, the review process did uncover possible flaws in some of the main, original results of this paper. The reviewers also expressed concerns that the experiments were unconvincing due to very small data sizes. The paper will benefit from a revision and resubmission to another venue, and is not ready for acceptance at ICLR 2019.
This work proposes to uses an energy based objective combined with generative adversarial networks for imitation learning. While most reviewers find the work easy to follow and come with theoretical justifications, albeit mostly followed from previous works, and good coverage of experimental results, all of them raised questions regarding the limited novelty and added contribution of the work, and missing more recent baselines. Please consider address these feedback in your future submissions.
All three reviewers strongly recommend accepting this paper. It is clear, novel, and a significant contribution to the field. Please take their suggestions into account in a camera ready version. Thanks!
This paper proposes a method for semi supervised semantic segmentation through consistency (with respect to various perturbations) regularization. While the reviewers believe that this paper contains interesting ideas and that it has been substantially improved from its original form, it is not yet ready for acceptance to ICLR 2020. With a little bit of polish, this paper is likely to be accepted at another venue.
The idea of learning unstable features from source tasks to help learn stable features for a target task is interesting and well motivated. As the proposed method and its theoretical analysis of learning unstable features from tasks are an incremental extension of an existing work [Bao et al. 2021], the technical contributions line in applying the idea of stable and unstable features learning to the setting of transfer learning. Therefore, the evaluation of this work is focused on the effectiveness of the proposed method in the transfer learning setting.   In transfer learning, one major goal is to make use of knowledge extracted from source tasks to help learn a precise target classifier even with a few or no labeled examples of the target task. It would be more convincing if experiments are conducted to show how the performance of the proposed method changes when the size of labeled data of the target task changes. This is to verify whether the exploitation of unstable features can help to learn a stable classifier for the target tasks more efficiently (i.e., with fewer labeled examples). In addition, as some baseline methods used for comparison do not need to access any labeled data of the target task (like unsupervised domain adaptation or domain generalization approaches), it is not fair to conduct comparison experiments in the setting where there are sufficient labeled examples of the target task since the original designs of such baselines may fail to fully exploit label information in the target task.   Another concern is whether the proposed method is realistic for real world transfer learning problems. Though in the rebuttal, the authors provided experimental results on a natural environment (CelebA), the constructed transfer learning problem is more like a toy problem. Indeed, there are many transfer learning benchmark datasets that contain multiple domains/tasks. It would be more convincing if experiments are conducted on those datasets.  By considering the above two concerns, this paper is on the borderline. My recommendation is a weak rejection based on the current form of this paper. Note that as some references listed by reviewers RJhJ and J8M5 are not really related to the proposed research here, the novelty of the proposed method compared with those references is NOT taken into consideration to make this recommendation.
Reviews were somewhat mixed here, but the consensus is to reject, with at least one voice (R2) urging rejection. Across reviewers, the recommendation to reject is primarily based on the level of originality with the proposed U Net architecture and on weakness of experiments, especially in comparing to baselines.  Reviewers found strengths in the paper s writing and in its demonstration of generalization to unseen geometries.  However, reviewers noted that the architecture does not win originality/significance points (including R3, the most positive reviewer): * R3: "The weakness of this paper is that it doesn t present any novel techniques. It s an existing architecture (U Net) applied in a new domain (wave simulation)." * R2: "The proposed approach is a straightforward application of U net to predict a spatial field given past few spatial fields (stacked together). However, U Nets, LSTMs, conv LSTMs and other architectures have been tried before. It is unclear what the novel contribution in this paper is [...] and why it would be instrumental in handling unseen geometries over longer periods of time." * R2 post response: "This paper is a clear reject. None of the contributions are novel [...]" * R4: "The paper lacks a novel contribution from the architectural and application side" * R1: "Some previous works also used the U net to predict wave dynamics [...] It is not clear what is the novelty (if any) in the proposed network architecture"  Reviewers also noted weaknesses in the experiments (acknowledged by R3, the most positive reviewer, though that review did not consider them a fatal flaw): * R1: "Not enough Experiments. How does the model generalize with more complicated initial conditions, for example, five or ten droplets? Furthermore, there is no comparison to other existing work." * R2: "There is no evaluation against the state of the art [...]" * R2 post response: "Application of DNNs to this problem, speed ups over numerical solvers, etc. have all been explored by SOTA works which have not been compared against. There is no clear articulation of the claimed novel contributions over the SOTA and empirical validation (or theoretical reasoning) of the same." * R4: "There are no comparisons to other baselines [...] " * R3: "Reviewer 4 brings up some fair points [about experimental issues]. I m not as concerned about the lack of a baseline comparison; that doesn t seem to be the point of this paper [...] there is only so much that can be done in an 8 page conference paper [...] However, given that the other 2 reviewers think the paper could use more work, it would be completely reasonable for the chairs to reject it based on those reviews."  Based on this consensus of reviews, my recommendation is to reject. I hope the feedback from the reviews is helpful to the authors.
I agree that reviewer Vxer was confrontational and abusive, especially in the response to the author s rebuttal, and believe that some form of sanction or reprimand is appropriate. That said, I do think that "performance" should be evaluated on both convergence rate and generalization.  Figure 3 does suggest some improvement on generalization for deep versions of resnet without batch normalization.  The three less offensive reviewers all indicated weak acceptance.  One reviewer pointed out the weakness of only getting positive results on deep versions of resnet with batch normalization removed.  Results on transformers, where Adam is typically used, would be more compelling.  This is my primary issue with the paper.  It has not demonstrated improvement in the standard practice of resnet (with batch normalization) and has not presented experiments on transformers.  The theoretical analysis is not aimed at explaining why the improvement is only observed on deep resnets with batch normalization removed or why L2 regularization seems to be of no value when batch normalization is present.  I understand that these are very difficult questions.  The paper has no champion and I am personally concerned about the significance of the contribution.
The paper describes an approach for learning context dependent entity representations that encodes fine grained entity types. The paper includes some good empirical results and observations, but the proposed approach is very simple but lacks technical novelty needed to top ML conference; the clarify of the presentation can also be improved. 
The reviewers found numerous issues in the paper, including unclear problem definitions, lack of motivation, no support for desiderata, clarity issues, points in discussion appearing to be technically incorrect, restrictive setting, sloppy definitions, and uninteresting experiments.  Unfortunately, little note of positive aspects was mentioned.  The authors wrote substantial rebuttals, including an extended exchange with Reviewer2, but this had no effect in terms of score changes. Given the current state of the paper, the committee feels the paper falls short of acceptance in its current form.
This work builds directly on McCoy et al. (2019a) and add a RNN that can replace what was human generated hypotheses to the role schemes. The final goal of ROLE is to analyze a network by identifying ‘symbolic structure’. The authors conduct sanity check by conducting experiments with ground truth, and extend the work further to apply it to a complex model. I wonder under what definition of ‘interpretable’ authors have in mind with the final output (figure 2)   the output is very complex. It remains questionable if this will give some ‘insight’ or how would humans parse this info such that it is ‘useful’ for them in some way.   Overall, though this is a good paper, due to the number of strong papers this year, it cannot be accepted at this time. We hope the comments given by reviewers can help improve a future version.  
The paper proposes an efficient RL based approach for solving the weighted maximum cut problem. The proposed approach shares high level insights with prior work such as ECO DQN (Barrett et al.) and S2V DQN; the key contribution is to demonstrate that the proposed cheap action decoding and stochastic policy strategy can improve the scalability without sacrificing much of the quality of the solution on the tasks considered in this paper.  The reviewers in general find the paper well presented, and especially note that the clear motivation for improving the efficiency of current GNN based RL baselines, particularly represented by ECO DQN.   A common concern among the reviewers is that the original title is misleading; the authors acknowledge that they should properly position the paper to avoid confusion that they were to address general combinatorial optimization problems (as the current title suggests). Notably, many combinational optimization problems can be reduced to max cut as suggested in the authors’ responses; demonstrating the performance in (some of) these problems via a max cut reduction would be helpful to support the significance of this work.  Beyond the title and positioning of this work, there were also initial confusions among the committee in terms of the choice of both (RL or supervised) learning based and heuristic based baselines. The authors did an excellent job in clarifying many of the questions in terms of related work and baselines (the clarity of the work has improved over the rebuttal phase). However, despite the additional ablation study and newly added baselines, there remain concerns/questions in the choice of task domains (lack of hard problem instances where existing solvers, learning  or heuristics  based may fail due to (possibly higher) computational complexity). Given the empirical focus of the paper, this appears to be an important concern, and not all reviewers are convinced the current empirical results are significant to warrant acceptance of this work.
Two reviewers are very positive about this paper and recommend acceptance, one indicates rejection and one is on the fence. Although all referees appreciate the extensive experiments and analysis presented in the paper, their main concerns are related to the limited superiority of the method wrt state of the art [R1], seemingly arbitrary choices and questionable assumptions [R4]. The rebuttal adequately addresses R1 s concerns by highlighting statistical significance of the results, and partially covers R4 s concerns. Although the proposed approach may be perceived as incremental [R1, R2, R3, R4], the authors argue that introducing self supervision to graph attention is not trivial, and emphasize their findings on how/when this is beneficial. Moreover, R2 and R3 acknowledge that the contribution of the paper holds promise, is worth exploring, and may be useful to the research community. Most reviewers are satisfied with the answers in the rebuttal. After discussion, three referees lean towards acceptance and the fourth reviewer does not oppose the decision. I agree with their assessment and therefore recommend acceptance. Please do include your comments regarding the choice of average degree and homophily in the final version of paper.
The authors propose an approach for a learnt attention mechanism to be used for selecting agents in a multi agent RL setting. The attention mechanism is learnt by a central critic, and it scales linearly with the number of agents rather than quadratically. There is some novelty in the proposed method, and the authors clearly explain and motivate the approach. However the empirical evaluation feels quite limited and does not show conclusively that the method is superior to the others. Moreover, the simple empirical results don t give any evidence how the attention mechanism is working or whether it is truly the attention that is affecting the results. The reviewers were split on their recommendation and did not come to a consensus. The AC feels that the paper is not quite strong enough and encourages the authors to broaden the work with additional experiments and analysis.
This paper presents a new metric for adversarial attack s detection. The reviewers find the idea interesting, but the some part has not been clearly explained, and there are questions on the reproducibility issue of the experiments. 
This paper introduces a NAS algorithm based on multi agent optimization, treating each architecture choice as a bandit and using an adversarial bandit framework to address the non stationarity of the system that results from the other bandits running in parallel.  Two reviewers ranked the paper as a weak accept and one ranked it as a weak reject. The rebuttal answered some questions, and based on this the reviewers kept their ratings. The discussion between reviewers and AC did not result in a consensus. The average score was below the acceptance threshold, but since it was close I read the paper in detail myself before deciding.  Here is my personal assessment:  " Positives: 1. It is very nice to see some theory for NAS, as there isn t really any so far. The theory for MANAS itself does not appear to be very compelling, since it assumes that all but one bandit is fixed, i.e., that the problem is stationary, which it clearly isn t. But if I understand correctly, MANAS LS does not have that problem. (It would be good if the authors could make these points more explicit in future versions.)  2. The absolute numbers for the experimental results on CIFAR 10 are strong.  3. I welcome the experiments on 3 additional datasets.  Negatives: 1. The paper crucially omits a comparison to random search with weight sharing (RandomNAS WS) as introduced by Li & Talwalkar s paper "Random Search and Reproducibility for Neural Architecture Search" (https://arxiv.org/abs/1902.07638), on arXiv since February and published at UAI 2019. This method is basically MANAS without the update step, using a uniform random distribution at step 3 of the algorithm, and therefore would be the right baseline to see whether the bandits are actually learning anything. RandomNAS WS has the same memory improvements over DARTS as MANAS, so this part is not new. Similarly, there is GDAS as another recent approach with the same low memory requirement: http://openaccess.thecvf.com/content_CVPR_2019/html/Dong_Searching_for_a_Robust_Neural_Architecture_in_Four_GPU_Hours_CVPR_2019_paper.html This is my most important criticism.  2. I think there may be a typo somewhere concerning the runtimes of MANAS. It would be extremely surprising if MANAS truly takes 2.5 times longer when run with 20 cells and 500 epochs than when run with 8 cells and 50 epochs. It would make sense if MANAS gets 2.5 slower when just going from 8 to 20 cells, but when going from 50 to 500 epochs the cost should go up by another factor of 10. And the text states specifically that "for datasets other than ImageNet, we use 500 epochs during the search phase for architectures with 20 cells, 400 epochs for 14 cells, and 50 epochs for 8 cells". Therefore, I think either that text is wrong or MANAS got 10x more budget than DARTS.  3. Figure 2 shows that on Sport 8, MANAS actually does *significantly worse* when searching on 14 cells than on 8 cells (note the different scale of the y axis). It s also slightly better with 8 cells on MIT 67. I recommend that the authors discuss this in the text and offer some explanation, rather than have the text claim that 14 cells are better and the figure contradict this. Only for MANAS LS, the 14 cell version actually works better.  4. The authors are unclear about whether they compare to random search or random sampling. These are two different approaches. Random sampling (as proposed by Sciuto et al, 2019) takes a single random architecture from the search space and compares to that. Standard random search iteratively samples N random architectures and evaluates them (usually on some proxy metric), selecting and retraining the best one found that way. The number N is chosen for random search to use the same computational resources as the method being compared. The authors call their method random search but then appear to be describing random sampling.  Also, with several recent papers showcasing problems in NAS evaluation (many design decisions affect NAS performance), it would be a big plus to have code available to ensure reproducibility. Many ICLR papers are submitted with an anonymized code repository, and if possible, I would encourage the authors to do this for a future version. "  The prior rating based on the reviewers was slightly below the acceptance threshold, and my personal judgement did not push the paper above the acceptance threshold. I encourage the authors to improve the paper by addressing the reviewer s points and the points above and resubmit to a future venue. Overall, I believe this is very interesting work and am looking forward to a future version.
The paper considers model based RL, and focuses on approaches that benefit from the differentiability of the model in order to compute the policy gradient. It theoretically shows that the error in the gradient of the model w.r.t. its input appears in an upper bound of the error in the policy gradient computing using the learned model. Motivated by this, it suggests a MBRL approach that learns two models, one of them minimizes the next state prediction error (as commonly done) and the other minimizes a combination of prediction error and the gradient error.  The paper empirically studies the method through extensive experiments.  Reviewers are generally positive about this work. They believe that the paper is insightful and the method is original. At first, there were some important concerns raised by the reviewers, but the authors revised their paper in the discussion period, and it appears that the reviewers are all satisfied now. I also read the paper during the rebuttal phase, and I should say that I have some concerns myself, especially on the theory part of the paper. Given that the authors did not have an opportunity to answer my questions, I do not put much weight on my concerns (and I believe most of them can be addressed with some clarifications). Considering the positive response of reviewers and promising results, I am going to recommend **acceptance** of this paper.  I strongly encourage the authors to consider the comments by reviewers, as well as the following ones, in the revision of their paper.   **Comments**  1) The true dynamics $f$ is defined as a stochastic one, i.e., $s_{t+1}   f(s_t, a_t, \epsilon_t)$ (just before Eq. 1), and similarly for the learned model. Here $\epsilon_t$ is the noise causing the stochasticity of the model. But later, when the errors on the model and its gradient are introduced (i.e., $\epsilon_f$ and $\epsilon_f^g$), the role of stochasticity becomes unclear. For example, we have $\|| \tilde{f}(s,a)   f(s,a) \||  \leq \epsilon_f$.  What happened to the noise term?  The same is true for Eq. (5). The next state s  (either according to the true dynamics or the learned model) is random. In that case, it is not obvious how to interpret Eq. (5). Is it the error of the expected gradient of the next state? Or is it something else?  In case the dynamics is assumed to be deterministic, this should be clarified early in the paper.  2) The upper bound in Theorem 1 might be vacuous if the Lipschitz constant $L_f$ of the model is larger than 1. To see this, consider Lemma 1. The constant $C_0$ is $\min [D/\epsilon_f, (1 L_f^{t+1})/(1   L_f)]$. If $L_f$ is larger than 1, for large enough t, the term $(1 L_f^{t+1})/(1   L_f)$ blows up and $C_0$ becomes $D/\epsilon_f$. Therefore, the upper bound of Lemma 1 becomes $D$. Here $D$ is the diameter of the state space, which is assumed to be bounded.  This carries to in the next lemmas. In Lemma 4, $C_5$ would be of the same order as $C_0$ (multiplied by an extra $L_1 L_f / (1   \gamma) )$, so the upper bound of this lemma becomes proportional to D too.  The $C_0$ s appearance continues in the proof of Theorem 1, in which $C_8$ is proportional to $C_0$ and $C_5$. So, $C_8$ is also become proportional to $D/\epsilon_f$. When we have $C_8 \epsilon_f$ in Eq. (34), we get a constant term $D$. A similar dependence appears in the proof of Theorem 2, where B_3 is proportional to $C_8 \epsilon_f$, which can be as large as $D$. And in Eq. (47), we have $B_3^2$. So the upper bound in Eq. (47), which seems to the be upper bound of Theorem 2, is proportional to $D^2$. This means that if $L_f$ is larger than one, the upper bound does not go to zero, no matter how small the model error $\epsilon_f$ is (unless it is actually zero). This makes the bound meaningless.  This might be unavoidable. I am not sure about it at the moment. But it definitely requires a discussion.  3) Assumption 2 has a term in the form of $E[\frac{s_{t_2}}{ s_{t_1}} ]$ (I have simplified the form). The states $s_{t_2}$ and $s_{t_1}$ are vectors in general. How is the division defined here?  4) Please improve the clarify of the proofs. For example, in Lemma 2 it seems that a negative sign is missing in Eq. (49). Also how do we get Eq. (50) and Eq. (52)? (I couldn t easily verify them).  5) I believe the "periodicity property" used in Assumption 1 should be "ergodicity property".  6) The paper still has a lot of typos, e.g., "To optimize the objective, One can ..." (P3), "argument data" (instead of augmented) (p4), "Superpose" (p5), "funcrion" (p6).
This paper presents an interesting mix of new theoretical and empirical results showing how learning temporally extended primitive behaviors can help improve offline (batch) RL.  Although 2/3 reviewers initially raised concerns regarding the motivation of the approach and some of the choices that were made, the authors did an excellent job at addressing these concerns in detail, and there is now a consensus towards acceptance.  I consider that this work is a meaningful contribution towards better offline RL, which is definitely a very important use case in practice. The authors have given convincing explanations to motivate their approach, and made several improvements to the paper. As a result, I am recommending it for acceptance, as a poster.
A nice idea: the latent prior is replaced by a GAN.  A general agreement between all four reviewers to reject the submission, based on a not thorough enough description of the approach, and possibly not being novel.
This paper focuses on unsupervised image denoising and proposes a method to do so. It shows that using a designed operator based on domain knowledge can help improve unsupervised image denoising. The authors also provide experimental results demonstrating that the proposed methods outperform existing unsupervised denoising and behave similar in performance to supervised methods. The reviewers liked the improvements but (1) limited novelty/simple extension of noise2self, (2) example not convincing, (3) lack of clarity in 2.3, (4) a variety of other technical concerns. The authors partially addressed these concerns. However, I concur with the reviewers that the paper still requires more work and is not ready for publication in its current form.
The general consensus amongst the reviewers is that this paper is not quite ready for publication, and needs to dig a little deeper in some areas.  Some reviewers thought the contributions are unclear, or unsupported.  I hope these reviews will help you as you work towards finding a home for this work.
This paper proposes a new knowledge distillation (KD) method for adversarial training. The key observation is inspiring: soft labels provided by the teacher gradually becomes less and less reliable during the adversarial training of student model. Based on that,  they propose to partially trust the soft labels provided by the teacher in adversarial distillation.   Reviewers unanimously agree that this paper has clear motivation, well sorted logic, and neat writing. While some reviewers initially posed concerns on evaluation completeness and detail clarification, they were well addressed during the rebuttal. AC reads the paper/discussion thread and agrees this is a worthy work to get accepted.
The authors make an argument for constructing an MDP from the formal structures of temporal logic and associated finite state automata and then applying RL to learn a policy for the MDP. This does not provide a solution for low level skill composition, because there are discontinuities between states, but does provide a means for high level skill composition.  The reviewers agreed that the paper suffered from sloppy writing and unclear methods. They had concerns about correctness, and were not impressed by the novelty (combining TL and RL has been done previously). These concerns tip this paper to rejection.
There were genuine differences of opinion here. I saw reviews of 8,6,5,5. In these cases, I do try to check if the 8 has a really compelling argument and err on the side of accepting, but here I think both the positive and negative reviews have fair points, so I am inclined to recommend rejection here.  I think the good news is that a lot of the negative stuff was around scoping/writing/related work, and so it should be (relatively) easy to shore up this submission into something that will get better reviews in the next conference cycle.
Given the changes to the paper, the reviewers agree that the paper meets the bar for publication at ICLR. There are some concerns regarding the practical impact on CPUs and GPUs. I ask the authors to clearly discuss the impact on different hardware. One can argue if adaptive quantization techniques are helpful, then there is a chance that future hardware will support them. All of the experiments are conducted on toy datasets. Please consider including some experiments on Imagenet as well.
The paper addresses the setting of continual learning. Instead of focusing on catastrophic forgetting measured in terms of the output performance of the previous tasks, the authors tackle forgetting that happens at the level of the feature representation via a meta learning approach. As rightly acknowledged by R2, from a meta learning perspective the work is quite interesting and demonstrates a number of promising results.  However the reviewers have raised several important concerns that placed this work below the acceptance bar:  (1) the current manuscript lacks convincing empirical evaluations that clearly show the benefits of the proposed approach over SOTA continual learning methods; specifically the generalization of the proposed strategy to more than two sequential tasks is essential; also see R1’s detailed suggestions that would strengthen the contributions of this approach in light of continual learning; (2) training a meta learner to predict the weight updates with supervision from a multi task teacher network as an oracle, albeit nicely motivated, is unrealistic in the continual learning setting   see R1’s detailed comments on this issue.  (3) R2 and R3 expressed concerns regarding i) stronger baselines that are tuned to take advantage of the meta learning data and ii) transferability to the different new tasks, i.e. dissimilarity of the meta train and meta test settings. Pleased to report that the authors showed and discussed in their response some initial qualitative results regarding these issues. An analysis on the performance of the proposed method when the meta training and testing datasets are made progressively dissimilar would strengthen the evaluation the proposed meta learning approach.  There is a reviewer disagreement on this paper. AC can confirm that all three reviewers have read the rebuttal and have contributed to a long discussion. Among the aforementioned concerns, (3) did not have a decisive impact on the decision, but would be helpful to address in a subsequent revision. However, (1) and (2) make it very difficult to assess the benefits of the proposed approach, and were viewed by AC as critical issues. AC suggests, that in its current state the manuscript is not ready for a publication and needs a major revision before submitting for another round of reviews. We hope the reviews are useful for improving and revising the paper. 
All three reviewers were positive about the paper, finding it to be on an interesting topic and with broad applicability. The results were compelling and thus the paper is accepted. 
The submission proposes an architecture to learn a similarity metric for graph matching. The architecture uses node graph information in order to learn a more expressive, multi level similarity score. The hierarchical approach is empirically validated on a limited set of graphs for which pairwise matching information is available and is shown to outperform other methods for classification and regression tasks.  The reviewers were divided in their scores for this paper, but all noted that the approach was somewhat incremental and empirically motivated, without adequate analysis, theoretical justification, or extensive benchmark validation.   Although the approach has value, more work is needed to support the method fully. Recommendation is to reject at this time.
Motivated by the connections between privacy and generalization, this paper studies the correlation between MI attack accuracy and OOD accuracy on synthetic and real world datasets. It shows that the measurements are not always correlated. I found the connection between the motivation and actual measurements performed in the experiments to be rather tenuous. Therefore it is hard to draw any insightful conclusions from the empirical results. It should also be noted that somewhat related disconnect between accuracy of MIA and generalization has already been observed in prior work.
The work proposes a modification to existing architectures applied to predict taxonomic labels from metagenomic sequences. Reviewers agreed that the problem was well motivated, but that current experiments lack comparisons with existing standard baselines in the area. I recommend the authors update their work to included the additional experiments suggested by the reviewers.
This paper investigates ways of using pretrained transformer models like BERT for classification tasks on documents that are longer than a standard transformer can feasibly encode.   This seems like a reasonable research goal, and none of the reviewers raised any concerns that seriously questioned the claims of the paper. However, neither of the more confident reviewers were convinced by the experiments in the paper (even after some private discussion) that the methods presented here represent a useful contribution.   This is not an area that I (the area chair) know well, but it seems as though there aren t any easy fixes to suggest: Additional discussion of the choice of evaluation data (or new data), further ablations, and general refinement of the writing could help.
All three reviewers are consistently negative on this paper. Thus a reject is recommended.
The reviewers think that incorporating class conditional dependencies into the metric space of a few shot learner is a sufficiently good idea to merit acceptance. The performance isn’t necessarily better than the state of the art approaches like LEO, but it is nonetheless competitive. One reviewer suggests incorporating a pre training strategy to strengthen your results. In terms of experimental details, one reviewer pointed out that the embedding network architecture is quite a bit more powerful than the base learner and would like some additional justification for this. They would also like more detail on the computing the MAML gradients in the context of this method. Beyond this, please ensure that you have incorporated all of the clarifications that were required during the discussion phase.
This paper studies through empirical analysis an interesting problem: distilling the (strong) inductive bias of a teacher model to the student model (of weak inductive bias). The main claim/finding is that not only the "dark knowledge" in the logits can be transferred, but also the inductive bias (e.g. recurrence in RNN and translation invariance in CNN) can be transferred to make the student model stronger. This conclusion looks not very surprising but does contribute some new ideas to the fields of both deep learning and transfer learning.  The paper receives insightful but controversial reviews. Throughout reading the lengthy rebuttal and discussions, the AC, as a neutral referee of both sides, thought that while some expressions in the discussions seem a bit urgent and strained, both reviewers and authors managed to participate in the academic debate with a professional attitude that focus only on the technical issues. These discussions are very extensive and helpful for drawing a thorough understanding of this paper, and of the important research problem.  After the public interactions with the authors, a private discussion was performed between all reviewers and the AC, and among the four reviewers, one argued for rejection, one voted for rejection, one voted for acceptance, and one argued for acceptance. The AC believed that one of the reject votes lacks enough support in the comments and thus discarded it. However, due to the wild disagreement between the reviewers, as well as between the reviewers and the authors, the AC read the paper carefully. The AC s main points are as follows:    The research problem is interesting, and this paper appears to be the first work that studies the inductive bias transfer problem.    The paper has made its endeavor to try to delve into this problem, through providing with extensive empirical results and analyses.    The biggest weakness of this paper is the experimentation approach towards quantitatively studying the inductive bias: comparing the teacher and student models through the relational similarity between the penultimate layer representation is simply not enough to justify that the inductive bias has been distilled/transferred.  Two reasons: + Due to the expressiveness of neural networks, it is not hard for the student to resemble the teacher s representations; in fact, this is a quite common result repeatedly used by researchers and practitioners, even when the student is only a smaller model. While the idea of distilling inductive bias is interesting, it simply cannot be sufficiently justified by the current experimentation design. + Inductive bias is something encoding our prior knowledge about the learning task and is often effective during the whole training procedure, which cannot be refreshed by the training data. However, albeit the distilled student model (transformer or MLP) resembles the representations of the teacher model (RNN or CNN), it is not certain whether the "distilled inductive bias" can linger in the student model if you further fine tune the student model to downstream tasks. That is, it is highly possible that such "distilled inductive bias" of the student model will be refreshed by the future training data. In contrast, if we directly fine tune the teacher model to downstream tasks, their inductive bias (recurrence of RNN or translation invariance of CNN) will be retained successfully in the fine tuned model. Basically, through this thinking, it is not clear whether the inductive bias has be distilled. If the distilled thing is refreshed out, it is probably not the inductive bias. More experimentation or a formal quantification of inductive bias is highly necessary here.  While Reviewer #1 was a bit skeptical in the comments and discussions (regarding which I had a private discussion with him/her), some of his/her comments are reasonable and should be well addressed before this paper could be accepted:   Be rigorous in scientific writing. While the experiments with bias variance tradeoff and calibration are interesting and relevant, the key concepts were used with less care. It is good to expand the authors  understanding of these concepts to make sure what they actually refer to.   Try to provide sufficient elaboration when you try to claim something. It is true that for now, in our field, there are quite a few papers claiming something very big in the title or abstract, but simply cannot fulfill their story through rigorous or sound technical study. I suggest to tone down some of the key claims such that "inductive bias can be transferred" if they are not clearly provable.  Finally, AC believes this paper studies a very interesting problem that may draw wide attention, and the paper is acceptable in a future version if the above comments are well addressed. Since this is already a resubmission (as mentioned by Reviewer #1), I d encourage the authors to focus on the technical parts of the comments and revise the paper substantially before submitting to yet another top venue.
This paper explores large scale supervised multi task training across 107 NLP tasks combined with self supervised C4 masked span infilling, using the T5 sequence to sequence model.  The results improve over prior strong T5 baselines on several NLP benchmarks such as SuperGLUE, GEM, and Rainbow.  The paper s main strengths are the scale and large number of tasks, the release of the trained models and data, as well as the clarity and presentation.  Reviewers had concerns with the novelty, limitations in the evaluation (to just T5, and to just SuperGLUE in portions of the paper), and the potential impact of hyperparameters on the results.  During the discussion period, the authors noted that it is not obvious a priori that their approach would work, and that their evaluations on other tasks made it unlikely to be overfitting to SuperGLUE.  They also noted that running the additional hyperparameter experiments suggested during the reviews were computationally prohibitive.  Overall, despite the drawbacks and relative lack of novelty, the extensive experiments and released models provide significant value and will be of interest to the research community.
The authors propose a novel distance metric learning approach. Reviews were mixed, and while the discussion was interesting to follow, some issues, including novelty, comparison with existing approaches, and impact, remain unresolved, and overall, the paper does not seem quite ready for publication. 
The paper focuses on anomaly detection in dynamical systems from time series measurement. The originality of the contribution is to detect anomalies not based on the detection of OOD observations but from identified parameters or statistics of the dynamical system. They are using “polynomial neural networks. All the reviewers agree that the paper is not yet mature both in the form and in the technical content. The authors did not provide a rebuttal.
This paper presents a method for attacking few shot learners with poisoning a subset of support set. I believe this might be the first work to address adversarial examples for meta learners (or few shot learners), which is a timely issue. A common concern raised by most of reviewers is in the novelty of this work, in the sense that the method builds on a basic attack strategy (such as PGD) in the standard adversarial example setting. Authors responded to this, summarizing what s new in this paper. Episodic training for few shot learners requires consuming support set (instead of single training data point). It is a nature of most meta learning methods. Thus, it is easily expected that the adversarial attack for few shot learners is naturally extended to poisoning a support set (or its subset) instead of a single data point. Certainly such extension may entail a new strategy. However, during the discussion period with reviewers, concerns on the novelty of such extension still remains. In particular, the few shot learning algorithms do not allow big changes in the original model. The algorithms analyzed are prototypical networks that do not utilize fine tuning, and MAML that fine tunes for a small number of pre fixed steps. So the transfer of adversarial samples may not be counted as a major contribution.  
Authors present a method for modeling neurodegenerative diseases using a multitask learning framework that considers "censored regression" problems (to model where the outputs have discrete values and ranges). Given the pros/cons, the committee feels this paper is not ready for acceptance in its current state.   Pro:   This approach to modeling discrete regression problems is interesting and may hold potential, but the evaluation is not in a state where strong meaningful conclusions can be made.  Con:   Reviewers raise multiple concerns regarding evaluation and comparison standards for tasks. While authors have added some model comparisons in response, in other areas comparisons don t appear complete. For example, when using MRI data, networks compared all use features derived from images, rather than systems that may learn from images themselves. Authors claim dataset is too small to learn directly from pixels in this data (in comments), but transfer learning and data augmentation have been successfully applied to learn from datasets of this size. In addition, new multitask techniques in the imaging domain have also been presented that dynamically learn the network structure, rather than relying on a hand crafted neural network design. How this approach would compare is not addressed.   
The paper proposes an unsupervised framework for domain adaptation in the context of person re identification to reduce the effect of noisy labels. They use refined soft labels and propose a soft softmax triplet loss to support learning with these soft labels.   All reviewers have unanimously agreed to accept the paper and appreciated the comprehensive experiments on four datasets and ablation studies which give some insights about the proposed method. I agree with the assessment of the reviewers and recommend that this paper be accepted.
PROS: 1. Clear, interesting idea. 2. Largely convincing evaluation 3. Good writing  CONS: 1. The model used in the evaluation is a Resnet 50 and could have been more convincing with a more SOTA model. 2. There is some concern about the whether the comparison of results (fig 6c) is really apples to apples. 
This paper presents a rigorous mathematical framework for knowledge graph embedding. The paper received 3 reviews. R1 recommends Weak Reject based on concerns about the contributions of the paper; the authors, in their response, indicate that R1 may have been confused about what the contributions were meant to be. R2 initially recommended Reject, based on concerns that the paper was overselling its claims, and on the clarity and quality of writing. After the author response, R2 raised their score to Weak Reject but still felt that their main concerns had gone unanswered, and in particular that the authors seemed unwilling to tone down their claims. R3 recommends Weak Reject, indicating that they found the paper difficult to follow and gave some specific technical concerns. The authors, in their response, express confusion about R3 s comments and suggest that R3 also did not understand the paper. However, in light of these unanimous Weak Reject reviews, we cannot recommend acceptance at this time.  We understand that the authors may feel that some reviewers did not properly understand or appreciate the contribution, but all three reviewers are researchers working at highly ranked institutions and thus are fairly representative of the attendees of ICLR; we hope that their points of confusion and concern, as reflected in their reviews, will help authors to clarify a revision of the paper for another venue.   
This paper experimentally shows that the block structure of similarities between layers typically appears for different models and such a structure is mainly induced by small set of dominant datapoints. Moreover, the dominant datapoints are not just noisy artifacts but represent some common image patterns such as background colors. The authors also found that the block structure can easily disappear by removing  the dominant datapoints, and the authors also proposed a method to suppress the block structure by regularizing PCs, Shake Shake regularization, and transfer learning.  This paper gives thorough experiments that clarify the mechanism of appearance of a block structure. However, its significance is a bit minor. Indeed, the block structure does not affect the generalization ability very much, and it can be removed without changing the predictive performance. I agree that investigating the behavior of the internal representation is of scientific interest as the authors pointed out, but on the other hand, its significance would not be convincing. Indeed, this concern was pointed out by several reviewers. Next, the main focus of this study is about the setting of large model with small data size. It is not clear whether it is universal across different model size relative to the dataset size. There is no theoretical investigation (for example, the block structure phenomenon could be explained by a high dimensional random matrix theory).   In summary, this paper investigates a somehow interesting phenomenon but its significance is not convincing. Thus, it would be a bit below the threshold of the acceptance.
This paper introduces an approach based on filter transform for designing networks equivariant to different transformation groups. Especially, the authors rely on the haramonic analysis view of steerable CNNs given in Weiler & Cesa (2019) to design an equivariant filter bank by computing simple transforms over base filters.   The reviewers finds the paper technically solid but difficult to read and with a limited contribution.  The AC carefully reads the paper and discussions. Although the connection between steerable CNNs and filter transform are interesting, the AC considers that the main contributions of the paper should be consolidated, especially the positioning with respect to Weiler & Cesa (2019). \ Therefore, the AC recommends rejection.
The paper investigates problems that can arise for a certain version of the dual form of the Wasserstein distance, which is proved in Appendix I. While the theoretical analysis seems correct, the significance of the distribution is limited by the fact, that the specific dual form analysed is not commonly used in other works. Furthermore, the assumption that the optimal function is differentiable is often not fulfilled neither. The paper would herefore be significantly strengthen  by making more clear to which methods used in practice the insights carry over.  
This paper presents a method for training neural networks with belief propagation based algorithms. The approach is to set a fully factorized prior over weights, compute a forward and backward pass of messages on a minibatch, then set the new prior to be a slightly higher temperature version of the minibatch approximate posterior. This new prior is then used for the next minibatch, and training iterates.  There is a huge range of opinions amongst reviewers. The main thing that reviewers appreciate is the novelty of using belief propagation instead of backpropagation for training neural networks. Finding alternatives to backprop with favorable properties could be hugely impactful, so even small gains in this direction are valuable. The posterior as prior update is interesting, and the authors have clearly put in care to getting things working. The main weaknesses are that some of the experiments aren’t always reasonable and fair, the paper is framed to overstate its contribution, and there’s not a clear advantage over standard approaches (e.g., MNIST error rates for a two hidden layer network are >2%).  In the end, this is a very borderline paper, but I find Rev nNGL’s position to be most informative. In particular, the paper frames the main contribution to be message passing as an alternative to SGD for training neural networks, but this is too broad of a framing given the existence of other closely related approaches like Soudry et al pointed out by Rev nNGL. I’d recommend that the authors frame their work as an advance over other message passing based approaches to training neural networks, and to focus on piecing apart precisely why the proposed approach improves over EBP and alternatives.
The paper presents a modification of the Winograd convolution algorithm that reduces the number of multiplications in a forward pass of a CNN with minimal loss of accuracy. The reviewers brought up the strong results, the readability of the paper, and the thoroughness of the experiments. One concern brought up was the applicability to deeper network structures. This was acknowledged by the authors to be a subject of future work. Another issue raised was the question of theoretical vs. actual speedup. Again, this was acknowledged by the authors to be an eventual goal but subject to further systems work and architecture optimizations. The reviewers were consistent in their support of the paper. I follow their recommendation: Accept. 
The reviewers agree that this paper studies an important problem, provides theoretically analysis to understand graph injection attack. The authors propose a new regularizer to improve the attack success. Extensive experimental results also show the effectiveness of the proposed method.
The paper introduces variants of RL algorithms that can consume factored state representations. Under the assumption that actions only affect a few factors, these factored RL algorithms can learn more efficiently than their vanilla counterparts. Learning a factored dynamics model (to be used in a model based algorithm) or representing factorized action selection policies (to be optimized by a model free RL algorithm) make intuitive sense in the problem settings that the paper considers. However, the paper should clarify the implicit assumptions being made about how the reward decomposes across factors. For instance, the factored DQN approach seems to require a linear reward decomposition across the factors. The factored DQN approach is also reminiscent of the Hydra algorithm on MsPacMan (https://papers.nips.cc/paper/2017/file/1264a061d82a2edae1574b07249800d6 Paper.pdf Section 4.2) which assigns an RL agent to each factor ("ghost" in MsPacMan) to learn a factor specific Q function. The linear aggregator that they use is identical to the factored DQN in this paper.  The reviewers all rate the paper as borderline. All reviewers suggest that being able to learn the factor graph (or at least parts of it) will greatly widen the scope of applications where the approach can be fruitfully applied   the paper acknowledges this as a compelling line of future work. The biggest weakness is originality   the core message of the paper is just that, where factored representations of state/actions exist RL algorithms must use it. This is not a surprising or novel message. The paper advocates for incorporating the factorization information in the most straightforward way (state masking, followed by action concatenation). Simple in retrospect is usually an excellent feature of an algorithm, not a bug; however, the proposal is literally the first idea a reader will likely think of. It might help to explore other ways of incorporating factorization information (e.g., rather than parameter sharing, have a separate network for each factor; rather than masking, have different width input layers to consume different number of parents in the DAG; etc.) and verifying that they are inferior to factored NN.
Dear authors,  Reviewers liked the idea of your new optimizer and found the experiments convincing. However, they also would have liked to get better insights on the place of AggMo in the existing optimization literature. Given that the related work section is quite small, I encourage you to expand it based on the works mentioned in the reviews.
This paper studies the stochastic shortest path (SSP) problem with a linear approximation to the transition model. The authors propose a doubling algorithm for regret minimization in this setting and bound its regret. This is a theory paper with no experiments.  This paper received three borderline reviews. All reviewers agreed on its strengths and weaknesses during the discussion. The strengths are that the paper is well written and that the results are novel. The weaknesses are that the proposed solution is standard and analyzed using standard tools. The reviewers noted departures from the standard analyses but these seem to be minor technical issues. Therefore, although well executed, this paper lacks novelty. No reviewer argued for the acceptance of this paper and therefore it is rejected.
Dear Authors,  Thank you very much for your very detailed feedback to the reviewers. They have highly contributed to clarifying some of the concerns raised by the reviewers and improved their understanding of this paper.  Overall, all the reviewers acknowledge the merit of this paper and thus I suggest acceptance of this paper. However, as Reviewer #4 pointed out, there are conceptual and theoretical issues that need to be more carefully addressed. Please clarify these issues in the final version of the paper.
This paper explores the use of multi step latent variable models of the dynamics in imitation learning, planning, and finding sub goals. The reviewers found the approach to be interesting. The initial experiments were a main weakpoint in the initial submission. However, the authors updated the experimental results to address these concerns to a significant degree. The reviewers all agree that the paper is above the bar for acceptance. I recommend accept.
All three reviewers feel that the paper needs to provide more convincing results to support their robustness claim, in addition to a number of other issues that need to be clarified/improved. The authors did not provide any response. 
The reviewers agree that the proposed method is theoretically interesting, but disagree on whether it has been properly experimentally validated.   My view is that the the theoretical contribution is interesting enough to warrant inclusion in the conference, and so I will err on the side of accepting.
The paper attempts to reduce computational cost of Transformer models. In this regard, authors generalizer PoWER BERT by proposing a variant of dropout that reduces training cost by randomly sampling a fraction of the length of a sequence to use at each layer. Further, a sandwich training method is used which trains a spectrum of randomly sampled model between the largest and the smallest size model. At test time, the best length configuration that balances the accuracy and latency tradeoff via evolutionary search is used. The reviewers found the general idea interesting, but raised a number of concerns. First, proper baselines should be used and related works be discussed. In particular, the method is built on top of Power BERT, yet it does not directly compare with it, and there was no good response when pointed out by a reviewer. Second, as the paper employs many tricks (some new some from prior work), but does not do any ablation studies to show how each of those contributes to the final accuracy gains. Finally, to showcase benefit compared to prior works in terms of computational cost a proper evaluation methodology and actual speedups for batch size 1 inference should be provided. Thus, an improved evaluation would benefit the paper a lot and paper in its current form is not ready for publication. 
This paper deals with the important topic of active transfer learning. All reviewers agree that while the paper presents some shortcomings , it is considered to be a worth contribution.
This paper proposes a method, SNOW, for improving the speed of training and inference for transfer and lifelong learning by subscribing the target delta model to the knowledge of source pretrained model via channel pooling.  Reviewers and AC agree that this paper is well written, with simple but sound technique towards an important problem and with promising empirical performance. The main critique is that the approach can only tackle transfer learning while failing in the lifelong setting. Authors provided convincing feedbacks on this key point. Details requested by the reviewers were all well addressed in the revision.  Hence I recommend acceptance.
This work examines how internal consistency objectives can help emergent communication, namely through possibly improving ability to refer to unseen referents and to generalize across communicative roles. Experimental results support the second hypothesis but not the first. Reviewers agree that this is an exciting object of study, but had reservations about the rationale for the first hypothesis (which was ultimately disproven), and for how the second hypothesis was investigated (lack of ablations to tease apart which part was most responsible for improvement, unsatisfactory framing). These concerns were not fully addressed by the response. While the paper is very promising and the direction quite interesting, this cannot in its current form be recommended for acceptance. We encourage authors to carefully examine reviewers  suggestions to improve their work for submission to another venue.
The paper compared different architectures of deep neural nets for learning full 3D turbulence simulations.  On coarse grids, the proposed method predicts more accurately than the classical solvers, especially on preserving the high frequency information.  The reviews think the paper is clearly written with strong experiments. Pls include the suggested references in the final version.
Dear authors,  All reviewers agreed that your work sheds new light on a popular class of algorithms and should thus be presented at ICLR.  Please make sure to implement all their comments in the final version.
The paper makes progress on a problem that is still largely unexplored, presents promising results, and builds bridges with  prior work on optimal control.  It designs input convex recurrent neural networks to capture temporal behavior of  dynamical systems; this then allows optimal controllers to be computed by solving a convex model predictive control problem.  There were initial critiques regarding some of the claims. These have now been clarified. Also, there is in the end a compromise between the (necessary) approximations of the input convex model and the true dynamics, and being able to compute an optimal result.   Overall, all reviewers and the AC are in agreement to see this paper accepted. There was extensive and productive interaction between the reviewers and authors. It makes contributions that will be of interest to many, and builds interesting bridges with known control methods.
This paper proposes a new method for conditional text generation that uses contrastive learning to mitigate the exposure bias problem in order to improve the performance. Specifically, negative examples are generated by adding small perturbations to the input sequence to minimize its conditional likelihood, while positive examples are generated by adding large perturbations while enforcing it to have a high conditional likelihood.   This paper receives 2 reject and 2 accept recommendations, which is a borderline case. The reviewers have raised many useful questions during the review process, while the authors has also done a good job during the rebuttal to address the concerns. After checking the paper and all the discussions, the AC feels that all the major concerns have been solved, such as more clarification in the paper, more results on non pretrained models, and small scale human evaluation.    On one hand, reviewers found that the proposed method is interesting and novel to a certain extent, the paper is also well written. On the other hand, even after adding all the additional results, the reviewers still feel it is not super clear that results would extend to better models, as most of the experiments are conducted on T5 small, and the final reported numbers in the paper are far from SOTA.   As shown in Table 1 & 2, the AC agrees that the final results are far from SOTA, and the authors should probably also study the incorporation of CLAPS into stronger backbones. On the other hand, the AC also thinks that T5 is already a relatively strong baseline to start with (though it is T5 small), and it may not be necessary to chase SOTA. Under a fair comparison, the AC thinks that the authors have done a good job at demonstrating its improvements over T5 MLE baselines.   As a summary, the AC thinks that the authors have done a good job during the rebuttal. On balance, the AC is happy to recommend acceptance of the paper. The authors should add more careful discussions to reflect the reviewers  comments when preparing the camera ready. 
The paper presents a careful analysis of SGD by characterizing the stochastic gradient via von Mises Fisher distributions. While the paper has good quality and clarity, and the authors  detailed response has further clarified several raised issues, some important concerns remain: Reviewer 1 would like to see careful discussions on related observations by other work in the literature, such as low rank Hessians in the over parameterized regime, Reviewer 2 is concerned about the significance of the presented analysis and observations, and Reviewers 2 and 4 both would like to see how the presented theoretical analysis could be used to design improved algorithms. In the AC s opinion, while solid theoretical analysis of SGD is definitely valuable, it is highly desirable to demonstrate its practical value (considering that it does not provide clearly new insights about the learning dynamics of SGD).
Strengths:  This paper introduces a clever construction to build a more principled disentanglement objective for GANs than the InfoGAN.  The paper is relatively clearly written.  This method provides the possibility of combining the merits of GANs with the useful information theoretic quantities that can be used to regularize VAEs.  Weaknesses:  The quantitative experiments are based entirely around the toy dSprites dataset, on which they perform comparably to other methods.  Additionally, the qualitative results look pretty bad (in my subjective opinion).  They may still be better than a naive VAE, but the authors could have demonstrated the ability of their model by comparing their models against other models both qualitatively and quantitatively on problems hard enough to make the VAEs fail.  Points of contention:  The quantitative baselines are taken from another paper which did zero hyperparameter search.  However the authors provided an updated results table based on numbers from other papers in a comment.  Consensus:  Everyone agreed that the idea was good and the experiments were lacking.  Some of the comments about experiments were addressed in the updated version but not all.
This paper introduces a new transformer architecture for representation learning in RL. The key ingredients of the proposed architectures are a novel combination of existing methods: (1) the use of LSTMs to reduce the need for large transformers and (2) a contrastive learning procedure that doesn t require human data augmentation.  The resulting approach requires less prior knowledge and provides higher sample efficiency. The paper is convincing, with comprehensive experiments on multiple challenging and well known benchmarks and an ablation study. The reviewers did expressed concerns that parts of the paper are a very difficult read and could use improvement, especially those relying on substantial external background. The intuition behind several components could be improved, and there are some clarity issues, as detailed in the individual reviews.
The initial reviews for this paper were diverging. After the rebuttal all reviewers have reached the consensus of recommending the paper s acceptance. Some reviewers have concerns regarding the novelty of the paper, however they appreciate that the paper is ell written and the empirical results are interesting. Following the reviewers recommendation, the meta reviewer recommends acceptance. In the final version of the paper the authors are encouraged to strengthen the weaknesses discussion as requested by one of the reviewers.
The paper studies the length distortion in a random (deep) ReLU network — namely, it bounds the expectation and higher moments of the length of the curve in feature space produced by applying a random ReLU work to a smooth curve. Because the product of layer norms grows exponentially in the depth, it might be natural to conjecture that the length grows exponentially in depth. Indeed, this has been claimed in previous theoretical work. The submission argues through rigorous mathematical analysis and corroborating experiments that this claim is incorrect. In fact, the length exhibits a slow (1/depth) contraction as the network depth increases. The paper also works out higher order moments and extensions to higher dimensional volumes. These results are obtained using nice (and natural) independence arguments and calculations.   Initial reviews were mostly positive, with the reservation that the initial submission may have slightly over claimed (the reviewer correctly notes that it is impossible to prove interesting results about the NTK of deep networks with the incorrect exponential growth hypothesis, and that related, and correct, arguments are embedded in the proofs of a number of NTK adjacent papers). After responses and revisions from the authors, the reviewers uniformly recommend acceptance. This is a solid paper, with an important conceptual point — length/volume contraction is critical to reasoning correctly about feature evolution in deep networks. In addition, it corrects existing errors in the literature, and provides relatively transparent justifications of its main claims.   The AC concurs with the reviewers’ evaluation of the paper, and recommends acceptance.
This paper studies whether adopting strategy adaptation mechanisms helps players improve their performance in zero sum stochastic games (in this case baseball). Moreover they study two questions in particular, a) whether adaptation techniques are helpful when faced with a small number of iterations and 2) what’s the effect of different initial strategies when both teams adopt the same adaptation technique. Reviewers expressed concerns regarding the fact that the author’s adaptation techniques improve upon initial strategies, which seems to indicate that their initial strategies were not Nash (despite the use of CFR). In the lack of theory of why this seems to happen at the current setup (and whether indeed the initial strategies are Nash and why do the improve), stronger empirical evidence from more rigorous experiments seem somewhat necessary for recommending acceptance of this paper.
The paper proposes a simple and effective way to stabilize training by adding consistency term to discriminator. Given the stochastic augmentation procedure $T(x)$ the loss is just a penalty on $D$. The main unsolved question why it help to make discriminator "smoother" in the consistency case for a standard GAN (since typically, no constraints are enforced). Nevertheless, at the moment this a working heuristics that gives new SOTA, and that is the main strength. The reviewer all agree to accept, and so do I.
This paper has been withdrawn by the authors.
This paper aims to do efficient epistemic uncertainty quantification for model based learning for control. It does so by augmenting the dataset with synthetic data around the true data points, and trying to classify whether a point is close to the training set or not. I agree with many of the criticisms that R3 and R5 brought fourth. Namely, it s not clear why a kernel density estimate couldn t be used instead (runtime complexity is cited as the reason, but could be addressed through approximations, inducing points etc). It is not clear how to set the sampling distribution for X_epi. Also, since efficiency is a motivation for the work, I suggest that the authors look at and cite:  https://arxiv.org/abs/2002.06715   I think at the moment the paper is not ready for publication, but the idea is interesting. Aside from comparing with the work above, what would improve this paper is an automatic way to select the distribution, or at least the covariance, of X_epi.  
Reviewer scores straddle the decision boundary but overall this does work does not meet the bar yet. Even after discussion with the authors, the reviewers reconfirmed there  reject  recommendation and the area chair agrees with that assessment.
The authors conduct extensive experiments to show that there were some errors in the original claims of the WMD paper and as opposed to what was claimed in the original paper, WMD does not outperform simpler baselines like BOW and TF IDF. The authors claim that this is significant because WMD is widely used in the literature and hence pointing out errors in the original paper may help the community.   Out of the 4 reviewers, 1 reviewer wrote a very short review and despite reminders did not elaborate on the reasons for a "Strong Accept". The other reviewer with a "Strong Accept" rating also did not champion the paper in the final discussions. The main objection of the two reviewers who were not in favor of accepting the paper were that (i) it focuses on cirticising a single paper and (ii) some of the criticism is not fair. In response, the authors claim that given the huge amount of derivative work which uses or builds upon the original WMD metric it is crucial to point out these errors.   Having read the reviews and the responses, it is not clear to me whether such a paper which focuses only on such criticism of a single paper (not matter how popular it is) has enough merit in being accepted. Alternatively, if such criticism was a part of a broader work (maybe a work on new document similarity metrics) then it would have more merit. Further, it should be noted that of the 4 misleading conclusions of the original paper identified by the authors at least 2 are debatable (one being an error in the dataset and the other being a normalisation technique which was not mentioned in the paper but used in the code). The authors have also rephrased one of the original 4 misleading points and from the discussion it seems that they agree it is not misleading. It would have been easier for me to accept the paper if it had a new metric and ablation studies which showed that (i) Hey, normalisation is important and should be done for all baseline algorithms that are being compared (ii) Hey, there are errors in the dataset which affect the results
The paper presents an approach for weakly supervised pre training for videos using textual information provided with web videos on Youtube and Instagram.  ## Strength * The work shows strong results with relative small dataset and computational resources compared to other work in the area of self/weakly supervised learning for videos. * Interesting ablations  ## Main Concerns * The authors don t discuss and compare to the weakly supervised work [Ghadiyaram et al. CVPR 19] adequately. Furthermore, the authors characterize the work incorrectly in their author response as detailed by R2. I agree to R2 here and like to highlight the concern is not that the method of [Ghadiyaram et al. CVPR 19] being similar to this work but the level/type of supervision. * Limited novelty over prior work.  ## Further Concerns * Some unclarities * The authors did not provide an updated revision of the pdf  Overall the paper received reject and borderline scores after author response and discussion (With the strongest score 6 from R1) due to the concerns concerns listed above apart from the ability to work with small number of data. I think the missing comparison to  [Ghadiyaram et al. CVPR 19] which operates in a similar setting weights strongly and I recommend reject.
This work develops a weight quantization method for deep neural networks that is suitable for a type of analog hardware system known as crossbar enabled analog computing in memory (CACIM).  The goal of this work is to train models on GPUs in such a way that they retain their predictive accuracy during inference when deployed on the analogue hardware system.  Pros: * Good adaptation of quantization methods to the CACIM system * Simple method * Validation of the proposed method on multiple datasets and models  Cons: * Lack of novelty: the proposed method is a simple combination of two popular methods, LLoyd s quantization and noise aware training  All reviewers appreciate the simplicity of the method and the good fit to the hardware.  The authors responded to all reviews and two reviewers acknowledged the authors  response.  The authors acknowledge some reviewer observations (motivation of quantization as reducing analogue noise, lack of experiments on the actual CACIM system), and the authors added an experimental evaluation on the actual physical CACIM system showing that their method performs well.  Overall the work is well executed and the proposed method is a good fit to the CACIM system.  However, the proposed quantization method is a straightforward adaptation of popular quantization methods.
This paper proposes to use self supervised learning in the context of "imbalanced regression", where some values of the outcome variables are rare, such as in long tailed regression. The author s proposal can be interpreted as a Monte Carlo approximation of a density smoothing technique, akin to Yang et al. 2021. They test their approach on three datasets. Overall, it provides marginal improvements, whose statistical significance are not assessed. All reviewers agreed that the paper has merits but that it should be further improved to demonstrate that the proposed method is indeed a step forward  in solving the problem of imbalanced regression. The authors should also provide stronger motivation for their pipeline details and experimental setup choices. I therefore recommend rejection, with encouragement for improvement in two directions: strengthening the experimental section, in particular by assessing statistical significance, and by improving the writing of the paper by developing a more rigorous exposition.
The authors propose a memory based continual learning method that decomposes the models  parameters and that shares a large number of the decomposed parameters across tasks. In other words, only a small number of parameters are task specific and the memory usage of storing models from previous tasks is hence a fraction of the memory usage of previous approaches. The authors take advantage of their method to propose specific ensembling approaches and demonstrate the strong performance of their methods using several datasets.  In the rebuttal, the authors were very reactive and provided many useful additional results during including a comparison of the computational cost of their method vs. others, results using two new datasets (CUBS & Flowers), and additional results on mini ImageNet. They also answered, through additional experiments, several reviewer questions including the robustness to different first tasks in the sequence.  Overall, the reviewers after the rebuttal/discussion period agree that this is a strong contribution: novel and fairly simple method with some theoretical justification, thorough empirical evaluation, well written and easy to follow manuscript. It also opens a few interesting avenues some of which the authors have already explored in their paper (e.g., ensembling).
While using self play for training a goal oriented dialogue system makes sense, the contribution of this paper compared to previous work (that the paper itself cites) seems too minor, and the limitations of using toy synthetic data further weaken the work. 
This manuscript presents an approach to handling abstract visual analogy tasks where panels of drawings are shown with a missing entry. One of a number of candidate drawings must be chosen to complete the panel. Reviewers brought up several concerns:  1. The task performed was made considerably easier by providing additional annotations at training time. This was not the case in the original task in prior work that the manuscript builds on. No convincing explanation was provided as to why this change is critical to accommodate the manuscript s contributions.  2. A key feature of the approach, the adaptive modular design, does not seem to contribute much. The authors rightly point out this may be a limitation of current benchmarks. Reviewers were sympathetic to this view but that leaves the manuscript in a tough spot: a central contribution cannot be evaluated. What is even worrisome is that without evaluating the effectiveness of the adaptive design we cannot know if it is working at all. What if adaptivity is required for some future analogy tasks but it turns out that this approach, despite seeming to be adaptive, falls short?  3. Another contribution, the multi task encoder does not seem to provide much value in ablation experiments. The manuscript would be improved if this feature was removed or its usefulness was demonstrated.  A number of smaller issues were also brought up by reviewers.  Throughout the responses to reviewers the authors highlight that their central contribution is incorporating a structure mapping prior "The central contribution of our method is introducing a structure mapping prior ...". I would like to draw the author s attention to the fact that they had to remind 3 of the 4 reviewers to focus on this rather than another aspect of the work. That clearly indicates that the manuscript and work needs a shift in focus. I would suggest that authors double down on their structural mapping prior, eliminate all other features which turned out to be controversial or impossible to evaluate, and demonstrate the utility of their idea in two domains, i.e., including another domain. This would really highlight the core contribution.  Unfortunately, what may turn out to be a good idea, the structural mapping prior, is lost among many other complexities. I hope the authors are not discouraged and that we see this line of work again in the future.
Summary: The paper discusses Markov games with general function approximation, and investigates in particular reinforcement learning algorithms that learn a Nash policy in a trial and error fashion. They consider two settings: the decoupled one where the player does not observe the opponent’s policy and the coordinated one where optimistic planning is performed for both. The main contribution is a new complexity measure called Minimax Eluder dimension which is used to control the regret of the proposed algorithms.   Discussions: The reviewers raised many minor concerns regarding the writing (typos of missing notation) and the clarity (missing discussions and explanations), which were addressed during the discussion phase. In light of this revision, the committee and myself judge that the paper should be accepted to ICLR.  Decision: Accept
The paper proposes an evolutionary architecture search method which uses weight inheritance through network morphism to avoid training candidate models from scratch.  The method can optimise multiple objectives (e.g. accuracy and inference time), which is relevant for practical applications, and the results are promising and competitive with the state of the art. All reviewers are generally positive about the paper. Reviewers’ feedback on improving presentation and adding experiments with a larger number of objectives has been addressed in the new revision.   I strongly encourage the authors to add experiments on the full ImageNet dataset (not just 64x64) and/or language modelling   the two benchmarks widely used in neural architecture search field.
All reviewers agree that the proposed SphereFace2 approach   training face recognition models by using multiple binary classification losses   is interesting and innovative. The reviewers agree that the paper is well written and are satisfied with the presented experimental study. The rebuttal addressed all additionally raised questions. I believe that the paper will be of interest to the audience attending ICLR and would recommend a presentation of the work as a spotlight.
The paper analyzes a 2 stage method for federated learning, first using FL with local steps, followed by a final phase of  always communicate  centralized SGD. For the convex case, the paper studies the influence of the data heterogeneity, a key parameter in FL, on the convergence of related schemes. Surprisingly the results of the 2 stage method seem to be basically identical to pure local training followed by the final centralized phase, and almost match the lower bound for communication.  Reviewers liked the interesting aspect of the heterogeneity induced error floor when the phases are switched, and its impact on the convergence rates, which can be substantial. Downsides are that the analysis only works for strongly convex setting, and the combination of the two methods and proof being relatively straight forward. Simplicity of the algorithm is a plus, while of the proof depends on novelty, about which reviewers are border line but positive.   Deep learning experiments should be expanded, as there the very opposite order of the two phases https://arxiv.org/abs/1808.07217 is more commonly used (i.e. more communication in early phase can help), which should be discussed. Also, in the experiments the tuning of hyperparameters in the single stage baselines needs to be improved to be more fair, which the authors have started but not fully finished for the Cifar case.  We hope the authors will incorporate the open points as mentioned by the reviewers.
This work studies an intriguing problem of searching optimal architectures for unsupervised domain adaptation. It is based on a two stage approach: (1) transferable architecture search via DARTS + MK MMD; (2) transferable feature learning via Backbone + MCD.  The reviews for this paper are very insightful, constructive and of high quality. While all reviewers acknowledge the contribution of a new research problem, they unanimously have suggestions for further improving the paper:   The novelty and soundness in the technical method are not fantastic. Authors should consider more elaborated loss designs for the approach, and unify the two stages with the same optimization objective.   The empirical evaluation is by far not extensive and insightful. More stronger NAS approaches and larger scale datasets should be included to give more evidence to support the claims that NAS DA is better.   A featured analysis of how non iid architecture (as searched out by this work) differs from iid architecture would make the paper much more interesting.  Authors did not participate in the rebuttal and discussion phase.  AC scanned through the paper and believes that this paper studies a promising research direction, but the work cannot be accepted before addressing the reviewers  comments. The weaknesses are quite obvious and will have a high probability of being asked by the reviewers of the next conference. So the authors need to make sure that they substantially revise their work before submitting to yet another top venue.
The paper studies knowledge distillation through the lens of semi parametric inference. There has been a lot of work on knowledge distillation in the past, but, as the paper points out, most of it is heuristic or empirical in nature, and thus theoretical understanding on the subject is lacking. The reviewers generally agree that this paper makes a useful theoretical contribution towards a better understanding of knowledge distillation. However, the reviewers also raised some concerns, especially regarding the clarity of the text, and they felt that the paper might be overstating its contribution. Still, all reviewers recommend acceptance, and on balance the merits of the paper seem to outweigh the weaknesses, so I d be happy to recommend acceptance.
This work deals with training generators of aligned pairs of images and segmentation maps. It is based on the recent DatasetGAN approach, which generates images and maps, but requires human annotations on a handful of generated images. This paper is addressing this problem by learning the annotation model over annotated real images instead of generated ones. To this end, the paper proposes a meta learning that uses the Gradient Matching Loss.  Overall, the rebuttal provides valuable insight and many issues raised by reviewers have been convincingly answered by the authors. On the whole, the reviewers converged positively, the novelty and the interest of the proposal stand out clearly, and this despite the lack of very convincing experiments, at least before the rebuttal. Authors are strongly encouraged to take all comments into account for their final version.
This paper presents an analysis of the kind of knowledge captured by pre trained word embeddings. The authors show various kinds of properties like relation between entities and their description, mapping high level commands to discrete commands etc. The problem with the paper is that almost all of the properties shown in this work has already been established in existing literature. In fact, the methods presented here are the baseline algorithms to the identification of different properties presented in the paper.  The term common sense which is used often in the paper is mischaracterized. In NLP literature, common sense is something that is implicitly understood by humans but which is not really captured by language. For example, going to a movie means you need parking is something that is well understood by humans but is not implied by the language of going to the movie. The phenomenon described by the authors is general language processing.  Towards the end the evaluation criteria for embedding proposed is also a well established concept, its just that these metrics are not part of the training mechanism as yet. So if the contribution was on showing how those metrics can be integrated in training the embeddings, that would be a great contribution.  I agree with the reviewer s critics and recommend a rejection as of now.
The authors provide in this manuscript a theoretical analysis to explain why deep neural networks become linear in the neighbourhood of the initial optimisation point as their width tends to infinity. They approach this question by viewing the network as a multi level assembly model.  All reviewers agree that this is an interesting, novel, and relevant study. The paper is very well written.  Initially, a weak point raised by a reviewer was that an empirical evaluation of the theory was missing. The authors addressed this issue in a satisfactory manner in their response.  In conclusion, this is a strong contribution worth publication.
The paper introduces a technique for randomised dynamic programming and uses it to scale a latent variable model that enables interpreting the hidden states of large pre trained models for text representation and generation.  The current version needs to be improved with regards to scope, which can be seen by the various confusions that it triggered, and which the authors tried to address in the rebuttal phase. It is somewhat unclear to all of us (myself included) whether the paper is about i) randomised dynamic programming (RDP), or ii) RDP s role in a particular LVM (with a CRF posterior approximation), or iii) RDP+LVM s ability to interpret deep Transformer models? Empirically, the paper is much more about (iii), somewhat about (ii, e.g. Table 1), very little about (i, e.g. Figure 2).  *Because the scope is now confusing*, the current version sometimes comes across as relatively incremental or even incomplete:  * Should the authors embrace interpretation. The overall strategy is *very interesting*, and it scales a neat model precisely in the way it needs to be scaled to do what it s meant to do, but this would change the focus of the paper, RDP would be all but a means to an end, and perhaps other techniques for interpretation would be needed.  * Should the authors embrace RDP itself (disentangled from its application to model interpretation). Some of us felt like the randomisation technique on its own is not too surprising (given the work of [Liu et al](http://proceedings.mlr.press/v97/liu19c/liu19c.pdf), for example), and, regardless of that, to push for RDP s significance, the paper would need more comparisons. The only alternative to RDP investigated in the paper is a heuristic top K gradient. There are deterministic gradients that are less heuristic, and which may become unbiased eventually as training progresses, see for example [[1]](https://aclanthology.org/D18 1108/) and [[2]](https://papers.nips.cc/paper/2020/hash/887caadc3642e304ede659b734f79b00 Abstract.html).  In the first round of reviews there were some comments that questioned the paper s fitness to ICLR, I would like to remark that this has been clarified, and the paper targets a problem of clear relevance to the conference.  I would personally like to add a minor comment: it would be nice to acknowledge some older literature on randomised DPs (see for example [[3]](https://papers.nips.cc/paper/2009/hash/e515df0d202ae52fcebb14295743063b Abstract.html) and [[4]](https://aclanthology.org/N10 1028/)).
+ an interesting task   learning to decompose questions without supervision    reviewers are not convinced by evaluation. Initially evaluated on MetaQA only, later relation classification on WebQuestions has been added.  It is not really clear that the approach is indeed beneficial on WebQuestion relation classification (no analysis / ablations) and MetaQA is not a very standard dataset.     Reviewers have concerns about comparison to previous work / the lack of state of the art baselines. Some of these issues have been addressed though (e.g., discussion of Iyyer et al. 2016)    
This paper considers augmenting LSTM language models with a form of residual connection that adds and additional feed forward layer before the softmax that integrates the output of the recurrent cell with the input embedding. This architectural variation is evaluated on the standard Penn Treebank and Wikitext 2 language modelling tasks and shown to lead to lower perplexities on the test sets, particularly when dynamic evaluation is used.   The reviewers agree that the proposed addition is well motivated, however they also observe that there has been substantial work in language modelling on various forms of residual and skip connections and it is not clear how this work relates to that body of work. The authors have provided some additional comparisons during the discussion, however the reviewers feel that further evaluation and analysis is needed. There was also some additional confusion about the varying hyperparamter tuning protocols employed in the different evaluations. The author’s have clarified this in their response so that it is clearer how the different results were obtained.  Overall this paper presents an promising initial result, but it would benefit from more complete evaluation, analysis, and hyperparameter tuning. This could include ablation studies and analysis to shed more light on what the proposed architectural addition is contributing, how this relates to other varieties of residual connection, and it’s positive interaction with dynamic evaluation. It would also be useful to include a tuned model with a comparison to previously reported Wikitext 2 results.
As the reviewers point out, the core contribution might be potentially important but the current execution of the paper makes it difficult to gauge this importance. In the light of this, this paper does not seem ready for appearance in a conference like ICLR.
The paper proposes a Bayesian optimization approach to creating adversarial examples. The general idea has been in the air for some years, and over the last year especially there have been a number of approaches using BayesOpt for this purpose. Reviewers raised concerns about differences between this approach and related work, and practical challenges in general for using BayesOpt in this domain (regarding dimensionality, etc.). The authors provided thoughtful responses, although some of these concerns still remain. The authors are encouraged to address all comments carefully in future revisions, which a sufficiently substantial that the paper would benefit from additional review.   
The authors presents an alternative view of Neural ODEs, offering a novel understanding of depth in neural networks. The reviewers were overall impressed by the novelty and potential for insights this work brings. There was some disappointment that the empirical results were not stronger (both in terms of pure performance and computational cost) and that it wasn t clear how the theoretical insights into depth actually translated into a practical insight. Nevertheless, I agree with the reviewers that this is a good submission and would I think make for an interesting addition to the conference programme.
This paper proposes a method for reinforcement learning with unseen actions.  More precisely, the problem setting considers a partitioned action space.  The actions available during training (known actions) are a subset of all the actions available during evaluation (known and unknown actions).  The method can choose unknown actions during evaluation through an embedding space over the actions, which defines a distance between actions. The action embedding is trained by a hierarchical variational autoencoder. The proposed method and algorithmic variants are applied to several domains in the experiments section.  The reviewers discussed both strengths and weaknesses of the paper.  The strengths described by the reviewers include the use of the hierarchical VAE and the explanatory videos.  The primary weakness is the absence of sufficient detail when describing the solution.  The solution description is not sufficiently clear to understand the details of the regularization metrics.  The details of regularization are essential when some actions are never seen in training.  The reviewers also mentioned that the experiment analysis would benefit from more care.  This paper is not ready for publication, as the solution methods and experiments are not presented with sufficient detail.
The paper focuses on the task of finding higher fidelity action proposals for temporal action proposal detection. As the reviewers mentioned, this task is a pre task to temporal activity localization/detection in video, which is the main task to be solved. The paper may be perceived differently if it were presented as a detection method instead. Apart from the scope of the paper, the reviewers also unanimously agree on the limited technical novelty of the proposed methodology in the paper. The proposed method can be seen as an application of self attention and transformer techniques on the problem of activity detection. The goal of these techniques is feature enrichment that serves to incorporate information across long term context, a concept that has appeared previously in other work but not necessarily with the same machinery (e.g. G TAD).   Despite its shortcomings and since it presents promising experimental results on well known proposal/detection benchmarks, the authors can benefit from considering the reviewers  comments and suggestions to produce a stronger and more compelling future submission.
there are two separate ideas embedded in this submission; (1) language modelling (with the negative sampling objective by mikolov et al.) is a good objective to use for extracting document representation, and (2) CNN is a faster alternative to RNN s, both of which have been studied in similar contexts earlier (e.g., paragraph vectors, CNN classifiers and so on, most of which were pointed out by the reviewers already.) Unfortunately reading this manuscript does not reveal too clearly how these two ideas connect to each other (and are separate from each other) and are related to earlier approaches, which were again pointed out by the reviewers. in summary, i believe this manuscript requires more work to be accepted.
Nice paper, providing a thorough investigation of a simple idea that may be useful to a wide range of practitioners. All reviewers are positive, and the discussion has led to significant improvements in exposition and overall in the quality of the submission.
This paper presents a new unsupervised training objective for sentence to vector encoding, and shows that it produces representations that often work slightly better than those produced by some prominent earlier work.  The reviewers have some concerns about presentation, but the main issue—which all three reviewers pointed to—was the lack of strong recent baselines. Sentence to vector representation learning is a fairly active field with an accepted approach to evaluation, and this paper seems to omit conspicuous promising baselines. This includes labeled data pretraining methods which are known to work well for English (including results from the cited Conneau paper)—while these may be difficult to generalize beyond English, this paper does not attempt such a generalization. This also includes more recent unlabeled data methods like ULMFiT or Radford et al. s Transformer which could be easily trained on the same sources of data used here. The authors argue in the comments that these language models tend to use more parameters, but these additional parameters are only used during pretraining, so I don t find this objection compelling enough to warrant leaving out baselines of this kind. Baselines of both kinds have been known for at least a year and come with distributed models and code for close comparison.
There was major disagreement between reviewers on this paper. Two reviewers recommend acceptance, and one firm rejection. The initial version of the manuscript was of poor quality in terms of exposition, as noted by all reviewers. However, the authors responded carefully and thoroughly to reviewer comments, and major clarity and technical issues were resolved by all authors.   I ask PCs to note that the paper, as originally submitted, was not fit for acceptance, and reviewers noted major changes during the review process. I do believe this behavior should be discouraged, since it effectively requires reviewers to examine the paper twice. Regardless, the final overall score of the paper does not meet the bar for acceptance into ICLR.
The paper proposes a set of conditions that enable a mapping from word embeddings to relation embeddings in knowledge graphs. Then, using recent results about pointwise mutual information word embeddings, the paper provides insights to the latent space of relations, enabling a categorization of relations of entities in a knowledge graph. Empirical experiments on recent knowledge graph models (TransE, DistMult, TuckER and MuRE) are interpreted in light of the predictions coming from the proposed set of conditions.  The authors responded to reviewer comments well, providing significant updates during the discussion period. Unfortunately, the reviewers did not engage further after their original reviews, and so it is hard to tell whether they agreed that the changes resolved all their questions.  Overall, the paper provides much needed analysis for understanding of the latent space of relations on knowledge graphs. Unfortunately, the original submission did not clearly present the ideas, and it is unclear whether the updated version addresses all the concerns. The paper in its current state is therefore not yet suitable for publication at ICLR.
This paper considers the problem of black box optimization over categorical variables using expensive function evaluations.    Fourier representation is proposed as surrogate model by treating the categorical input as the direct sum of cyclic groups. The parameters are learned using exponentially weighted update algorithm.   To select the inputs for evaluation, simulated annealing and MCTS are employed as search algorithms to optimize the learned surrogate function.    Experiments are performed on two synthetic problems and RNA sequence design problems.  The proposed fourier representation is novel and the results show the promise of this method in terms of computational efficiency over state of the art COMBO method.  There are two unsatisfactory aspects of this paper. 1. In expensive black box optimization problems, number of function evaluations to find better solutions is critical. This paper takes a non Bayesian approach to improve computational efficiency (over prior Bayesian optimization methods), but this same advantage comes at the expense of sample efficiency (number of function evaluations) due to lack of exploration.  2. In fourier representation, mapping categorical values to different group elements may change which basis are used for modelling. From a practitioner s perspective, it is important to verify that the performance is not significantly affected by this choice. This can be verified empirically. Even though one reviewer raised this point, authors  haven t responded though it is an easy experiment to do.  Due to the above shortcomings, the paper is judged to be not ready for publication at the current stage. I strongly encourage to resubmit the paper after addressing the above two concerns.
The paper considers learning an NMT systems while pivoting through images. The task is formulated as a referential game. From the modeling and set up perspective it is similar to previous work in the area of emergent communication / referential games, e.g., Lazaridou et al (ICLR 17) and especially to Havrylov & Titov (NIPS 17), as similar techniques are used to handle the variable length channel (RNN encoders / decoders + the ST Gumbel Softmax estimator).  However, its multilingual version is interesting and the results are sufficiently convincing (e.g., comparison to Nakayama and Nishida, 17). The paper would more attractive for those interested in emergent communication than the NMT community, as the set up (using pivoting through images) may be perceived somewhat exotic by the NMT community. Also, the model is not attention based (unlike SoA in seq2seq / NMT), and it is not straightforward to incorporate attention (see R2 and author response).  + an interesting framing of the weakly supervised MT problem + well written + sufficiently convincing results   the set up and framework (e.g., non attention based) is questionable from practical perspective 
This paper proposes an online distillation method for efficient object recognition. The main idea is to employ a binary student network to learn frequently occurring classes using the pseudo labels generated by a teacher network. In order to identify rare vs frequent classes, an attention triplet loss is used. The proposed scheme is empirically evaluated on CIFAR 100 and tiny imagenet datasets.  The major and common concern from reviewers about this draft is the quality of presentation, which has made it difficult to read and understand the ideas and their underlying motivations. While specific instances of this were mentioned by the reviewers, and responded by the authors, the readability issues go beyond these instances. In fact, I could independently observe presentation issues different from those mentioned by the reviewers. For example, in Eq (1), what is eta (never defined before), what is the loss function here for which the gradient update leads to Eq (1)? The proof also is not clear and seems to have issues. For example, in Eq (13) what is the meaning of multiplying two vectors? If it means dot product, it should be denoted more clearly, e.g. as w^T w^* or <w,w^*>. In Eq (12), can alpha be negative? If not, should be clarified why, and if it can be negative, then (<w^*,w>)^2 >  (alpha n)^2 does not need to hold, but this inequality is used in Eq (14) anyway.  Overall, I think the submission can benefit from an overhaul of the writing. I encourage authors to resubmit after improving on that.
The paper presents yet another approach for modeling words based on their characters. Unfortunately the authors do not compare properly to previous approaches and the idea is very incremental.
This paper presents a light weight hybrid model using both convolutions and Transformer layers resulting in models with lower computational cost and good performance. Reviewers find the paper interesting and agree that the paper did a good job in presenting convincing experimental results. There were questions about role of different components in the proposed model, which author s addressed in the response with additional ablation studies. One reviewer expressed concerns about lack of theoretical foundations for the proposed approach. However they also agree that the paper presents a good and useful experimental study. I think overall the paper has good contributions that others can build on in the future and recommend acceptance.
This paper proposes a new softmax like operator, to be used instead of eps greedy or softmax in Q learning algorithms. There has been some previous work in this direction, most notably Mellowmax, but the proposed operator is more computationally efficient, and there is some experimental evidence that it improves DQN performance. The reviews were mixed, with two mildly positive reviewers (6), who found the work interesting, and two negative reviewers (3,5), who raised issues about the impact of the work when taken as a part of a larger RL algorithm, and about the generality of the work w.r.t. to other RL algorithms like policy gradients. During the discussion, the reviewers did not reach an agreement. My decision to reject the paper is based on the following: while the idea is novel, and the contraction analysis is appropriate, the main interest to the community in such an idea is either experimental   can it be used to push the state of the art RL algorithms? or theoretical   can we glean new theoretical insights using this method? In its current presentation, there is not enough evidence in the paper to support either of these.  I encourage the authors to either dig deeper into the experimental evaluation and produce more convincing results, or dig deeper into the theory and show some theoretical benefit of Resmax.
The main concern is that the results in this paper are based on strong asymptotic assumptions. (At least) more empirical results are needed. 
This paper provides a new approach for progressive planning on discrete state and action spaces. The authors use LSTM architectures to iteratively select and improve local segments of an existing plan. They formulate the rewriting task as a reinforcement learning problem where the action space is the application of a set of possible rewriting rules. These models are then evaluated on a simulated job scheduling dataset and Halide expression simplification. This is an interesting paper dealing with an important problem. The proposed solution based on combining several existing pieces is novel. On the negative side, the reviewers thought the writing could be improved, and the main ideas are not explained clearly. Furthermore, the experimental evaluation is weak.
This paper studies how (two layer) neural nets extrapolates. The paper is beautifully written and the authors very successfully answered all the questions. They managed to update the paper, clarify the assumptions and add additional experiments. 
Pros:   Addresses an important medical imaging application   Uses an open dataset  Con:   Authors do not cite original article describing challenge from which they use their data: https://arxiv.org/pdf/1612.08012.pdf , or the website for the corresponding challenge: https://luna16.grand challenge.org/results/   Authors either 1) do not follow the evaluation protocol set forth by the challenge, making it impossible to compare to other methods published on this dataset, or 2) incorrectly describe their use of that public dataset.   Compares only to AlexNet architecture, and not to any of the other multiple methods published on this dataset (see: https://arxiv.org/pdf/1612.08012.pdf).   Too much space is spent explaining well understood evaluation functions.   As reviewers point out, no motivation for new architecture is given. 
The problem addressed here is an important one: What is a good evaluation metric for generative models?  A good selection of popular metrics are analyzed for their appropriateness for model selection of GANs.  Two popular approaches are recommended: the kernel Maximum Mean Discrepancy (MMD) and the 1 Nearest Neighbour (1 NN) two sample test.  This seems reasonable, but the present work was not recommended for acceptance by 2 reviewers who raised valid concerns.  From a readability perspective, it would be nice to simply list the answer to question (1) directly in the introduction.  One must read more than a few pages to get to the answer of why the metrics that are advocated were picked.  It need not read like a mystery.  R4: "The evaluations rely on using a pre trained imagenet model as a representation. The authors point out that different architectures yield similar results for their analysis, however it is not clear how the biases of the learned representations affect the results. The use of learned representations needs more rigorous justification"  R2: "First, it only considers a single task for which GANs are very popular. Second, it could benefit from a deeper (maybe theoretical analysis) of some of the questions."   the first point of which is also related to a concern of R4.  Given the overall high selectivity of ICLR, the present submission falls short.
The paper proposes an approach for forecasting diverse object trajectories using determinantal point processes (DPP). Past trajectory is mapped to a latent code and a conditional VAE is used to generate the future trajectories. Instead of using log likelihood of DPP, the propose method optimizes expected cardinality as a measure for diversity. While there are some concerns about the core method being incremental in novelty over some existing DPP based methods, the context of the paper is different from these papers (ie, diverse trajectories in continuous space) and reviewers have appreciated the empirical improvements over the baselines, in particular over DPP NLL and DPP MAP in latent space. 
This work proposed a method for encouraging an agent showing altruistic behaviour towards another agent (leader) without having access to the leader s reward function. The basic idea is based on the hypothesis that having the ability to reach many future states (i.e., called choice) is useful for the leader agent, no matter what it reward function is. The altruistic agent learns a policy that maximizes the choice of the leader agent. The paper defines three notions of choice, and evaluates them on four environments.  The reviewers believe that this work attempts to solve an important problem, proposes a novel approach, and performs reasonably good experiments. The reviewers are all on the positive side at the end of the discussion phase. Therefore, I recommend acceptance of the paper. I also suggest a spotlight presentation for this work because of the novelty of the problem, which might be of interest to other researchers.  The authors have already done some revisions to their paper (including adding a new environment). I encourage them to consider any remaining comments from reviewers in their final version.
There are some interesting ideas discussed in the paper, but the reviewers expressed difficulty understanding the motivation and the theoretical results. The experiments do not seem convincing in showing that SQDML achieves significant gains. Overall, the the paper needs either stronger and clearer theoretical results, or more convincing experiments for publication at ICLR.
The paper studies the lottery ticket hypothesis in the context of deep image priors. Deep image priors are convolutional neural networks that are imposed as a prior for image reconstruction problems. A deep image prior can be an un trained convolutional network, or it can be a trained generator, and the paper considers both types of priors. Deep image priors are often highly over parameterized and thus the paper under review asks the question on whether the networks really have to be heavily parameterized or whether a small subnetwork will also do. The paper performs experiments on image restoration with an entirely un trained DNN (this constitutes the larges part of the paper) and on image restoration with a pre trained network.   The paper received four reviews out of which three recommend weak acceptance and one strong acceptance.   Reviewer wMsj finds it interesting that the paper shows that some networks are more suitable as deep image priors than others, but finds that the paper lacks evidence on why some structures are better than others.   Reviewer Kjf3 strongly recommends acceptance (8) in a relatively generic review. The reviewer finds the paper is interesting as it provides a novel application of the lottery ticket hypothesis. However, the review also criticizes that the experimental evaluating lacks rigor.   Reviewer jFko appreciates that the paper studies the lottery ticket hypothesis for the first time for un trained and pre trained image prior, and that the results are interesting as they suggest that the models can be made smaller and that this can even improve performance. The reviewer criticizes that the paper s presentation is confusing, and points out a few weaknesses in the empirical evaluation. The authors responded and after a brief discussion, the reviewer raised their score.    Reviewer vf8k provides a relatively brief review and criticizes that to find sparse networks, one requires the ground truth image, which is not accessible. The authors clarify that the method is transferable in that the network identified can be used for other images and is thus transferable. The reviewer was satisfied with this response.   The score of this paper 6.5 after the discussion period. Three of the reviewers are on the fence about the paper, one reviewer is not, and that reviewer significantly impacted the score. This reviewer, however, did not provide convincing arguments about the merits of the paper. All reviewers find that `3: Some of the paper’s claims have minor issues. , and I agree with that statement.  I do not recommend acceptance of the paper in its current form, because of insufficiently rigorous experiments to justify the claims:   Specifically, the paper s goal is to address the research question  do they [neural network based priors for image reconstruction] really have to be heavily parameterized?  The literature already answers this question since as the paper under review reads on page 3, the literature found that an under parameterized non convolutional model can function as an un trained image prior. Those underparameterized networks perform well and have fewer parameters than the best performing networks found in the paper under review.   The paper argues that a sparse network can give better performance. This claim is based on an at most 0.1dB difference, which can only be achieved when choosing the optimal sparsity level, which is not clear to do without knowing the ground truth image.
The paper aims to do out of distribution (OOD) detection in multi label classification. However,  the challenges of extending energy based  OOD methods in multiclass to multi label setting is not big. This paper just defines the label wise free energy. The key challenging issues in MLC is the label dependency. The paper did not consider modeling the label dependency and devide it to several binary classification issues. And the paper did not provide the theory gurantee. The paper is far below the bar of top conferences.
This paper tackles an important problem and includes experiments on a new domain (Russian documents vs English documents). Unfortunately, all reviewers agree that this paper lacks novelty for publication in its current state. Additional details and clarifications to the proposed approach, notably through a more thorough performance analysis, would improve the significance of the paper.
  pros:   good, sensible idea   good evaluations on the domains considered   good analysis   cons:   novelty, broader evaluation  I think this is a good and interesting paper and I appreciate the authors  engagment with the reviewers.  I agree with the authors that it is not fair to compare their work to a blog post which hasn t been published and I have taken this into account.  However, there is still concern among the reviewers about the strength of the technical contribution and the decision was made not to accept for ICLR this year. 
This paper presents a zero shot generation approach by disentangling representations into swappable components (each component corresponding to an attribute) and then conditioning on any desired combination of attributes to do zero shot synthesis of samples containing those attributes.  There were some concerns raised in the original reviews which the authors have addressed in the rebuttal and the revised submission. Post the discussion phase, all reviewers see merit in the proposed ideas and unanimously recommend acceptance. Based on my own reading of the paper and the reviews/author responses, I agree with the assessment. 
The paper proposed a GAN approach to robust learning against adversarial examples, where a generator produces adversarial examples as perturbations and a discriminator is used to distinguish between adversarial and raw images. The performance on MNIST, SVHN, and CIFAR10 demonstrate the effectiveness of the approach, and in general, the performance is on par with carefully crafted algorithms for such task.   The architecture of GANs used in the paper is standard, yet the defensive performance seems good. The reviewers wonder the reason behind this good mechanism and the novelty compared with other works in similar spirits. In response, the authors add some insights on discussing the mechanism as well as comparisons with other works mentioned by the reviewers.   The reviewers all think that the paper presents a simple scheme for robust deep learning based on GANs, which shows its effectiveness in experiments. The understanding on why it works may need further explorations.  Thus the paper is proposed to be borderline lean accept.   
This paper proposes an unsupervised text style transfer model which combines a language model prior with an encoder decoder transducer. They use a deep generative model which hypothesises a latent sequence which generates the observed sequences. It is trained on non parallel data and they report good results on unsupervised sentiment transfer, formality transfer, word decipherment, author imitation, and machine translation. The authors responded in depth to reviewer comments, and the reviewers took this into consideration. This is a well written paper, with an elegant model and I would like to see it accepted at ICLR. 
The paper proposes to regularize the decoder of the VAE to have a flat pull back metric, with the goal of making Euclidean distances in the latent space correspond to geodesic distances. This, in turn, results in faster geodesic distance computation. I share the concern of R2 that this regularization towards a flat metric could result in "biased" geodesic distances in regions where data is scarce. I suggest the authors discuss in the next version of the paper if there are situations where this regularization might have drawbacks and if possible, conduct experiments (perhaps on toy data) to either rule out or highlight these points, particularly about scarce data regions. 
In this paper, a novel machine learning based method for solving TSP is presented; this method uses guided local search in conjunction with a graph neural network, which is trained to predict regret. Reviewers disagree rather sharply on the merits of the paper. Three reviewers think that the paper is novel, interesting, and has good empirical results. Two reviewers think that the fact the results are not competitive with the best non learning based ("classic") solvers mean that the paper should be rejected. This area chair believes that research is fundamentally not about beating benchmarks, but about new, interesting, and sound ideas. The conceptual novelty of this method, together with the good results compared with other learning based methods, is sufficient for accepting the paper.
The paper presents a new algorithm for data augmentation in graph neural networks. The algorithm works by learning a conditional model of a node s neighbor features, and augment the neighborhood representation using the generative model.   In response to the reviews, the authors provided long answers and clarified much of the text. Nonetheless, after the discussion, two main concerns remained. First, the presention still felt subpar, too notationally heavy for what was presented. Second, the gains with respect to the baselines were assessed as not sufficiently significant to justify the approach which is substantially more complex than a baseline such as GRAND.
Two reviewers recommended rejection, and one was on the edge. There was no rebuttal to address the concerns and questions posed by the reviewers.
One of the four reviewers failed to engage in discussion, two acknowledged the author s response and paper revision without changing their scores, and one reviewer engaged in considerable discussion resulting in a score increase to a weak accept.  No reviewer gave the paper a strong endorsement.  I do appreciate the large effort that the authors put into revising their paper and addressing reviewers concerns.  However, major post submission revision puts an inappropriate burden on reviewers.  In any case, there is not strong support for this paper even from the one heavily engaged reviewer.
The authors presents a method for adapting models to new tasks in a zero shot manner using learned meta mappings.  The reviewers largely agreed that this is an interesting and creative research direction.  However, there was also agreement that the writing was unclear in many sections, that the appropriate metalearning baselines were not compared to, and that the power of the method was unclear due to overly simplistic domains.  While the baseline issue was mostly cleared up in rebuttal and discussion, the other issues remain.  Thus, I recommend rejection at this time.
After the rebuttal phase, all scores are borderline (6) or negative (4). Among the most confident reviewers (confidence 5), one gives 6 and one gives 4. The reviewer with confidence 4 gives overall score 6 but states they cannot support the paper. There were several concerns about the novelty of the task and method, the challenge of the experimental settings, missing comparisons to recent prior work in the original paper, etc. While the reviewers see merit, the paper can benefit from another revision before being accepted, including to better position the novelty of its method and perhaps reduce claims of novelty of the task. 
The paper presents a construction for deep learning on point clouds that evolve over time. The key characteristics of the data are irregular sampling in the spatial domain and regular sampling in the temporal domain. The presented construction addresses both these aspects of the data. The review by R3 was negative but was addressed by the authors and R3 did not participate in the discussion. The AC supports acceptance.
The paper proposed a method for training multiple transformers with tied parameters and enabling dynamic choice of the number of encoder and decoder layers. The method is evaluated in neural machine translation and shown to reduce decoding costs without compromising translation quality. The reviewers generally agreed that the proposed method is interesting, but raised issues regarding the significance of the claimed benefits and the quality of overall presentation of the paper. Based on a consensus reached in a post rebuttal discussion with the reviewers, I am recommending rejecting this paper.
This paper studies how much overparameterization is required to achieve zero training error via gradient descent in one hidden layer neural nets. In particular the paper studies the effect of margin in data on the required amount of overparameterization. While the paper does not improve in the worse case in the presence of margin the paper shows that sometimes even logarithmic width is sufficient. The reviewers all seem to agree that this is a nice paper but had a few mostly technical concerns. These concerns were sufficiently addressed in the response. Based on my own reading I also find the paper to be interesting, well written with clever proofs. So I recommend acceptance. I would like to make a suggestion that the authors do clarify in the abstract intro that this improvement can not be achieved in the worst case as a shallow reading of the manuscript may cause some confusion (that logarithmic width suffices in general).
I recommend acceptance based on the positive reviews. The paper analyzes critical points for linear neural networks and shallow ReLU networks. Getting characterization of critical points for shallow ReLU networks is a great first step.
The paper was evaluated by 4 knowledgeable reviewers and got mixed scores. While most reviewers appreciated the new intuitive approach to meta RL. there were severe concerns about algorithmic choices and the evaluations that led to a poor score from some reviewers. These concerns are summarized below:   The motivation of experience relabeling for out of distribution samples is not clear (R2)   It is unclear why experience relabeling does not work for in distribution samples (R2, R4)   The reported performance is not a fair comparison as it is typically not known when a task is in distribution or out distribution, so we would either have to take always experience relabeling or never (or learn when do use which algorithm)   The paper falls short in terms of evaluations (R3, R4), in particular it remains unclear to me if MIER can, under realistic circumstances. It is suggested to use more established benchmarks such as Meta World to evaluate the performance of MIER.   For the given reasons, I recommend that the authors do these corrections and  go through another round of reviews at another conference. 
The paper is proposed a rejection based on majority reviews.
The authors argue that tighter relaxations for certified robustness suffer from a worse loss landscape and thus are outperformed by the much simpler and less tight IBP relaxation and come up with a new relaxation to overcome this problem.   After the rebuttal there still remain doubts about the reasoning regarding the loss landscape (even though I acknowledge that the authors have invested significant amount of work to support their hypothesis). Moreover, the differences to existing certified training methods is small or the proposed method performs worse while being significantly more expensive (in particular if one takes into account the results which are reported on the IBP Crown github page where the reported numbers are significantly lower than reported in the present paper) so that the benefit is unclear.  Thus the majority of the reviewers still suggests rejection and I agree with that even though I think that the paper has its merits and I encourage the authors to continue this line of work. For a next version, the authors should evaluate all the methods ideally with an exact verification method resp. use the best relaxation for all methods. Otherwise the differences can come just from the weaker relaxation but not from a difference in real robustness.  
This paper proposes the Mutual Information Gradient Estimator (MIGE) for estimating the gradient of the mutual information (MI), instead of calculating it directly. To build a tractable approximation to the gradient of MI, the authors make use of Stein s estimator followed by a random projection. The authors empirically evaluate the performance on representation learning tasks and show benefits over prior MI estimation methods. The reviewers agree that the problem is important and challenging, and that the proposed approach is novel and principled. While there were some concerns about the empirical evaluation, most of the issues were addressed during the discussion phase. I will hence recommend acceptance of this paper. We ask the authors to update the manuscript as discussed.
This paper presents an approach for interactive imitation learning while avoiding an adversarial optimization by using ensembles. The reviewers agreed that the contributions were significant and the results were compelling. Hence, the paper should be accepted.
All the reviews like the theoretical result presented in the paper which relates the gating mechanism of LSTMS (and GRUs) to time invariance / warping. The theoretical result is great and is used to propose a heuristic for setting biases when time invariance scales are known. The experiments are not mind boggling, but none of the reviewers seem to think that s a show stopper. 
The paper explores the usefulness of intermediate layers for linear probing, aiming at improving out of distribution transfer with significantly less cost than fine tuning. Two reviewers recommended borderline acceptance, while two others recommended borderline rejection as final rating. The main concerns raised by the reviewers were the limited novelty of the proposed method (e.g., compared to Elmo), unconvincing results in the natural and structure categories of VTAB, and lack of experiments to justify the claims, as well as the demonstration of the method in other tasks beyond image classification. The rebuttal has clarified several other questions. The AC really likes the simplicity of the approach, and also finds the problem of improving the efficiency of transfer learning very important. In addition, the paper is very well written and easy to follow, as acknowledged by all reviewers. However, the AC agrees with R2 and R3 that the paper, in its current form, does not pass the bar of ICLR, unfortunately. First, the novelty is limited, as pointed out by R1, R2, and R3. In addition to the related works mentioned in the reviews like Elmo, note that the idea of selecting intermediate features, concatenating them, and running a linear classifier for OOD transfer has also been explored in [Yunhui Guo et al, A broader study of cross domain few shot learning, ECCV 2020]. Second, while the approach has advantages in terms of efficiency, the accuracy drop (compared to fine tuning) for in domain tasks limits its applicability. Finally, even though the AC agrees with the authors this is not a requirement, a more comprehensive set of experiments on more tasks would make the paper stronger, especially given that the novelty is incremental. The authors are encouraged to improve the paper for another top conference.
One referee supports acceptance, whereas three referees lean towards rejection. All referees agree that the idea introduced in the paper is interesting but find that the motivation and evaluation of the proposed aggregation functions could be significantly strengthened. The rebuttal addresses R1 s concerns about novelty and unfair comparisons, R2 s concerns about computational efficiency of the methods, R3 s concerns about motivation of the proposed approach and some missing baselines, and R4 s concerns about motivation. However, the rebuttal does not address the reviewers  concerns related to improvements achieved by the proposed approach, statistical significance nor appropriate comparison with SOTA. I agree with the reviewers that the paper tries to address a relevant problem and proposes interesting ideas, which are worth exploring. However, after discussion, the referees agree that further work should be devoted to strengthen the contribution. I agree with their assessment and hence must reject. In particular, I would strongly recommend to follow their suggestions to either provide strong theoretical motivation to support the claims of the paper or work on a strengthened empirical evaluation, following OGB guidelines to report the std of the results and including a proper comparison with the state of the art. 
This paper investigates convolutional LSTMs with a multi grid structure. This idea in itself has very little innovation and the experimental results are not entirely convincing.
Although the problem studied in the paper is interesting, all the reviewers believe that the current draft has limited technical contributions. Moreover, there are serious issues with the writing and presentation of the work. Also the experiments are rather limited and their results are not significant. I strongly recommend the authors to take the reviewers  comments into account and improve different aspects of their work for future conferences.
This paper has been reviewed by three reviewers and received scores such as 3/3/6. The reviewers took into account the rebuttal in their final verdict. The major criticism concerned the somewhat ad hoc notion of interpretability, the analysis of vanishing/exploding gradients in  TPRU is experimental lacking theory. Finally,  all reviewers noted the paper is difficult to read and contains grammar issues etc. which does not help. On balance, we regret that this paper cannot be accepted to ICLR2020.  
This paper uses chemical reaction data as a means to help train molecule embeddings, by requiring embeddings to satisfy known reaction equations. The idea is nice and clear, and the paper includes strong empirical evaluation. All four reviewers agreed the paper could be accepted, with two of them raising their scores after a detailed author rebuttal and discussion, which included additional experiments.
The paper introduces a novel decoding algorithm that allows to dynamically integrate external knowledge with generative  LMs. The proposed technique is plug and play, it does not require re training or fine tuning LMs with knowledge based objectives.  The author report a series of experiments on several datasets and tasks showing improvements over competitive baselines and in some  cases above SOTA. The authors have addressed the reviewers  queries and added experimental results (additional baselines) as well as clarifications of their approach (e.g., Figure A1 in the Appendix). I think the topic  (e.g., constrained LM decoding) is of general interest to the ICLR community and the approach compelling.
The authors provide a theory for training feed forward spiking neural networks (SNNs) on input to output spike train mappings. They utilise for this heterogenous neurone and skip connections.  The resulting method is tested on DVS Gesture, N Caltech 101 and sequential MNIST. It achieved very good performance. The reviewers agreed that the results are interesting and significant.  In the initial reviews, the reviewers pointed out some doubts about the theory and clarity of writing. These doubts and objections were addressed in the revision and the reviewers were quite satisfied with that.  In conclusion, the manuscript presents interesting results for SNNs with a solid theory and very good experimental results. All reviewers vote for acceptance.
This paper proposes to create an explanation space to describe the relationships between input data and prototypes (and also between the prototypes themselves). It constructs such a space suing VAEs and conducts experiments to validate the effectiveness and interpretability of the method.  Strengths:   The proposed method is interesting and intuitive  Weakness:   Novelty of the idea is limited   Missing experiment comparison with some important previous work   Some claims are not well supported by the empirical results
The authors propose a graph residual flow model for molecular generation.  Conceptual novelty is limited since it is simple extension and there isn t much improvement over state of art.
This paper proposes a new method for zero shot policy transfer in RL. The authors propose learning the policy over a disentangled representation that is augmented with attention. Hence, the paper is a simple modification of an existing approach (DARLA). The reviewers agreed that the novelty of the proposed approach and the experimental evaluation are limited. For this reason I recommend rejection.
This paper proposes a “guided” evolution strategy method where the past surrogate gradients are used to construct a covariance matrix from which future perturbations are sampled. The bias variance tradeoff is analyzed and the method is applied to real world examples.  The method is not entirely new, and discussion of related work as well as comparison with them is missing. The main contribution is in the analysis and application to real world examples, and the paper should be rewritten focusing on these contributions, while discussing existing work on this topic thoroughly.  Due to these issue, I recommend to reject this paper. 
This is an interesting piece of work that provides solid evidence on the topic of bootstrapping in deep reinforcement learning.
The work focuses on detecting whether a certain data sample was used to train a deep network based conditional image synthesis model. The key idea is not to rely on just reconstruction error but normalizing it via a proposed difficulty score. The reviewers found the problem statement important and the paper easy to follow. However, a common concern in the discussion was the approach is specific to image to image translation problems. Many other minor questions were answered by the authors in the rebuttal. Upon discussion post rebuttal, the reviewers decided to maintain their score. AC and reviewers believe that the paper will benefit from better analysis and description of difficulty score and its correlation with reconstruction error. It would be ideal to see how these ideas are useful for a broader problem than image translation. Please refer to the reviews for final feedback and suggestions to strengthen the submission.
In this paper, the authors consider linear quadratic network games (also known as graphical games) and they discuss a number of conditions and procedures to learn the underlying graph of the game from observations of best response trajectories (or possibly infinite sets thereof) in the game.  The reviewers  initial assessment was overall negative, with two reviewers recommending rejection and one giving a borderline positive recommendation. The authors  rebuttal did not address the concerns of the reviewers recommending rejection, and the authors did not provide a revised paper for the reviewers to see how the authors would implement the suggested changes, so the overall negative assessment remained.  After my own reading of the paper, I concur with the majority view that the paper has several weaknesses that do not make it a good fit for ICLR (especially regarding the lack of precision in the theorems and the statement of the relevant assumptions), so I am recommending rejection.
All three reviewers agree that the research question—should pretrained embeddings be used in code understanding tasks—is a reasonable one. However, there were some early issues with the way in which the paper reported results (involving both metrics and baselines). After some discussion with the reviewers, it seems that the paper now presents a clear picture of the results, but that these results are not sufficiently strong to warrant acceptance.   I m wary to turn down a paper over what are basically negative results, but for results like this to be useful to the community, they d have to come from a very thorough experiment, and they d have to be accompanied by a frank and detailed discussion. Neither of the two more confident authors are convinced that this paper meets that bar.
Three out of four of the reviewers are leaning (weakly or strongly) towards rejecting this paper. Unfortunately, the authors only responded to the concerns of the most positive reviewer, making it difficult to disregard the concerns from the three more negative reviewers.  I also took a look at the paper myself, and share a number of the reviewers  concerns. First, the proposed method appears to be performing transductive inference for its predictions, while many baselines it compares with rely on inductive inference. Transductive inference is generally known to outperform inductive inference, therefore some of the improvements in accuracy can potentially be accounted to that. The authors did mention in their one author response that they generated results in the inductive setting and still saw an improvement, however the submission was not updated with details around that new experiment, making it hard to rely on it. Second, the paper is using a 224x224 resolution for images, while the original mini ImageNet benchmark (and the majority of baselines evaluated on it) assume a 84x84 resolution. Here too, using the former resolution is known to outperform the latter. Third, I too found the paper to lack clarity at a number of places in the writing.  I also notice that the final predictions is made following the averaging of features from two models (A and B, as in Eq. 5). This is a form of model ensembling, which generally is a principle know to help improve generalization. It seems appropriate to wonder whether the baselines are worse partly due to not relying on any ensembling at all.   Finally, I ve found a recent method from ICJAI 2021 (Cross Domain Few Shot Classification via Adversarial Task Augmentation) which appears to beat the proposed method in the cross domain setting for the majority of domains.   Given the above, and the lack of rebuttals to the reviewers with the most concerns, I m afraid I must recommend this paper be rejected at this time.
The authors introduce an RNN model, ProtoryNet, which uses trajectories of sentence protoypes to illuminate the semantics of text data.  Good points were brought up and addressed in discussion, which have improved the paper   including a helpful suggestion from Rev 3 to fine tune BERT sentence embeddings in ProtoryNet, which led to significant performance gains.  Unfortunately the tone of discussion with one reviewer slipped below the respectful standards to which we aspire, but rest assured that only substantive points on the paper were considered.  Reviewers were split but in discussion converged to leaning against acceptance, allowing the authors to reflect on, and incorporate new results carefully in an updated manuscript.
Non autoregressive modelling for text to speech (TTS) is an important and challenging problem. This paper proposes a deep VAE approach and show promising results. Both the reviewers and the authors have engaged in a constructive discussion on the merits and claims of the paper. This paper will not be the final VAE contribution to TTS but represents a significant enough contribution to the field to warrant publication. It is highly recommended that the authors take into account the reviewers  comments.
None of the reviewers championed the paper. Many weaknesses were shared across the reviewers: none of the individual contributions is individually novel, paper is not well written and the results do not show significant improvement over the prior state of the art. No rebuttal was provided. The AC agrees with the reviewers that the paper is not ready for publication at ILCR.
The paper provides an algorithm for the stochastic multi armed bandit (MAB) problem in the regime with fairness constraints. It continues a line of work that in high level define fairness as a requirement to ensure a minimum amount of exploration for every arm. The main concern I found in the reviews regards the definition of fairness in this paper. Although it follows the same high level narrative of previous works its exact definition and difference from previous papers is not convincingly motivated, and seems to be tailored to the proposed algorithm rather to a real world fairness constraint. This issue could have been mitigated by a novel or generalizable technique, or insightful experiments, but this does not seem to be the case given the reviewers comment about the limited novelty and basic experiments.
In this paper, a method to solve the segmentation problem by continuous optimization is proposed by using a soft differentiable warping function. The proposed method is theoretically sound, and interesting experiments such as the data analysis of covid19 are also presented. This is a good paper in terms of both theory and application.
The authors develop a novel strategy, Deep Partition Aggregation, to train models to be certifiably robust to data poisoning attacks based on flipping labels of a small subset of the training data or introducing poisoned input features. They improve upon existing certified defences against data poisoning and are the first to establish certified guarantees against general poisoning attacks.  Most reviewers were in support of acceptance. Reviewer concerns were raised in the rebuttal phase but were convincingly addressed in the rebuttal phase. One reviewer did raise concerns on the weakness of experimental results on CIFAR 10, but the fact that this method has established the first certified defence in the general poisoning setting and that the results are stronger on other datasets certainly warrant acceptance. I would encourage the authors to clarify this in the final version.  
This paper provides a very large scale study on the pretraining of image recognition models. Specifically, three scaling factors (model sizes, dataset sizes, and training time) are extensively investigated. One important phenomenon observed by this paper is that stronger upstream accuracy may not necessarily contribute to stronger performance on downstream tasks actually sometimes these two types of performance could even be at odds with each other  Overall, all the reviewers enjoy reading this paper and highly appreciate the empirical results presented in this paper. There were only a few concerns raised by the reviewers but most were well addressed during the discussion period. All reviewers reach a consensus on accepting this paper and believe this study is worthy to be heard by the community.
The reviewers consider the paper to promising, but raise issues with the increase in the complexity of the MDP caused by the authors  parameterization of the action space, and comparisons with earlier work (Pazis and Lagoudakis).   While the authors cite this work, and say that they that they needed to make changes to PL to make it work in their setting (in addition to adding the deep networks), they do not explicitly show comparisons in the paper to any other discretization schemes.   
This paper considers a generalized weighted least squares optimization method for the random Fourier feature model. Generalization error analysis is carried out under both the over parametrized and under parametrized schemes, and under both noise free and noisy scenarios.  Reviewers generally agree that this is a solid theoretical work that considerably extends previous work (Belkin et al. (2020), Xie et al. (2020)) in the literature and that the paper is ready for publication.  While some reviewers express reservation about the relevance of the work to ICLR, I believe this is a nice work that would be of interest to the machine learning community at large.
The paper suggests that whitening the data harms generalization and optimization performance when learning models of the form h(W x) i.e. those that are based on a linear projection of the inputs (which includes DNNs for instance). The main concern of the reviewers is that their theoretical developments were not that convincing; it seemed more along the lines of providing some specific anecdotes. But more broadly, the caveat is that their development seems very simple: their results in high dimensional settings (where d   dim(x) > number of samples n) is not that relevant since vanilla whitening is anyway fraught in such high dimensions, since the sample covariance matrix is not a good estimator in high dimensions anyway. And when n > d, their result focuses on linear models, where they say that whitening reduces information about the singular vector directions where the input data might mostly lie on. But if the data lies on a lower dimensional linear manifold, then whitening is again fraught: the covariance matrix is singular. The linear manifold assumption also seems very specific given the general title of the paper. Overall, the paper needs to narrow their focus on specific settings where whitening is harmful, but the specific settings above in and of themselves do not necessarily say anything other than to estimate the covariance matrix carefully before doing whitening. 
This work presents a method to combine EBMs and VAEs in two stages. First, the VAE model is learned; second, an EBM based correction term is learned via MLE. The methodology is novel and of interest to the ICLR community.
The paper considers the difference between GD and ADAM in terms of implicit bias. It considers a specific distribution and architecture where the two algorithms converge to different solutions while perfectly fitting the training data. The authors highlight the fact that this happens while adding regularization, which does not happen in the linear case. The reviewers found some of the insights and analysis interesting. However, they also had reservations about the impact of the results given that it is known that GD and ADAM have different implicit biases, and that the distribution appears specifically crafted towards  showing this effect for the architecture studied. In future versions, the authors are encouraged to better motivate the chosen distribution, use more standard neural architecture (e.g., standard relu), and provide more explanation about the role of regularization in their result.
Thanks for your detailed feedback to the reviewers, which clarified us a lot in many respects. This paper potentially discusses an interesting problem, and the concern raised by Review #2 was addressed in the revised paper. However,  given the  high competition at ICLR2020, this paper is unfortunately below the bar. We hope that the reviewers  comments are useful for improving the paper for potential future publication.  The 
The paper addresses a very important issue in GNN, the definition of a well defined pooling function for node aggregation. The proposed Graph Multiset Transformer, although not entirely new, seems to be useful in practice. Issues related to experimental results, as well as problems with presentation, have been solved by the authors s rebuttal, that presented solid experimental results and analysis. Concerns about the real expressivity of the proposed approach when compared to Weisfeiler Lehman graph isomorphism test do not affect the contribution delivered by the paper, that seems, at this point, significant. 
This paper investigates defense against backdoor attacks for models that have already been trained. It proposes, in particular, a min max formulation for backdoor defense, in which the inner maximum seeks a powerful trigger that leads to a high loss, while the outer minimum seeks to suppress the "adversarial loss", so as to unlearn the injected backdoor behaviors. To solve the minimax, the authors also propose a method, Implicit Backdoor Adversarial Unlearning (I BAU). In addition, the authors also provide theoretical analysis including the convergence bound and generalization bound. Extensive experiments demonstrate the effectiveness and efficiency of the proposed method.   The proposed method is interesting and the implementation is nice. Overall, there is a fundamental flaw in the formulation: if the trigger is not additive (where there are many such examples of poisoning attacks that are not additive) this approach should fail completely. Not having experiments that discuss such triggers that are not additive is a significant flaw in the presentation of the paper. Another flaw is that the trigger is assumed to be small norm. Unlike adversarial examples attacks (at test time), there is no reason for backdoor triggers to be of small norm. Given that the defense critically relies on these two flawed assumptions, and the extent of how the proposed algorithm is sensitive to these assumptions are not properly addressed in the experiments, this paper is on the border line.
While the reviewers generally appreciated the idea behind the method in the paper, there was considerable concern about the experimental evaluation, which did not provide a convincing demonstration that the method works in interesting and relevant problem settings, and did not compare adequately to alternative approach. As such, I believe this paper is not quite ready for publication in its current form.
The authors propose an approach to learn perturbation sets from data and go beyond the mathematically sound L_p adversarial perturbations towards more realistic real world perturbations. To measure the quality of the learned perturbation set the authors put forward two specific criteria and prove that an approach based on conditional variational autoencoders (cVAE) can satisfy these criteria. In particular, given access to paired data (instance and its perturbation), the authors train a cVAE which can then be used to generate novel perturbations similar to the ones observed during training. Leveraging this generative model the authors train models which are robust to such perturbations while improving the generalisation performance on clean data.  The studied problem is of high significance and the proposed solution is sufficiently novel. The reviewers agree that the paper presents a significant step in the right direction and will be of interest to the ICLR community. The authors addressed all major concerns raised by the reviewers. In my opinion, given the inherent tradeoff between the two terms in Assumption 1, and the approximation gap due to the design choices of the particular cVAE, I feel that a hard problem was reduced to an almost equally hard problem. Nevertheless, the principled approach coupled with promising empirical results are sufficient to recommend acceptance. I strongly advise the authors to incorporate the remaining reviewer feedback and try to tone down claims such as “certifiably robust” given the issues pointed out above.  
This work performs fast controllable and interpretable face completion, by proposing a progressive GAN with frequency oriented attention modules (FOAM).  The proposed FOAM encourages GANs to highlight more to finer details in the progressive training process. This paper is well written and is easy to understand. While reviewer #1 is overall positive about this work, the reviewer #2 and #141 rated weak reject with various concerns, including unconvincing experiments, very common framework, limited novelty, and the lack of ablation study. The authors provided response to the questions, but did not change the rating of the reviewers. Given the various concerns raised, the ACs agree that this paper can not be accepted at its current state.
Four knowledgable reviewers recommend accept. Good job!
This paper presents an algorithm for distributed optimization in that aims to be "Byzantine robust", in the sense that it learns successfully when some of the workers send arbitrary messages.  The goal in this work is to remain robust when each worker samples data from a different distribution.  While reviewers found the work interesting, issues about the theoretical development arose during the discussion period, and it appears that the paper cannot be accepted in its current form.  The most serious issue was with Proposition I, which appears to be incorrect.  In its putative proof, the authors write that each gradient is sampled at most $s$ times.  This naturally leads to the conclusion that, in Algorithm 2, when the Break statement is reached,  $g_{j_i}$ is not used to compute $\bar{g}_t$.  Given this interpretation of Algorithm 2, it seems that Proposition I cannot be true. For example, if $s 1$, once $t \geq n/2$, fairly often, gradients will be sampled that had previously been sampled. In such cases, would be zero, so that, on average, $\bar{g}_t$ would be biased toward zero in later rounds.  Their putative proof of Proposition I refers to a whole chapter of a statistics text. We couldn t find anything in that chapter that implies what they claim about Algorithm 2 (or that treats a sampling scheme like Algorithm 2 at all).  Throughout the paper, when the authors took expectations, it was not always clear what was random and what was fixed.  After some discussion, disagreement remained about how to interpret some of the assumptions.  This was true in particular about the assumption in the first displayed equation on page two.  
The paper studies the problem of OOD classification: the test data and training data distribution can have different spurious feature class dependencies.  The reviewers have stated that the proposed procedure is a natural choice, with simple implementation. Another positive point is that it could easily be incorporated in many off the shelf machine learning training algorithms.  Yet, the technical novelty was mentioned to be limited. The bilevel optimization point of view and the connection with min max optimization problems raised some concerns, as the vocabulary used could be misleading. It was also raised that the paper lacks theoretical supports: no formal analysis, most explanations are ad hoc, etc.
The paper proposes a new approach for weakly supervised learning, based on conditional normalizing flows. Reviewers generally found the paper to have an interesting, novel proposal with empirical promise. However, some concerns were raised: to name a few,  (1) _Clarity._ Several reviewers found portions of the technical content hard to follow, e.g., the description of constraints in Sec 4.  (2) _Scalability compared to data programming._ One reviewer was unsure of how the present approach compares in terms of inference time and/or accuracy to a two stage data programming approach.  (3) _Infeasibility of sampling from Equation 2._ One reviewer suggested the paper discuss and compare to a simpler baseline, which is to perform rejection sampling from the constraint set.  (4) _Suitability of point cloud problem._ One reviewer was unsure of whether the point cloud problem, considered as an experimental setting in this paper, is reflective of weakly supervised learning.  (5) _Practicality of knowing weak labeler error rates._ The paper assumes knowledge of the weak labeler error rates in constructing constraints. Some reviewers raised concerns on the practical viability of this assumption.  For point (2), the relevant reviewer was not convinced following the discussion. The suggestion is to treat LLF as a label model, which serves as input to a non MC predictor. The question then is what the predictive performance of this combined approach looks like, as opposed to the LLF s themselves.  For point (3), the response clarified that the number of constraints might make rejection sampling infeasible. This appears to be true, but it is suggested that the paper at a minimum discuss this, and ideally also clarify claims about the general purpose need for the proposed approach (since in some cases one might be able to do rejection sampling).  For point (4), the discussion was somewhat inconclusive. It is suggested that the authors explicitly discuss some of the points brought up in the response.  For point (5), while the assumption not wholly uncommon in the literature, it would be better for the authors to perform some sensitivity analysis against misspecification of the error rates.  Overall, the paper has some interesting ideas that are well worth exploring. The present execution appears to have some scope for improvement, with the reviews providing a range of suggestions of areas of the paper that could be made clearer or strengthened. The paper would be best served by incorporating these comments and undergoing a fresh review.
Authors apply dense nets and LSTM to model dependencies among labels and demonstrate new state of art performance on an X Ray dataset.  Pros:   Well written.   New improvement to state of art  Cons:   Novelties are not strong. One combination of existing approaches are used to achieve state of art on what is still a relatively new dataset. (All Reviewers)    Using LSTM to model dependencies would be affected by the selected order of the disease states. In this sense, LSTM seems like the wrong architecture to use to model dependencies among labels. This may be a drawback in comparison to other methods of modeling dependencies, but this is not thoroughly discussed or evaluated. (Reviewer 1 & 3)    There is a large body of work on multi task learning with shared information, which have not been evaluated for comparison. Because of this, the contribution of the LSTM to model dependencies between labels in comparison to other available approaches cannot be verified. (Reviewer 1 & 3)    Top AUC performance on this dataset does not carry much significance on its own, as the dataset is new (CVPR 2017), and few approaches have been tested against it.    Medical literature not cited to justify with evidence the discovered dependencies among disease states. (Reviewer 1)  
After much back and forth about prior work, 3 reviewers score this paper as an 8 and one scores it as a 3. Other reviewers have written to the 3 and told them they believe that their review is now too harsh, in light of clarifications w.r.t. related work. I tend to agree, though I must admit that I am not an expert on this topic.  Given that there is almost unanimous support for accepting and it s possible that the one hold out has not seen some of the extra information, I recommend acceptance.  Given the praise from the other three reviewers, I moreover recommend a spotlight.
The authors develop theoretical results showing that policy gradient methods converge to the globally optimal policy for a class of MDPs arising in econometrics. The authors show empirically that their methods perform on a standard benchmark.  The paper contains interesting theoretical results. However, the reviewers were concerned about some aspects: 1) The paper does not explain to a general ML audience the significance of the models considered in the paper   where do these arise in practical applications? Further, the experiments are also limited to a small MDP   while this may be a standard benchmark in econometrics, it would be good to study the algorithm s scaling properties to larger models as is standard practice in RL.  2) The implications of the assumptions made in the paper are not explained clearly, nor are the relative improvements of the authors  work relative to prior work. In particular, one reviewer was concerned that the assumptions could be trivially satisfied and the authors  rebuttal did not clarify this sufficiently.  Thus, I recommend rejection but am unsure since none of the reviewers nor I am an expert in this area.
The paper proposes to fine tune the belief states of a MDP, for later using the learned model for decision time planning, e.g. via search. The contribution is well presented, motivated and focused to a specific scenario, which is generally considered challenging in the literature. This scenario is exemplified by the cooperative card game Hanabi, which takes the role of the benchmarking setting for the empirical evaluation of the fine tuning procedure.  The major concern raised in the review and discussion phases are about the limited evaluation, which is centered around only Hanabi, as well as the magnitudes of the improvements over previous baselines. However, three knowledgeable reviewers agreed that since the setting has been historically challenging, the reported improvements are in fact significant and potentially inspiring future works in this direction.   The paper is accepted provided that the authors include and polish in the camera ready the additional experiments over the parameter sensitivities, the ablation tests and the discussions highlighted by the reviewers in the comments.
The paper presents an extension of FID for conditional generation settings. While it s an important problem to address, the reviewers were concerned about the novelty and advantage of the proposed method over the existing methods. The evaluation is reported on toy datasets, and the significance is limited.
This paper proposes a method for out of distribution modeling and evaluation in  the human motion prediction task. Paper was reviewed by four expert reviewers who identified the following pros and cons.  > Pros:   New benchmark for testing out of distribution performance [R1]   Compelling performance with respect to the baselines [R1,R4]   Paper is well written and easy to  follow  [R2]   Generative model in the context  of  out of distribution modeling of human motion is novel [R1,R2,R4]  > Cons:   Lack of support for interpretability claim  [R1]   Validity and usefulness of the metric [R1]   Lack of "effectiveness" of the proposed approach [R2,R4]   Technical contributions are not significant [R3,R4]   Experimental validation lacks comparisons to other state of the art in motion prediction  methods [R3]    Lack of evaluation on additional datasets and for the main task [R4]  Authors tried to address the comments in the rebuttal, but  largely unconvincingly to the  reviewers.  On balance, reviewers felt that negatives outweighed the positives and unanimously suggest rejection. AC concurs and sees no reason to overturn this consensus.  
Thank you for your submission to ICLR.  All the reviewers are in agreement that this paper presents a nice contribution to the field, highlighting a class of DEQ models that correspond to optimization problems.  This provides a nice perspective on what kinds of computations DEQ models may perform, and I think provides a valuable contribution to the field.  The comments provided by the authors in their responses were satisfactory, and all reviewers were ultimately in agreement  that the paper should be accepted.  One comment: the authors mention that monDEQ models are restricted by requiring that the activation be a prox function, but actually [Revay et al., 2021] (https://arxiv.org/abs/2010.01732) showed that any monotone Lipchitz <  1 function can be used there.  I believe this captures the settings the authors consider here, so it s not clear to me that the formulation indeed provides greater expressivity that monDEQs, and this point should be considered in the paper.  More broadly, however, it is true that the perspective of the monDEQ techniques are different, but I would try to be as precise in this point as possible.
The paper addresses the question of why warm starting could result in worse generalization ability than training from scratch.  The reviewers agree that increasing the circumstances in which warm starting could be applied is of interest, in particular to reduce training time and computational resources.  However, the reviewers were unanimous in their opinion that the paper is not suitable for publication at ICLR in its current form.  Concerns included that the analysis was not sufficiently focused and the experiments too small scale.  As the analysis component of the paper was considered to be limited, the experimental results were insufficient on the balance to push the paper to an acceptable state.
In this paper, the authors investigate a multi task RL actor critic technique, where a single actor is used while multiple critics are trained (one per task, where each task corresponds to a different reward function). Experiments on several environments demonstrate that this method works quite well in practice.  All reviewers found the proposed approach sensible and effective, in spite of its simplicity. The main concerns were:   Lack of novelty: although this is indeed not a particularly original idea, the specific instantiation in the actor critic setup is novel and well motivated   Some confusing / unconvincing experimental results: after receiving this feedback, the authors were able to upload a new revision that addressed the main concerns   Focusing on the "multi style" aspect when this is essentially a multi task algorithm: although I agree that framing it as a specific case of multi task learning would make sense and would probably make more appealing to multi task RL researchers, I do not consider this to be a major issue  In spite of being a relatively straightforward paper, I believe it is good to have strong empirical evaluation of such basic techniques disseminated to the research community, and I thus recommend acceptance, in accordance with reviewers  recommendations after the discussion period.
The paper studies the effectiveness of few shot learning techniques in settings where the training labels are imbalanced. While addressing an interesting practical problem, reviewers raised concerns about the paper s technical depth, insufficient distinction to existing techniques for coping with label imbalance, and limited qualitative conclusions from the results. The authors incorporated some of these comments in their revision, but a more comprehensive update on the latter two points appears appropriate.
The paper proposes to study "implicit competitive regularization", a phenomenon borne of taking a more nuanced game theoretic perspective on GAN training, wherein the two competing networks are "model[ed] ... as agents acting with limited information and in awareness of their opponent". The meaning of this is developed through a series of examples using simpler games and didactic experiments on actual GANs. An adversary aware variant employing a Taylor approximation to the loss.   Reviewer assessment amounted to 3 relatively light reviews, two of which reported little background in the area, and one more in depth review, which happened to also be the most critical. R1, R2, R3 all felt the contribution was interesting and valuable. R1 felt the contribution of the paper may be on the light side given the original competitive gradient descent paper, on which this manuscript leans heavily, included GAN training (the authors disagreed); they also felt the paper would be stronger with additional datasets in the empirical evaluation (this was not addressed). R2 felt the work suffered for lack of evidence of consistency via repeated experiments, which the authors explained was due to the resource intensity of the experiments.   R5 raised that Inception scores for both the method and being noticeably worse than those reported in the literature, a concern that was resolved in an update and seemed to center on the software implementation of the metric. R5 had several technical concerns, but was generally unhappy with the presentation and finishedness of the manuscript, in particular the degree to which details are deferred to the CGD paper. (The authors maintain that CGD is but one instantiation of a more general framework, but given that the empirical section of the paper relies on this instantiation I would concur that it is under treated.)  Minor updates were made to the paper, but R5 remains unconvinced (other reviewers did not revisit their reviews at all). In particular: experiments seem promising but not final (repeatability is a concern), the single paragraph "intuitive explanation" and cartoon offered in Figure 3 were viewed as insufficiently rigorous. A great deal of the paper is spent on simple cases, but not much is said about ICR specifically in those cases.   This appears to have the makings of an important contribution, but I concur with R5 that it is not quite ready for mass consumption. As is, the narrative is locally consistent but quite difficult to follow section after section. It should also be noted that ICLR as a venue has a community that is not as steeped in the game theory literature as the authors clearly are, and the assumed technical background is quite substantial here. For a game theory novice, it is difficult to tell which turns of phrase refer to concepts from game theory and which may be more informally introduced herein. I believe the paper requires redrafting for greater clarity with a more rigorous theoretical and/or empirical characterization of ICR, perhaps involving small scale experiments which clearly demonstrates the effect. I also believe the authors have done themselves a disservice by not availing themselves of 10 pages rather than 8.  I recommend rejection at this time, but hope that the authors view this feedback as valuable and continue to improve their manuscript, as I (and the reviewers) believe this line of work has the potential to be quite impactful.
The paper considers the problem of controlling the dynamics of a networked dynamical system, under partial observations, considering a reduced order system from coarse data, and providing approximation bounds and an empirical evaluation.  Reviewers agree it is a borderline paper.  Technical results are nontrivial, and it introduces new questions, but the main contribution is rather narrow and it could be better written.
PROS: 1. Good results on CLEVER datasets 2. Writing is clear 3. The MAC unit is novel and interesting. 4. Ablation experiments are helpful  CONS: The authors overstate the degree to which they are doing "sound" and "transparent" reasoning.  In particular statements such as "Most neural networks are essentially very large correlation engines that will hone in on any statistical, potentially spurious pattern that allows them to model the observed data more accurately. In contrast, we seek to create a model structure that requires combining sound inference steps to solve a problem instance." I think are not supported.  As far as I can tell, the authors  do not show that the steps of these solutions are really doing inference in any sound way  I also found the interpretability section to be a bit unconvincing.  The reviewers and I discussed this and there was some attempt to assess what the operations were actually doing but it is not clear how the language and the image attention are linked.  I wonder whether the learned control activations are abstract and re used across problems the way that the accompanying functional solution s primitives are.  Have you looked at how similar the controls are across problems which are identical except for a different choice of attributes?  To me, one of the hallmarks of a truly "compositional" solution is one in which the pieces are re used across problems, not just that there is some sequence of explicit control activations used to solve each individual problem.
This paper proposes a method that uses conditional moment restriction methods to estimate causal parameters in non parametric instrumental variable settings.  This is done by converting to an unconditional moment restriction setting common in the econometrics causal inference literature.  The paper was reviewed quite favorably by reviewers, and the authors updated the manuscript to address specific issues raised by reviewers.
All reviewers agree that the paper is not quite ready for publication. 
The paper presents an unsupervised visual abstraction model, used for reinforcement learning tasks. It is trained through intrinsic rewards, generated from temporal differences of inputs. This is similar to "learning to control pixels". The method is tested in DM Lab (3D environment, 2D navigation tasks) and Atari (Montezuma s Revenge).  The paper is at times hard to follow, and it seems the improvements accompanying the rebuttals did not convince reviewers to change their notes significantly. The experiments do not contain enough comparisons to other models, baselines, nor ablations, to sustain the claims.  In its current form, this is not acceptable for publication at ICLR.
This is a mostly theoretical paper concerning online and stochastic optimization for convex loss functions that are not Lipschitz continuous. The authors propose a method for replacing the Lipschitz continuity condition with a more general Riemann Lipschitz continuity condition, under which they are able to provide regret bounds for the online mirror descent algorithm, as well as extending to the stochastic setting. They follow up by evaluating their algorithm on Poisson inverse problems.   The reviewers all agree that this is a well written paper that makes a clear contribution. To the best of our knowledge, the theory and derivations are correct, and the authors were highly responsive to reviewers’ (minor) comments. I’m therefore happy to recommend acceptance.
The paper proposes a mechanism for A* planning with learned policy and value functions. The experiments (restricted to the Sokoban domain) show that the runtime of guided search follows a heavy tailed distribution, suggesting that in many cases, the problem is either solved quickly or takes a long time. An abstract model is proposed to explain this distribution, and a number of mechanisms are proposed to overcome its challenges.  The reviewers thought the paper had some interesting ideas but found the experimental section to be especially weak. While the paper starts out with quite general claims, the experiments only consider a single domain. Also, key details about the experiments were missing. Finally, the writing feels rushed   the original submission had many typos and lacked proofs for two theorems.   I agree with these objections and recommend rejection. Please revise the paper following the reviews and resubmit to a different deadline.
The article studies the behaviour of binary and full precision ReLU networks towards explaining differences in performance and suggests a random bias initialisation strategy. The reviewers agree that, while closing the gap between binary networks and full precision networks is an interesting problem, the article cannot be accepted in its current form. They point out that more extensive theoretical analysis and experiments would be important, as well as improving the writing. The authors did not provide a rebuttal nor a revision. 
This paper is consistently supported by all three reviewers and thus an accept is recommended.
Borderline decision.  The idea is nice, but the theory is not completely convincing.  That makes the results in this paper not be significant enough.
The paper proposes a new objective function called ICE for metric learning.  There was a substantial discussion with the authors about this paper. The two reviewers most experienced in the field found the novelty compared to the vast existing literature lacking, and remained unconvinced after the discussion. Some reviewers also found the technical presentation and interpretations to need improvement, and this was partially addressed by a new revision.  Based on this discussion, I recommend a rejection at this time, but encourage the authors to incorporate the feedback and in particular place the work in context more fully, and resubmit to another venue.
This paper proposes a method for neural architecture search (NAS) based on adversarial methods. It uses a discriminator trained to distinguish between random vs. good architectures, letting the discriminator s scores serve as a reward signal for an autoregressive generator. I agree with AR1: this is a nice and clever idea. Reviewers generally agreed that the method was interesting, e.g. it s quite flexible in that it s able to incorporate constraints, and that the evaluation is rather extensive and shows that the method performs well across the board. Many minor criticisms were raised and addressed well by the authors in their responses and manuscript updates.  The major criticism shared by most reviewers was the high methodological complexity of the proposed approach, and the proportionally small gains shown over much simpler baselines. This criticism remained despite the authors  responses. The method is indeed complex: the same method without any adversarial component already performs well, and many important details of the model are relegated to Appendix A.2. (I would recommend, for example, moving Fig. 2 to the main text if at all possible. Also, the Appendix can/should be included in the main PDF for ICLR, rather than in supplementary material, as AR1 mentions.) It was not clear to reviewers that the adversarial component of the approach has a significant benefit. The authors respond by pointing to Table 7 showing that the discriminator reduces the number of queries and points out that in reality these queries correspond to expensive evaluations. If this is a major selling point of the method (it sounds like it could be), it should be highlighted and analyzed far more   at least moved to the main text rather than an Appendix   ideally with a real world evaluation showing a practical large improvement in overall wall clock time, rather than a benchmark where these evaluations are free. Perhaps the exclusive reliance on these benchmarks, though undoubtedly useful for quick experimentation, in the end holds back the paper and prevents the method s benefits from becoming apparent to the readers.  As a minor point (also raised by AR1), the paper is formatted incorrectly for ICLR: the font color is off, and more importantly the PDF is unsearchable (text cannot be selected, ctrl F does not work), which makes it very difficult to quickly reference and review. Please try not to stray from the conference provided style file for future submissions.  I appreciate the cleverness of the method, the extent of the evaluation, and the thorough responses to the reviews. However, unfortunately with the current presentation, it is too difficult to discern the benefit of the proposed approach from the manuscript. The approach is nonetheless intuitively appealing and seems quite promising, and I hope the authors will take the reviewers  good feedback into account and resubmit the paper in the future.
The paper proposes hierarchical Bayesian optimization (HiBO) for learning control policies from a small number of environment interaction and applies it to the postural control of a humanoid. Both reviewers raised issues with the clarity of presentation, as well as contribution and overall fit to this venue. The authors’ response helped to clarify these issues only marginally. Therefore, primarily due to lack of clarity, I recommend rejecting this paper, but encourage the authors to improve the presentation as per the reviewers’ suggestions and resubmitting.
The paper proposes a stochastic network, named Ensebmle in One (EIO), to increase adversarial robustness. EIO replaces the layers in a given architecture by so called random gated blocks (RGBs) in which a random gate switches between multiple copies of the original layers. By sampling from the random gates different subnetworks can be sampled which can be arranged to form an ensemble. During training non robust feature distillation (as proposed in previous work) between models is applied. For inference in the experiments a single subnetwork is sampled, and the robustness of that subnetwork is compared against several ensemble methods and adversarial training.    One reviewer was worried about model capacity and recommended to perform experiments on image net to demonstrate scalability to large datasets. In turn, authors added experiments on CIFAR 100 during rebuttal period. Another critique was that the model does not show a significant advantage over vanilla adversarial training (AT), which can easily tuned with different perturbation strengths and only takes half of the training time. While other ensemble techniques, like DVERGE, can be combined with AT to improve their robustness, combing EIO with AT does not lead to improvements as shown by experiments performed during the rebuttal period. Two reviewers stated that adding a theoretical analysis will improve the paper. Another suggestion for improving the paper was to add a comparison to stochastic path networks, which is related work, and to investigate model performance when results from several sub networks are aggregated.   Overall, the paper can not be accepted in its current state, but I would recommend the authors to continue the direction of work and to incorporate reviewers suggestions in a future version of the manuscript.
The reviewers are rather critical about the paper and the authors did not take a part in the discussion phase. Let me also add that the paper ignores a vast number of papers dealing with a similar problem. The column generation algorithm is a core of LPBooting also used for rule learning ("Rule Learning with Monotonicity Constraints", "The Linear Programming Set Covering Machine"). There are many other papers also using linear relaxation of integer programming to build rule models. Logical analysis of Data is also a well known method being close to such approaches. There is also a plenty other rule learning systems that should be compared in the experimental study such as Ripper, Slipper, MLRules, or Ender (to mention only a few of them).
This paper investigates a promising direction on the important topic of interpretability; the reviewers find a variety of issues with the work, and I urge the authors to refine and extend their investigations.
This paper suggests an architecture with a deterministic initialization which has only 0/1 values. The reviewers were mostly (marginally) negative, mainly because of the low novelty and significance of this work.   Specifically, the main novelty issues were: 1) Improving convergence speed and removing BatchNorm: was already done, in a quite similar manner, and it achieves better or similar results. (Fixup , ReZero: https://arxiv.org/abs/1901.09321, https://arxiv.org/abs/2003.04887, and few others as well) 2) Initializing a network with deterministic initialization: was also done (ConstNet, https://arxiv.org/abs/2007.01038). I think the main difference from the previous work is the additional Hadamard connections, which help break the symmetry. However, it is unclear what is the benefit of this modification, as the previous work could train without it (albeit on CIFAR).  Specifically, the main significance issues were: 1) Reducing standard deviation: The authors  response confirmed there is no statistically significant benefit (p ~ 0.1) for variance reduction when comparing with Kaiming initialization for ImageNet. 2) General network performance: The results do not seem better than the baseline (Xavier init is not a proper baseline in a network with ReLUs). 3) Sparsity claims: The network appears to be losing accuracy even with 20% sparsity, which isn t even useful for efficiency. For comparison, the lottery ticket hypothesis showed you can get to 90% sparsity and get better results. So, this is a nice observation, but not a major contribution.  Therefore, I recommend the authors to better distinguish themselves from previous works (What are the changes? Why are these important?), and improve their empirical results so they highlight the usefulness of the suggested method (e.g., improve the SOTA in some benchmark).
The paper studies the problem of character generation using reinforcement learning for generation/parsing. All the reviewers recommended reject due to insufficient experimental investigation to support the ideas. The authors did not provide a rebuttal. Hence, the reviewers  opinion still remains the same. AC agrees with the reviewers and believes that the paper is not yet ready for publication.
This paper presents an algorithm for combining various feature types when training recurrent networks. The features are handled by modifying the update rules and cell states based on the features  type   dense, sparse, static, w/ decay, etc.  Strengths   The model handles each feature according to its type and handles cell state and transitions appropriately.    Extends earlier work to handle more feature types, like sparse features.  Weaknesses   Limited novelty. Models similar to various aspects of the proposed system have been presented in prior works. For example: TLSTM, which the authors use as a baseline. Although some components are novel, like the treatment of sparse features, contributions, in my opinion, are not sufficient to be accepted at ICLR.   Presentation: Confusing and not enough information for reproducing results; multiple reviewers raised concerns about presentation of the feature types and experimental results. There were suggestions to improve, which the authors did consider during revision, but some concerns still remain.  In the end, the reviewers agreed about the limited novelty of this work, given existing literature. The recommendation, therefore, is to reject the paper.  
For this paper initially the reviews were 6,8,5,5. All the reviewers have provided constructive and substantial feedback. The authors have incorporated changes to address some of these comments and some of the comments could not be addressed. The main criticism of the reviewers have been that the Reviewer tkQp finds two clear limitations in the paper, Reviewer 3o7Z finds that the proposed idea is similar to the parameter space adversarial attacks and Reviewer sCeW questions the generalisability of the method to other tasks. After the rebuttal the reviewers have reached the consensus that the paper may not be above the acceptance threshold (final scores: 6,6,5,5). Following the reviewers  recommendations, the meta reviewer recommends rejection.
This paper explores the use of partial rejection control (PRC) for improved SMC based variational bounds. While an unbiased SMC variant with PRC has been previously introduced by Kudlicka et al. (2020), this work introduces innovations that can help apply such ideas to variational inference. These bounds result in improvements in empirical performance.   This paper was heavily discussed, with significant engagement by both the authors and the reviewers. Most reviewers recommended acceptance of this paper, with one reviewer (R4) recommending against acceptance. R4 s central concerns regard the novelty of the proposed approach and its positioning relative to the existing SMC literature. The authors argued vigorously in the comments that this paper should be judged as a contribution to the VI literature and not the SMC literature.  Unfortunately, I will recommend that this paper is rejected. It is my opinion that R4 s concerns were not fully addressed.  On the one hand, I agree with the authors that there is significant value to be had in exploring variants of SMC for VI. Indeed, some prior art, like FIVO and IWAE, contributed little to the Monte Carlo literature. I believe that these were good contributions.  On the other hand, I am concerned that the current draft does not clearly circumscribe its contributions. I read the sections that disuss the works of Schmon et al. (2019) and Kudlicka et al. (2020), and the writing did not leave me with a clear enough sense of the differences. I also read the abstract and introduction of the paper. The introduction of the paper positions this work clearly within the VI literature, but does not clearly discuss prior SMC art, e.g., it does not cite Kudlicka et al. (2020). Despite citing rejection control for SMC, the writing of the abstract and introduction left me with the impression that this work was the first to introduce *unbiased, partial* rejection control for SMC. I believe that impressions matter and that the machine learning community should be generous to adjacent communities when assigning credit.  I realize that my decision is a matter of taste. I also want to say that I am confident that the authors have a clear sense of where their contribution sits, and I suspect that it is a valuable contribution. However, I cannot recommend the draft in its current form. If this is a contribution to the VI literature, as the authors argue, then the authors should not hesitate to give full credit to prior SMC art. My reading of the current draft still leaves me confused about which aspects of the SMC estimator are actual contributions.
This paper proposes a novel approach for pruning deep neural networks using non parametric statistical tests to detect 3 way interactions among two nodes and the output. While the reviewers agree that this is a neat idea, the paper has been limited in terms of experimental validation. The authors provided further experimental results during the discussion period and the reviewers agree that the paper is now acceptable for publication at ICLR 2020. 
This paper tackles the problem of confidence on neural network predictions for out of distribution (OOD) samples. The authors propose an approach for training neural networks such that the OOD prediction is uniform across classes. The approach requires samples from in  and out of distribution and relies on a mixture of Gaussians for modelling the distributions, allowing to obtain theoretical guarantees on detecting OOD samples (unlike existing techniques).  The main concerns of the reviewers have been addressed during the rebuttal. If this approach does not outperform state of the art in practice, providing such theoretical guarantees is an important contribution.  All reviewers agree that this paper should be accepted. I therefore recommend acceptance.
Summary: This paper provides an approach for causal inference in observational survival dataset in which the outcome is of time to event type with right censored samples.  To this end, the paper adapts the balanced representation learning approach proposed in (Shalit et al, 2017) to the context of survival analysis. The paper adapts an approach that uses flexible models to learn nuisance models, common in machine learning.  The authors validated their approach via simulation study and a set of application datasets: a EHR based cohort study of cardio vascular health, an RCT dataset of HIV patients, and a semi synthetic dataset.  The main concerns of reviewers were due to perceived lack of originality relative to the original proposal in (Shalit et al, 2017) 
This work studies the relation between graph heterophily and the robustness of GNNs and theoretically shows that effective structural attacks on GNNs for homophilous graphs lead to increased heterophily level, while for heterophils graphs they alter the homophily level contingent on node degrees under some specific assumptions.  Overall, the findings in the paper are interesting and can be useful for other researchers trying to improve GNNs  robustness on homophilic and heterophilic datasets. After the discussion and rebuttal, the main concerns are:    while the paper has shown some interesting observations, no new methodology was proposed based on these findings.    The authors have attempted to relax assumptions and justified their setup on experiments, however the explanations are still limited. For example, Theorem 1 does not allow attention mechanism, different choices of aggregator, skip connection, and more GNN layers.
To summarize the pros and cons:  Pro: * Interesting application * Impressive results on a difficult task * Nice discussion of results and informative examples * Clear presentation, easy to read.  Con: * The method appears to be highly specialized to the four bug types. It is not clear how generalizable it will be to more complex bugs, and to the real application scenarios where we are dealing with open world classification and there is not fixed set of possible bugs.  There were additional reviewer complaints that comparison to the simple seq to seq baseline may not be fair, but I believe that these have been addressed appropriately by the author s response noting that all other reasonable baselines require test cases, which is an extra data requirement that is not available in many real world applications of interest.  This paper is somewhat on the borderline, and given the competitive nature of a top conference like ICLR I feel that it does not quite make the cut. It is definitely a good candidate for presentation at the workshop however.
The authors present a hierarchical factorization of the Poisson matrix and explain why sparcity in the encoder is important for interpretability. The reviewers appreciated the contribution of the paper and highlighted the advantage of such an approach for users. The authors have improved their initial version by adding more detail on inferences and experiments.  The decision is to accept the paper.
This paper considers the problem of active learning (AL) with data drawn from multiple domains. This framing motivates integrating work on domain shift detection and adaptation into standard AL approaches.   The reviewers agreed that the work reports a robust set of experiments, which is a clear strength. However, they also raised key concerns, namely: (i) The heterogeneous setting considered is not particularly well motivated; (ii) The technical contributions of this work are limited. The latter would not be a major issue if the empirical evaluation addressed a clear open question (since this would constitute a useful contribution in and of itself), but the empirical contribution is somewhat limited given the unique setting considered and the relevant prior work (some of which seems to have been overlooked by the authors).
The reviewers are all weakly positive. The author response clarified important aspects of the paper. The new human evaluation was critical. However, the human evaluation result presentation is flawed: presenting Likert scores as means does not reflect them well. The authors should use something similar to a Gantt chart to fully reflect the distribution across Likert categories. Another detail in the human evaluation that are troubling: it does not reflect interaction with the system, but judgements through observation. Therefore, the human evaluation does not reflect the ability of the learned dialogue system to interact with users. Overall, the paper makes a nice, original contribution, but despite author improvement there are evaluation flaws (even if they are common in papers using these benchmarks).
The authors provide an extension to GCNs of Kipf and Welling in order to incorporate information about higher order neighborhoods. The extension is well motivated (and  though I agree that it is not trivial modification of the K&W approach to the second order,  thanks to the authors for the clarification).  The improvements are relatively moderate.  Pros:   The approach is well motivated   The paper is clearly written Cons:   The originality and impact (as well as motivation) are questioned by the reviewers 
The authors discuss how to predict generalization gaps. Reviews are mixed, putting the submission in the lower half of this year s submissions. I also would have liked to see a comparison with other divergence metrics, for example, L1, MMD, H distance, discrepancy distance, and learned representations (e.g., BERT, Laser, etc., for language). Without this, the empirical evaluation of FD is a bit weak. Also, the obvious next step would be trying to minimize FD in the context of domain adaptation, and the question is if this shouldn t already be part of your paper? Suggestions: The Amazon reviews are time stamped, enabling you to run experiments with drift over time. See [0] for an example.   [0] https://www.aclweb.org/anthology/W18 6210/
This paper provides convergence results for Non linear TD under lazy training.  This paper tackles the important and challenging task of improving our theoretical understanding of deep RL. We have lots of empirical evidence Q learning and TD can work with NNs, and even empirical work that attempts to characterize when we should expect it to fail. Such empirical work is always limited and we need theory to supplement our empirical knowledge. This paper attempts to extend recent theoretical work on the convergence of supervised training of NN to the policy evaluation setting with TD.  The main issue revolves around the presentation of the work. The reviewers found the paper difficult to read (ok for theory work). But, the paper did not clearly discuss and characterize the significance of the work: how limited is the lazy training regime, when would it be useful? Now that we have this result, do we have any more insights for algorithm design (improving nonlinear TD), or comments about when we expect NN policy evaluation to work?   This all reads like: the paper needs a better intro and discussion of the implications and limitations of the results, and indeed this is what the reviewers were looking for. Unfortunately the author response and paper submitted were lacking in this respect. Even the strongest advocates of the work found it severely lacking explanation and discussion.  They felt that the paper could be accepted, but only after extensive revision.  The direction of the work is important. The work is novel, and not a small undertaking. However, to be published the authors should spend more time explaining the framework, the results, and the limitations to the reader.  
This paper proposes an approach to learn the causal structure underlying a dataset with acyclicity and other structure constraints, and then used the inferred structure to compute partial causal effects. The authors show that, on simulated data, the proposed method outperforms others in the literature. The manuscript also contains an analysis of real world data that describes the causal effects of the lockdown of cities in the Hubei province (China) to reduce the spread of COVID 19.   Overall, the reviewers think that this is a well structured and written paper. From a novelty viewpoint, the main contribution consists in formalising the causal contribution of mediators, as the method for computing the causal structure is based on a small modification to previous literature.  The main concern raised by the reviewers were on the experimental evaluation. Some of these concerns were addressed by the authors during rebuttal, whilst some on the number of nodes remained. We encourage the authors to consider these concerns in the final version of the manuscript.
I thank the authors and reviewers for the discussions. Reviewers raised major concerns regarding the significance of the results and experiments. Given all, I think the paper needs more work before being accepted. I encourage authors to address comments raised by the reviewers to improve their paper.    AC
This paper proposes an input dependent dropout strategy, using variational inference to infer the rates.  While the idea is a fairly straightforward variant of recent probabilistic dropout methods, the paper demonstrates consistent improvements across several types of NN layers (dense, convolutional, and attention) in large scale experiments (e.g. ImageNet).  The reviewers unanimously agreed on accepting the paper.
This paper provides a global convergence guarantee for feedforward three layer networks trained with SGD in the MF regime. By introducing the novel concept of neuronal embedding of a random initialization procedure, SGD trajectories of large width networks  are shown to be well approximated by the MF limit, a continuous time infinite width limit (Theorem 3). Furthermore, under some additional assumptions the MF limit is shown to converge to the global optimum when the loss is convex (Theorem 8, case 1) and for a generic loss when $y y(x)$ is a deterministic function of input $x$ (Theorem 8, case 2). The global convergence guarantee presented in this paper is based on less restrictive assumptions compared with existing studies. All the reviewers rated this paper quite positively, with less confidence however, seemingly because of mathematical thickness of the proofs. Although the reviewers did not manage to check every detail of the proofs, they agreed that the reasoning seems mathematically sound as far as they can tell. The authors response adequately addressed minor concerns raised by the reviewers. I am thus glad to recommend acceptance of this paper.  Pros:   Introduces the idea of a neuronal embedding, which allows establishing relation between SGD on large width three layer networks and its MF limit in a quantitative way with a less restrictive setting.   Provides a global convergence guarantee under the iid initialization, in the sense that if the MF limit converges it attains the global optimum.   Shows that the global convergence guarantee does not require convexity of the loss when a deterministic function is to be learned.  In particular, the uniform approximation property, rather than the convexity of the loss, plays a crucial role in proving the  global convergence guarantee (it allows translation of the vanishing gradient in expectation at convergence into the almost sure vanishing gradient), which is a quite original contribution of this paper.
This paper proposes a combination of SVGD and SLGD and analyzes its non asymptotic properties based on gradient flow. This is an interesting direction to explore. Unfortunately, two major concerns have been raised regarding this paper:  1) the reviewers identified multiple technical flaws. Authors provided rebuttal and addressed some of the problems. But the reviewers think it requires significantly more improvement and clarification to fully address the issues. 2) the motivation of the combination of SVGD and SLGD, despite of being very interesting, is not very clearly motivated; by combining SVGD and SLGD, one get convergence rate for free from the SLGD part, but not much insight is shed on the SVGD part (meaning if the contribution of SLGD is zero, then the bound because vacuum). This could be misleading given that one of the claimed contribution is non asymptotic theory of  SVGD style algorithms" (rather than SLGD style..). We encourage the authors to addresses the technical questions and clarify the contribution and motivation of the paper in revision for future submissions.   
There was certainly some interest in this paper which investigates learning latent models of the environment for model based planning, particularly articulated by Reviewer3.  However, the bulk of reviewer remarks focused on negatives, such as:   The model based approach is disappointing compared to the model free approach.  The idea of learning a model based on the features from a model free agent seems novel but lacks significance in that the results are not very compelling.  I feel the paper overstates the results in saying that the learned forward model is usable in MCTS.   the paper in it’s current form is not written well and does not contain strong enough empirical results   
This paper makes progress on the open problem of text generation with GANs, by a sensible combination of novel approaches.   The method was described clearly, and is somewhat original.   The only problem is the hand engineering of the masking setup. 
This paper introduces a benchmark for experimental design algorithms for an important cellular biological question, causal discovery of effective genetic knock out interventions. It uses existing datasets.  The paper was discussed by the reviewers after the authors correctly pointed out that methodological machine learning novelty is not a necessary condition for accepting papers. Two reviewers increased their scores and all are slightly positive. The benchmark was seen as valuable, and one reviewer even commented they might use it in their own research. However, the paper is still on the borderline as this benchmark is only a first step. It has not been shown yet that machine learning insights can be produced with it, as the authors have not  actually used it for benchmarking yet. In other words, the benchmark can be considered a potentially excellent idea which has not been tested empirically yet.  This seems a highly promising research direction and the authors are strongly encouraged to continue to providing the benchmarks and releasing the method to the community so that others can help them in that.
The paper presents a new approach for distinguishing synonyms and antonyms via an extension of a parasiamese neural network, called "the repelling parasiamese network".  The strengths of the paper, as identified by reviewers, are a novel architecture for antonymy detection, a new dataset, and solid empirical results. However, there are major drawbacks identified by reviewers w5dj and hoTU. Specifically, there are clarity issues in writing, lack of a proper justification to the proposed architecture, insufficient details about the quality of datasets, insufficient contextualization in prior work. The scores are borderline, but unfortunately, the authors did not use the rebuttal opportunity to sufficiently address these questions/concerns raised by the reviewers. I thus recommend to reject the paper.
This paper introduces a new technique for discovering closed form functional forms (ordinary differential equations) that explain noisy observed trajectories x(t) where the "label" x (t)   f(x(t), t) is not observed, but without trying to approximate it. The method first tries to approximate a smoother trajector x^hat(t), then relies on a variational formulation using a loss function over functionals {C_j}_j, defined in terms of an orthonormal basis {g_1, …, g_S} of sampling functions such that the sum of squares of all the C_j approximates the theoretical distance between f(x) and the solution f*(x). These sampling functions are typically chosen to be a basis of sine functions. The method is evaluated on several canonical ODEs (growth model, glycolitic oscillator, Lorenz chaotic attractor) and compared to gaussian processes based differentiation, to spline based differentiation, regularised differentiation, and applied to model the temporal effect of chemotherapy on tumor volume.   Reviewers found that the paper was well motivated and easy to follow (EBvJ), well evaluated (EBvJ), offering new perspectives to symbolic regression (79Ft). Reviewer vaG3 had their concerns addressed. Reviewer ZddY had concerns about the running time (a misunderstanding that was clarified) and the lack of comparison to a simple baseline consisting in double optimisation over f and x^hat(0) using Neural ODEs (the authors have added a Neural ODE baseline but were in disagreement with ZddY and 79Ft about their limitations).  Reviewers engaged in a discussion with the authors, and the scores are 6, 6, 8, 8. I believe that the paper definitely meets the conference acceptance bar and would advocate for its inclusion as a spotlight in the conference.
While the reformulation of RNNs is not practical as it is missing sigmoids and tanhs that are common in LSTMs it does provide an interesting analysis of traditional RNNs and a technique that s novel for many in the ICLR community. 
The authors propose an algorithm for meta rl which reduces the problem to one of model identification. The main idea is to meta train a fast adapting model of the environment and a shared policy, both conditioned on task specific context variables. At meta testing, only the model is adapted using environment data, while the policy simply requires simulated experience. Finally, the authors show experimentally that this procedure better generalizes to out of distribution tasks than similar methods.  The reviewers agree that the paper has a few significant shortcomings. It s unclear how hyper parameters are selected in the experimental section; the algorithm does not allow for continual adaptation; all policy learning is done through data relabelled by the model.   Overall, the problem the paper addresses is very important, but we do not deem the paper publishable in its current form.
The authors present a new training procedure for generative models where the target and generated distributions are first mapped to a latent space and the divergence between then is minimised in this latent space. The authors achieve state of the art results on two datasets.  All reviewers agreed that the idea was vert interesting and has a lot of potential. Unfortunately, in the initial version of the paper the main section (section 3) was not very clear with confusing notation and statements. I thank the authors for taking this feedback positively and significantly revising the writeup. However, even after revising the writeup some of the ideas are still not clear. In particular, during discussions between the AC and reviewers it was pointed out that the training procedure is still not convincing. It was not clear whether the heuristic combination of the deterministic PGA parts of the objective (3) with the likelihood/VAE based terms (9) and (12,13), was conceptually very sound. Unfortunately, most of the initial discussions with the authors revolved around clarity and once we crossed the "clarity" barrier there wasn t enough time to discuss the other technical details of the paper. As a result, even though the paper seems interesting, the initial lack of clarity went against the paper.   In summary, based on the reviewer comments, I recommend that the paper cannot be accepted.  
In light of the reviews and the rebuttal, it seems that the paper needs to be rewritten to head off some of the confusions and criticisms that the reviewers have made. That said, the main argument seems to contradict some of the lower bounds recently established by Madry and colleagues, showing the existence of distributions where the sample complexity for finding robust classifiers is arbitrarily larger than that for finding low risk classifiers. I recommend the authors take a closer look at this apparent contradiction when revising.
Graph neural networks are an increasingly popular topic of research in machine learning, and this paper does a good job of studying the representational power of some newly proposed variants. The framing of the problem in terms of the WL test, and the proposal of the GIN architecture is a valuable contribution. Through the reviews and subsequent discussion, it looks like the issues surrounding Theorem 3 have been resolved, and therefore all of the reviewers now agree that this paper should be accepted. There may be some interesting followup work based on studying depth, as pointed out by reviewer 1, but this may not be an issue in GIN and is regardless a topic for future research.
**Overview** The paper provides a simplified offline RL algorithm based on BCQ. It analyzes the algorithms using a sampling based maximization of the Q function over a behavior policy for both Bellman targets and for policy execution   the EMaQ. Based on this, the paper then proposes to use more expressive autoregressive models (MADE) for learning the behavior policy from replay buffer data. The methods work well for harder tasks in the D4RL benchmark.   **Pro**    The method is relatively novel    Algorithms are simple modifications of existing ones   Empirical results are strong, matching or exceeding BEAR on D4RL while at the same time matching SAC for online learning   Work for both and offline   ablation study on the choice of generative model for μ(a|s)  **Con**   The current form of the complexity measure is somewhat not practical.   Theoretical results are not strong enough   Algorithmic contributions appear incremental  **Recommendation** The paper is on the borderline. It contributes simple and nice algorithmic ideas and these ideas work well empirically. These results demonstrate that a good choice of the behavior policy generative model is important for some tasks. At the same time, the reviewers are concerned about the theoretical parts, e.g., issues relates new complexity measure. Overall, the meta reviewer believes that the paper might not be in a status ready for publication.
The paper extends the earlier work on Prototypical networks to semi supervised setting. Reviewers largely agree that the paper is well written. There are some concerns on the incremental nature of the paper wrt to the novelty aspect but in the light of reported empirical results which show clear improvement over earlier work and given the importance of the topic, I recommend acceptance. 
This paper explores the brain s activity in response to language, specifically targeting the signatures of syntax in the brain.  The authors specifically investigate the signatures of specific syntactic elements against the "typical" effort based syntax measures from some previous work.    The title and abstract of the paper are clear and compelling, but the text of the paper muddies the message and this was expressed in the reviews.  There may be some debate in the literature as to if syntax and semantics are dissociable, and to what degree we can actually measure syntax in the brain, but I (and your reviewers) have trouble believing that any one actually thinks there is *no* syntax representations in the brain.  Certainly this is not a claim made by either the Federenko or Pylkkanen papers the authors cite. Federenko says "lexico semantic and syntactic processing are deeply inter connected and perhaps not separable" but doesn t claim that the brain doesn t "do" syntax. Pylkkanen says "Syntax in the brain is necessary to explain the fact that humans are exquisitely skilled at judging syntactic well formedness, even for sentences that have no coherent meaning."  I suggest this paper either rephrase the arguments, more clearly articulate the issues they wish to address, or find another venue where the reviewers might be more read to debate *if* syntax is encoded in the brain.  That seems outside of the scope of ICLR.
The paper presents a reinforcement learning technique for problems with continuous actions. The proposed approach consists in learning  a discretization of continuous action spaces from human demonstrations. This discretization returns a fixed number of actions for each input state. By discretizing the action space, any discrete action deep RL technique can be readily applied to the continuous control problem. Experiments reported in the paper show that the proposed approach outperforms several RL baselines such as SAC.  The key criticism from the reviewers relates to the incremental nature of this paper s contribution. While the precise equation proposed by in this paper for learning discrete actions from demonstrations may be novel, there have been several very similar techniques in the literature. For example, Gaussian Mixture Models (GMMs), a closely related model, have been widely studied in the context of learning policies from demonstrations.   In summary, the reviewers are not convinced that the paper contains sufficiently novel ideas for an ICLR publication.
The paper describes a method to improve generalization by mixing examples in the hidden space. Experiments on CIFAR 10 and CIFAR 100 showed that the proposed method improves the generalization of the networks. The reviewers found these results promising, but argue that the experimental section was too weak in its current form   notable lacking experiments on larger scale datasets such as Imagenet. Notably the paper should compare more with the relevant baselines to better understand its significance.
This paper proposes an approach for predicting transcription factor (TF) binding sites and TF TF interaction.  The approach is interesting and may ultimately be valuable for the intended application.   But in its current state, the paper has insufficient technical novelty (e.g. relative to matching networks of Vinyals 2016), insufficient comparisons with prior work, and unclear benefit of the approach.  The reviewers also had some concerns about clarity.  
This paper presents work on scene graph grounding under weak supervision.  The reviewers appreciated the consideration of this task and formulation of a solution for it.  However, concerns were raised over the importance of this weakly supervised grounding task, how it addresses challenges in previous methods, the empirical evaluation, insights obtained, motivation, and clarity of exposition.  After reading the authors  response, the subsequent discussion and reconsideration resulted in a sense that while the task is new, the overall contribution and remaining questions over empirical evaluation mean the paper is not yet ready for publication at ICLR.
The reviewers are unanimous that this is an interesting paper, but that ultimately the empirical results are not sufficiently promising to warrant the added complexity.
This manuscript proposes spread divergences as a technique for extending f divergences to distributions with different supports. This is achieved by convolving with a noise distribution. This is an important topic worth further study in the community, particularly as it related to training generative models.  The reviewers and AC opinions were mixed, with reviewers either being unconvinced about the novelty of the proposed work, or expressing issues about the clarity of the presentation. Further improvement of the clarity, combined with additional convincing experiments would significantly strengthen this submission.
This paper deals with a particular model structure selection problem: inferring the order of a given sequence of latent variables. This problem is closely related to the matching problem that involves discrete optimization. The authors propose to cast the problem into a one step Markov Decision problem and optimize it using the policy gradient.  The proposal here is using Variational Order Inference (VOI) using and using a Gumbel Sinkhorn distribution to construct a proposal over approximate permutations. The approach is mathematically sound and novel.  Empirical results on image caption and code generation show promising results: method outperforms the previous Transformer InDIGO and other baselines (Random, L2R, Common, Rare). This paper further analyzes the learned orders globally and locally, and conducts ablations.  The reviewers were overall very enthusiastic.  
The reviewers were largely agreed that the paper presented an interesting idea and has potential but needs a better empirical evaluation.  It seems that the authors largely agree and are working to improve it.  PROS: 1. Improving the speed of program synthesis is a useful problem 2. Good treatment of related work, e.g. CEGIS  CONS: 1. The approach likely does not scale 2. The architecture is underspecified making it hard to reproduce 3. Only 1 domain for evaluation
This paper analyzes the implicit bias of gradient descent of infinite width 2 layer neural networks with ReLU activation. It is shown that the dynamics of gradient descent to optimize the 2 layer NN converges to the optimization dynamics on the random feature model in the infinite width limit. Then, it is shown that the gradient descent converges the minimal L2 norm solution from the initial parameters which yields regularization on a weighted integration of second order differentiation. Although this type of analysis has been given in the existing work, this paper gives its explicit form in 1 dimension input setting.  This paper reveals an interesting fact about the implicit regularization that would be educationally valuable. On the other hand, I should mention that there is room for improvement in its theoretical contribution and moreover its novelty is rather limited. 1. Although the explicit formulation of the implicit regularization is informative, the minimum norm bias itself is already pointed out by existing work and this work follows the line. Especially, regularization on the second order derivative has been already pointed out by previous work (although they are 1 norm regularization). 2. The logical jump from the original data to the adjusted data is still not convincing. It is explained that some numerical experiments show the linear term is negligibly small, which means the problems (15) and (17) are very close. However, this excuse does not make sense for this kind of "theoretical" work. The logic used here should be clarified to make the theoretical framework complete.  The evaluations by the reviewers indicate that this paper is on the borderline, and I also feel that some more additional strong point would be required so that this paper is accepted. I encourage the authors to go in this direction and make the analysis more detailed so that the theoretical framework would get more completed.  Minor comment: Theorem 4 overlaps the result given by the following paper [R1]. It is recommended that the relation and novelty compared with that paper is discussed.  [R1] E, W., Ma, C. & Wu, L. A comparative analysis of optimization and generalization properties of two layer neural network and random feature models under gradient descent dynamics. Sci. China Math. 63, 1235–1258 (2020).
This paper develops a ``preference conditioned” approach to approximate the Pareto frontier for Multi Objective Combinatorial Optimization (MOCO) problems with a single model (thus dealing with the thorny problem that there can be exponentially many Pareto optimal solutions). It appears to provide  flexibility for users to obtain various preferred tradeoffs between the objectives without extra search. The basic idea is to use end to end RL to train the single model for all different preferences simultaneously.   The technical soundness and practical performance are strong. This work s approximation guarantee depends on the ability to approximately solve several (weighted) single objective problems. This may be challenging due to the NP hardness of the latter. However, this limitation seems to also apply to other end to end learning based approaches.   One area where the novelty is somewhat limited is that the paper borrows some number of ideas from neural single objective optimization. The contribution overall seems noteworthy for hard multi objective problems.
This paper proposes a two stage adversarial training approach for learning a disentangled representation of style and content of anime images. Unlike the previous style transfer work, here style is defined as the identity of a particular anime artist, rather than a set of uninterpretable style features. This allows the trained network to generate new anime images which have a particular content and are drawn in the style of a particular artist. While the approach works well, the reviewers voiced concerns about the method (overly complicated and somewhat incremental) and the quality of the experimental section (lack of good baselines and quantitative comparisons at least in terms of the disentanglement quality). It was also mentioned that releasing the code and the dataset would strengthen the appeal of the paper. While the authors have addressed some of the reviewers’ concerns, unfortunately it was not enough to persuade the reviewers to change their marks. Hence, I have to recommend a rejection.
This paper proposes a simple idea for using expert data to improve a deep RL agent s performance. Its main flaw is the lack of justification for the specific techniques used. The empirical evaluation is also fairly limited. 
The reviewers pointed out several opportunities for improvements and concurred that the paper needs significant work before it is ready for publication.  The authors did not provide a rebuttal. We hope the review process was useful to the authors. 
The paper proposes a method for performing active learning on graph convolutional networks. In particular, instead of performing uncertainty based sampling based on an individual node level, the authors propose to look at regional based uncertainty. They propose an efficient algorithm based on page rank. Empirically, they compare their method to several other leading methods, comparing favorably.    Reviewers found the work poorly organized and difficult to read. The idea to use region based estimates is intuitive but feels like nothing more than just that. It s not clear if there is a mathematical basis to justify such a method (e.g. an analysis of sample complexity as has been accomplished in other graph active learning problems, Dasarathy, Nowak, Zhu 2015).   The idea requires further study and justification, and the paper needs an improved exposition. Finally, the authors were not anonymized on the PDF. 
The paper makes an interesting attempt at connecting graph convolutional neural networks (GCN) with matrix factorization (MF) and then develops a MF solution that achieves similar prediction performance as GCN.   While the work is a good attempt, the work suffers from two major issues: (1)  the connection between GCN and other related models have been examined recently. The paper did not provide additional insights; (2) some parts of the derivations could be problematic.   The paper could be a good publication in the future if the motivation of the work can be repositioned. 
Three reviewers agreed to reject and the other reviewer also suggested it is below the threshold.
The paper is presenting an important empirical finding. When the learning algorithms are initialized with the same point, the continual and multitask solutions are connected by linear and low error paths. Motivated by this finding, the paper proposes a new continual learning algorithm based on path regularization. The paper received unanimously good scores. I agree with the reviews and recommend acceptance. 
This paper considers the so called partial label learning problem and proposes a class activation map that is better at making accurate predictions than the model itself on selecting the true label from candidate labels. The authors investigate the approach in experimental results on four benchmark image datasets.   The reviewers appreciated the simplicity of the approach and its effectiveness in practice. The reviewers raised questions how to apply the approach to another weakly supervised learning problem such as semi supervised learning and whether the approach is an identification based strategy. The reviewers also raised several questions asking for more details.  The authors submitted responses to the reviewers  comments. After reading the response, updating the reviews, and discussion, the reviewers who took part in the discussion considered that their  “questions have been well addressed” and that the “authors’ responses basically provided the answers to the questions”.    The feedback provided was already fruitful and the final version should be already improved.   Accept. Poster.
The paper extends the work of “slimmable networks” in that it aims to find a single set of weights suitable for multiple FLOP/accuracy tradeoff (or memory/accuracy tradeoff). The main novelty of the paper is in adapting known techniques from bayesian optimization (BO) to the setting at hand, resulting in a modified training technique. The experiments show a performance lift when compared against the original slimmable networks, as well as other approaches called “two stage” that alternate between optimizing the weights and the architecture. The paper provides a practical approach to an important problem yielding non trivial results. The main weakness of the paper seems to be its novelty. Although it is not possible to naively apply the multi objective optimization with NAS techniques, the reviews seem to indicate that the innovation required to do so is not sufficient to meet the ICLR bar. This is indeed a borderline case, but given the competing papers, my tendency is towards rejecting the paper. 
This paper proposes a graphon based search space for neural architecture search. Unfortunately, the paper as currently stands and the small effect sizes in the experimental results raise questions about the merits of actually employing such a search space for the specific task of NAS. The reviewers expressed concerns that the results do not convincingly support graphon being a superior search space as claimed in the paper.  
All reviewers agreed that this paper is essentially a combination of existing ideas, making it a bit incremental, but is well executed and a good contribution.  Specifically, to quote R1:  "This paper proposes a generative model architecture for molecular graph generation based on autoregressive flows. The main contribution of this paper is to combine existing techniques (auto regressive BFS ordered generation of graphs, normalizing flows, dequantization by Gaussian noise, fine tuning based on reinforcement learning for molecular property optimization, and validity constrained sampling). Most of these techniques are well established either for data generation with normalizing flows or for molecular graph generation and the novelty lies in the combination of these building blocks into a framework. ... Overall, the paper is very well written, nicely structured and addresses an important problem. The framework in its entirety is novel, but the building blocks of the proposed framework are established in prior work and the idea of using normalizing flows for graph generation has been proposed in earlier work. Nonetheless, I find the paper relevant for an ICLR audience and the quality of execution and presentation of the paper is good."
After reading the paper, reviews and authors’ feedback. The meta reviewer agrees with the reviewers that the paper touches an important topic(scale up training). However, as some of the reviewers pointed out, the paper could be further improved by clarifying the novelty and more thorough evaluation justification of the metric being used. Therefore this paper is rejected.  Thank you for submitting the paper to ICLR.  
In this paper, the authors consider the offline RL with only realizability and partial coverage assumption, under which a model based pessimistic policy optimization algorithm has been proposed and rigorously justified. Moreover, variety of special MDP models, including kernelized nonlinear regulator and linear mixture MDP, have been plugged into the general framework, which leads to different specific algorithm and refined guarantees.   In general, the reviewers are positive to the submission. However, there are still issues need to be further discussed,     *Computation feasibility*: most of the reviewers raise the same concern about the computation feasibility and efficiency. Specifically, the proposed algorithm is too complicated, and thus, may not be practical.     *Comparison with existing statistical results*: both reviewers and I appreciate the summary in the paper about the coverage assumptions in the existing methods. However, a similar table for summarizing the complexity of existing algorithms, as well as detailed discussion, is also necessary for a better position of the proposed method among the literature, including both model based and model free RL.
The authors propose a new approach to topology optimization to address over smoothing in GCNs. This is a borderline paper. Topology optimization is clearly important and relevant and the approach tries to optimize the topology (add/delete edges) by viewing the problem as a latent variable model and aiming to optimize the graph together with the GCN parameters to maximize the likelihood of observed node labels. A number of related joint topology optimization approaches exist, however, as discussed in the reviews and the responses. The proposed methodology is termed variational EM but is a bit heuristic in the sense that E and M steps do not follow a consistent criterion (the direction of KL is flipped between the steps). A number of comparisons are provided with consistent gains though the gains appear relatively small. No error bars are provided despite request to add them to better assess the significance of these results. It remains unclear whether the gains are worth the added complexity.  
Reviewers raised concerns about the paper s clarity (interchangeable use of subtly different terms, notation, typos), and how realistic/practical certain assumptions are. The authors are encouraged to incorporate the reviewers  detailed comments for a future submission.
The paper provides variance reduction techniques for GCN training. When training a GCN it is common to sample nodes as in SGD, but also subsample the nodes’ neighbors, due to computational reasons. The entire mechanism introduces both bias and variance to the gradient estimation. The authors decompose the gradient estimate into its variance and bias error, allowing them to apply more targeted variance (and bias) reduction techniques.  The results and improvement over existing GCN methods seem to be solid. The main weakness of the paper is its novelty. As pointed out in the reviews the techniques seem to be quite close to papers [5],[11] (referring to the authors posted list). It therefore boils down to the question of whether the authors simply applied existing techniques, achieving a better implementation than previous art, or did they develop a truly new algorithm that will encourage further research and deepen the understanding of GCNs. Given the decisive opinions of reviewers 1 and 4, that remained after taking the response into account, I tend to believe that the improvement provided here is either too incremental or not stated in a crisp enough manner in order to be published in its current form 
All of the reviewers are impressed by this paper s empirical results and they agree that this is a good paper and should be accepted. Some questions about the theoretical justification of the proposed method and its potential practical impact remain open, but the empirical results are impressive and can result in more research in understanding Cyclic Precision Training (CPT) and improving quantized training of neural nets. I suggest acceptance as a spotlight presentation.
Dear Authors,  The paper was received nicely and discussed during the rebuttal period. There is consensus among the reviewers that the paper should be accepted:     This paper does contribute solidly to a timely topic of theoretical understanding of sparisty recovery with deep unroling.     The original version had very limited experiments and only synthetic ones, which raised concerns about whether the setting is motivated and whether the algorithm works on actual real data. The revision fixed that to an extent with some experiments on real data.  Yet, there are still some concerns that we suggest to be tackled for the final version:   The capacity analysis is carried out inside a strongly convex regime while the algorithm is advocated for nonconvex sparsity recovery (see, e.g., the Discussion at the end of Section 2.1 );   The analysis is relatively loosely connected to the adopted fist order optimization procedure;   While the depth of network plays a role in the upper bound of Equation (15), its real impact on generalization gap looks quite limited.  The above are just suggestions to be looked more carefully, but there are not necessary.   The current consensus is that the paper deserves publication.  Best AC
The paper proposes a data augmentation approach that extends Mixup with high  and low pass filtering operations, in order to regularize deep networks towards focusing on low frequency components of the input signal.  Reviewers are unconvinced about the significance of the contribution.  Reviewer 5zdd notes that the method does not improve over standard Mixup in the absence of corruption error.  Reviewer 3E2o notes that "the idea of spectral mixing itself is not particularly novel", and also asks for ablation studies concerning the hyperparameters of the method; the author response unfortunately does not provide enough detail on ablation experiments.  The AC agrees with the reviewers and does not believe the author response has addressed weaknesses in a satisfactory manner.
The reviewers in general like the idea of using the Cramer Wold kernel, noting that its heavy tails and closed form solution are appealing properties that lead to increased stability and improved training. The main concern was novelty, as this paper can be seen as simply changing the kernel in WAE MMD. One suggestion is to more heavily highlight the CW distance, and in particular to find another useful application for it outside of WAE MMD.  The paper emphasizes frequently that the closed form loss function is a critical feature of this approach, however I don’t see any experiments that optimize WAE MMD under the CW distance while sampling from the Gaussian. This is important to measure the degree to which any improvement is attributable to a closed form solution, or to the distance measure itself. 
The authors propose to use the information bottleneck to learn state representations for RL. They optimize the IB objective using Stein variational gradient descent and combine it with A2C and PPO. On a handful of Atari games, they show improved performance.  The reviewers primary concerns were: *Limited evaluation. The method was only evaluated on a handful of the Atari games and no comparison to other representation learning methods was done. *Using a simple Gaussian embedding function would eliminate the need for amortized SVGD. The authors should compare to that alternative to demonstrate the necessity of their approach.  The ideas explored in the paper are interesting, but given the issues with evaluation, the paper is not ready for acceptance at this time.
The paper combines a few different ideas for representation learning on sequential data and is able to achieve competitive WER on the Librispeech ASR dataset. I appreciate the fact that the authors engaged with reviewers and tried to improve the paper. While I get a sense that the final system has many moving parts, I believe the paper meets the bar for acceptance at ICLR.
The authors propose to enforce interpretability and controllability on latent variables, like affect and speaking rate, in a speech synthesis model by training in a semi supervised way, with a small amount of labeled data with the variables of interest labeled.   The idea is sensible and the results are very encouraging, and the authors have addressed the initial concerns brought up by the reviewers.
The work proposes a simple neural network framework for detecting anomalies on sequential data. The manuscript is quite rough. The paper needs significant editing. The authors should take the reviewers  recommendations to heart and make deeper changes to the paper.
As expressed by most reviewers, the idea of the paper is interesting:  using summarization as an intermediate representation for an auto encoder.  In addition, a GAN is used on the generator output to encourage the output to look like summaries.  They just need unpaired summaries.  Even if the idea is interesting, from the committee s perspective, important baselines are missing in the experimental section:  why would one choose to use this method if it is not competitive with other baselines that have proposed work in this vein?  One reviewer brings up the point that the method is significantly worse than a supervised baseline.  Moreover, the authors mention the work of Miao and Blunsom, but could have used one of their experimental setups to show that at least in the semi supervised scenario, this work empirically performs as well or better than that baseline.
The paper addresses an important problem of finding a good trade off between generalization and convergence speed of stochastic gradient methods for training deep nets. However, there is a consensus among the reviewers, even after rebuttals provided by the authors, that  the contribution is somewhat limited and the paper may require additional work before it is ready to be published.
This paper proposes an approach for abstractive summarization of multi domain dialogs, called SPNet, that incrementally builds on previous approaches such as pointer generator networks. SPNet also separately includes speaker role, slot and domain labels, and is evaluated against a new metric, Critical Information Completeness (CIC), to tackle issues with ROUGE. The reviewers suggested a set of issues, including the meaningfulness of the task, incremental nature of the work and lack of novelty, and consistency issues in the write up. Unfortunately authors did not respond to the reviewer comments. I suggest rejecting the paper.
The pros and cons of this paper cited by the reviewers (with a small amount of my personal opinion) can be summarized below:  Pros: * The method itself seems to be tackling an interesting problem, which is feature matching between encoders within a generative model  Cons: * The paper is sloppily written and symbols are not defined clearly * The paper overclaims its contributions in the introduction, which are not supported by experimental results * It mis represents the task of decipherment and fails to cite relevant work * The experimental setting is not well thought out in many places (see Reviewer 1 s comments in particular)  As a result, I do not think this is up to the standards of ICLR at this time, although it may have potential in the future.
This paper observes that a fully convolutional model in the style of recent MLP Mixer and ViT variants can have surprisingly good initial performance. As this paper attracts certain amount of attentions, three expert reviewers have provided very detailed and serious comments, and two actively engaged with author discussions. AC also carefully read the paper as well as all discussion threads.  AC agrees the authors should not be penalized by not achieving the best performance, nor not comparing with very recent work. The main legitimate critiques, however, focus on three aspects: (1) over claimed contribution; (2) experiment solidness/competitiveness; and (3) writing completeness/clarity.  First, this paper established an interesting ablation experiment that a very simple model, that uses only standard convolutions to achieve the mixing steps, can roughly "do the work". However, AC disagrees this is a very "surprisingly new" result, on top of MLP mixer: given convolutions are increasingly re injected into ViTs to gain the vision inductive bias, their similar role in MLP mixer should be expected too. Moreover, as in general agreement by reviewers, the paper title might have over claimed   the authors cannot directly prove this concept "patch is the most critical component" yet. The authors later also agreed and changed some confusing wording, which is a good move (but also, making their contribution now even less obvious).   Second, this method does not achieve noteworthy competitive results compared to others, in order to justify its merit (simplicity alone is good to have, but insufficient to justify a strong work). Importantly, it has been pointed out by two reviewers that the model throughput is much worse than the competitors. AC also noticed that the comparison was not very rigorous, e.g., comparing ConvMixer patch size 7 with DeiT B 16 patch size 16 doesn t help draw much fair informative conclusion. The cifar 10 results alone did not provide strong support and were later de emphaszied by authors too.   Third, while NOT being the main reason of rejection, AC personally suggests the authors to responsibly enrich their main text, and to remove the  “A note on paper length” paragraph. The authors intentionally kept the paper length unusually short. Reviewers generally dislike this idea. Being an innovative writer is good, but very relevant details and discussions were left in the supplemental as a result. Especially, AC agrees the whole section A and part of section B of the supplemental should have been in the main paper at very least.  In summary, the authors strive to tell an interesting story, but it is not yet a well settled story. The experiments are not solid enough to support their bold claims. The authors are suggested to improve their work further by taking into account reviewer comments.
The paper addresses image translation by extending prior models, e.g. CycleGAN, to domain pairs that have significantly different shape variations. The main technical idea is to apply the translation directly on the deep feature maps (instead of on the pixel level).  While acknowledging that the proposed model is potentially useful, the reviewers raised several important concerns: (1) ill posed formulation of the problem and what is desirable, (2) using fine tuned/pre trained VGG features, (3) computational cost of the proposed approach, i.e. training a cascade of pairs of translators (one pair per layer).  AC can confirm that all three reviewers have read the author responses. AC suggests, in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper. 
The paper proposes a new adaptive optimization algorithm which is claimed to have better convergence properties and lower susceptibility to gradient variance. Reviewers found the idea of normalizing on the fly to be interesting, but raised some important concerns. Although similar to AdaGrad, Expectigrad has a very important differentiation due to division by $n_t$. Assuming $\beta 0$ in my opinion is also ok and many papers assume this for analysis. Even after accounting for these two facts, during discussions the reviewers considered the work to be incremental and a more thorough evaluation is needed to determine the benefits of algorithm. Specifically, please compare to important and relevant baselines (like AdamNC and Yogi), because sometimes it felt like baselines were picked and dropped randomly. The empirical improvement provided by Expectigrad compared to SOTA is not clear (both on synthetic problems from Reddi et al and real problems). Thus, unfortunately, I cannot recommend an acceptance of the paper in the current form. However, I would strongly encourage authors to resubmit after improving according to reviewer suggestions.   Some other minor points that came up during discussion are:  1. choice of hyperparameters was not clear to reviewers, e.g. different optimizer may behave very differently for same set of hyperparameters, so it would not be fair to compare them as is. 2. gradients would never be exactly zero in deep networks, so is current definition of $n_t$ good enough?
The paper presents a simple and intuitive method to prune the missing value in the learning and inference steps of the neural networks, leading to similar prediction performance as other methods to impute missing value. It has some really useful insights, but could benefit from one more round of revision for a strong publication:  1. improving the writing so that its sets up the right expectations on the contributions of the paper;  2. providing discussions on its connections (and differences) with zero imputation and missing indicator methods;  3. thoroughly investigating the experiment results to illustrate the advantages of the proposed method.    The recommendation of reject is made based on the technical aspect of the paper.   During the rebuttal phase, the authors misused the interactive and transparent (for the better or worse) openreview system by writing inappropriate comments with personal accusations to the reviewers who write negative reviews. We would like to extend the apologies to the reviewers for this unpleasant experience and thank the reviewers for their engagement and work, as well as their fair assessment of the paper.
This paper presents a detailed comparison of different bonus based exploration methods on a common evaluation framework (Rainbow) when used with the ATARI game suite. They find that while these bonuses help on Montezuma s Revenge (MR), they underperform relative to epsilon greedy on other games. This suggests that architectural changes may be a more important factor than bonus based exploration in recent advances on MR.  The reviewers commented that this paper makes no effort to present new techniques, and the insights discovered could be expanded on. Despite this, it is an interesting paper that is generally well argued and would be a useful contribution to the field. I recommend acceptance.
This paper proposes an ensemble based active learning approach to select a subset of training data that yields the same or better performance. The proposed method is rather heuristic and lacks novel technical contribution that we expect for top ML conferences. No theoretical justification is provided to argue why the proposed method works. Additional studied are needed to fully convincingly demonstrate the benefit of the proposed method in terms computational cost. 
While the paper contains interesting ideas, the reviewers agree the experimental study can be improved. 
Main content:  Blind review #1 summarizes it well:  This paper is about learning an identifiable generative model, iFlow, that builds upon a recent result on nonlinear ICA. The key idea is providing side information to identify the latent representation, i.e., essentially a prior conditioned on extra information such as labels and restricting the mapping to flows for being able to compute the likelihood. As the loglikelihood of a flow model is readily available, a direct approach can be used for learning that optimizes both the prior and the observation model.	     Discussion:  Reviewer questions were mostly about clarification, which the authors addressed during the rebuttal period.     Recommendation and justification:  All reviewers agree the paper is a weak accept based on degree of depth, novelty, and impact.
Very solid paper exploring an interpretation of LSTMs. good reviewss
The paper proposes a simple modification to how data augmentation is done in image based RL. This results in some improvements on benchmark tasks. The change essentially amounts to adapting data augmentation strategies that are already understood in other fields to deep RL. However, the effect of data augmentation in simple image based deep RL tasks is already known. As such, I think the contribution in this paper is quite incremental   the notion that data augmentation in deep RL helps is already known, and the particular augmentation strategy proposed here is not especially novel. So while it s good in terms of producing improved results on some benchmark tasks, it doesn t seem to be of high significance to the study of reinforcement learning or machine learning more broadly. As such, I think it could be a valuable contribution to a more narrow venue, or as a technical report, but is too incremental and narrow in scope for ICLR.  A note to the authors (this did not impact the paper decision): due to the unfortunately lackluster quality of the reviews, I read and reviewed the paper myself as well to be able to produce a more accurate meta review. In the balance, I see the point the authors make in the response that some of the results in prior work (e.g., CURL) are unfortunately unreliable. That s not the fault of the authors, it s the fault of the prior works. I took this into account in my assessment. In this sense, I do think the comparison to prior work is sensible. On the other hand, I think the practice of reporting only very specific checkpoints (e.g., 100k and 500k), though borrowed from prior work, is not a good way to report results, as it hides the real performance of the methods.
Strengths:  This paper gives a detailed treatment of the connections between rate distortion theory and variational lower bounds, culminating in a practical diagnostic tool.  The paper is well written.  Weaknesses:  Many of the theoretical results existed in older work.  Points of contention:  Most of the discussion was about the novelty of the lower bound.  Consensus:  R3 and R2 both appear to recommend acceptance (R2 in a comment), and have both clearly given the paper detailed thought.
The paper provides a unique contribution to the scalability of Bayesian inference to Pyramidal Bayesian Models with application to neuroimaging. The major point of concern by the reviewers is around how close is the inference approach to the more classical Mean Field VI. However, in my opinion, the authors have addressed these concerns in the rebuttal. Therefore, I recommend Accept.
This paper constitutes interesting progress on an important topic; the reviewers identify certain improvements and directions for future work, and I urge the authors to continue to develop refinements and extensions.
This paper proposes a new framework for improving supervised learning via invariant mechanisms. The reviewers agree that overall, this paper is well written and contributes to a growing body of work on invariant prediction and causality in supervised learning. At the same time, there are some concerns regarding novelty and significance in light of previous work, as well as the overall organization of the paper, which could be improved to highlight the main contributions more clearly. Ultimately, this was a borderline decision, but it is clear that the paper needs a major revision before acceptance. Although the authors have already incorporated some of the minor comments which is appreciated, the authors are urged to consider the major comments (e.g. see R2 s comments regarding presentation) when revising the paper.
This paper proposes "HyperDynamics" a framework that takes into account the history of an agents recent interactions with the environment to predict physical parameters such as mass and friction. These parameters are fed into a forward dynamics model, represented as a neural network, that is used for control.  Pros:   addresses an important problem (adapting dynamics models to "new" environments) and provides strong baselines   well written and authors have improved clarity even further based on reviewers comments  Cons:   I agree with the reviewer that it is currently unclear how well this will transfer to the real world   The idea of predicting physical parameters from a history of environment interactions is not not novel in itself (although the proposed framework is, as far as I know). The authors should include related work along the lines of (1) (this is just one paper that comes to mind, others exist)  (1) Preparing for the Unknown: Learning a Universal Policy with Online System Identification
This paper aims to analyze CNN representations in terms of how well they measure the perceptual severity of image distortions.  In particularly, (a) sensitivity to changes in visual frequency and (b) orientation selectivity was used. Although the reviewers agree that this paper presents some interesting initial findings with a promising direction, the majority of the reviewers (three out of four) find that the paper is incomplete, raising concerns in terms of experimental settings and results. Multiple reviewers explicitly asked for additional experiments to confirm whether the presented empirical results can be used to improve results of an image generation. Responding to the reviews, the authors added a super resolution experiment in the appendix, which the reviewers believe is the right direction but is still preliminary.  Overall, we believe the paper reports interesting findings but it will require a series of additional work to make it ready for the publication.
The reviewers attempted to give this paper a fair assessment, but were unanimous in recommending rejection.  The technical quality of motivation was questioned, while the experimental evaluation was not found to be clear or convincing.  Hopefully the feedback provided can help the authors improve their paper.
Important problem (modular continual RL) and novel contributions. The initial submission was judged to be a little dense and hard to read, but the authors have been responsive in responding and updating the paper. I support accepting this paper. 
This paper falls in the borderline area and there are still some concerns (for instance by AnonReviewer5 and AnonReviewer2) that deserve further treatment. Given that most ideas can only be validated in experiments (as the results are not theoretical), some points that remain are the comparison with other approaches (there are reasonable comparisons, but there are very famous contenders missing such as xgboost, ok that LightGBM is, but why not the other?), details about the tuning, the significance of results (practical and statistical is not complete/detailed enough), and the reasoning about situations with many rules and interpretability seems to be worth exploring/discussing further.
The reviewers point out that most of the results are already known and are not novel. There are also issues with the presentation. Studying only depth 2 and depth 3 networks is very limiting.
This paper gives a new algorithm for the CCA problem. The main idea of the new algorithm is to reformulate the matrices in the CCA problem as a product of three matrices: one orthonormal matrix, one rotation and one upper diagonal matrix. The algorithm then performs remannian gradient descent to these components. The per iteration complexity of the algorithm is O(d^2k) while the (local) convergence rate is O(1/t). Overall the reformulation is interesting and the algorithm seems effective in practice. On the other hand the convergence rate proof relies on local strong convexity and it s not clear why the algorithm converges globally (or even what is the radius of convergence locally).
This paper considers the problem of agents learning to autonomously navigate the web, specifically by focusing on filling out forms. The focus is on using adversarial environment generation to form a curriculum of training tasks. Thank you for the revisions to the manuscript, which have particularly improved readability. The presented problem is really interesting and seems an important real world problem for RL. Despite this, as the paper stands the results are not completely convincing. It seems like there is also scope to rigourously analyse the proposed method on other, better known domains to better quantify its limitations.
The paper addresses a pressing problem for applications involving clinical time series and introduce a pipeline that handle many of the issues pertaining to data preprocessing.  An important contribution is the software that makes the processing more seamless, which will, without a doubt, be useful to the community given the need for reproducibility.  The authors have responded suitably to reviewer comments with the main  leftover criticism  being that such a paper may not be the best fit for ICLR. This isn t a typical paper. However, something that introduces this level of automation and flexibility in handling time series has not been presented at this conference (or other ML conferences) to the best of my knowledge. It seems it could work in conjunction (as opposed to competing) with any new time series models/techniques that may be introduced.
While the paper contains some interesting ideas, the reviewers felt that overall the paper is not theoretical well supported, and likewise the experiments are not fully convincing. Even after the rebuttal, these concerns still persist.
This paper introduces a new linear attention mechanism for transformer based models.  This is accomplished by replacing the softmax in the standard transformer self attention with a cosine based re weighting mechanism.  The empirical results are good, and cosFormer generally outperforms existing efficient transformers for autoregressive language modeling, fine tuning, and on the long range arena.  The reviewers were generally positive regarding the paper, with all reviewers voting to accept.  The discussion period focused on particular choices regarding the ReLU activation function vs. other non negative activation functions, further motivating the cosine operation, and comparing the speed of cosFormer vs. other efficient transformers.  The authors responded by providing additional ablations to empirically validate the choice of ReLU, motivated the cosine operation by noting that it introduces a locality bias, and further described the computation requirements of their transformer vs. prior work.  Overall, this is an interesting addition to the linear / efficient transformer literature, with solid empirical results supporting the various design decisions.
There is a substantial contribution in identifying novel questions/issues, as this paper certainly does. Neither I nor the reviewers have seen this issue of transient non stationary before, and the authors make a compelling case for it, especially in the supervised setting with the CIFAR experiments. It is less compelling through the RL experiments. As such, this paper is likely to inspire new work within the field. To me, Figure 1 is the most interesting aspect of the whole paper.  The initial approach by the authors is questionable in its effectiveness, and is likely to be improved by others in the future. Some of the results in Figure 3 are questionable, especially when you look at the individual curves in Figure 8. So overall, this means that the authors have identified a truly novel issue, and proposed an initial method that is just okay.  They ve done a nice job investigating this in a supervised setting, and need to push further in the RL setting.  The question is whether the novel contribution of the problem outweighs that the algorithm and its evaluation could use improvement.  The reviewers debated this in the discussion, with points on both sides, but the novelty of the question/issue (even if the investigation could use work) is likely to inspire further work in this direction.  Other notes: The authors could have evaluated the (impractical) version of their algorithm proposed in the first paragraph of Section 4.2. This would inform 1) whether their parallel training approximation is close to the optimal algorithm, and 2) whether the optimal (impractical) algorithm is capable of improving generalization significantly. If the latter is true, it would leave open a huge avenue of investigation to find better approximate solutions.
The paper makes some novel and interesting observation pertaining the relationship between data heterogeneity and personalization. Reviewers like the paper and ideas in general but raised several concerns. The rebuttal rectified several confusions and provided more clarification which convinced the reviewers that the paper is above bar for publication.
The paper aims to integrate Stein variational inference methods into the existing probabilistic programming language NumPyro. The implemented methods include variantions of Stein variational gradient descent with different types of kernel functions, non linear scaling of update terms, and matrix valued kernels. The paper includes empirical results with a comparsion with existing baselines in real world problems. Using this framework, the authors developed a new Stein mixture algorithm for deep Markov models, which shows better performance than existing methods.  Strengths:    The paper is overall well written and the method is clearly explained.    The literature review is thorough.   Integration of SteinVI into numpyro seems useful. Users can easily take advantage of the state of the art SteinVI algorithms for their own Bayesian modelings.   Extending the stein mixture method to deep Markov models is a novel application.  Weaknesses:    The originality is low the authors propose algorithms that are very similar to previous work and there is a lack of experiments to verify the usefulness of the proposed method, for example,   to verify the decreased variance of the gradient estimates claimed by the authors.   Efforts are required to illustrate why ELBO within Stein is preferred over the existing work.   Some important Stein VI methods seem lacking.   No experiments to support the usefulness of EinSteinVI for Non linear Stein VI, Matrix valued kernel stein VI, and message passing stein VI.  All reviewers vote for rejection. I recommend the authors to addrss the limitatoins mentioned above and improve the paper before its resubmission to another venue.
The reviewers liked the overall idea presented in this paper. Although the idea as well as relevant tooling for incorporating constraints in the latent space has been studied a lot in the past, the authors differentiate their work by applying it in a new interesting problem. At the same time, some confusions about relation to prior work remain after rebuttal. Firstly, the theoretical additions to prior work (Srinivas et al. 2010) are still unclear in terms of significance   they feel more like observations made on top of an existing theorem rather than fresh significant insights. Furthermore, even if prior work has not considered exactly the same set up, it would still be needed to understand what the performance would be when considering prior models or prior datasets used in similar domains (e.g. suggestions by R2, R3). The latter would be desirable especially since the experimental set up used in this paper is deemed by the reviewers too simple (while the motivation of the paper is to solve an issue essentially manifesting in complex scenarios).
The submission proposes a method that combines sparsification and low rank projections to compress a neural network.  This is in line with nearly all state of the art methods.  The specific combination proposed in this instance are SVD for low rank and localized group projections (LGP) for sparsity.  The main concern about the paper is the lack of stronger comparison to sota compression techniques.  The authors justify their choice in the rebuttal, but ultimately only compare to relatively straightforward baselines.  The additional comparison with e.g. Table 6 of the appendix does not give sufficient information to replicate or to know how the reduction in parameters was achieved.  The scores for this paper were  borderline, and the reviewers were largely in consensus with their scores and the points raised in the reviews.  Given the highly selective nature of ICLR, the overall evaluations and remaining questions about the paper and comparison to baselines indicates that it does not pass the threshold for acceptance.
The paper considers ways to understand label smoothing methods, which are widely used in many applications.  There is some theory on the performance of SGD with and without the methods of the paper, but there is s significant gap in terms of how the theory offers insight into label smoothing.  There are some empirical results, but they are insufficient and there is not much description of the experimental setup.  There was a diversity of reviews.  But, after a discussion among reviewers, it was felt that, overall, another iteration on improving the coherence and presentation of the paper will make it much better for the community. 
The papers makes progress on the important question of implicit bias in gradient based neural learning. Remarkably they derive reasonable conditions for global optimality.
This paper studies the interplay between adversarial examples and generalization in the uniform setting (not specific assumptions on the architecture) in a toy high dimensional setting. In particular, the authors show a fundamental tradeoff between generalization error and the average distance of adversarial examples.  Reviewers were skeptical about the possible significance of this work, but the paper underwent a major revision that greatly improved the quality of presentation. That said, the results are still preliminary since they only consider a toy dataset (concentric spheres). The AC recommends re submitting this work to the workshop track.
Some expert reviewers have raised novelty issues, that the authors have addressed in detail. Still, these expert reviewers are not entirely convinced. If this were a journal, I would recommend a major revision or reject and resubmit in order to allow the authors to anticipate the reviewers  concerns in the body of the paper and get some fresh reviews. I compliment the authors on the diligence they have put into the rebuttal stage, and look forward to reading the next version of the work. I will note that the bounds by Bartlett, Foster, and Telgarsky (and then the PAC Bayes versions by Neyshabur et al.) are numerically vacuous empirically, and so whether those bounds or these bounds for RNNs explain generalization is up for debate.
The reviewers conclude the paper does not bring an important contribution compared to existing work. The experimental study can also be improved. 
I concur with two of the reviewers: the work is somewhat incremental in terms of technical novelty (it s effectively CycleGANs for domain adaptation with a couple of effective tricks) and the need/advantage of the cycle consistency loss is not demonstrated sufficiently. The only solid ablation evidence seems to the the SVHN >MNIST experiment from post submission; I would personally like to see this kind of empirical proof extended much further (the fact that Shrivastava et al. s method doesn t work well on GTA >Cityscapes is not itself proof that cycle consistency is needed). With more empirical evidence I can see this paper being a good candidate for a computer vision conference like CVPR or ICCV.
The paper introduces MUSIC, a method for unsupervised learning of control policies, which partitions state variables into exogenous and endogenous collections and maximizes mutual information between them. Reviewers were uniformly positive, agreeing that the  approach was interesting and well motivated, and the experiments convincing. Some concerns were raised as to clarity, which were addressed through several revisions of the manuscript. I am happy to recommend acceptance.
The paper generalizes the concept of "hindsight", i.e. the recycling of data from trajectories in a goal based system based on the goal state actually achieved, to policy gradient methods.  This was an interesting paper in that it scored quite highly despite all three reviewers mentioning incrementality or a relative lack of novelty. Although the authors naturally took some exception to this, AC personally believes that properly executed, contributions that seem quite straightforward in hindsight (pun partly intended) can be valuable in moving the field forward: a clean and didactic presentation of theory backed by well designed and extensive empirical investigation (both of which are adjectives used by reviewers to describe the empirical work in this paper) can be as valuable, or moreso, than a poorly executed but higher novelty works. To quote AnonReviewer3, "HPG is almost certainly going to end up being a widely used addition to the RL toolbox".  Feedback from reviewers prompted extensive discussion and a direct comparison with Hindsight Experience Replay which reviewers agreed added significant value to the manuscript, earning it a post rebuttal unanimous rating of 7. It is therefore my pleasure to recommend acceptance.
This paper presents a method, called Zest, to measure the similarity between two supervised machine learning models based on their model explanations computed by the LIME feature attribution method.  The technical novelty and significant are high, and results are strong.  Reviewers had clarifying questions regarding experiments and suggestions to add experiments, which involve additional domains (text and audio) and different families of classifiers, and more contexts based on prior literatures. These were adequately addressed by the authors. Overall, this paper deserves borderline acceptance.
This paper addresses a promising method for order learning and applies the new ideas of multiple chain learning and anchor selection to age estimation and aesthetic regression. The decision regarding instance class is made by comparing it with anchor instances in the same chain and maximizing the consistency among the comparison results. In a multi chain setting, each chain may correspond to a higher level attribute class, for example, gender or ethnic group. Supervised and unsupervised learning of multiple ordered chains is proposed.  As rightly acknowledged by R4: “What more promising is the unsupervised chains, which could automatically search for a more optimal multi chain division scheme than the pre defined data division.” All three reviewers and AC agree that the proposed approach is interesting and shows promising results. There are several potential weaknesses and suggestions to further strengthen this work: (1) more quantitative results are needed for assessing the benefits of this approach (R3, R4)   see R3’s request to complete the results for FG Net, to include the results for CLAP2016 and a comparison with the SOTA method BridgeNet. Pleased to report that the authors have revised the manuscript and have included performance of the arithmetic scheme as well as the geometric scheme for FG Net. Also the authors have provided some initial evaluations of BridgeNet and promised to report the final results as well as the results for CLAP2016 in the final version.  (2) R3 and R4 have expressed concerns regarding using the geometric ratio of the class distances in age estimation and that the improvement may be caused by the data distribution that favours it (R4) or because the baseline methods are not fine tuned in the same manner (R3). The authors have partially addressed this concern in the rebuttal.  There is a large body of work in computer vision that is focused on relative comparison of samples based on attributes (e.g. age) that is not clearly articulated in the discussions / baseline comparisons (1CH)   see the seminal work [Relative attributes by Parikh and Grauman, ICCV2011] and the follow up works.  Considering the author response, the AC decided that the most crucial concerns have been addressed in the revision and that the paper could be accepted, but the authors are strongly urged to include additional results that were promised in the rebuttal for the final revision.  
The paper presents an algorithm for audio super resolution using adversarial models along with additional losses, e.g. using auto encoders and reconstruction losses, to improve the generation process.   Strengths   Proposes audio super resolution based on GANs, extending some of the techniques proposed for vision / image to audio.   The authors improved the paper during the review process by including results from a user study and ablation analysis.  Weaknesses   Although the paper presents an interesting application of GANs for the audio task, overall novelty is limited since the setup closely follows what has been done for vision and related tasks, and the baseline system. This is also not the first application of GANs for audio tasks.    Performance improvement over previously proposed (U Net) models is small. It would have been useful to also include UNet4 in user study, as one of the reviewers’ pointed out, since it sounds better in a few cases.   It is not entirely clear if the method would be an improvement of state of the art audio generative models like Wavenet.  Reviewers agree that the general direction of this work is interesting, but the results are not compelling enough at the moment for the paper to be accepted to ICLR. Given these review comments, the recommendation is to reject the paper.
Accept. The paper proposes Deformable DETR that builds on DETR and solves the slow convergence and limited spatial resolution problem while getting impressive results. The authors should think about comparing with other linear attention mechanisms to show the applicability of the method.
This paper proposed a new measure of effective gradient flow (EGF), and also compared sparse vs. dense networks on CIFAR 10 and CIFAR 100. The notion of EGF would be interesting, but the paper did not present enough evidence to support this notion.
The reviews were a bit mixed, with some concerns on the novelty and experimental evaluation. While the authors  efforts during rebuttable were appreciated, the overall sentiment is that this work, in its current form, cannot be accepted to ICLR yet. Please consider revising your work based on the excellent reviews. Some more comments from the AC s independent assessment:   (a) Further elaboration on the novelty is needed. Currently the main message appears to be that if we combine two existing approaches (AT and EntM or LS) then we get better results. This is perhaps not too surprising and more elaboration on the significance would be appreciated.   (b) More comparisons in the experiments, including the SOTA performances and alternative defenses (some below).   (c) The analysis in Section 6 adds more confusion than clarification. It is clear that EntM and LS would largely decrease M_f, but why would they also decrease the Lipschitz constant even more sharply? If this explanation is useful, why not directly regularize the Lipschitz constant and maximize the margin M_f? There is in fact a large body of work on this, see for example:  1. Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation  2. Parseval Networks: Improving Robustness to Adversarial Examples  3. L2 Nonexpansive Neural Networks  4. and the many references since.  
This paper presents two new representation learning tasks (losses) based on contrastive learning that when combined with a language modeling loss result in a better multilingual model. Experiments on machine translation and XTREME demonstrate the benefit of the proposed method compared to strong baselines.  I think this is an interesting paper that advances multilingual representation learning. The authors have incorporated many suggestions from the reviewers to improve the paper during the rebuttal period. I recommend to accept the paper, but also strongly suggest the authors to make an official submission to XTREME to validate their results.
Paper presents and interesting new direction, but the evaluation leaves many questions open, and situation with respect to state of the art is lacking
All reviewers vote for rejecting this paper. The main points of criticism shared by the reviewers are missing novelty and missing/unclear significance of the contribution. There was no rebuttal, so this is a clear reject.
This paper presents a decentralized cooperative approach in multi agents using Markov games theory. After reviewing the paper and reading the comments from the reviewers, here are my comments:     The paper is well written, quite difficult to follow, but very informative.   The contribution is clearly stated and the results support it.   Theoretical results are interesting for the RL community.   The main concern is about learning the epsilon approximate Nash equilibrium policy which is a fundamental part of the paper.
This paper has conducted extensive experiments to examine the scaling and transferring laws of LMs for machine translation and has concluded several interesting findings which could be inspiring to the future work.  The main concerns from reviewers are that the novelty of this paper is not enough. In addition, the experiments are not well designed and the clarity of this paper can be further improved. We hope the reviews can help authors improve their paper.
All three reviewers advocated acceptance. The AC agrees, feeling the paper is interesting. 
This paper proposes a self supervised learning method for learning representations for graph structured data, with both local and global objectives. The local objective aims to maximize the mutual information between two correlated graphs generated with attribute masking [Hu et al. 19], with the InfoNCE loss [van den Oord et al. 18], and the global objective aims to cluster the graphs using the RPCL [Xu et al. 93] objective, which pulls the sample toward the closest cluster while pushing it away from the rival clusters. The proposed method is validated on standard graph classification benchmarks by training a linear classifier on top of the GNN pre trained with it, and the results show that it largely outperforms existing graph pre training methods.   This paper fell into a borderline case, receiving split reviews with two of the reviewers learning toward rejection, and two others proposing to accept. The reviewers in general agreed that the experimental validation is thorough (except for one reviewer), and some of the reviewers mentioned that the proposed idea of performing self supervised learning at both local and global level makes sense. However, the negative reviewers were concerned with the limited novelty of the proposed method, since the proposed method seems like a simple combination of two objectives each of which are based on existing ideas (although the latter has not been explored for GNN pre training). The reviewers had interactive discussions with the authors, and the authors provided detailed feedback. Yet, the reviewers were not convinced that the method has sufficient novelty to warrant publication even after the internal discussion period, and decided to keep their negative ratings.  I believe that this is a simple yet effective pre training method for GNNs on graph structured data. The proposed method of combining the local and global objective seems like a promising solution to learn a metric space that well captures the graph level similarity and also is well separated for discriminative classification, and it may have some practical impact given its good performance on benchmark datasets. However, as the two negative reviewers mentioned, the paper in its current form is presented as a simple combination of existing approaches. The local objective is a slight modification of attribute masking strategy of [Hu et al. 19], and the global objective of clustering has been explored in self supervised learning of CNNs for image data [Asano et al. 20]. Thus, I lean toward rejecting the paper, considering its relative novelty and quality.   However, I find the proposed work highly promising, and encourage the authors to further develop the method while also improving on the paper writing. I suggest the authors to focus more on the main idea of learning with both local and global objectives, without specifically tying each objective to any of the existing methods. The authors may consider various techniques for both local and global objectives (such as hinge loss based contrastive loss with k means clustering as shown in the response to R3), and suggest the proposed work as a more general framework.    [Asano et al. 20] Self Labeling via Simultaneous Clustering and Representation Learning, ICLR 2020
The paper develops a novel provable defense against patch based adversasrial attacks on image classification system, by combining a novel architecture and certification procedure. The theoretical and experimental contributions are convincing and clearly advance the state of the art in provable defenses against adversarial perturbations.  The questions raised by the reviewers were addressed convincingly by the authors during the rebuttal phase, leading to unanimous consensus amongst reviewers towards acceptance. I recommend acceptance.
This paper considers a domain adaptation setting where a source domain model trained on a server is adapted on a client using target domain dataset. The paper considers the setting where the client only has a modest memory footprint (e.g., an edge device) and uses a recently proposed technique "TinyTL" (NeurIPS 2020) which is based on freezing the network weights but only updating the biases and adding a lightweight residual module. The basic idea of the paper is also based on SHOT (ICML 2020).  While the reviewers appreciate the problem setting and the basic idea, there were several concerns, some of which included:    Limited novelty: The paper s key ideas are largely based on SHOT and TinyTL and a simple combination of these with not such significant challenges or insights offered.   Federated setting not considered adequately: Although the paper title and the abstract/introduction talk about the federated setting, the paper largely focuses on a single source and single client setting.   Inadequate baselines and experiments: The federated learning baselines used in the paper are fairly basic ones (e.g., FedAvg). Some of the experimental results are not convincing enough.  The paper received mixed scores and the reviewers engaged in discussions with the authors. However, the concerns still linger. Based on the reviews, discussion, and my own reading and assessment of the paper, I think the paper falls short of the acceptance threshold. The authors are advised to consider the reviewers  concerns to improve the manuscript for a future submission.
The paper presents a backdoor attack approach against pre trained models that may affect different downstream languages tasks with the same trigger. The paper shows that the downstream models can inherit security holes from upstream pre trained models.   The paper is on the borderline and disagreement remains after discussion and author responses. In general, the new setting introduced in the paper is interesting and well motivated. However, the options split in how realistic the setting is (e.g., use of uncommon trigger), the evaluation of stealthiness, and the novelty of the idea. After checking the paper, I believe the ideas and insights are justifiable for an ICLR paper and they differ significantly enough from the prior work. I do agree with reviewers that they are some  limitations of the proposed techniques (mostly inherited from the prior work it based on). However, as backdoor attack in NLP is a relative new area, I would be more lenient on these weaknesses.   The reviewers also provide constructive suggestions on how to improve the evaluation and writing. I hope the authors can address all the comments in the next revision.
In this work, the authors interpret the Transformer as a numerical ODE modelling multi particle convection. Guided by this connection, the authors take the Transformer that uses a feed forward net over attentions, and create a variant of transformer which instead uses an FFN attention FFN layer, thus the name macaron net. The authors present experiments in the GLUE dataset and in two MT datasets, and they overall report improved performance using their variant of Transformer. Thus, the main selling point of the paper is how seeing Transformer under his new light can potentially improve results through the construction of better models. The main criticisms from the authors is that  this story is not entirely convincing because the proposed variant departs a bit from the theory (R1 and comment about the Strang Marchuk splitting) and the papers does not consider an evaluation of accuracy of Macaron in solving the underlying set of ODEs (comment from R3). As such, I cannot recommend acceptance of this paper   I believe another set of revisions would increase the impact of this paper.
The paper extends the FNP model to multimodal settings using the mixture of graphs. However, there are legitimate concerns about the quality of experiments, such as baselines, as the reviewers mention. For example, mRNP is supervised, and comparison to DeepIMV is not fair. I encourage the authors to address them appropriately in the next version of the paper.   The authors can significantly improve the presentation of ideas. Please avoid making hyperbole and excessively bold statements, as the reviewers have pointed out. This way, there will be room for a better demonstration of the novel parts of the paper. For example, the authors misuse the term "generative" for the proposed mRNP. There are multiple hand waving statements about the role of uncertainty that are not well supported in the current draft. I believe this paper can be a good paper by addressing the reviewers  comments.
This paper adds an attention mechanism to deep variational autoencoders.  The authors develop a global + local attention method and achieve better log likelihoods than a variety of recent methods on MNIST and OMNIGLOT.  Overall the reviewers found this paper strong (8, 8, 8, 6), particularly after the author rebuttal.  They found the paper to be clear, the contribution sensible and novel and the experiments thorough and compelling.  In particular, the authors added additional experimental results on a larger dataset which addressed a common concern among the reviewers.  Thus the recommendation is to accept the paper.
The paper studies the problem of uncertainty estimation of neural networks and proposes to use Bayesian approach with noice contrastive prior.  The reviewers and AC note the potential weaknesses of experimental results: (1) lack of sufficient datasets with moderate to high dimensional inputs, (2) arguable choices of hyperparameters and (3) lack of direct evaluations, e.g., measuring network calibration is better than active learning.  The paper is well written and potentially interesting. However, AC decided that the paper might not be ready to publish in the current form due to the weakness.
This paper proposes a new approach to learning deep generative models with induced structure in the latent representation. All four reviewers gave the same score of 6 to this paper, showing a consensus that the paper is above the bar for acceptance. The authors did a commendable job of detailed replies to reviewer comments, which as R1, R3, and R4 all note has improved the clarity and quality of the paper, addressing their concerns.
While there was some interest in the analysis, the consensus view was that the original treatment was not sufficiently well motivated, and the revision was too dissimilar from the original submission for it to be evaluated for publication in this year s ICLR.
The authors show evidence that an RL agent with a new neural architecture with an external memory is superior on a version of the concentration game to a baseline.   However, other works have proposed neural architectures with episodic memories, and the reviewers feel that the proposed model was not adequately compared to these.  Furthermore, there are concerns about the novelty of the proposed model.
All reviewers feel this paper addresses and important topic, and has many merits. However, it is difficult to recommend publication at this time. The primary concern is that the paper has its theoretical optimality as an important contribution, but the reviewers and myself (in a non public thread) were unable to verify the correctness of the proofs. In part unfortunately this is due to edits to the proofs happening late in the revision period, too late for further discussion with the authors. Some of the particular questions in the proof of theorem 1 (appendix B) include: clarifying the value of $\rho$ which makes the unnumbered equation above equation (6) equivalent to definition 1, and in particular whether the $1/|X_k|$ term should be inside or outside the absolute value; and clarifying various undefined symbols which are introduced in the equation at the top of page 13, but are never defined, including $M$, $b$, and $z_i$. Reviewers also had some concern that the algorithm should be benchmarked against more recent / better performant baselines than Kamiran et al. (2012).
This manuscript proposes and analyses a weighting approach to improve the conformance of adversarial training in federated learning. The authors observe that adversarial training seems to degrade during the late stages of training, and suggest that this degradation is a consequence of exacerbated cross device bias in federated averaging. They suggest and analyze a weighted scheme to fix this issue.  During the review, the main concerns are related to the novelty of the work compared to existing work, the clarity of the technical contributions, and unclear technical statements. The authors respond to these concerns and partially satisfy the reviewers. After discussion, reviewers remain mixed, with multiple weak rejects and one strong accept. No fatal flaws are noted.   The opinion of the area chair is that while there are no fatal flaws, there is very limited enthusiasm for this paper. This limited enthusiasm seems to be a result of intuition for observed phenomena that seem incorrect or insufficient to reviewers. Overall, I think this paper outlines and addresses an interesting issue of real concern. Flaws in the intuition building/explanation, and issues with clarity of presentation need to be improved for this work to have some impact.
# Quality: The algorithm is thoroughly evaluated and several interesting experiments are included in the appendix.   # Clarity: The paper is generally well written.  # Originality: The proposed approach is a small but novel improvement over existing algorithms (to the best of the reviewers and my knowledge). The concept of "deployment efficiency" is, in my opinion not novel, since it seems mostly a rebranding of what the MBRL community traditionally refers to as "data efficiency"   although I agree that deployment efficiency is indeed a more accurate term.  # Significance of this work:  The paper deal with a relevant and timely topic. However, the paper does not compare to the larger MBRL literature. Hence, it is difficult to gauge the significance of this work.  # Overall: This manuscript offers a good contribution to the topic of model based reinforcement learning algorithms.  # Minor comments:    I suggest removing the word "impressive" from the abstract. This is a subjective term, which should be avoided.   In my personal opinion, it would be nice to include experiments with more state of the art baselines such as PETS and POPLIN, for which code is available online. It is unclear to me how much the improvement in performance depends on the algorithm itself compared to just having larger batch sizes. From this perspective, Figure 5 in Appendix B is probably the most interesting insight of the manuscript, to me.
This paper makes use of the unlikelihood objective from Welleck et al (2019) which was shown in NLP to the problem of forecasting motion trajectories on roads. The unlikelihood term is meant to lower the probability mass in non driveable areas. The paper makes use of Trajectron++, and existing trajectory forecasting model to demonstrate the idea. While the idea is interesting, the notion of using negative examples to lower the likelihood outside a valid domain has been used in multiple occasions. The paper mentions contrastive learning, but I did not see a meaningful discussion on the difference between unlikelihood training and contrastive learning, beyond what exists in the related works section. Also, due to the unlikelihood term having appeared in Welleck et al, reviewers are hesitant to acknowledge novelty of the method. One of the reviewers also questions the significance of the results, which the authors countered by saying that their method reduces the violation rate from 10.6% to 8.9% in their predictions. This is good, but combined with the former issue implies that the paper needs more work before publication.    
After reading the reviews, rebuttal, and looking through the paper I do feel that UPL setting is one that we need to consider. However is not clear to me that proposed approach matches the conditions described by the authors. In particular the scalability constraints seem important. I do feel that for the UPL setting makes sense particularly in the large case scenario of many examples and classes and how the system behaves under strict computational budgets for learning and inference. And I m wondering whether in that limit parametric models would actually becomes relevant again, and whether there is a "burn in" that one has to pay to use parametric models.  That said I don t think current approaches (CURL, AGEM etc.) will do well even in that setting, partially because they were not necessarily thought for that.  So in summary, I find the problem interesting, probably more so than the solution and particularly in an large scale setting.  However I think for the paper to have the impact it needs, and be ready for acceptance it needs a bit more. I think looking at a larger scale setting, and relying on that to motivate the problem will considerably help with its impact.  Also is not clear to me how the proposed solution scales (non parametric approaches don t always do well in large scale settings), which I think is needed for it to be convincing. 
The authors propose a semi supervised novelty detection method which tries to identify out of distribution samples in the unlabeled data (consisting of in  and out distribution samples) using a disagreement score of an ensemble. The ensemble is generated by fine tuning the trained classiifer on the labeled training data plus the unlabeled data which all get a fixed label (which is repeated several times to generate the ensemble). The main idea is that one uses early stopping based on an in distribution validation set in order to avoid overfitting on the unlabeled points which allows then identification of the out distribution points via the disagreement score.  The reviewers appreciated the simplicity of the approach and the extensive experimental results. The authors did a good job in trying to answer all questions and concerns of the reviewers.   However, some concerns remained:   the setting assumes that the OOD data is fixed which was considered as partially unrealistic and thus evaluation of the OOD detection performance on unseen OOD distributions was requested in order to understand the limitations of the method (this was only partially done by the authors).    the theoretical result is for a two layer network and completely based on previous work. As the authors use much deeper networks later on in the experiments, this result cannot be used to theoretically justify the approach.    there remained concerns about the necessary diversity of the ensemble and the early stopping procedure  While I think that the paper has its merits, it is not yet ready for publication. I encourage the authors to to take into account the above points and other remaining concerns of the reviewers in a revised version.
The authors propose an intriguing way to designing competitive online algorithms. However, the state of the paper and the provided evidence of the success of the proposed methodology is too preliminary to merit acceptance.
The paper proposed a useful incremental extension to the monotonic GMM attention by incorporating source content. It has shown comparable performance for online and long form speech recognition, but falls behind on the machine translation task. For online ASR, it would be more convincing to include latency comparisons across different streaming models besides WERs.  The presentation of the paper can be further improved although it already got better based on reviewers  comments. As in the discussion, a more accurate description of the method would be "multi head Gaussian attention" instead of GMM attention.  The main factor for the decision is limited novelty and clarity can be further improved.
this submission proposes an efficient parametrization of a recurrent neural net by using two transition functions (one large and one small) to reduce the amount of computation (though, without actual improvement on GPU.) the reviewers found the submission very positive.  please, do not forget to include all the result and discussion on the proposed approach s relationship to VCRNN which was presented at the same conference just a year ago.
This submission has been withdrawn.   The reviews are of good quality. The authors should consider writing two separate papers: one about the problem and solution from an ML perspective, and the other about the application to radiology. Papers that provide a new method in the context of a single application domain run the risk of making a contribution to neither, and of being evaluated by reviewers who are not experts in both.
the paper undoubtedly tackles an interesting problem in the mainstream of learning with partial / unknown / weak / noisy / complementary labels. The authors have had a set of constructive suggestions and questions from the reviewers (and external comments), some positive, some negative. I find it a bit unsettling that to several major questions, the main feedback from the authors was a citation in the paper with no further action; (a) ablation tests of R2 end up in citing papers from a public comment, (b) R4 raised a key point in comment 2, with the links to partial labels learning. The authors’ answer is not satisfying as one would have hoped at least of a partial justification of the author’s approach in this context. The authors would have had time to develop at least elements of a formal comparison. Just citing the work is not enough;  
This paper introduces an simple but potentially effective off policy TD algorithm.   Overall, the reviewers felt the work was incomplete and not yet ready for publication. The all recognized the authors made significant updates to the paper, but serious issues remain with the empirical work: studying the impact of the proposed extension on other algorithms, missing baselines (e.g., TDRC), scope of environments limited similar chain like domains, significant questions about how best parameter settings where chosen for comparison, etc.  This is clearly an interesting direction. If the authors can improve the experiments and better situate their method if the literature (connecting to the lit in off policy RL about accelerating and improving off policy TD methods this will become a solid contribution.
The paper presents a structured VAE, where the model parameters depend on a local structure (such as distance in feature or local space), and it uses the meta learning framework to adjust the dependency of the model parameters to the local neighborhood.  The idea is natural, as pointed by Rev#1. It incurs an extra learning cost, as noted by Rev#1 and #2, asking for details about the extra cost. The authors  reply is (last alinea in first reply to Rev#1): we did not comment (...) because in essence, using neighborhoods in a naive way is not affordable.  The area chair would like to know the actual computational time of Local VAE compared to that of the baselines.    More details (for instance visualization) about the results on Cars3D and NORB would also be needed to better appreciate the impact of the locality structure. The fact that the optimal value (wrt Disentanglement) is rather low ($10^{ 2}$) would need be discussed, and assessed w.r.t. the standard deviation.    In summary, the paper presents a good idea. More details about its impacts on the VAE quality, and its computation costs, are needed to fully appreciate its merits. 
The paper presents a deep learning approach encodes codebases as databases that conform to rich relational schemas. Based on this, a biased graph walk mechanism efficiently feeds this structured data into a transformer and deepset approach. The results shown a quite good, compared to other approaches present at ICLR. Moreover, one reviewer is strongly voting for accepting the paper, arguing that "that this paper is of significance to the ML4Code research community, as it shows how to offload the engineering cost of extracting semantic information from programs to a standard tool." Overall, I have really enjoyed reading the paper, and the use of relational database as codebase together with a transformers is sweat. On the other, it also presented in a rather engineering way, as pointed out by several reviewers, suggesting that some software engineering venue might be a better place for the work. But then ICLR had similar papers, and the present paper demonstrates a benefit of using a relational encoding. Thus, I weight the leaning towards rejects borderlines votes less and suggest an accept overall. We all should keep in mind that also deep neural architecture are full of design choices.
This paper studies the problem of estimating the value function in an RL setting by learning a representation of the value function. While this topic is one of general interest to the ICLR community, the paper would benefit from a more careful revision and reorganization following the suggestions of the reviewers.
Pros:   novel idea of endowing RL agents with recursive reasoning   clear, well presented paper   thorough rebuttal and revision with new results  Cons:   small scale experiments  The reviewers agree that the paper should be accepted.
The reviewers felt that the method was natural and the writing was mostly clear (although could be improved by providing better signposting and fixing typos). However, there was also general agreement that comparison to other methods was weak; one reviewer also points out that the way that the reported numbers compare the methods on different sets of data, which might be an inaccurate measure of performance (this is more minor than the overall issue of lack of comparisons). While the authors provided more comparison experiments during the author response, it is in general the responsibility of authors to have a close to final work at the time of submission.
This paper studies the problem of distilling the knowledge present in different GAN based image generation tasks. The paper received mixed reviews. The reviewers had difficulty understanding some details regarding the approach, and requests for ablations and clarifications on existing empirical evaluation. The authors provided a strong thoughtful rebuttal that addressed many of those concerns. The paper was discussed and two reviewers updated their reviews in the post rebuttal phase. Reviewers generally agree that the paper should be accepted but still have concerns regarding contribution and writing. AC agrees with the reviewers and suggests acceptance. However, the authors are urged to look at reviewers  feedback and incorporate their comments in the camera ready.
The paper presents a new variation of neural (re) rendering of objects, that uses a set of two deep ConvNets to model non Lambertian effects associated with an object. The paper has received mostly positive reviews. The reviewers agree that the contribution is well described, valid and valuable. The method is validated against strong baselines including Hedman et al., though Reviewer4 rightfully points out that the comparison might have been more thorough.   One additional concern not raised by the reviewers is the lack of comparison with [Thies et al. 2019], which is briefly mentioned but not discussed. The authors are encouraged to provide a corresponding comparison (as well as additional comparisons with Hedman et al) and discuss pros and cons w.r.t. [Thies et al] in the final version.
Although some reviewers still had concerns about the novelty of the proposed method, most of the other concerns have been addressed in a satisfying manner according to reviewers. They globally have a positive opinion about the paper after revision. 
This paper presents a bias/variance decomposition for Boltzmann machines using the generalized Pythagorean Theorem from information geometry. The main conclusion is that counterintuitively, the variance may decrease as the model is made larger. There are probably some interesting ideas here, but there isn t a clear take away message, and it s not clear how far this goes beyond previous work on estimation of exponential families (which is a well studied topic).  Some of the reviewers caught mathematical errors in the original draft; the revised version fixed these, but did so partly by removing a substantial part of the paper about hidden variables. The analysis, then, is limited to fully observed Boltzmann machines, which have less practical interest to the field of deep learning. 
Auto Seg Loss uses a differentiable surrogate parameterized loss function that approximates using RL some of the non differentiable metrics for segmentation. Auto Seg Loss outperform cross entropy and other loss functions through a great number of experiments. The main concerns rised by the reviewers (More clarity on the abstract and intro, extending the related work, and performance experiments) has been addressed. Accordingly I recommend the paper to be accepted at ICLR 2021.
All three reviewers recommend acceptance after the rebuttal stage, and the AC found no reason to disagree with them. The proposed method is simple and effective, and the concerns raised about experimental validation and novelty seem well addressed in the rebuttal. 
While the revised paper was better and improved the reviewers assessment of the work, the paper is just below the threshold for acceptance. The authors are strongly encouraged to continue this work.
This work proposes a method, inspired by Cellular Automata, to generate 3D objects in voxel space. By *only* using local update rule for each location, the method can probabilistic generate high resolution models of everyday objects in the dataset. Due to the ability to incrementally generate details, the quality of the samples are seemingly higher than tradition approaches using Voxel based GANs.  Most reviewers and myself agree this is a strong and interesting paper that will spark good discussion in the ICLR community. It is also well written and ideas are clearly explained. During the review process, the authors improved the work by conducting additional experiments to analyze the sensitivity of hyper parameters and took in and incorporated various suggestions from the reviewers. After the revision, I believe the work to be in good shape to be accepted at ICLR2021, and I will recommend that this paper be accepted (Poster).
The paper explores in more detail the "RL as inference" viewpoint and highlights some issues with this approach, as well as ways to address these issues. The new version of the paper has effectively addressed some of the reviewers  initial concerns, resulting in an overall well written paper with interesting insights.
The paper investigates the order of Transformer modules and its effect on performance. The proposed approach IOT, consists of several pre defined encoder and decoders with different orderings (and weight sharing), along with a light predictor which is trained to choose the best configuration per instance.    Most reviewers found the general idea of predicting the order of Transformer modules at instance level quite intriguing. Other strengths included wide range of evaluation tasks, major empirical gains, and novelty.     R1 and R4 raise valid and important concerns on validity of results when the model size and training time are controlled.   However, after carefully reading the author response and the revised paper, I feel that this issue is resolved.   The authors provide comparison with larger models, ensemble models, and models trained longer, and in all cases the gains are still obvious.    Overall, I feel that the general idea behind this paper is very exciting and could inspire more research in this direction. Therefore I recommend accept. 
This paper proposed a new optimization framework for pruning CNNs considering coupling between channels in the neighboring layers. Two reviewers suggested acceptance and two did rejection. The main concerns of the negative reviewers are (a) limited novelty, (b) limited performance metrics and (c) limited baselines. The authors  response did not fully clarify the reviewers  concerns during the discussion phase, and AC also agrees that they should be resolved to meet the high standard of ICLR. Hence, AC recommend rejection.  Here is additional thought from AC. The authors propose ours c and ours cs. The latter is reported to outperform the former in terms of FLOPs, but AC thinks the former may have merits in other more important performance metrics, e.g., the actual latency and/or memory consumption on a target device. More discussions and results for this would strengthen the paper.
The paper proposes an approach for solving constrained optimization problems using deep learning. The key idea is to separate equality and inequality constraints and "solve" for the equality constraints separately. Empirical results are given for convex QPs and for a non convex problem that arises in AC optimal power flow. There was much discussion of this paper between the reviewers and the area chair. THe key question was whether the empirical evaluation is sufficient to convince that the method is more effective than existing solvers. The current experiments do not show that the method achieves better solutions than existing solvers. For the convex case this is to be expected since solvers are optimal. But in the non convex case, it would have been nice to see that the method indeed can find better solutions. This leaves the advantage of the method in its speedup over existing methods. However, as the authors acknowledge, it is possible that this speedup is due to better use of parallelization than the methods they compare to. It is true that deep learning is particularly easy to parallelize, but this is not impossible for other methods (e.g., for linear algebra operations etc). Thus, taken together the empirical support for the current method is somewhat limited. The method itself does make sense, and this was indeed appreciated by the reviewers.   
The paper proposes a new metric to measure GAN performance by training a classifier on the true labeled dataset and then comparing the distribution of the labels of the generated samples to the true label distribution. Reviewers find that the paper is well written but lacks novelty and is quite experimental does not present any new insights. The paper investigates well known model collapse and diversity issues. Reviewers are not convinced that this is a good metric to measure sample quality or diversity as the generator can drop examples far away from the boundary and still achieve a good score on this metric.
This paper proposes to address the problem of domain adaption using Knothe Rosenblatt transport withe the method denoted as KRDA . The main idea is to perform density estimation of the different distributions with mixture of Gaussians and then estimate a  an explicit mapping between the distribution using  Knothe Rosenblatt. Experiments show that the proposed method works well on toy and real life datasets.   The paper had low score during the reviews (3,3,3,3). While the reviewers appreciated the idea, they felt that the originality of the method is not well justified compared to a number of existing UDA approaches using OT. Also the reviewers noted several important references missing and that should also be compared during the numerical experiments. A discussion about the limits of the method in high dimension would also be very interesting.  The authors did not provide a reply to the reviewers  comments so their opinion stayed t same during the discussion. The paper is then rejected and the AC strongly suggests that the authors take into account the numerous comments from the reviewers before re submitting ton a new venue.
This paper presents a learning based approach to detect and fix bugs in JavaScript programs. By modeling the bug detection and fix as a sequence of graph transformations, the proposed method achieved promising experimental results on a large JavaScript dataset crawled from GitHub.  All the reviews agree to accept the paper for its reasonable and interesting approach to solve the bug problems. The main concerns are about the experimental design, which has been addressed by the authors in the revision.   Based on the novelty and solid experiments of the proposed method, I agreed to accept the paper as other revises. 
This submission proposes a black box method for certifying the robustness of smoothed classifiers in the presence of adversarial perturbations. This work goes beyond previous works in certifying robustness for arbitrary smoothing measures.  Strengths:  Sound formulation and theoretical justification to tackle an important problem.  Weaknesses  Experimental comparison was at times not fair.  The presentation and writing could be improved.  These two weaknesses were sufficiently addressed during the discussion. All reviewers recommend acceptance.
This paper studies the problem of how to train an agent to understand relationships and dependencies among available (and potentially changing) actions in an RL environment to more efficiently solve a task. For instance, in the absence of a hammer for the task of putting up a painting on a wall, the agent could use an alternative tool like adhesive strips if available. The paper s main technical contribution is to use train a graph attention network to learn action space relationships under a given action representation. The paper demonstrates the effectiveness of this strategy on a range of environment benchmarks.  The reviewers initially brought up several lacunae in their assessment of the paper. These included the opaqueness in the explanation of the graph network structure, incremental nature of the improvement over the paper of Jain et al 2020, the lack of clear ablation studies and their message, comparisons with baselines drawn from other existing approaches potentially relevant to the setting, and the role of hyperparameters and their tuning.  In response, the author(s) provided detailed clarifications and additional experimental results. Namely, they clarified the details of the graph attention network, added ablation studies to help understand the role of this component, discussed the relevant and (in)applicability of other existing work, and supplied details about hyperparameter tuning. The author response was adequate to convince the reviewers to arrive at a consensus reflecting the positive impression of the paper.  In view of the unanimous opinion of the reviwers, I recommend acceptance of the paper.
This paper introduces a few training methods to fit the dynamics of a PDE based on observations.  Quality:  Not great.  The authors seem unaware of much related work both in the numerics and deep learning communities.  The experiments aren t very illuminating, and the connections between the different methods are never clearly and explicitly laid out in one place. Clarity:  Poor.  The intro is long and rambly, and the main contributions aren t clearly motivated.  A lot of time is spent mentioning things that could be done, without saying when this would be important or useful to do.  An algorithm box or two would be a big improvement over the many long english explanations of the methods, and the diagrams with cycles in them. Originality:  Not great.  There has been a lot of work on fitting dynamics models using NNs, and also attempting to optimize PDE solvers, which is hardly engaged with. Significance:  This work fails to make its own significance clear, by not exploring or explaining the scope and limitations of their proposed approach, or comparing against more baselines from the large set of related literature.
The submission introduces a theoretically justified solution to  client drift  in federated learning.  Generally, the reviewers agreed that this could be a strong paper, and several of their minor concerns have been addressed in the rebuttal. Unfortunately, a major issue raised during the discussion was the correctness of Lemma 7. Despite the extra clarifications provided by the authors, Reviewer2 still believes it is incorrect and R4 also sided with them.  As theoretical analysis is the major contribution of this work, we have to reject the submission. I would strongly encourage the authors to fix the issue (or clarify the proof), and resubmit it to one of the upcoming top ML conferences. 
The paper addresses the problem of prior selection in Bayesian neural networks by proposing a meta learning framework based on PAC Bayesian theory. The authors optimize a PAC bound called PACOH in the space of possible posterior distributions of BNN weights. The method does not rely on nested optimization schemes, instead, they directly minimize PAC bound via a variational algorithm called PACOH NN which is based on SVGD and the reparameterization trick.  The method is evaluated on experiments with both synthetic and real world data showing improvements in both predictive accuracy and uncertainty estimates.  Initially many reviewers were positive about the paper. However, it was noticed by one reviewer that the submitted paper presents a very significant overlap with  Jonas Rothfuss, Vincent Fortuin, and Andreas Krause. PACOH: Bayes Optimal Meta Learning with PAC Guarantees. arXiv, 2020.  Another reviewer mentioned that they were actually reviewing for AISTATS the above manuscript by Rothfuss et al. The ICLR program chairs were contacted for a possible violation of the dual submission policy for ICLR:   "Submissions that are identical (or substantially similar) to versions that have been previously published, or accepted for publication, or that have been submitted in parallel to this or other conferences or journals, are not allowed and violate our dual submission policy."   The ICLR program chairs decided that the similarities between the two papers are not enough to issue a desk rejection. However, in the discussion period,  three reviewers out of 4 pointed out that, even though the authors did revise Sections 4 and 5 in the current version, these modifications do not seem to be strong enough to make up for the really strong overlaps between the two papers. The reviewers agreed on rejection and stated that this paper should either be merged with the Rothfuss et. al. one (assuming the authors are the same), or its content should be developed to the point of making both of them clearly distinct. 
We encourage the authors to improve the mentioned aspects of their work in the reviews. 
This paper present novel formulations to address the problem of unbalanced Gromov. The Conic formulation is very interesting but stays theoretical until optimization algorithms are available. The Unbalanced Gromov is a nice extension of Gromov and comes with relatively efficient solvers. Some very limited numerical experiment show the proposed UGW used between 2D distributions (two moons) and graphs.  The paper had some mixed reviews with reviewers acknowledging the novelty of the approach (albeit an extension similar to unbalanced OT) and of the theoretical results. The detailed a very well written response to the reviewers comment has been appreciated. But all reviewers also noted a lack of numerical experiments outside of the very simple illustrations in the paper. This paper is a very nice contribution to the theory of optimal transport but fails at illustrating its relevance to the ML community.  Despite acknowledging the theoretical contributions of the paper, the  AC recommends a reject but strongly encourages the authors to complete the experimental section with some ML applications or at least proof of concepts (graph classification, domain adaptation, ...).  
The paper investigates the sensitivity of a QA model to perturbations in the input, by replacing content words, such as named entities and nouns, in questions to make the question not answerable by the document. Experimental analysis demonstrates while the original QA performance is not hurt, the models become significantly less vulnerable to such attacks. Reviewers all agree that the paper includes a thorough analysis, at the same time they all suggested extensions to the paper, such as comparison to earlier work, experimental results, which the authors made in the revision. However, reviewers also question the novelty of the approach, given data augmentation methods. Hence, I suggest rejecting the paper.
The paper proposes a Bayesian approach to learning in contextual MDPs where the contexts can dynamically vary during the episode. The authors did well in their rebuttal and alleviated most of the reviewers  concerns. During the discussion there was an agreement that the paper should be accepted. Please take all reviewer comments into account when preparing the final version.
This paper proposes an algorithm for hyperparameters optimization that exploits a formulation as an MDP and thus makes use of a model based reinforcement learning approach.  The formulation of HPO as an MDP although not novel (Jomaa et al. is not the only one to have considered this case, and the connection between the two was already known in the community) is indeed an interesting topic that could be impactful for the community. Unfortunately, the current manuscript is not providing much new insight into the topic.  After carefully reading the paper, I agree with Reviewers cKwe and 8eE4 that the current manuscript has several points of concern: 1) the formulation as a sequential decision making problem is not fully elaborated  2) lacking comparison to look ahead (i.e., non myopic) HPO algorithms (there is plenty of literature on Bayesian Optimization for doing this). This also makes it difficult to understand if the performance benefits come from the look ahead or from the MDP formulation  3) the writing is generally understandable, but some of the important design choices and details of the algorithms are not easy to find in the manuscript   improving the clarity of the text would be very beneficial.  I encourage the authors to incorporate the feedback from the reviewer and to polish this paper into the shiny gem that it deserves to be.  Suggestions:    The MDP formulation for HPO might actually prove very beneficial for hyperparameters control (i.e., dynamically adjusting parameters during the learning process) where there is a real transition function rather than hyperparameters optimization. Might be worth reading https://arxiv.org/abs/2102.13651 which attempts to do hyperparameters control in the context of MBRL.    Adding better visuals to explain formulation and algorithm might go a long way.   Tables 1 and 2 could be replaced by learning curves for a more intuitive way of visualizing the results.
This paper presents work on video scene segmentation.  The reviewers appreciated the introduction of a boundary aware pre training method.  However, concerns were raised regarding limited novelty, empirical effectiveness, and generic applicability.  The reviewers engaged in significant discussion based on the other reviews and authors  responses.  Based on these discussions the reviewers concluded that while the proposed method does have differences with respect to BSP, the overall contributions were not sufficient for inclusion in ICLR.
In this paper, the authors propose an RL based method for learning DAGs based on searching over causal orders instead of graphs. Order search for learning DAGs is a well studied problem, and it is well known that this can relieve some of the burden of searching through the space of DAGs. Several reviewers raised legitimate concerns regarding the experiments,  and without identifiability or theoretical results to advance the state of the art, the contribution of this work is limited.
This work extends the previously introduced NMN for VQA for handling reasoning over text using symbolic reasoning components that can perform counting, sorting etc and can be compositionally combined. Moreover, to successfully train the model, the authors introduce a simple unsupervised auxiliary loss for training the IE components as well heuristically incorporating inductive biases in the behaviour on couple of components. All reviews agreed that this is a challenging topic and an interesting approach to symbolic reasoning over text. At the same time, reviewers did point that experiments are borderline thin, since the authors start with DROP and drop questions that are not particularly suited for symbolic reasoning, resulting in a substantially smaller dataset. Despite the fact that the experiments could probably be stronger, I’m recommending acceptance cause this topic is very interesting and this is a good paper to raise discussions at ICLR,
In this paper, the authors propose a method to find disentangling embeddings of the structure and the attribute of the graph. Overall, this is an interesting paper and the paper is well written and easy to follow, and the paper has some merits. However, the reviewers were still not convinced by the response, and the paper is still below the acceptance threshold.  I encourage authors to revise the paper based on the reviewer s comments and resubmit it to a future venue.   
The paper seeks to obtain faster means to count or approximately count of the number of linear regions of a neural network. The paper improves bounds and makes an interesting contribution to a long line of work.   A consistent concern of the reviewers is the limited applicability of the method. The empirical evaluation can serve to better assess the accuracy of theoretical bounds that have been obtained in previous works, but the practical utility is not as clear yet.   This is a borderline case. The reviewers lean towards a positive rating of the paper, but are not particularly enthusiastic about the paper. The paper makes good contributions, but is just not convincing enough.   I think that the work program that the authors suggest in their responses could lead to a stronger paper in the future. In particular, the exploration of necessary and sufficient conditions for different neural networks to be equivalent and the use of number of linear regions when analyzing neural networks, seem to be very promising directions. 
This submission proposes a method to pass sanity checks on saliency methods for model explainability that were proposed in a prior work.  Pros:  The method is simple, intuitive and does indeed pass the proposed checks.  Cons:  The proposed method aims to pass the sanity checks, but is not well evaluated on whether it provides good explanations. Passing these checks can be considered as necessary but not sufficient.  All reviewers agreed that the evaluation could be improved and most reviewers found the evaluation insufficient.  Given the shortcomings, AC agrees with the majority recommendation to reject. 
The paper proposes a rather complex algorithm for unsupervised doamin adaptation. While the paper provides detailed explanation, some motivation and some experimental resulst, it does not provide any theoretical guarantees for its performance. More concerning, since domain adaptation can only succeed when there is a close relationship between the source and target tasks, and only with algorithms  that take that relationship into account, any scientific proposal for domain adaptation should include a clear discussion of the assumptions driving the proposed algorithms and of the circumstances under which the proposed approach  may or may not work. This is missing in the current submission.   More specifically, a similar ocncern was voiced by Reviewer 3  Namely ".The generalization error (both theoretically and empirically) of the gradient approximation is unclear. It is necessary to analyze how effective and under what conditions the proposed approximation can work for the expected target loss optimization." Thsi point was not addressed in teh authors  rebuttal.  Anotehr key concerning point that was also brought up by reviewer 3 read: "It needs elaboration why the density ratios can be directly replaced as discriminator predictions, which seems not straight forward and is the main difference to the conventional DRL." In response the authors cite the paper by Bickel et al 2007 but it falls short of addressing the well know fact that density ratio cannot be reliably estimated from samples of bounded size. The authors should have explained specific assumptions that can make this step of their algorithm og through.
This paper proposes a new training method for an end to end contract bridge bidding agent. Reviewers R2 and R3 raised concerns regarding limited novelty and also experimental results not being convincing. R2 s main objection is that the paper has "strong SOTA performance with a simple model, but empirical study are rather shallow."  Based on their recommendations, I recommend to reject this paper. 
This paper proposes an efficient training free NAS method, NASI, which exploits Neural Tangent Kernels (NTK)’s ability to estimate the performance of candidate architectures. Specifically, the authors provide a theoretical analysis showing that NAS can be realizable at initialization, and propose an efficient approximation of the trace norm of NTK that has a similar form to gradient flow, to alleviate the prohibitive cost of computing NTK. Since the method is training free, NASI is also label  and data agnostic. The experimental validation shows that NASI either outperforms or performs comparably to existing training based and training free NAS methods, while being significantly more efficient.  The below is the summary of pros and cons of the paper, after :  Pros   The idea of using NTK to predict the performance of candidate neural architectures is both novel and promising, and the proposed analysis and efficient approximation are non trivial.   The paper provides sufficient theoretical proof of its claims, including the assumptions made.   The method is highly efficient in terms of search cost, and the searched architectures obtain good performance on benchmark datasets.   The method is data/label free and thus allows transfer architectures across tasks.   The paper is well written.  Cons   There is no result on ImageNet obtained by directly applying NASI on it.   The initial reviews were split, due to other concerns regarding whether the proposed method finds good architectures, missing comparison against certain training free baselines, and some unclear descriptions. However, they were addressed away by the authors during the rebuttal period which led to a consensus to accept the paper.   In sum, this is a strong paper that proposes a novel idea for training free NAS, and the proposed method seems to be both effective, efficient, and generalizes well across tasks. One remaining concern is the computational cost of running the method on larger datasets, such as ImageNet, and I suggest the authors report the results and the running time in the final paper.  Another suggestion is to include discussion of, or comparison to other efficient NAS methods based on meta learning, such as MetaD2A [Lee et al. 21], which is not training free but is more efficient than the proposed NASI.   [Lee et al. 21] Rapid Neural Architecture Search by Learning to Generate Graphs from Datasets, ICLR 2021
1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.     The paper tackles an interesting and challenging problem with a novel approach.   The method gives improves improved performance for the surface reconstruction task.  2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.  The paper   lacks clarity in some areas   doesn t sufficiently explain the trade offs between performing all computations in the spectral domain vs the spatial domain.   3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it’s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.  Reviewers had a divergent set of concerns. After the rebuttal, the remaining concerns were:   the significance of the performance improvements. The AC believes that the quantitative and qualitative results in Table 3 and Figures 5 and 6 show significant improvements with respect to two recent methods.   a feeling that the proposed method could have been more efficient if more computations were done in the spectral domain. This is a fair point but should be considered as suggestions for improvement and future work rather than grounds for rejection in the AC s view.  4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.  The reviewers did not reach a consensus. The final decision is aligned with the more positive reviewer, AR1, because AR1 was more confident in his/her review and because of the additional reasons stated in the previous section. 
The reviewers found that the paper needs more compelling empirical study.
The paper does not provide theory or experiment to justify the various proposed relaxations. In its current form, it has very limited scope.
In the context of recurrent neural networks, the motivation of the paper is to explore the "space" between fully trained models and almost not trained models, e.g. echo state networks, using a formal approach. In fact, a modular approach has proven to be very successful in many practical applications, and in addition brain seems to adopt this strategy as well. The addressed theoretical issue is stability of the network (i.e., the network implements a contraction map.) Specifically, it is assumed that a network is composed of a set of subnetworks that meet by construction some stability condition, and the problem is to design a mixing weight matrix, interconnecting the latent spaces of the subnetworks, able to give stability guarantees during and after training. Some novel stability conditions are proposed as well as two different approaches to design a successful mixing weight matrix. The original submitted paper was not easy to read, and after revision major problems with presentation have been resolved, although the current version looks more like an ordered collection of results/statements than a smooth and integrated flow of discourse. The revision has also addressed some concerns by reviewers on the role of size and sparsity of the modules, as well as the sensitivity of the stabilization condition on the mixing weight matrix has been experimentally assessed, obtaining interesting results. Overall the paper reports interesting results, however the novelty of the contribution seems to be a bit weak, e.g. stability conditions on recurrent networks (although different from the reported ones) were already presented in literature. Also the idea of exploiting, in one of the proposed models,  the fact that the matrix exponential of a skew symmetric matrix is orthogonal to maintain the convergence condition during training, is not novel. Moreover, the experimental assessment does not provide a direct comparison, under the same architectural/learning setting, of the novel stability results versus the ones already presented in literature. Empirical results are obtained on simple tasks (using datasets with sequences of identical length), and relatively small networks, which limits a bit the scope of the assessment, as well as it is not clear if the observed improvements (where obtained) are statistically significant (especially when compared with results obtained by networks with the same order of parameters.) The quality of the assessment would increase significantly by considering datasets with sequences of different lengths, and involving more challenging tasks that do require larger networks.
This paper implements a novel architecture for inferring loop invariants in verification (though the paper bridges to compilers).  The idea is novel and the paper is well executed.  It is not the usual topic for ICLR, but not presents an important application of deep learning done well, and it has interesting implications for program synthesis.  Therefore, I recommend acceptance.
The reviewers are largely in agreement that this proposal would benefit from more clarity and comparison to key papers/findings in this space. While one reviewer is leaning towards acceptance, and their points were considered by the other reviewers, there wasn t a consensus towards aligning towardsa an acceptance. Thus, I recommend that the authors take advantage of the reviewers  comments to further improve their manuscript.
This paper studies training of the generative adversarial networks (GANs), specifically the discriminator, as a continual learning problem, where the discriminator does not forget previously generated samples. This model can be potentially used for improving GANs training and for generating synthetic datasets for evaluating continual learning methods. All the reviewers and AC agree that showing how continual learning techniques applied to the discriminator can alleviate mode collapse in GANs training is an important direction to study.  There is reviewer disagreement on this paper. AC can confirm that all three reviewers have read the author responses and have contributed to the final discussion.  While acknowledging that continual learning setting is potentially useful, the reviewers have raised several important concerns: (1) low technical novelty in light of EWC++ and online EWC methods (R1 and R3)   methodological and empirical comparison to these baselines is required to assess the difference and benefits of the proposed approach; the authors response to these concerns (and also R2’s comments in the discussion) were insufficient to assess the scope of the contribution. (2) More diverse/convincing empirical findings would strengthen the evaluation (e.g. assessing whether or not generator could help to overcome forgetting; showing that memory replay strategy by storing sufficient fake examples from previously generated samples cannot prevent mode collapse in GANs training – see the R3’s comment; showing the benefits of the generated samples for evaluating continual learning methods). (3) R1 left unconvinced that GAN training can be improved via continual learning training, as the relation between the proposed view and the minimax optimization difficulties in GANs is not addressed – see R1’s comment about this. The authors briefly discussed in their response to the review that the proposed approach is orthogonal to these works. However, a better (possibly theoretical) analysis of GANs training and continual learning would indeed help to evaluate the scope of the contribution of this work.  Regarding the available datasets that exhibit a coherent time evolution   see the Continuous Manifold Based Adaptation for Evolving Visual Domains by Hoffman et al, CVPR 2014.   Among (1) (3):  (2) and (3) did not have a substantial impact on the decision, but would be helpful to address in a subsequent revision. However, (1) makes it very difficult to assess the benefits of the proposed approach, and was viewed by AC as a critical issue.   AC suggests that in this current state the paper can be considered for a workshop and recommend to prepare a major revision before resubmitting it for the second round of reviews.    
The authors propose an approach for pre training that involves "taking notes on the fly" for rare words. The paper stirred a lively discussion on the reasons for the reported results, which the authors followed up with new experiments and findings that convinced the reviewers that indeed their approach is valid and interesting. Thus, I am recommending acceptance.
The paper addresses the important question of determining the intrinsic dimensionality, but there remain several issue, which make the paper not ready at this point: unclear exposition, lack of contextualisation of existing work and seemingly limited insights. The reviewers have provided many suggestions to improve the paper which we hope will be useful to improve the paper.
The paper got mixed scores of 4 (R1), 6 (R3), 8 (R2). R1 initially gave up after a few pages of reading, due to clarity problems. But looking over the revised version was much happier, so raised their score to 7. R2, who is knowledge about the area, was very positive about the paper, feeling it is a very interesting idea. R3 was also cautiously positive. The authors have absorbed the comments by the reviewers to make significant changes to the paper. The AC feels the idea is interesting, even if the experimental results aren t that compelling, so feels the paper can be accepted. 
The paper describes a new method to improve the generalization of model based RL by means of interventional data augmentation. The key idea is to intervene the value of a particular variable (e.g., object property) in the learned dynamic model for episode simulations. Experimental results show that it improves (i) the generalization ablity in the OoD scenarios with respect to the intervened variable, (ii) sample efficiency in the presence of unbalanced training distribution.  Strengths:    connects data augmentation to counterfactual property generation   clearly written   novel about applying counterfactual data augmentation to DYNA, as opposed to standard data augmentation techniques in other areas of machine learning   The paper well demonstrates the benefits of counterfactual data augmentation for model based RL  Weaknesses:    a lack of explanation for how the model is supervised to be equivariant to different data augmentations.   empirical results seem to suggest that the proposed data augmentation does not have much of an effect on performance   The claimed connection between the SCM and the proposed dynamic model seems vague   The technical contribution seems limited and involves very strong assumptions.   The structural causal model it introduces does not appear to be used by the method at all.   the presentation does not cleanly separate counterfactual reasoning from intervention   he greatest weakness of the method, acknowledged by the authors, is that there is no way to train the model on altered data. Thus, the performance of the policy on these altered data hinges on the extent to which the model, trained without such data, happens to make accurate predictions  All the reviewers voted for rejection. I recommend the authors to use the reviewrs  comments to improve the paper and resubmit to another venue.
The approach proposed here have raised major concerns from multiple reviewers especially concerning the novelty and the experimental validation procedure.
The paper considers test time adaptation to distribution shift which is a very important and impactful problem. The authors propose an empirical method that has different pieces, the most important ones being input transformation and confidence maximization and using likelihood ratio loss.  There were various concerns that got addressed during the rebuttal period such as, novelty of the proposed method, ablation study of different parts of the model, novelty and importance of diversity regularizer, choice of optimization. However there are still three remaining concerns that addressing them will improve the paper significantly: First, clear motivation behind the method for the cases when the model is certain but we have data imbalance. Second, analysis in the online setting of batch by batch prediction and adaptation. Third, establishing the claim regarding data subset experiment that it enable the model to adapt on a subset of data and later switch to complete execution mode without adaptation for efficient run time and improved throughput. How is the method to know the data distribution has changed, or that it has sufficiently adapted to it when the data distribution is not changing?
The reviewers were split (with all scores hovering around borderline) and I found it difficult to reach a conclusion. I like the paper, and agree with the authors that it may offer an interesting "middle ground" between bottom up and top down approaches. On the other hand, I was concerned with some of the execution flaws that were brought up in the reviews, in particular, insufficient comparisons to other embedding methods, lack of results on COCO, and to a significant degree, lack of focus in presentation. I think this could be a much stronger paper, and it will benefit from additional time to line up those missing components. To clarify the concern re: experiments on COCO, since the authors bring up computational constraints: I agree that running these experiments during the rebuttal period is not a reasonable expectation. But the conclusion is that these results should have been in the original submission! Semantic/panoptic segmentation is now a very mature area, and COCO (along with CityScapes) is one of the standard benchmarks. (BTW, "different methods often perform similarly"  on COCO and CityScapes, but not always   partially due to the significant differences in the statistics of the two datasets). I don t think it s reasonable to have a submission in this area which does not include results on it, since it makes it very hard to assess how much empirical progress is being made.
This manuscript outlines a method to improve the described under fitting issues of sequential neural processes. The primary contribution is an attention mechanism depending on a context generated through an RNN network. Empirical evaluation indicates empirical results on some benchmark tasks.  In reviews and discussion, the reviewers and AC agreed that the results look promising, albeit on somewhat simplified tasks. It was also brought up in reviews and discussions that the technical contributions seem to be incremental. This combined with limited empirical evaluation suggests that this work might be preliminary for conference publication. Overall, the manuscript in its current state is borderline and would be significantly improved wither by additional conceptual contributions, or by a more thorough empirical evaluation.
Regarding clarity, while the paper definitely needs work if it is to be resubmitted to an ML venue, different revisions would be appropriate for a physics audience. And given the above comment, any suggested changes are likely to be superfluous.
The problem studied in this paper is interesting and the high level motivation of the proposed research is reasonable. However, as pointed out by reviewers, it is not convincing that the developed components in the proposed method are able to address the issues mentioned in the high level motivation. Furthermore, the experimental results are not convincing to verify the motivations either. Though the authors provided some clarifications in the rebuttal, reviewers  major concerns still remain.    The authors are encouraged to take reviewers  concerns into consideration to revise the proposed method to make it a stronger work for future submission. Based on its current form, this work is not ready for publication at ICLR.
This paper presents a secure aggregation method to ensure byzantine robustness. The reviewers thought that the idea was interesting, but had the following concerns. * Relaxing the assumptions used in the theoretical analysis as much as possible * Run more extensive experiments I encourage the authors to their feedback into account when preparing the revised draft.   
This paper studies generalized label smoothing (GLS), which unifies positive label smoothing (PLS) and negative label smoothing (NLS), and studies its connections to existing loss functions. It also shows the benefit of NLS in the high noise regime.  Although the reviewers acknowledge that the idea of NLS in this paper is interesting, they also expressed the concerns that: the practicality of GLS is not thoroughly evaluated against prior works; the empirically best setting of parameter r is only verified in a limited number of datasets; the theoretical results  difference with prior works is limited. We encourage the authors to take the reviewers  feedback to strengthen the paper in the next iteration.
The reviewers unanimously agree that the paper is timely, well motivated and correct, with potential to significantly impact digital contact tracing. 
This paper proposes a technique to improve membership inference attacks by carefully applying "difficulty calibration" to improve the attack success rate. The reviewers are split on this paper. They all generally agree on the facts: the paper introduces a (somewhat) new technique and performs a solid evaluation, but the novelty on top of prior work isn t all that high.  On the whole I believe this paper should be accepted. This paper has identified a very clear problem with existing attacks (poor performance at low false positive rate) and has carefully developed a way to improve on this metric. A thorough evaluation has convinced the reviewers that this paper does what it set out to do.  It comes down to a question of novelty then. And here the question is this: does someone who reads this paper learn something new that wasn t obvious before? Part of this can be novelty in the method and I agree with the reviewers that this paper lacks novely in the method. However the paper does not lack novelty in the ideas on the whole. While Long et al., Sablayrolles et al., and Carlini et al. do all use some kind of low false positive rate evaluation and calibrate for low loss, none of these papers actually go out of their way to evaluate this fact explicitly. And so even if this paper had no technical contribution at all, the simple measurement study in and of itself would be a useful insight.  Machine learning research at present focuses fairly heavily on novelty of the techniques. While this is good, it s also important to go back and actually evaluate what we have. That s what this paper does, and it does it well enough to be worth accepting.  The paper would definitely be improved by following some of the advice of the reviewers and including comparisons to prior work (e.g., especially clarifying the relationship to Sablayrolles et al. and if it is true that this attack is a simplification of this prior one and is thus less effective) and I hope the authors will take the opportunity to do this.
This paper explores the connections between reward maximization (RM) with REINFORCE and distribution matching (DM) with distributional policy gradients (DPG) for fine tuning language models. Based on this, the paper proposes to apply a baseline (an idea in reinforcement learning) in DM to reduce variance and improve sample efficiency. Reviewers have concerns on the technical novelty as claimed in the paper, since the application of baseline is a straightforward practice and the resulting method is a simple addition to the existing method. More analysis (such as on the tradeoff between prior and constraint satisfaction, etc) was also suggested.
This paper presents a differentiable coarsening approach for graph neural network. It provides the empirical demonstration that the proposed approach is competitive to existing pooling approaches. However, although the paper shows an interesting observation, there are remaining novelty as well as clarity concerns. In particular, the contribution of the proposed work over the graph kernels based on other forms of coarsening such as the early work of Shervashidze et al. as well as higher order WL (pointed out by Reviewer1) remains unclear. We believe the paper currently lacks comparisons and discussions, and will benefit from additional rounds of future revisions. 
The overall consensus after an extended discussion of the paper is that this work should be accepted to ICLR. The back and forth between reviewers and authors was very productive, and resulted in substantial clarification of the work, and modification (trending positive) of the reviewer scores.
The authors first consider a mean field two player zero sum game and consider quasistatic Wasserstein gradient flow dynamics for solving the problem. The dynamics is proved to be convergent under some assumptions. Finally, the authors provide a discretization of the gradient flow and using this proposes an algorithm for solving min max optimization problems. They use this algorithm for GAN s as the main example. Experimental results claim that the algorithm outperforms langevin gradient descent especially in high dimensionas.  This paper sits right at the border. But subsequent to the author response, one of the reviewers has updated the score and seems more positive about the paper. In view of this, I am leaning towards  an accept.
The paper introduces a GAN architecture for generating small patches of an image and subsequently combining them. Following the rebuttal and discussion, reviewers still rate the paper as marginally above or below the acceptance threshold.  In response to updates, AnonReviewer3 comments that "ablation experiments do make the paper stronger" but it "still lacks convincing experiments for its main motivating use case: generating outputs at a resolution that won t fit in memory within a single forward pass".  AnonReviewer2 points to the major shortcoming that "throughout the exposition it is never really clear why COCO GAN is a good idea beyond the fact that it somehow works. I was missing a concrete use case where COCO GAN performs much better."  Though authors provide additional experiments and reference high resolution output during the discussion phase, they caution that these results are preliminary and could likely benefit from more time/work devoted to training.  On balance, the AC agrees with the reviewers that the paper contains some interesting ideas, but also believes that experimental validation simply needs more work, and as a result the paper does not meet the bar for acceptance. 
The paper proposes a GAN approach for unsupervised learning of 3d object shapes from natural images. The key idea is a two stage generative process where the 3d shape is first generated and then rendered to pixel level images. While the experimental results are promising, the experimental results are mostly focused on faces (that are well aligned and share roughly similar 3d structures across the dataset). Results on other categories are preliminary and limited, so it s unclear how well the proposed method will work for more general domains. In addition, comparison to the existing baselines (e.g., HoloGAN; Pix2Scene; Rezende et al., 2016) is missing. Overall, further improvements are needed to be acceptable for ICLR.    Extra note: Missing citation to a relevant work Wang and Gupta, Generative Image Modeling using Style and Structure Adversarial Networks https://arxiv.org/abs/1603.05631
Summary: This paper introduces a novel type of reward shaping (not potential based) that can work for MDPs with 0/1 rewards (e.g., systems with goal states). I think the paper contains a nice nice of empirical results, clear explanations/insights, and theoretical contributions. As long as the authors do a good job saying up front what kinds of MDPs the method does/does not address, this would provide an interesting addition to practical methods for solving sparse reward RL tasks. The paper could be strengthened with additional empirical results, but that is almost always true, and I think the number and quality of experiments are well above the bar. The paper could also be strengthened if it better compared with other ways of using demonstrations. This is done in the text, but not empirically, as the authors wanted to focus on reward shaping methods. I think this is a relatively minor point and does not outweigh the many positives.  Discussion: One reviewer remains against accepting this paper. I respectfully disagree with their evaluation and unfortunately they did not update their review after the responses. Hopefully they would agree the final version of this paper is indeed a useful contribution.  Recommendation: I believe this paper should be accepted based on the science. 
In general the reviewers found the work to be interesting and the results to be promising.  However, all the reviewers shared significant concerns about the clarity of the paper and the correctness of technical claims made.  This paper would significantly benefit from rewriting and restructuring the paper to improve clarity, better motivate the approach and provide more careful exposition of related work and technical claims.
The paper considers a variant of the point goal navigation problem in which the agent additionally receives an audio signal emitted from the goal. The proposed framework incorporates a form of acoustic memory to build a map of acoustic signals over time. This memory is used in combination with an egocentric depth map to choose waypoints that serve as intermediate subgoals for planning. The method is shown to outperform state of the art baselines in two navigation domains.  The reviewers all agree that the paper is very well written and that the evaluations are thorough, showing that the proposed framework offers clear performance gains. The idea of combining acoustic memory as a form of map with an occupancy grid representation as a means of choosing intermediate goals is interesting. However, the significance of the contributions and their relevance are limited by the narrow scope of the audio video navigation task, which seems a bit contrived. The paper also overstates the novelty of the work at times (e.g., being the first use of end to end learned subgoals for navigation). The author response resolves some of these concerns, but others remain.
The reviewers had some concerns regarding clarity and evaluation but in general liked various aspects of the paper. The authors did a good job of addressing the reviewers  concerns so acceptance is recommended.
The paper was seen positively by all reviewers. The strength of the paper are:   Intuitive and interesting combination of Koopman Operators and Optimal Control for Reinforcement Learning   Convincing experiments on challenging benchmark tasks   All of the issues of the reviewers (advantages to SAC, gaps in the theory and missing references) have been properly addressed in the rebuttal.  I therefore recommend acceptance of the paper.
Although the paper considers a somewhat limited problem of learning a neural network with a single hidden layer, it achieves a surprisingly strong result that such a network can be learned exactly (or well approximated under sampling) under weaker assumptions than recent work.  The reviewers unanimously recommended the paper be accepted.  The paper would be more impactful if the authors could clarify the barriers to extending the technique of pure neuron detection to deeper networks, as well as the barriers to incorporating bias to eliminate the symmetry assumption.
The paper proposes an algorithm for semi supervised learning, which incorporate biased negative data into the existing PU learning framework.  The reviewers and AC commonly note the critical limitation of practical value of the paper and results are rather straightforward.  AC decided the paper might not be ready to publish as other contributions are not enough to compensate the issue.
In general, the reviewers were lukewarm about the paper. They all acknowledged the strength of the paper: it is well written, HCCL showed (somewhat) improvements over previous methods, and it is easy to implement. However, it still feels incremental, and the improvement over the full training setting is small due to the natural limitation of consistency assumption. The AC feels that while there is merit of the proposed method, the impact seems to be limited to specific scenarios such as limited epochs.
The authors consider the problem of active plasticity in the mammalian brain, seen as being a means to enable lifelong learning. Building on the recent paper on differentiable plasticity, the authors propose a learnt, neuro modulated differentiable plasticity that can be trained with gradient descent but is more flexible than fixed plasticity. The paper is clearly motivated and written, and the tasks are constructed to validate the method by demonstrating clear cases where non modulated plasticity fails completely but where the proposed approach succeeds. On a large, general language modeling task (PTB) there is a small but consistent improvement over LSTMS. The reviewers were very split on this submission, with two reviewers focusing on the lack of large improvements on large benchmarks, and the other reviewer focusing on the novelty and success of the method on simple tasks. The AC tends to side with the positive review because of the following observations: the method is novel and potentially will have long term impact on the field, the language modeling task seems like a poor fit to demonstrate the advantages of the dynamic plasticity, so focusing on that benchmark overly much is misleading, and the paper is high quality and interesting to the community. 
Unfortunately some of the reviewers  reactions to the author feedback won t be visible to the authors. The reviewers highly appreciated the replies and revision of the paper  Pros:   The paper renders Generalized Exploration tractable for deep RL.   The idea is applicable to many DRL methods and is potentially very valuable to deal with the headaches associated to DRL.  Cons:   R2 and R4 are still concerned about whether  smart  exploration will always be advantageous, and whether the added complexity is a good trade off for the (potentially) better performance. A comparison to  pure  exploration would still be insightful.    the new  SAC with Deep Coherent Exploration  only partially addresses the concerns of R2 and R4, especially in terms of performance  While the paper has improved drastically during the reviewing process, there are still a few too many doubts.
Although sharing data between tasks benefits multitask RL, this requires that rewards be relabeled across tasks. This paper shows that, for binary rewards, directly reusing data from other tasks with constant reward relabels is effective, and the paper develops a method around this idea that is highly effective.  The reviewers found that the idea and execution were impressive, that the paper was well written, and that the empirical analysis was convincing.   In response to concerns in the preliminary reviews about certain shortcomings in the empirical analysis and some lack of theoretical analysis, the authors provided substantial revisions to the paper. Due to some lack of reviewer response to the discussion, this meta reviewer examined whether those revisions were sufficient to address the reviewers  concerns. The authors did a good job in providing the requested improvements and the analysis is stronger, but remaining similarities to existing methods (CDS) means that this paper still remains borderline. These same concerns were also shared by reviewers that continued to engage in discussion with the authors. To remedy this, the authors are encouraged to better and more substantially address differences with prior work in the writing and motivation throughout the entire paper. In addition, although space is a concern, it would be beneficial to integrate the high level takeaways from the new analyses in the appendices into the main paper.
The paper provides a neural generalization of decision trees with the idea of maintaining interpretability. The approach falls a bit short on theoretical grounds. For example, the main theorem portraying interpretability isn t properly defined and some definitions appear implicitly in the proof. The view of decision trees as a sequence of soft decisions appears to need to model how the full probability distribution over the nodes propagates at each depth. A much stronger case for interpretability (rather than assuming that each T_i is interpretable) should be made if this is kept as one of the main arguments for the architecture. Interpretability of decision trees does not directly carry over to these models.   
This paper proposes a sensor placement strategy based on maximising the information gain. Instead of using Gaussian process, the authors apply neural nets as function approximators. A limited empirical evaluation is performed to assess the performance of the proposed strategy.  The reviewers have raised several major issues, including the lack of novelty, clarity, and missing critical details in the exposition. The authors didn’t address any of the raised concerns in the rebuttal. I will hence recommend rejection of this paper.
The paper proposes "HyperGrid Transformers" a modified transformer architecture for learning a single model for multiple tasks in NLP.  The proposed method was evaluated on popular GLUE/SuperGLUE tasks and reported competitive results with the baselines (the improvements are somewhat marginal). The paper contains some interesting idea of using a decomposable hypernetwork to learn grid wise projections for different tasks, which may not be particularly novel in machine learning context but seems new for multitask NLP. Reviewers generally agree the paper is above acceptance bar, however some concerns were raised about clarity of baselines and fairness of experimental comparison as well as stronger baselines. Authors improved some of them in the rebuttal, but there is still some room to further improve the quality of presentation and writing in the final version. 
This paper describes a new method for explaining the predictions of a CNN on a particular image. The method is based on aggregating the explanations of several methods.   They also describe a new method of evaluating explanation methods which avoids manual evaluation of the explanations.  However, the most critical reviewer questions the contribution of the proposed method, which is simple.  Simple isn t always a bad thing, but I think here the reviewer has a point.  The new method for evaluating explanation methods is interesting, but the sample images given are also very simple   how does the method work when the image is cluttered?   How about when the prediction is uncertain or wrong?
This paper provides theoretical justifications on why the data augmentation technique, Mixup (convex combinations of pairs of data examples) , can help in improving robustness and generalization of GLMs and ReLUs. The authors rewrote a Mixup loss function as the summation of a standard empirical loss and some regularization terms regularizing gradient, Hessian and some higher order terms. Using the quadratic approximation of the Mixup loss (ignoring the higher order terms), the authors proved that the quadratic approximation of the Mixup loss was equivalent to an upper bound of the second order Taylor expansion of an adversarial loss, providing justifications for why Mixup loss training could improve robustness against small attacks. Using the same quadratic approximation of the Mixup loss, the regularization term controlled the hypothesis class to have a smaller Rademacher complexity.  Overall, the paper provides insightful theoretical interpretations for a commonly used data augmentation technique in DL. The paper also supports its claims by numerical experiments. Although there is some minor concerns on using the quadratic approximation of the Mixup loss, as well as R3 term s regularization effect on a broader family of models, the paper provides unique and novel insights on Mixup.; all reviewers acknowledge the authors applying the existing proof techniques to analyze Mixup s effect on robustness and generalization.  Therefore, I recommend accepting this paper.
# Quality:  While the paper presents an interesting approach, Reviewer 2 raised relevant questions about the assumption of the theoretical justification that needs to be thoroughly addressed. Moreover, as noted by Reviewer4, the quality of the paper would also benefit from a more clear connection to existing model based reinforcement learning literature, besides [Pan et al.]. For example, how much of the proposed approach and results can be applied in other algorithms?  # Clarity:  While the paper is generally well written and only minor suggestions from the reviewers should be implemented.  # Originality: The proposed approach is a small but novel improvement over existing algorithms (to the best of the reviewers and my knowledge).  # Significance of this work:  The paper deal with a relevant and timely topic. However, it is currently very difficult to gauge the significance of this work, and it unclear if the results can be extended beyond toy benchmarks and to other RL algorithms. Several reviewers suggested additional experiments to strengthen the paper.  # Overall: This paper deal with an interesting topic and presents new interesting results. However, the current manuscript is just below the acceptance threshold. Extending the experimental evaluation and improving the clarity of the paper would crucially increase the quality of the paper. 
In this paper, the authors theoretically analyzed the attacking mechanisms of the two kinds of adversarial examples in the Gaussian mixture data model case and proved that adversarial robustness can be disentangled in directions of the data manifold. The reviewers commonly felt that the idea and theoretical analysis in this paper are interesting, but experiments are not satisfactory.  At the current status, they still have a main concern regarding the correctness of comparison between the results of Theorem 4 and Corollary 3 (which is the heart of their theoretical claims, the main message of the paper and the main motivation for experiments).  As a whole, this paper has some merits but the authors still cannot clarify some concerns raised by some reviewers.  
This is an interesting contribution to the Boltzmann machine (BM) literature that makes a nice connection to DEQ models. On a positive note, reviewers found that it was well written, clear, and interesting. Unfortunately, there were significant concerns with the manuscript that were not fully addressed in the revision: inappropriate or incomplete baselines, insufficient credit given to previous works, and the fact that this model is limited as compared to its BM relatives.  I would recommend that the authors take into account the reviewers  feedback in a revision of the work.
This paper proposes a novel methodology for applying convolutional networks to spherical data through a graph based discretization.   The reviewers all found the methodology sensible and the experiments convincing.  A common concern of the reviewers was the amount of novelty in the approach, as in it involves the combination of established methods, but ultimately they found that the empirical performance compared to baselines outweighed this.
There was consensus that though  the paper introduces an interesting question, but not enough exploration has been made. The reviews point out several mathematical in accuracies, and points out  several issues including that the delta criterion needs to be examined.
This paper presents an analytic approach for estimating the optimal reverse variance schedule given a pre trained score based model. The experimental results demonstrated the efficacy of the proposed method on several datasets across different sampling budgets. Given the recent interest in score based generative models, I believe that the paper will find applications in various domains. I am pleased to recommend it for acceptance.
The authors identify a source of bias that occurs when a model overestimates the importance of weak features in the regime where sufficient training data is not available. The bias is characterized theoretically, and demonstrated on synthetic and real datasets. The authors then present two algorithms to mitigate this bias, and demonstrate that they are effective in experimental evaluations.   As noted by the reviewers, the work is well motivated and clearly presented. Given the generally positive reviews, the AC recommends that the work be accepted. The authors should consider adding additional text describing the details concerning Figure 3 in the appendix. 
The reviewers unisono do not accept the paper, because it is (a) not well written; (b) experimentally not convincing, but addresses a nice problem.  I suggest that the authors address the issues in a subsequent paper, and resubmit to one of the main conferences.
The paper empirically evaluates the effectiveness of ensembles of deep networks against adversarial examples. The paper adds little to the existing literature in this area: an detailed study on "ensemble adversarial training" already exists, and the experimental evaluation in this paper is limited to MNIST and CIFAR (results on those datasets do not necessarily transfer very well to much higher dimensional datasets such as ImageNet). Moreover, the reviewers identify several shortcomings in the experimental setup of the paper. 
The paper extends the maximum entropy inverse reinforcement learning (IRL) framework by changing the optimal criterion used in reinforcement learning (RL). This novel criterion is an expectation of the Q values over a weighted distribution over states and actions induced by a policy, which is in contrast to the standard criterion that is an expectation over the initial state distribution.  All the reviewers agree that the topic addressed in this paper is interesting and novel. On the other hand, there are some concerns about the technical novelty and relevance of the paper. Since the authors have not provided any feedback, the reviewers did not solve their concerns and they reach a consensus on rejecting this paper.
The paper proposes a personalized federated learning method, which personalizes by computing a weighted combination of neighboring compatible models. Reviewers uniformly liked the quality of writing and level of novelty, and agree on the relevance of the problem and solution. The solution was deemed creative and particularly impactful in the important case of heterogeneous data on each node, and experiments showed convincing improvements. The discussion between reviewers and authors was constructive and has lead to further improvements of the paper. Slight concerns remained on privacy with all models stored on the server, and breath of personalized FL benchmarks used, but reviewers agreed the contributions overall are still significant enough. Future work remains on the theory of the proposed model.
This paper studies test time adaptation in the context of adversarial robustness. The key idea is to use a maximin framework, which illustrates non trivial robustness (under transfer attack) using domain adversarial neural network (DANN) to Linf norm and unseen adversarial attacks. The approach is sound, well grounded, and quite logical. Results demonstrate the effectiveness.  However, there exists some limitations, for example, 1) The adaptive attack results are concerning. Comparing Table 1 and Table 3, with the adaptive attack (J FPAM), the accuracy in the homogeneous setting is below that of adversarial training (Adv S) in Table 1, which somehow echoes my concern on not testing the transductive setting using strong attacks. It seems that adversarially trained models can better defend the adaptive attacks. 2) The paper says "This threat model enables us to study whether a large test set can benefit a defender for adversarial robustness", yet there is no any experiments in the main paper that correspond to this discussion. The appendix seems lacking this discussion either. 3) Due to the page limit, a lot of details have been moved into the appendix, making the paper difficult to read.  In the end, I think that this paper may not be ready for publication at ICLR, but the next version must be a strong paper if above limitations can be well addressed.
The paper empirically investigates the behaviour of graph neural networks, as a function of topology, structural noise, and coupling between nodal attributes and structure. While the paper is interesting, reviewers in general felt that the presentation lacked clarity and aspects of the experiments were hard to interpret. The authors are encouraged to continue with this work, accounting for reviewer comments in subsequent versions. 
All reviews were negative for this paper, due to various issues. I think the main issue was that the experimental results were too weak to be convincing. For example, the reviewers were not sure if the differences in performance between different activations are significant. The reviewers also required more datasets and more experiments. The authors added std to results, more experiments and argued that the current datasets are sufficient, but the reviewers seemed to remain unconvinced.
All reviewers recommended reject, and there were no responses from authors.
The paper proposes to use a convolutional/de convolutional Q function over on screen goal locations, and applied to the problem of structured exploration. Reviewers pointed out the similarity to the UNREAL architecture, the difference being that the auxiliary Q functions learned are actually used to act in this case.  Reviewers raised concerns regarding novelty, the formality of the writing, a lack of comparisons to other exploration methods, and the need for ground truth about the sprite location at training time. A minor revision to the text was made, but the reviewers did not feel their main criticisms were addressed. While the method shows promise, given that the authors acknowledge that the method is somewhat incremental, a more thorough quantitative and ablative study would be necessary in order to recommend acceptance.
Thank you for submitting you paper to ICLR. ICLR. Although there revision has improved the paper, the consensus from the reviewers is that this is not quite ready for publication.
The authors integrate an interpolation based regularization to develop a graph neural network for semi supervised learning. While reviewers enjoyed the paper, and the authors have provided a thoughtful response, there were remaining questions about clarity of presentation and novelty remaining after the rebuttal period. The authors are encouraged to continue with this work, accounting for reviewer comments in future revisions.
All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.
The authors present a deep model for probabilistic clustering and extend it to handle time series data.   The proposed method beats existing deep models on two datasets and  the representations learned in the process are also interpretable.  Unfortunately, despite detailed responses by the authors, the reviewers felt that some of their main concerns were not addressed. For example, the authors and the reviewers are still not converging on whether SOM VAE uses a VAE or an autoencoder. Further, the discussion about the advantages of VAE over AE is still not very convincing. Currently the work is positioned as a variational clustering method but the reviewers feel that it is a clustering method which uses a VAE (yes, I understand that this difference is subtle but needs to be clarified).   The reviewers read the responses of the author and during discussions with the AC suggested that there were still not convinced about some of their initial questions. Given this, at this point I would prefer going by the consensus of the reviewers and recommend that this paper cannot be accepted.
## A Brief Summary of the Paper   In offline decentralized MARL, the discrepancy between the offline data and the interactions of agents in the environment can cause discrepancy and as a result, the policies will perform suboptimally. This issue in ORL is known as extrapolation error. This paper is trying to address the issue of extrapolation error with offline decentralized agents. It is possible to alleviate this problem by combining offline RL with online fine tuning. This paper introduces Online Transition Correction (OTC) approach to address this problem which aims to correct the biased transition dynamics with a form of importance sampling based on embedding and value based distance metrics.    ## Summary of the Reviews    Below I will outline some important concerns raised by the reviewers.    ### Reviewer T8NR  **Pros:**    Interesting and practical setting.    Extensive experiments to evaluate OTC.  **Cons:**    Lack of enough discussions about closely related works, for example, the MABCQ algorithm.     Computational cost of OTC.    Experiments: comparisons against baselines (MA ICQ), Large variance in Figure 4. Small scale (only two agent) setting, can it scale to more agents?   ### Reviewer 2xuS    **Pros:**    The bias of transition dynamics in offline decentralized MARL problem is interesting.  **Cons**    Key baselines such as [[BREMEN]] and [[MUSBO]] that looks into the deployment constrained ORL is not compared against in this paper.    The proposed OTC algorithm can easily be applied to Single agent settings. MARL vs SARL comparisons would be interesting.    Large computational cost incurred from OTC. Because of the search procedure of finding most similar examples to the examples in the dataset.  ### Reviewer a2PP **Pros:**   Well written.  **Cons:**   Analysis on the behavior of the policy, in particular on novelty seeking during the online finetuning phase.   The missing details of the transition function.   Compute constraints and budget.   Unclear experimental protocol: states vs observations...   Missing baselines.  ### Reviewer ZgL6 **Cons**   Lack of novelty.   Limited experimental results: transfer learning scenarios, lack of experiments on multiagent environments such as Starcraft II.  ## Key Takeaways and Thoughts  Overall, the authors did a good job addressing the concerns raised by the reviewers. For example, the authors ran additional experiments and compared single agent BCQ with and without OTC on some D4RL tasks. The authors gave detailed responses to the questions related to the computational cost. However, the initial submission version of this paper feels rushed as it is submitted to the conference. I would recommend the authors, go through the reviews carefully and address the points raised by the reviewers carefully in a future version of this paper. As it stands now, it is difficult to evaluate the results reported by the authors during the rebuttal, due to the lack of clarity about their experimental details.  I think the writing can be improved further. There are several typos in the paper, and most reviewers were confused about the novelty of the paper. I would recommend the authors to provide more detailed discussions about the differences from other similar approaches in the literature. Also, justify the selected experimental protocol better.
This paper begins to formalize a connection between value decomposition and difference rewards. Whilst we are in agreement with the authors that papers do not need to make new algorithmic contribution and purely theoretical papers that deepen our understanding of established methods can be significant contributions, all reviewers had doubts on the maturity of the theoretical contribution of this paper.  Given the concerns raised by the authors for the attention of the area chair, I would like to reassure the authors that the majority of reviewers engaged in discussion after the rebuttal but remained unconvinced of the significance of the theoretical results. As these are representative of the potential audience at ICLR, it is clear further improvements to the motivation given in the paper and/or weakening of the assumptions within the theory are needed to engage the interest of the wider machine learning community.  The empirical studies in the paper also seem disconnected from the theoretical contribution and more like a continuation of the paper "Qplex: Duplex dueling multi agent q learning." Given the theoretical connection to difference rewards (e.g. COMA as explicitly noted by the authors in Implication 1) I would expect these methods to be included in the experiments to demonstrate how this theoretical connection affects performance in practical applications.
This paper proposes a CNN that is invariant to input transformation, by making two modifications on top of the TI pooling architecture: the input dependent convolutional filters, and a decoder network to ensure fully transformation invariant. Reviewer #1 concerns the limited novelty, unconvincing experimental results. Reviewer #2 praises the paper being well written, but is not convinced by the significance of the contributions. The authors respond to Reviewer #2 but did not change the rating. Reviewer #3 especially concerns that the paper is not well positioned with respect to the related prior work.  Given these concerns and overall negative rating (two weak reject and one reject), the AC recommends reject.
All reviewers still argue for rejection for the submitted paper. The AC thinks that this paper should be published at some point, but for now it is a "revise and resubmit".
This paper introduces two new quantum neural networks with specific structures: TT QNNs and SC QNNs. The main contribution of this work is to show a theoretical lower bound that the gradient of the two neural networks (at random initialization) with respect to certain training objectives is well lower bounded by 2^{ 2 L}, where L is the number of layers in the network. Previously, the known work only manage to prove this lower bound with less realistic QNNs with 2 design, or prove an 2^{ poly(n)} lower bounds for random QNNs, where the input of the neural network is an n qubit. This paper makes a first step towards solving the vanishing gradient problem of QNNs at random initialization.     The major concern of the paper is the usefulness of these QNNs with proposed architectures: The proposed QNNs might be theoretically easier to train, but what if they can only learn a significantly smaller class of functions? In classical world, such phenomenons are very common: Linear classifiers (or even linear functions over prescribed feature mappings) are much easier to train and have much better theoretical properties, but they fail short in terms of representation power comparing to real neural networks.    In this paper, on the theory side, there is no argument about the representation power of these QNNs: It is unclear which set of functions they can represent efficiently, which limits their theoretical interests to machine learning committee. On the empirical side, the reviewers all agree that the empirical results are weak at this point: The proposed new QNNs did not show significant advantages over random QNNs (especially with early stopping), and other types of QNNs were not compared. Moreover, there seems to be some efficiency issue regarding implementing these QNNs   More convincing empirical evidence or theoretical evidence about the power of these QNNs need to be addressed.  
Perron Frobenius operator (P) is a well known tool which maps the density of a dynamical system at time t (p_t) to that at t+1 (p_{t+1}): p_{t+1}   P p_t. The idea has recently been extended (kernel Perron Frobenius operator (kPF); Klus et al. 2020) to map a probability measure p_Z to p_X via covariance operators (4) associated to a reproducing kernel; this corresponds to the transformation of the kernel mean embedding of Z to that of X as it is recalled in (5). The authors use the kPF technique in generative modelling to map the known prior (p_Z) to the data generating distribution (p_X), and illustrate the idea numerically.  While the focus of the paper (generative modelling) is relevant, the reviewers had several severe issues with the submission: 1) the manuscript lacks clarity of presentation at multiple points, 2) the reviewers had concerns with the scalability of the approach (which unfortunately has not been analyzed), 3) the submission is a straightforward application of a well established tool (kPF) in the literature; the paper lacks novelty.  Significantly more effort is required before publication.
The paper proposes a distributional perspective on the value function and uses it to modify PPO for both discrete and continuous control reinforcement learning tasks. The referees had noticed a number of wrong/misleading statements in the initial version of the submission, and the AC had also pointed out several problematic statements in a revised version. While the authors had acknowledged these mistakes and made appropriate corrections, there are several places that still need clear improvement before the paper is ready for publication. The paper seems to introduce a novel actor critic algorithm. However, the correctness of its key step, the  SR($\lambda)$ algorithm, has not been rigorously justified. For example, it is unclear how the geometric random variables would arise in that algorithm. For experiments, the AC seconds the comments provided by Reviewer 2 during the discussion: "The empirical comparisons are overall still lacking: for the smaller scale experiments, whilst the authors have been actively engaged in improving these comparisons during the rebuttal, at present, they are still in need of updating to make a fair comparison, for example in terms of the number of parameters included. The authors have acknowledged this, although the rebuttal period ran out before they were able to post new plots. The large scale empirical results are still lacking reasonable baselines against existing distributional RL agents."  
This paper provides a valuable survey, summary, and empirical comparison of many generalization quantities from throughout the literature. It is comprehensive, thorough, and will be useful to a variety of researchers (both theoretical and applied).
This paper proposed a transformer based routing network which removes the constraints in the original routing network such as the depth of a network. Multi Task learning (MTL) based on routing has been an interesting topic in the deep learning research community.  Our reviewers have serious concerns on the experiments. The presented empirical results do not seem to be able to sufficiently support the claims in this paper. Comparing with SOTA MTL methods is needed to make the proposed method convincing.
All the reviewers agreed that the paper lacks novelty. The overall framework is based on FOMM (Siarohin et al. 2019); the mixing operation is similar to CutMix (Yun et al. 2019). The improvements over the prior work are very subtle. R1 and R3 mentioned the paper is not well written. The rebuttal didn’t change the reviewers’ mind. In particular, the reviewers pointed out that only one out of six videos showed clear visual improvement in the added video results. After reading the paper, reviewer’s comments, the rebuttal with added results, the AC agrees with the reviewers that the paper is not ready for publication.
This paper proposes a novel method for considering translations in both directions within the framework of generative neural machine translation, significantly improving accuracy.  All three reviewers appreciated the paper, although they noted that the gains were somewhat small for the increased complexity of the model. Nonetheless, the baselines presented are already quite competitive, so improvements on these datasets are likely to never be extremely large.  Overall, I found this to be a quite nice paper, and strongly recommend acceptance, perhaps as an oral presentation.
The authors present and implement a synchronous, distributed RL called Decentralized Distributed Proximal Policy Optimization. The proposed technique was validated for pointgoal visual navigation task on recently introduced Habitat challenge 2019 and got the state of art performance.  Two reviews recommend this paper for acceptance with only some minor comments, such as revising the title. The Blind Review #2 has several major concerns about the implementation details. In the rebuttal, the authors provided the source code to make the results reproducible.  Overall, the paper is well written with promising experimental results. I also recommend it for acceptance. 
Interesting paper applying memory networks that encode external knowledge (represented in the form of triples) and conversation context for task oriented dialogues. Experiments demonstrate improvements over the state of the art on two public datasets.  Notation and presentation in the first version of the paper were not very clear, hence many question and answers were exchanged during the reviews.  
This paper is concerned with improving data efficiency in multitask reinforcement learning problems. This is achieved by taking a hierarchical approach, and learning commonalities across tasks for reuse. The authors present an off policy actor critic algorithm to learn and reuse these hierarchical policies.  This is an interesting and promising paper, particularly with the ability to work with robots. The reviewers did however note issues with the novelty and making the contributions clear. Additionally, it was felt that the results proved the benefits of hierarchy rather than this approach, and that further comparisons to other approaches are required. As such, this paper is a weak reject at this point.
This work combines steerable MLPs with equivariant message passing layers to form Steerable E(3) Equivariant GNNs (SEGNNs). It extends previous work such as Schnet and EGNNs, by allowing equivariant tensor messages (in contrast to scalar or vector messages). The paper also provides a unifying view of related work which is a nice overview for the ML community. It is overall well written, but would benefit from further revision to improve readability in some parts (in particular section 3, cf. reviews). It shows strong empirical results on the IS2RE task of the OC20 dataset and mixed results on the QM9 dataset.
This work proposes an EM type of approach for domain adaptation under covariate shift. The approach well motivated and developed and experimentally evaluated on synthetic data.  Pro:   The EM type of framework is simple and natural and  promising direction for DA, which should be explored and analyzed further.  Con:   The presentation is highly overselling the results. Both in terms of the generality of the findings and in terms references to privacy preserving properties. Both would need a solid formal analysis which this submission does not provide.   Several reviewers have stated that, while the authors promised updates to their manuscript during the author response phase, no such updated submission has been made.   The work bases their approach by referring to a well known theoretical DA bound by Ben David et al (2010). The theorem is not stated correctly. The most important component in that work is to restrict the models to a class of bounded capacity.   The claim of the authors of "solving the problem" under covariate shift are overstated. It is reasonable to expect that the authors provide a more thorough analysis of the limitations or their approach, that is, clearly state the conditions under which it would succeed and fail. Below are some references on lower bounds of DA under covariate shift.   Given that the theoretical analysis is limited, a more thorough experimental exploration would be expected.  Refs on difficulty of DA learning under covariate shift and bounded d_H distance:  Shai Ben David, Ruth Urner: On the Hardness of Domain Adaptation and the Utility of Unlabeled Target Samples. ALT 2012: 139 153  Shai Ben David, Tyler Lu, Teresa Luu, Dávid Pál: Impossibility Theorems for Domain Adaptation. AISTATS 2010: 129 136 
The paper proposes an algorithm for unsupervised skill transfer between robots with different kinematics. Integral to the approach is the idea that while the robots differ, they may use similar strategies to perform similar tasks. Without access to paired data, the paper formulates the problem of learning correspondences between robots as one of matching skill distributions across robots. Drawing insights from work in machine translation, the paper proposes an unsupervised objective that encourages the model to learn to align the distribution over skill sequences. Experimental results demonstrate the ability to use learned skill correspondences to support transfer across different robots in different domains.  As several reviewers point out, the problem of learning to transfer skills across robots with different kinematic designs from video demonstrations raises a number of interesting challenges that are relevant to the robotics and learning communities. Among them, a fundamental contribution of the paper is the ability to learn skill correspondences in an unsupervised manner based on unlabeled demonstrations. The approach by which this is achieved (i.e., using distribution matching) is sensible and clearly described. While the reviewers agree on the significance of the research problem, they raise a few key concerns regarding the initial submission. Among them are questions about the nature and extent of the domain variations that the method can handle (e.g., between robots with different degrees of freedom); the significance of the contributions; and how this work is situated in the context of existing approaches to robot skill learning. Several reviewers question the definition of morphological variation and comment that these variations may violate the assumption that task strategies are similar across designs. The authors provided detailed feedback to each of the reviews, which helps to clarify several of these concerns. Unfortunately, several reviewers did not respond to multiple requests to update their reviews. The one who did decided to maintain their score.  The paper tackles an important problem in robot learning and the work has the potential to have significant impact on the way in which robots acquire new skills. The original submission together with the author responses suggest that there is are solid contributions here. The authors are strongly encouraged to revisit the discussion of the approach to more clearly convey the novelty of the approach and to consider experimental evaluations that better support these claims.
This submission has been evaluated by 5 reviewers with 3 leaning towards borderline accept and 2 leaning towards borderline reject. Reviewers have been consistently concerned about several aspects of this work, i.e. that *the method is only demonstrated on toy datasets*, that there is an issue with the scalability to larger substructures, that the proposed approach did not excel *in the simple task of triangle counting* or even that *the authors did not perform any other experiments even on a toy dataset*, and that comparisons on Deep LRP re. efficiency were not provided, and *more complex settings and sensitivity* were not investigated. Reviewers also noted that the general idea of recursion did already appear in GNNs in one or another setting.  In making this decision, AC agrees that there is some potential in the proposed analysis and reviewers also highlighted this as a positive side of the submission. Yet, it is really hard to overlook at the same time the rebuttal where authors had the chance to address all reviewers comments regarding the experiments, their various details, and their variations.  Failing to address these comments to the satisfaction of the majority of reviewers makes it impossible for AC to recommend the acceptance even tough there is every chance that the paper will ultimately make it to a high quality venue after a thorough revision (reviewers have really given a fair number of good suggestions that should assist authors).
All reviewers expressed interest in this promising approach, but raised questions that were not addressed by the authors during the discussion period. As concerns raised included insufficient repeats of empirical experiments to draw conclusions and the paper appearing to be in an early draft format, we cannot support acceptance for publication at this time. I strongly encourage the authors to act on the feedback given to improve the paper for a future submission.  
This paper trains an expert style DNN that routes input examples to appropriate expert modules resulting in high accuracy on ImageNet with less compute. Reviewers have been positive about the strong empirical results. However the paper itself is not written well and reviewers had hard time figuring out actual architecture and training methodology. For example reviewers couldn t easily figure out the differences between LGM, WGM and SRM.   The paper itself is sparse on why some of the choices have been made, their relation to existing methods and how do they affect the final performance. For example   In eq2, TCP objective has been normalized for each expert separately with a vague No Superiority Assumption. What motivates this assumption? Why is it reasonable? Eq 4 is quite similar to the load balancing loss in Switch Transformer paper. However there has been no discussion about the similarities and differences.  I think the paper needs to rewritten with clear explanation of the actual architecture, in what aspects it is similar/differs to existing expert models. What key components are the reason for the superior performance?  While I appreciate the authors for the ablations studies they presented during response phase, I think the paper requires major rewriting and cannot recommend acceptance at this stage.
The reviewers appreciate the idea of hyperparameter planning and the thorough experimentation. Some concerns remain regarding the comparison between this method and SlowFast that require to be addressed. Also, the scope of the paper that targets hyperparameter optimization networks for action recognition specifically, may be too narrow for an ICLR audience. 
After reading the author’s response, all reviewers recommend accepting the paper.   The authors provided an extensive response carefully considering all reviewers  comments. After incorporating the feedback, the manuscript improved in terms of presentation, relation to the literature and empirical results.  The paper is very well written and motivated. On top of the insightful analysis, experimental results are strong, obtaining comparable performance to that of a ResNet 18 on ImageNet.  R1 and R3 strongly support the paper while R2 and R4 consider it borderline.  R2 raised questions about experimental details and reproducibility. While R2 did not comment, these concerns were very clearly addressed by the authors in the view of the AC.  R4 was initially concerned with the novelty of the approach, but changed their mind after the author s response. The AC encourages the authors to further consider the feedback provided by the reviewer after the discussion period was over. 
At least two of the reviewers found the proposed approach novel and interesting and worthy of publication at ICLR. The reviewers raised concerns regarding the paper s terminology, which may lead to some misunderstanding. I agree that upon a quick skim, a reader may think that the paper performs the crossover operation outlined at the bottom right of Figure 1. Please consider improving the figure and the caption to prevent such a misunderstanding. You can even slightly change the title to reflect the policy distillation operation rather than naive crossover. Finally, including some more complex baselines benefits the paper. I am curious whether performing policy gradient on an ensemble of 8 policies + periodic removal of the bottom half of the policies will provide similar gains.
The paper studies the noisy labels problem in semi supervised node classification and proposes a method that leverages pairwise interactions to explicitly force the embeddings for certain node pairs to be close to each other leading to better robustness.  The reviewers agreed the proposed method is promising. However, the reviewers also had concerns about the novelty, and that certain aspects of the method could be justified better and the experiments should consider larger scale settings to make the paper more convincing. These were the key reasons for rejection.
After discussion, all reviewers agree to accept this paper. Congratulations!!
This  paper presents an LLE based unsupervised feature selection approach. While one of the reviewers has acknowledged that the paper is well written with clear mathematical explanations of the key ideas, it also lacks a sufficiently strong theoretical foundation as the authors have acknowledged in their responses; as well as novelty in its tight connection to LLE. When theoretical backbone is weak, the role of empirical results is paramount, but the paper is not convincing in that regard.
Dropout s Dream Land: Generalization from Learned Simulators to Reality  This work explores the use of dropout inside a learned dynamics model, so that when an agent is trained inside an environment generated by this model (rather than the actual environment), the policy learned would do better in the actual environment. In a sense, this is a form of domain randomization in a learned simulator. They show that their approach has better transfer capabilities compared to the baseline World Models method, where an entropy injection via temperature adjustment is used to make transfer more effective, and this is particularly evident in the CarRacing "learn in latent simulation" experiment.  While this work is interesting to me, and I believe it has something to offer to the ICLR community, after reading the reviews and also the detailed author / reviewer discussion (and after understanding and clarifying all of the nuanced points in the experiments), the reviewers and myself believe that this paper needs more work before meeting the bar of ICLR conference acceptance. I believe the author clarified many issues and misunderstandings with the reviewers, and we have made sure the reviewers took that into consideration. Based on the reviews, I have summarized recommendations below to help the authors improve this work. There are 2 dimensions of the work that can be improved:  1) Novelty and Connections with prior works that used dropout in RL  While this work is not using MC dropout (it applies dropout inside the LSTM M), there has been sufficient work (as listed by R2) using MC dropout with RL, and the reviewers  impression that while they may not be exactly the same, it does bring to question of the novelty in this work. It is recommended to discuss not only that the approach is not MC dropout, but also connections with previous work that used MC dropout (or even other forms of dropout) dynamics models to prevent overfitting, and also discuss why this particular approach is needed (or is better, via experiments), over MC dropout, if that can also be used to generate novel dream environments. As R3 mentioned, the idea of applying dropout (of whichever form) on a learned dynamics model *is* new, and this point should be emphasized very clearly to the reader, so I recommend the authors improve the writing to incorporate discussions and relations to previous work (R2 provides a good list), and emphasize what is considered novel in this particular work.  2) Experimental design  The authors show that the proposed method offers a clear advantage over the baseline WM approach for CarRacing, where they show that DDL can do the "train in dream / deploy in real env" transfer much better than the original baseline can. For the Doom experiment, I m less concerned about the exact score compared to the baseline (as noted by R1), given the high variance of these results, and also the high randomness in the process used to collect data via a random policy, and see that their result is within the margin of statistical error. Just noting the replication effort that went in as a footnote and citing existing results, noting the high variance, should suffice.  What would really improve the work, IMO, is to compare to GameGAN on PacMan environment. Would DDL offer improvements vs GameGAN on PacMan? This will be a strong data point for the proposed method s effectiveness, compared to more trivial tasks such as DoomTakeCover. R3 also brings out a good point that the paper offers better sim to sim transfer, rather than sim to real transfer. That is another avenue to explore, if this work is to be improved.  The authors have cited PlaNet / Dreamer / SimPle papers, but mentioned that these works don t deal with the issue of reality gap, but I would argue that the iterative learning (data gathering / retraining) aspect of these algorithms is actually one method to address the gap. The tasks studied in these more recent papers, such as DM Control from Pixels, or Atari, have more datapoints, or "leaderboard" participants, using this paper s terminology, so they can also be considered if the authors wish to try DDL to see if this method can help improve the performance of these newer approaches which are based on world models with iterative training and data collection. One can explore whether this approach can lead to better sample efficiency gains, in addition to absolute performance after training, when combined with iterative training in PlaNet / Dreamer type approaches.  Overall, the work in its current form would make a good workshop paper, but I look forward to seeing more work done in the experiments to see better convincing results, in addition to clarifying the writing on related approaches and making contributions / novelty more clear, which I believe will really improve the work for a future submission.
The paper presents a new idea for detection of model stealing attacks. The new method generates "fingerprint", i.e., adversarial examples that transfer to surrogate models (extracted in model stealing attacks) but not to reference models (i.e., models obtained independently from the same data). If a model owner suspects that some model is stolen, fingerprints can be used for verifications of such claims.   The paper s contribution is novel and significant. It is the first practical tool, to my knowledge, suitable for a reliable characterization of stolen models. The empirical results are quite impressive demonstrating the detection of stolen models with an AUC   1.0. Some presentations issues have been addressed by the authors during the revision.  
 The question the authors address is relevant and interesting mostly in the UDA setting. However, there exists several recent works that have  highlighted the importance of label distribution ratio in DA (Wu et al., Combes et al. etc.), hence the main contribution of the paper is to propose a novel analysis and results in the multi source setting. That said, the paper has mixed reviews and after going through the paper, the reviews and the discussion, I tend to agree with some of the reviewers that while  the idea is interesting, the paper lacks in several points that makes it unsuitable to publication, for now.  Here are the main points leading to the decision.   A) UDA is usually the most frequent situation that occurs in domain adaptation and the most difficult to handle.  The theoretical novelty of the bound comes only from the multi source aspect that seems to be original  B) there is a strong contradiction in the paper. In the intro, they state that the paper addresses situations where conditional distributions differ. However in 4.2 they assume that they are finally equal.  In section 4.1, the authors show that for optimizing their problem, they need to have labels, mostly for estimating the class conditional distributions. When these labels are available in the target domain, the problem is pretty simple and there exists many simple baselines that can handle this problem. However, in a UDA setting, they do not have label and proposes a method for estimation label proportion by assuming S_t(z|y)   T(z|y), which is in contradiction with their initial hypotheses S_t(z|y) !  T(z|y). Hence under their assumption, the left hand side of Lemma 1 is zero and the equality is useless. Hence, I would suggest the authors to avoid such a contradiction.  Under equality of S_t(z|y)   T(z|y), the approach proposed by the authors bears strong similarity with the work of Redko et al 2019 (cited in their paper). So I would highly to recommend them to compare with that algorithm. .   C) the authors use a lot a trick related to filtering, moving average.... I guess those parts is important for making the approach works and they are not properly analyzed.  D) The paper is  confusing in its writing and somehow this confusion makes the theoretical details hard to understand. For instance, in section 3 the loss function is defined as having two variables but used one line after with only 1. In the theorem, it is not clear whether the true labelling function intervenes or how the y in h(x,y) is related to the true labels. I guess a clarification is needed here for making the soundness of the theoretical results.
The paper conveys interesting ideas but reviewers are concern about an incremental nature of results, choice of comparators, and in general empirical and analytical novelty.
Three reviewers recommended an acceptance (rating 7) while R1 deviated much from them (rating 3). After reading R1 s concerns carefully and the authors  rebuttal, I found some of the criticisms to be invalid. The authors provided a satisfactory response, addressing concerns and clarifying potential misunderstandings. Because R1 did not update the review after the rebuttal period, I am assuming the concerns have been adequately addressed. The three other reviewers all unanimously agreed that this paper tackles a timely topic, proposes a simple and effective approach, and shows convincing empirical results. I concur with the reviewers  recommendations.
In this manuacript, the authors develop feature fool attacks with feature level adversarial perturbations using deep image generators and a novel optimization objective. They further show that the feature fool attacks are versatile and can generate targeted feature level attacks at the ImageNet scale that are simultaneously interpretable, universal to any source image, and physically realizable. The reviewers agree that the paper is well motivated and the authors have addressed some concerns. However, the reviewers still do not satisfy with some concerns so as to keep the initial scores. In comparison with the manuscripts I m handling, I have to recommend to reject it!
I (and some of the reviewers) find the general motivation quite interesting (operationalizing the Gricean maxims in order to improve language generation). However, we are not  convinced that the actual model encodes these maxims in a natural and proper way.  Without this motivation, the approach can be regarded as a set of heuristics which happen to be relatively effective on a couple of datasets.  In other words, the work seems too preliminary to be accepted at the conference.  Pros:   Interesting motivation (and potential impact on follow up work)   Good results on a number of datasets Cons:   The actual approach can be regarded as a set of heuristics, not necessarily following from the maxims   More serious evaluation needed (e.g., image captioning or MT) and potential better ways of encoding the maxims  It is suitable for the workshop track, as it is likely to stimulate an interesting discussion and more convincing follow up work.  
This paper proposes a neural network based method for computing committor functions, which are used to understand transitions between stable states in complex systems. The authors improve over the techniques of Khoo et al. with a method to approximately satisfy boundary conditions and an importance sampling method to deal with rare events. This is a good application paper, introducing a new application to the ML audience, but the technical novelty is a bit limited. The reviewers see value in the paper, however scaling w.r.t. dimensionality appears to be an issue with this approach.
This work proposes to extend the invariance/equivariance properties of GNNs by focusing on distance preserving and angle preserving transformations, given respectively by the Euclidean and Conformal group. Preliminary experiments are reported that demonstrate the advantage of such architectures.  Reviewers found this work generally interesting, tackling an important problem and proposing a valid solution. However, they also raised important concerns, namely the relatively minor novelty relative to recent models (such as EGNN), as well as the lack of convincing real world experiments that would validate the modeling assumptions. Taking all these considerations into account, the AC recommends rejection at this time, and encourages the authors to address the points raised by reviewers in a revision.
The paper analyzes the performance of CNN models when data is mislabelled in different manners.  The reviewers and AC note the critical limitation of novelty of this paper to meet the high standard of ICLR.  AC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.
This paper studies the tensor principal component analysis problem, where we observe a tensor T   \beta v^{\otimes k} + Z where v is a spike and Z is a Gaussian noise tensor. The goal is to recover an accurate estimate to the spike for as small a signal to noise ratio \beta as possible. There has been considerable interest in this problem, mainly coming from the statistics and theoretical computer science communities, and the best known algorithms succeed when \beta \geq n^{k/4} where n is the dimension of v. The main contribution of this paper is to leverage ideas from theoretical physics and build a matrix whose top eigenvector is correlated with v for sufficiently large \beta using trace invariants. On synthetic data, the algorithms achieve better performance than existing methods.  The main negative of this paper is that it is not so clear how tensor PCA is relevant in machine learning applications. The authors gave some references to applications of tensor methods, but I want to point out that all of those works are about using tensor decompositions, which despite the fact that they are both about tensors, are rather different sorts of tools. Many of the reviewers also found the paper difficult to follow. I do think exposition is particularly challenging when making connections between different communities, as this work needs to introduce several notions from theoretical physics. I am also not sure how novel the methods are, since a somewhat recent paper Moitra and Wein, "Spectral Methods from Tensor Networks", STOC 2019 also uses tensor networks to build large matrices whose top eigenvalue is correlated with a planted signal, albeit for a different problem called orbit retrieval. 
The paper is proposed as probable accept based on current ratings with a majority accept (7,7,5).
# Paper Summary  This paper proposes "variance based sample weighting" (VBSW). The key observation is that, in areas where the labeling function is changing rapidly, more samples may be required to achieve a good fit. In Section 3, they justify this intuition by showing that areas in which the label gradient is higher disproportionately impact the generalization performance, and go on to propose upweighting examples proportionately to the local label variance.  This label variance is approximated, for each training example, by finding its k nearest neighbors (either in the feature space, or a latent space they use the latter in their experiments), and calculating the sample variance of the labels. There s a bit of a leap here: the authors started advocating for drawing more samples from rapidly changing regions, but ended up upweighting the existing samples. These are not the same thing, but this issue is not discussed.  The experiments are well thought out and comprehensive (although Reviewer 4 complained about a lack of nontrivial baselines, and I agree). The first and fourth set of experiments are particularly impressive. The first uses toy datasets to enable easy visualization, while the fourth (added during the response based on a comment of Reviewer 1) explores how robust VBSW is to hyperparameter choices and label noise. The second and third sets of experiments are on "realistic" problems, and while the gains are arguably marginal, VBSW does show consistently positive results.  # Pros  1. The reviewers agreed that the fundamental idea was intuitive and well explained 1. The algorithm is general, and easy to implement 1. Reviewer 1 asked how robust VBSW was to hyperparameter choices and label noise, and the authors added a new section that I think does an excellent job alleviating such concerns 1. Experiments are comprehensive, including both toy datasets on which the behavior of the algorithm can be easily visualized, and more realistic experiments on MNIST, CIFAR 10,  RTE, STS B AND MRPC. They show consistently positive results (although not especially large ones)  # Cons  1. The reviewers had mixed opinions on the writing quality, although they generally agreed that it was well organized. There are a large number of typos, grammatical errors, and awkward phrases 1. Reviewer 4 noted that Section 3 assumes that there is no label noise, which seems unrealistic 1. Reviewers 2 and 4 complained about a lack of nontrivial baselines in the experiments (reviewer 4 called this "inexcusable"). In response, the authors added a new experiment comparing with active bias, on which the results were arguably positive, but not convincingly so. Regardless, this is only one experiment there should be baselines for all experiments (except perhaps for the ones on UCI datasets), ideally multiple baselines (would a curriculum learning approach be appropriate on any of these tasks?)  # Conclusion  Three of the four reviewers recommended acceptance, but reviewer 4 gave a very negative score (3: clear rejection). Most of this reviewer s criticisms were fairly minor, with the two major ones being (i) that Section 3 unrealistically assumes that there is no label noise, and (ii) that the experiments have no nontrivial baselines. This first criticism is, I think, not a *huge* deal: Section 3 is only intended to provide intuition. The authors attempted to address the second criticism by adding one experiment comparing to "active bias", but I think that this is insufficient. In addition, while the paper is well structured, I agree with the reviewers who complained about the paper s lack of polish.  All reviewers praised the fundamental idea, and said that the authors gave good intuition for their approach. The experimental results are also fairly comprehensive (aside from the lack of nontrivial baselines), and show positive results. The new section on robustness (in response to Reviewer 1) is a great addition that, I think, fills in most of the remaining "gaps" in this work. The major outstanding issues, in my opinion, are the writing quality, and the lack of nontrivial experimental baselines. These are very fixable, but I think that they re too significant for the paper to be accepted. Overall, I think that this is a borderline paper, but it s on the rejection side of the boundary.
Reviewers mostly recommended to accept after engaging with the authors. I have decided to reduce the weight of AnonReviewer3 because of the short review. Please take reviewers  comments into consideration to improve your submission for the camera ready. 
This paper proposes a class of neural processes that lifts the limitations of conditional neural processes (CNPs) and produces dependent/correlated outputs but that, as CNPs, is inherently scalable and it is easy to train via maximum likelihood. The proposed model is extended to multi output regression and to capture non Gaussian output distributions. Results are presented on synthetic data, an electroencephalogram dataset and on a climate modeling problem. The paper parameterizes the prediction map as a Gaussian, where the mean and covariance are determined using neural networks. Non Gaussian prediction maps are obtained using copulas.   Technically speaking, the reviewers found the approach to be incremental and only marginally significant and I agree with them. Issues such as estimates of computational cost, using fixed lengthscales for the covariances and relationships/using normalizing flows have been addressed by the authors satisfactorily. Empirically, the contribution of the paper is somewhat significant, as it provides similar flexibility to other more computationally expensive processes and more general assumptions than conditional neural processes.
This work studies the performance of several end to end CNN architectures for the prediction of biomedical assays in microscopy images. One of the architectures, GAPnet, is a minor modification of existing global average pooling (GAP) networks, involving skip connections and concatenations. The technical novelties are low, as outlined by several reviewers and confirmed by the authors, as most of the value of the work lies in the empirical evaluation of existing methods, or minor variants thereof.   Given the low technical novelty and reviewer consensus, recommend reject, however area chair recognizes that the discovered utility may be of value for the biomedical community. Authors are encouraged to use reviewer feedback to improve the work, and submit to a biomedical imaging venue for dissemination to the appropriate communities.  
Reviewers unanimous on rejection. Authors don t maintain anonymity. No rebuttal from authors. Poorly written
All reviewers recommend rejection due to limited novelty and insufficient experimental analysis. The author’s response has addressed several other questions raised by the reviewers, but it was not sufficient to eliminate the main concerns about novelty (as the method is a combination of existing techniques) and missing comparisons to justify the effectiveness of the proposed approach.
The paper proposes a novel method for training a non autoregressive machine translation model based on a pre trained auto regressive model. The method is interesting and the evaluation is carried out well. It should be noted, however, that the relative complexity of the training procedure (which involves multiple stages and external supervision) might limit the practical applicability and impact of the technique.
While the paper has good quality and clarity and the proposed idea seems interesting, all three reviewers agree that the paper needs more challenging experiments to justify the proposed idea. The authors are not able to include additional experiments (such as these based on different transformations) into their revision to better convince the reviewers. In addition, the AC feels that the technical novelty of the paper is rather minor (some incremental change to VAE). In particular, related to some concerns of Reviewer 3, the AC feels the proposed idea is not too much different than introducing certain kind of side information for supervision; the main novelty seems to be distorting the data itself somehow to provide these side information (which does not seems to be that novel). 
The reviewers agreed that there are some promising ideas in this work, and useful empirical analysis to motivate the approach. The main concern is in the soundness of the approach (for example, comments about cumulative learning and negative samples). The authors provided some justification about using previous networks as initialization, but this is an insufficient discussion to understand the soundness of the strategy. The paper should better discuss this more, even if it is not possible to provide theory. The paper could also be improved with the addition of a baseline (though not necessarily something like DeepStack, which is not publicly available and potentially onerous to reimplement).  
This paper introduces a novel architecture and loss for estimating PSD matrices using neural networks.  There is some theoretical justification for the architecture, and a small scale but encouraging experiment.  Overall, I think there is a sensible contribution here, but there are so many architectural and computational choices presented together at once that it s hard to tell what the important parts are.  The main problems with this paper are: 1) The scalability of the approach O(N^3) 2) The derivation of the architecture and gradient computations wasn t clear about what choices were available and why.  Several alternative choices were mentioned but not evaluated.  I think the authors also need to improve their understanding of automatic differentiation.  Backprop through eigendecomposition is already available in most autodiff packages.  It was claimed that a certain kind of matrix derivative provided better generalization, which seems like a strong claim to make in general. 3) The experimental setup seemed contrived, except for the heteroskedastic regression experiments, which lacked competitive baselines.  Why were the GP and MLPs homoskedastic?  As a matter of personal preference, I found that having 4 different "H"s differing only in font and capitalization for the network architecture was hard to keep track of.  I agree that R1 had some unjustified comments and R2 s review was contentless.  I apologize for these inadequate reviews. 
The pros and cons of this paper can be summarized as follows:  Pros: * It seems that the method has very good intuitions: consideration of partial rewards, estimation of rewards from modified sequences, etc.  Cons: * The writing of the paper is scattered and not very well structured, which makes it difficult to follow exactly what the method is doing. If I were to give advice, I would flip the order of the sections to 4, 3, 2 (first describe the overall method, then describe the method for partial rewards, and finally describe the relationship with SeqGAN) * It is strange that the proposed method does not consider subsequences that do not contain y_{t+1}. This seems to go contrary to the idea of using RL or similar methods to optimize the global coherence of the generated sequence. * For some of the key elements of the paper, there are similar (widely used) methods that are not cited, and it is a bit difficult  to understand the relationship between them: ** Partial rewards: this is similar to "reward shaping" which is widely used in RL, for example in the actor critic method of Bahdanau et al. ** Making modifications of the reference into a modified reference: this is done in, for example, the scheduled sampling method of Bengio et al. ** Weighting modifications by their reward: A similar idea is presented in "Reward Augmented Maximum Likelihood for Neural Structured Prediction" by Norouzi et al.  The approach in this paper is potentially promising, as it definitely contains a lot of promising insights, but the clarity issues and fact that many of the key insights already exist in other approaches to which no empirical analysis is provided makes the contribution of the paper at the current time feel a bit weak. I am not recommending for acceptance at this time, but would certainly encourage the authors to do clean up the exposition, perhaps add a comparison to other methods such as RL with reward shaping, scheduled sampling, and RAML, and re submit to another venue.
This submission has been assessed by three reviewers who scored it 3/1/3, and they have remained unconvinced after the rebuttal. The main issues voiced are the difficult readability of the paper, cryptic at times due to a mix of physical and DL notations, and a lack of sufficient experimentation to support all claims. The reviewers acknowledge the authors  efforts to resolve the main issues but find these efforts insufficient. Thus, this paper cannot be accepted to ICLR2020.
The submission provides a theoretical framework on the learning of group based disentanglement representations and proposes a novel method to learn such representations.  The reviewers appreciated the novel perspective of the paper in introducing the concept of group based disentanglement in unsupervised VAE. Furthermore, the approach was considered to be soundly theoretically motivated and experiments to be extensive. There was a lively discussion between reviewers and authors about certain ambiguities in the manuscript; however, they seem to have been largely resolved to the reviewers  satisfaction.  While there was a reviewer with a very low confidence recommending rejection, this paper brings an indisputably interesting novel perspective to the learning of unsupervised representations and I thus recommend acceptance.
The reviewers generally thought the proposed algorithm was a straightforward extension of Yin et al., 2017, and not enough for a new paper.  They also objected to a lack of test results (to show generalization), but the authors did provide these in their revision.  Pros: + Adaptive batch sizing is useful, especially if the larger batches license parallelization.  Cons:   Small, incremental change to the algorithm from Yin et al., 2017   Test performance did not improve over well tuned momentum optimization, which limits the appeal of the method. 
In this paper, the authors propose to use segmentation priors for black box attacks such that the perturbations are limited in the salient region. They also find that state of the art black box attacks equipped with segmentation priors can achieve much better imperceptibility performance with little reduction in query efficiency and success rate. Hence, the auithors propose the Saliency Attack, a new gradient free black box attack, that can further improve the imperceptibility by refining perturbations in the salient region. The reviewers think that the proposed method is simple and important, and the authors have responded properly to some comments.  However, the reviewers still are not satisfied with the experimental evaluation and comparisons, as the authors can only try to compare with other ideas and test more models in the future. In summary, I think the manuscript at its current staus cannot be accepted.
In this paper, the authors propose a test for subgroup treatment effects in settings where data is obtained online, via a method they call SUBTLE.  The authors adopt a semi parametric (generalized linear model) approach to modeling nuisance functions.  The authors derive the form of the distribution of their test statistic in (12), which is based on asymptotic normality the influence function based estimator.  The authors evaluate their methods via simulation studies, and on a dataset of user clicks from Yahoo!  The opinion of the reviewers was somewhat split on this paper.  One reviewer felt the paper was out of scope for ICLR, although this did not influence the overall evaluation of the paper   since ICLR s scope has now broadened and solicits work on all areas of machine learning and related areas of data science (as the ICLR website makes clear).  However, reviewers raised a number of concerns about the paper (in particular, see reviewer 2) that on balance did not persuade them that the paper is ready for publication in the current state.
This paper proposes an approach for imitation learning from unsegmented demonstrations. The paper addresses an important problem and is well motivated. Many of the concerns about the experiments have been addressed with follow up comments. We strongly encourage the authors to integrate the new results and additional literature to the final version. With these changes, the reviewers agree that the paper exceeds the bar for acceptance. Thus, I recommend acceptance.
The manuscript brings up an important issue: that current methods and datasets don t generally highlight interactions when it comes to trajectory prediction. This is despite the fact that it would seem that current methods incorporate agent interactions and that datasets appear to require reasoning about agent interactions. This qualitative and quantitative observation should lead to better datasets in the future as well as more refined metrics pushing the field forward. Reviewers were in agreement that this is a strong submission. The authors responded with substantive new experiments that cleared up any lingering issues.
This paper proposes an 8 bit quantization strategy for rapid DNN deployment. 3 reviewers all rated this paper as marginally below acceptance threshold due to lack of novelty. 8 bit quantization (including channel wise) is a well studied task. The paper lacks comparison with peer work. 
The paper proposes several simple alternatives to generate adversarial examples for deep reinforcement learning algorithms based on image distortions such as lighting change, blurring and rotation, and show the performance of DRL agents degenerate as a result. Most reviewers appreciate the simplicity and computational efficiency of the proposed attacks. The results revealed by the work is however rather unsurprising, given similar attacks have been evaluated for DNNs. The authors did not offer much more insight on the presented results beyond that, for example, robustness of different DRL algorithms with regards to these attacks as mentioned by reviewer 2, sensitivities of the parameters for each attack proposed, effectiveness of different attacks on different environment and possible combination of attacks. 
All reviewers believed that the novelty of the contribution was limited.
This paper proposed to use Gumbel softmax to optimize the routing matrix in routing network for multitask learning.  All reviewers have a consensus on rejecting this paper.  The paper did not clearly explain how and why this method works, and the experiments are not sufficient.
The authors present an algorithm CHOCO SGD to make use of communication compression in a decentralized setting. This is an interesting problem, and the paper is well motivated and well written. On the theoretical side, the authors prove the convergence rate of the algorithm on non convex smooth functions, which shows a nearly linear speedup. The experimental results on several benchmark datasets validate the algorithm achieves better performance than baselines. These can be made more convincing by comparing with more baselines (including DeepSqueeze and other centralized algorithms with a compression scheme), and on larger datasets. The authors should also clarify results on consensus.
The authors propose a conditional GAN based approach for generating faces consistent with given input speech.  The technical novelty is not large, as the approach is mainly putting together existing ideas, but the application is a fairly new one and the experiments and results are convincing.  The approach might also have broader applicability beyond this task.
This paper analyses interpretation methods that use probes to evaluate the information in individual neurons of a deep network and shows that it confounds probe quality and ranking quality, and encoded information and used information. The paper proposes a new method which does not suffer from the same drawbacks. The reviewers were positive about this paper, and the discussion between the reviewers and authors resulted in the authors adding multiple clarifications. I ask the authors to try to optimize the paper for clarity further. I recommend acceptance.
This paper proposed Residual Gradient Compression as a promising approach to reduce the synchronization cost of gradients in a distributed settings. It provides a useful approach that works for a number of models. The reviewers have a consensus that the quality is below acceptance standard due to practicality of experiments and lack of contribution.
There is a concern from one of the reviewers that the paper needs deeper analysis. On the other hand, applying finite horizon techniques to deep RL is relatively unexplored, and the paper does provide some interesting results in that direction. 
Paper addresses the problem of sim2real (training with synthetic data and then applying the  learned model on real data) in the context of scene graph generation.  The paper was reviewed by four expert reviewers  who identified the following pros and cons of  the method.   > Pros:   Paper addresses relevant and important problem [R1, R2, R4]   Paper containing compelling results with respect to a number of baselines [R2, R4]  > Cons:   Lack of clear motivation, focus, and explanation of novelty [R4]   Missing details, which makes paper hard to follow [R3, R4]   Lack of explanations for baselines [R4]   Lack of focused analysis of specific contributions [R4]   Lack of discussion on the limitations of the approach [R1, R2]   Presented evidence is largely on toy data or very domain specific [R2]  A number of the shortcoming were addressed by the authors during the rebuttal through revisions.  However, opinion of reviewers on the paper remained split, with paper receiving  following scores:    5: Marginally below acceptance threshold   6: Marginally above acceptance threshold   7: Good paper, accept   5: Marginally below acceptance threshold  Overall, all reviewers and AC agree that the paper addresses an important and interesting problem. At the same time  AC agrees with R2 and others that point out that there are significant limitation in terms of applicability of the approach in more complex scenarios, where readily available simulator may not exist. On balance, and considering the large number of high quality submissions to ICLR this year, the paper was deemed marginally below the acceptance threshold.  
The paper aims to find locally interpretable models, such that the local models are fit (w.r.t. the ground truth) and faithful (w.r.t. the global underlying black box model). The contribution of the paper is that the local model is trained from a subset of points, selected via an optimized importance weight function. The difference compared to Ren et al. (cited) is that the IW function is non differentiable and optimized using Reinforcement Learning.   A first concern (Rev#1, Rev#2) regards the positioning of the paper w.r.t. RL, as the actual optimization method could be any black box optimization method: one wants to find the IW that maximizes the faithfulness. The rebuttal makes a good job in explaining the impact of using a non differentiable IW function.  A second concern (Rev#2) regards the interpretability of the IW underlying the local interpretable model.   There is no doubt that the paper was considerably improved during the rebuttal period. However, the improvements raise additional questions (e.g. about selecting the IW depending on the distance to the probes). I encourage the authors to continue on this promising line of search. 
This paper presents an encoder decoder based approach to construct a compressed latent space representation of each molecule. Then a second neural network segments the output and assigns an atomic number. Unlike previous works using 1D or 2D representations, the proposed method focuses on the 3D representations.  The reviewers have several major concerns. Firstly, the novelty of the paper seems to be limited as the proposed method mainly use the existing techniques. Secondly, there is no clear baseline to compare with. Finally, there is no clear quantitative results to measure the proposed method. The rebuttal did not well address these problems.  Overall, this paper did not meet the standard of ICLR and I choose to reject the paper. 
The submission proposes a complex, hierarchical architecture for continuous control RL that combines Hindsight Experience Replay, vision based planning with privileged information, and low level control policy learning. The authors demonstrate that the approach can achieve transfer of the different control levels between different bodies in a single environment.  The reviewers were initially all negative, but 2 were persuaded towards weak acceptance by the improvements to the paper and the authors  rebuttal. The discussion focused on remaining limitations: the use of a single maze environment for evaluation, as well as whether the baselines were fair (HAC in particular). After reading the paper, I believe that these limitations are substantial. In particular, this is not a general approach and its relevance is severely limited unless the authors demonstrate that it will work as well in a more general control setting, which is in their future work already.   Thus I recommend rejection at this time.
The paper proposed what is termed Relational State Space Model (R SSM) that can be used for modeling interacting time series data. The model essentially consists of a set of (nonlinear) state space models whose states are jointly evolved in a way that take into account a known interaction structure between them (the relational part, even though technically it is just a coupling structure   the term relational structure in the past has been used for models with objects and classes, for example see the difference between "coupled HMM" vs "relational HMM"). The authors also proposed a graph normalizing flow operation to model the joint state evolution. The main weakness of the paper is in the complexity of the model. However, from a modeling point of view, R SSM seems suitable in situation when the interaction structure is known, and this is demonstrated in the experimental results when comparing against the baselines. 
Two knowledgable reviewers recommend accepting the paper, and the less familiar reviewer is also positive. The final decision is to accept the paper. It s an interesting and timely topic with insightful results.
The consensus of the reviews is to accept the paper. I agree.  Reviewers highlighted many strengths, including a compelling main idea: * R5: "The paper presents an interesting and motivating case for Bayesian inference in probabilistic generative models: a problem that has inherent uncertainty along with the ability to incorporate domain knowledge that can reduce the inference complexity." * R3: "Overall, the idea is interesting and supported by correct mathematical derivations and experimental proofs of concept." * R4: "the generative approach is novel. Adding domain knowledge is relevant and significant when dealing with real world applications"  As well as compelling experiments, substantially improved in the discussion period: * R1: "The authors have shown some promising results in modeling particle dynamics." * R5: "The addition of Appendix H, in my opinion, considerably strengthens the paper s story and case for acceptance. [... T]he authors have addressed most of my major concerns."  And clear writing: * R5: "In general, the paper is well written (apart from some higher level structural issues discussed below) and the notation is clear and unambiguous." * R4: "The paper is very well written, clear"  The main weaknesses highlighted were in experiments (lacking good baselines, as well as ablations), and in discussing some choices in the model s construction. These were effectively addressed in the discussion (though R5 still points to some places that could be improved).
There is insufficient support to recommend accepting this paper.  The authors provided detailed responses, but the reviewers unanimously kept their recommendation as reject.  The novelty and significance of the main contribution was not made sufficiently clear, given the context of related work.  Critically, the experimental evaluation was not considered to be convincing, lacking detailed explanation and justification, and a sufficiently thorough comparison to strong baselines, The submitted reviews should help the authors improve their paper.
The authors present a toy stacking task where the goal is to stack blocks to match a given configuration, and a method that is a slightly modified DQN algorithm where the target configuration is observed by the network as well as the current state. There are a few problems with this paper. First, the method lacks novelty   it is very similar to DQN. Second, the claims of learning physical intuitions is not borne out by the method or experimental results. Third, the tasks are very simple and there is no held out test set of target configurations. 
This paper proposes a generalization of the standard Transformer attention mechanism in which keys and queries represent abstract concepts (which must be specified a priori). This in turn yields "concept embeddings" (and logits) as intermediate network outputs, providing a sort of interpretability. Reviewers agreed that this is a simple (in a good way) and interesting approach, and may lead to follow up work that builds on this architecture.   Some concerns regarding the relation of this method to prior work—in particular the "Concept Bottleneck" model—were raised and addressed in discussion; the authors might incorporate additional discussion in future drafts of the work.
This paper proposes a method that uses subgoals for planning when using video prediction. The reviewers thought that the paper was clearly written and interesting. The reviewer questions and concerns were mostly addressed during the discussion phase, and the reviewers are in agreement that the paper should be accepted.
This paper presents a graph neural network based approach to forecasting multiple time series. It incorporates structure learning similar in some ways to NRI (Kipf et al.) and a recurrent graph convolution forecaster given the inferred graph based on DCRNN (Li et al.). The paper shows consistently improved performance over several different kinds of baselines (classical, deep learning, and graph based deep learning) on three datasets, two of which are public and one proprietary.  Reviewer 1 thought the paper was “well presented and easy to understand”. I agree. The reviewer liked the simplicity of the approach but wondered whether the empirical improvement was sufficient. The reviewer asked about applicability outside the time series domain and the authors provided a satisfying response.  Reviewer 3 thought that simultaneously learning graph structure and forecasting was an “understudied topic”. The reviewer pointed out several strengths including: the end to end nature of the approach, the reduction in training cost due to the direct parameterization of the adjacency matrix structure, the optimal structural regularization scheme, and the extensive experimentation. Like R1, they thought the paper was well written. They suggested several points of improvement, which were mainly seeking clarification. They made a good point regarding the PMU dataset in that only one month was considered, which was not enough to capture long term seasonalities. This seems like a limitation to me. The authors responded to each of the points in turn. Regarding the point about the PMU dataset, the authors stated that the data was extremely noisy, so they settled on a month that was comparatively clean.  The review from R4 was not as positive as the other reviews. R4 proposed that the present work may be a “simplification of NRI” rather than being novel/different because NRI was a windowed based approach, while GTS was based on the entire time series. The reviewer also pointed out what appears to be a highly relevant paper (Wu et al.); though this appeared in KDD 2020 and in my opinion it’s understandable if the authors missed it. Finally, R4 raised several issues with the empirical evaluation. They make a very good point that the analysis on regularization used the kNN graph where a “ground truth” graph was not available. Why not evaluate the regularizer on the other datasets where ground truth is available? The authors responded with an updated paper addressing this point. The authors responded to other points of criticism and a fairly extensive debate ensued. The key points of the debate were: A difference in opinion between the significance in departure between the structure learning mechanism in this work (GTS) and NRI (Kipf et al.) Overlap between a recently proposed paper (Wu et al., KDD 2020). I am fairly sympathetic with the authors here. That work is fairly late breaking and they do point out a key difference: "the advantage of our method over MTGNN is clear: we can get structures we desire, not restricted to a degree k graph. This advantage is an important contribution, because MTGNN hard wires the graph parameterization and enforces that all nodes have the same number of (out) neighbors. What if one desires instead a graph that approximately obeys spatial proximity, like that in the case of METR LA? Our method yields such a graph." Some minor concerns with Fig. 2 and the structural prior/regularization analysis  I have read the paper, and while R4’s concerns are legitimate, I think this paper is clearly over the bar. I support this paper’s acceptance and ask the authors to take the reviews into consideration when revising their paper. As R4 suggests, they could add a controlled set of experiments on a perhaps synthetic dataset to show that the proposed GTS is better than NRI. If the scalability of NRI is a concern, then this can be highlighted to the same extent as LDS.
The paper addresses generalization to compositions of rare and unseen sequences. It proposes an unstructured data augmentation, that achieves comparable generalization to structured approaches (e.g. using grammars). The idea is based on recombining prototypes and oversampling  in the tail.   The paper provides a novel approach to an important problem. All four reviewers recommended accept.   
The work presents a method of imposing harmonic structural regularizations to layers of a neural network. While the idea is interesting, the reviewers point out multiple issues.  Pros: + Interesting method + Hidden layer coherence tends to improve  Cons:   Deficient comparisons to baselines or context with other works.   Insufficient assessment of impact to model performance.   Lack of strategy to select regularizers   Lack of evaluation on more realistic datasets
This paper proposes a new design space for initialization of neural networks motivated by balancing the singular values of the Hessian. Reviewers found the problem well motivated and agreed that the proposed method has merit, however more rigorous experiments are required to demonstrate that the ideas in this work are significant progress over current known techniques. As noted by Reviewer 2, there has been substantial prior work on initialization and conditioning that needs to be discussed as they relate to the proposed method. The AC notes two additional, closely related initialization schemes that should be discussed [1,2]. Comparing with stronger baselines on more recent modern architectures would improve this work significantly.  [1]: https://nips.cc/Conferences/2019/Schedule?showEvent 14216 [2]: https://arxiv.org/abs/1901.09321.
Three referees support accept and one indicates reject. The issues pointed out by the reviewer who proposed rejection should be properly reflected in the final version.  First, regarding the synthetic experiment that illustrates the shortcomings of the existing GNN models, three reviewers, including myself, judged quite interesting. However, note the opinion of one reviewer that it is more appropriate to separate the influence of feature x and graph structure in the label generation method and each independently contribute to label generation. This part should be more justified in the final version.  In addition, it was pointed out that the expressive power of the model may be limited according to the parameterization type of the precision matrix, and there is a limitation that there may be a disadvantage in inference because it is a copula based probabilistic model. I think this characteristic is actually a fundamental limitation of the proposed method. However, three reviewers, including myself, thought that it was an interesting framework as a role that can complement the message passing architecture, and decided that the possibility of the proposed method was worth publishing. However, in order to reinforce this argument a little more, it would be better if the final version verifies it with more diverse GNN architectures and datasets.
The paper focuses on learning speech representations with contrastive predictive coding (CPC). As noted by reviewers, (i) novelty is too low (mostly making the model bidirectional) for ICLR (ii) comparison with existing work is missing.
This submission presents a technique to improve generalization of urban scenes segmentation.  Based on a pre trained deep net on synthetic data, the approach aims at adapting statistics on real target domain such as Cityscapes, BDD or IDD datasets using an Instance adaptive Batch Normalization (IaBN) at test time. Results are reported on several synthetic to real scenarios.  Most of the reviewers were not convinced by the approach and have raised several issues. After rebuttal and discussion, no one really changed her/his mind. The novelty of the proposed method is limited to the use of the existing IaBN in this context except the one sample adaptation.  Although the proposed method is effective on some benchmarks, the extra processing time may be a significant limitation. Additional comparisons are necessary. We encourage the authors to consider the reviewers feedbacks for future publication.
The revised paper is a solid improvement.  However, all reviewers and I find that there are still a number of issues that prevent the paper from being acceptable at the current stage.  For example, some important parts are still unclear, especially the definition of STI effect.  The observation of STI effect requires more theoretical or empirical investigation, in addition to a toy example.
 The paper investigates mixed integer linear programming methods for neural net robustness verification in presence of adversarial attckas. The paper addresses and important problem, is well written, presents a novel approach and demonstrates empirical improvements; all reviewers agree that this is a solid contribution to the field.
This paper proposes to speed up Bayesian deep learning at test time by training a student network to approximate the BNN s output distribution. The idea is certainly a reasonable thing to try, and the writing is mostly good (though as some reviewers point out, certain sections might not be necessary). The idea is fairly obvious, though, so the question is whether the experimental results are impressive enough by themselves to justify acceptance. The method is able to get close to the performance achieved by Monte Carlo estimators with much lower cost, although there is a nontrivial drop in accuracy. This is probably worth paying if it achieves 500x computation reduction as claimed in the paper, though the practical gains are probably much smaller since Monte Carlo methods are rarely used with 500 samples. Overall, this seems a bit below the bar for ICLR. 
All reviewers rated this paper as a weak reject. The author response was just not enough to sway any of the reviewers to revise their assessment. The AC recommends rejection.
The authors use the Cayley transform representation of an orthogonal matrix to provide a parameterization of an RNN with orthogonal weights.   The paper is clearly written and the formulation is simple and elegant.  However,  I share the concerns of reviewer 3 about the significance of another method for parameterizing orthogonal RNN, as there has not been a lot of evidence that these have been useful on real problems (and indeed, on most of the toys used show the value of orthogonal RNN, one can get good results just by orthogonal initialization, e.g. as in Henaff et. al. as cited in this work).    This work does not compare experimentally against many of the other methods, e.g. https://arxiv.org/pdf/1612.00188.pdf,  the two Jing et. al. works cited, simple projection methods (either full projections at each step or stochastic projections as in Henaff et. al.).  It does not cite or compare against the approach in https://arxiv.org/pdf/1607.04903.pdf.  
I m quite concerned by the conversation with Anonymous, entitled "Why is the dependence...". My issues concern the empirical Rademacher complexity (ERC) and in particular the choice of the loss class for which the ERC is being computed. This  class is obviously data dependent, but the Reviewers concerns centers on the nature of its data dependence. It is not valid to define the classes by the Jacobian s norm on the input data, as this _structure_ over the space of classes is data dependent, which is not kosher. The reviewer was gently pushing the authors towards a very strong assumption... i m guessing that the jacobian norm over all data sets was bounded by a particular constant. This seems like a whopping assumption. The fact that I can so easily read this concern off of the reviewer s comments and the authors seem to not be able to understand what the reviewer is getting at, concerns me.  Besides this concern, it seems that this paper has undergone a rather significant revision. I m not convinced the new version has been properly reviewed. For a theory paper, I m concerned about letting work through that s not properly vetted, and I m really not certain this has been. I suggest the authors consider sending it to COLT.
This paper proposes to use constrained Bayesian optimization to improve the chemical compound generation. Unfortunately, the reviewers raises a range of critical issues which are not responded by authors  rebuttal. 
The reviews are split. The most significant concern seems to be the narrow focus of the paper: insensitivity of a very specific architecture, ViT to some patch based transformations of the image. The paper aims to "understand and improve" the behavior of ViTs in this respect, but as the reviewers point out, the understanding (what exactly is the mechanism for this insensitivity) is lacking. Furthermore, there is a good reason to believe that other transformer architectures might not have a similar behavior. Ultimately both the lack of depth and the lack of breadth of the investigation suggest that the impact may be limited. I think this is not a good fit for ICLR.
This paper proposes to speed up finetuning of pretrained deep image classification networks by predicting the success rate of a zoom of pre trained  networks without completely running them on the test set. The idea is that a sensible measure from the output layer might well correlate with the performance of the network. All reviewers consider this is an important problem and a good direction to make the effort. However, various concerns are raised and all reviewers unanimously rate weak reject. The major concerns include the unclear relationship between the metrics and the fine tuning performance, non  comprehensive experiments, poor writing quality. The authors respond to Reviewers’ concerns but did not change the major concerns. The ACs concur the concerns and the paper can not be accepted at its current state.
This work presents a novel approach to improving text decoding. This is backed up by a solid analysis of cross entropy growth with top k vs top p and an interesting demonstration of repetition correlating with probability. The paper is well written and well organized. The authors  rebuttal was effective in convincing the reviewers. The human evaluation (added during the rebuttal phase) is a good demonstration of the effectiveness of the approach and so this paper s proposed decoding algorithm is likely to be impactful.  Pros:   Well written.   Solid theoretical analysis of cross entropy and its relation to top p and top k decoding. Good demonstration of how repetition is related to probability.   Interesting, novel and effective decoding algorithm.   Human evaluation of the algorithm s output.  Cons:   The approach has not been tested with a variety of language models.   Decoding quality still depends on a target perplexity which may need to be tuned.   Unnecessary dependence on Zipf s law in the basic decoding algorithm.
This paper analyzes the non convergence issue in Adam in a simple non convex case. The authors propose a new adaptive gradient descent algorithm based on exponential long term memory, and analyze its convergence in both convex and non convex settings. The major weakness of this paper pointed out by many reviewers is its experimental evaluation, ranging from experimental design to missing comparison with strong baseline algorithms. I agree with the reviewers’ evaluation and thus recommend reject.
While the reviewers in general liked the ideas proposed in the paper, the experimental evaluation has several issues that need fixing before it can be accepted.
This submission proposes to use neural networks in combination with pairwise choice markov chain models for choice modelling. The deep network is used to parametrize the PCMC and in so doing improve generalization and inference.  Strengths: The formulation and theoretical justifications are convincing. The improvements are non trivial and the approach is novel.  Weaknesses: The text was not always easy to follow. The experimental validation is too limited initially. This was addressed during the discussion by adding an additional experiment.  All reviewers recommend acceptance. 
This paper presents work on semantic action conditioned video prediction.  The reviewers appreciated the interesting task and use of capsule networks to address it.  Concerns were raised over generalization ability of the proposed approach, points on clarity, scalability, and handling of uncertainty/diversity by the method.  After reading the authors  response, the reviewers engaged in discussion.  Over the course of this discussion, the reviewers converged on a reject rating, noting that the concerns raised above were not sufficiently addressed to warrant publication at this stage.
The paper studies reinforcement learning in the presence of (adversarial) perturbations in the underlying system dynamics. The main (novel) observation is that  agents trained against a single policy may overfit  to that policy and hence will lack robustness to new/unseen policies. The paper proposes a population based augmentation to the Robust RL formulation in which a population of adversaries are randomly initialized and samples from during training. The authors seek to show that their method generalizes well to unseen policies at test time.  Most reviewers agree that the paper provides a range of solid experimental results (with in distribution and out of distribution tasks) showing robustness and generalization of their methods on several robotics benchmarks while avoiding a ubiquitous domain randomization failure mode. However, all the reviewers (and myself) agree that some of the conceptual claims of the paper may not be precise. For example, some of the reviewers disagree with the authors on finding the (mixed) Nash equilibria. Such general claims are hard to validate (may not even be true) and need theoretical justification. Hence, it is not conceptually clear why using multiple adversaries would not suffer from the same limitations as in the single adversary case.  Also, in the discussion phase, the reviewers agreed that the results/claims of the paper (i.e. overfitting to a single adversary and the need for multiple adversaries) are very interesting, but at the same time need to be confirmed by more extensive experiments.    Indeed, if the above are addressed, the paper would make a strong contribution to the area of RL.   
This paper presents a new benchmark for evaluating continual learning(CL) algorithms on transferability and scalability. It also introduces a data driven prior to reduce the architecture searching space. Experiments show the new benchmark helps to analyze the properties of CL algorithms and the proposed algorithm performs better than baselines.  The reviewers raised concerns about evaluation metric, weak baselines, and limited experimental cases for evaluating transferability. The authors added more experiments with stronger baselines and revised the paper based on the reviewers  suggestions. However, the authors also admit that how to evaluate transferability is still an open question.   Despite the concerns, the reviewers generally agreed that the paper is well written, and the new benchmark is an important contribution for evaluating continual learning algorithms on transferability and scalability.  Hence it makes a worthwhile contribution to ICLR and I m recommending acceptance of the paper.
This paper presents a method that hybridizes the strategies of linear programming and interval bound propagation to improve adversarial robustness.  While some reviewers have concerns about the novelty of the underlying ideas presented, the method is an improvement to the SOTA in certifiable robustness, and has become a benchmark method within this class of defenses.
The reviewers think the proposed method is well motivated and interesting. However, the novelty needs to be improved. At the moment, the paper seems to be a minor improvement over existing works.
This paper demonstrates that deep networks can memorize adversarial examples of training data with completely random labels, which motivates some analyses on the convergence and generalization of adversarial training (AT). The authors identify a significant drawback of memorization in AT that could result in robust overfitting and propose a new algorithm to mitigate this drawback. Experiments on benchmark datasets validate the effectiveness of the proposed algorithm. One of the reviewers is concerned about (1) the validity of stability analysis where 80% of the data labels are noisy, and the perturbation (64/255) is large, (2) the gap between theory and practice, and (3) novelty. The authors have made a great effort to address these concerns. Although there is still no consensus after the author s response, the majority of the reviewers are in strong support. I, therefore, recommend acceptance.
The reviewers agreed that this is a technically novel and interesting paper with results for a very natural problem and all voted for acceptance. The paper gives more evidence for the wide ranging compatibility between the goals of sketching and of privacy.
This paper proposes to improve the exploration in the PPO algorithm by applying CMA ES. Major concerns of the paper include: paper editing can be improved; the choices of baselines used in the paper may be not reasonable; flaws in comparisons with SOTA. It is also not quite clear why CMA can improve exploration, further justification required. Overall, this paper cannot be published in its current form.
The meta reviewer agrees with the reviewers that this is a marginal case. Conditioned on the quality of content and comparisons to other works: Constrained Reinforcement Learning With Learned Constraints (https://openreview.net/forum?id akgiLNAkC7P) Parrot: Data Driven Behavioral Priors for Reinforcement Learning (https://openreview.net/forum?id Ysuv WOFeKR) PERIL: Probabilistic Embeddings for hybrid Meta Reinforcement and Imitation Learning (https://openreview.net/forum?id BIIwfP55pp)  We believe that the paper is not ready for publication yet. We would strongly encourage the authors to use the reviewers  feedback to improve the paper and resubmit to one of the upcoming conferences. 
Meta learning for offline RL is an understudied topic with lots of potential impact in the research community. This paper takes the first stab against that challenging problem by proposing a solution similar based on PEARL and distance metric learning. The results look good and it seems like the authors have addressed some of the concerns raised by the reviewers. As a result, I suggest to accept this paper.  However, this paper still has some shortcomings as reviewers suggested more baselines with more experiments on standardized benchmarks such as D4RL or RL Unplugged could make the paper stronger.
The paper gives a way of constructing a dataset of programs aligned with invariants that the programs satisfy at runtime, and training a model to predict invariants for a given program.  While the overall idea behind the paper is reasonable, the execution (in particular, the experimental evaluation) is problematic. As a result, the paper cannot be accepted in its present form. Please see the reviews for more details.
The paper proposed new version of LSTM which is claimed to abandon the redundancies in LSTM.  It is weak both in theory and experiments.  All reviewers gave clear rejects and the AC agree.
The paper presents an approach to multi agent coordination using goal driven exploration on subspaces of the observation space.  The results of the paper show that the authors  approach performs baselines on grid worlds and two tasks from the StartCraft Multi agent Challenge. While the rebuttal clarified many points raised by the reviewers, there was an agreement that the paper should be more convincing regarding the applicability of the approach. The reviewers were concerned with the scalability of the approach to larger environment, as well as the amount of hand crafting/domain knowledge required to apply the approach. Overall, while the paper contributes interesting results showing that such domain knowledge can help when properly leveraged, it feels like the approach needs be validated on more challenging environments before acceptance. 
Several visualizations are shown in this paper but it is unclear if they are novel.
This paper proposes an application of capsule networks to code modeling.  I see the potential in this approach, but as the reviewers pointed out, in the current draft there are significant issues with respect to both clarity of motivating the work, and in the empirical results (which start at a much lower baseline than previous work). I am not recommending acceptance at this time, but would encourage the reviewers to clarify the issues raised in the reviews for future submission.
This paper presents a number of experiments involving the Model Agnostic Meta Learning (MAML) framework, both for the purpose of understanding its behavior and motivating specific enhancements.  With respect to the former, the paper argues that deeper networks allow earlier layers to learn generic modeling features that can be adapted via later layers in a task specific way.  The paper then suggests that this implicit decomposition can be explicitly formulated via the use of meta optimizers for handling adaptations, allowing for simpler networks that may not require generic modeling specific layers.  At the end of the rebuttal and discussion phases, two reviewers chose rejection while one preferred acceptance.  In this regard, as AC I did not find clear evidence that warranted overriding the reviewer majority, and consistent with some of the evaluations, I believe that there are several points whereby this paper could be improved.  More specifically, my feeling is that some of the conclusions of this paper would either already be expected by members of the community, or else would require further empirical support to draw more firm conclusions.  For example, the fact that earlier layers encode more generic features that are not adapted for each task is not at all surprising (such low level features are natural to be shared).  Moreover, when the linear model from Section 3.2 is replaced by a deep linear network, clearly the model capacity is not changed, but the effective number of parameters which determine the gradient update will be significantly expanded in a seemingly non trivial way.  This is then likely to be of some benefit.  Consequently, one could naturally view the extra parameters as forming an implicit meta optimizer, and it is not so remarkable that other trainable meta optimizers might work well.  Indeed cited references such as (Park & Oliva, 2019) have already applied explicit meta optimizers to MAML and few shot learning tasks.  And based on Table 2, the proposed factorized meta optimizer does not appear to show any clear advantage over the meta curvature method from (Park & Oliva, 2019).  Overall, either by using deeper networks or an explicit trainable meta optimizer, there are going to be more adaptable parameters to exploit and so the expectation is that there will be room for improvement.  Even so, I am not against the message of this paper.  Rather it is just that for an empirically based submission with close ties to existing work, the bar is generally a bit higher in terms of the quality and scope of the experiments.  As a final (lesser) point, the paper argues that meta optimizers allow for the decomposition of modeling and adaptation as mentioned above; however, I did not see exactly where this claim was precisely corroborated empirically.  For example, one useful test could be to recreate Figure 2 but with the meta optimizer in place and a shallower network architecture. The expectation then might be that general features are no longer necessary.
This work studies corset based pruning strategies for neural networks, and highlights the looseness of approximation bounds, the difference between approximation error and probability, and the importance of considering post pruning fine tuning. I found the empirical findings and concerns raised around the utility of approximation bounds for pruning guarantees interesting and important, and appreciated the benchmark with varying levels of difficulty. However, the empirical analysis was limited to coreset based methods and a simple LeNet architecture, and could benefit from considering additional non coreset based approaches, architectures, and datasets. While I agree with the authors that new methods are not required for their work to be valuable, I believe that a more thorough empirical analysis is needed to support that their claims that current approximation bounds are not useful across wider experimental settings.
The reviewers generally found the idea interesting and the contribution of the paper significant. I agree, I think this is quite a neat idea to investigate, and the paper is written well and is engaging to read.  I would encourage the authors to take into account all of the reviewer suggestions when preparing the camera ready version. Of particular importance is the name: I think it s bad form to appropriate a name already used in other prior work (proto value functions, which are very well known in the RL community), so I think it is very important for the final to change the name to something that does not conflict with an existing technique. Obviously this does not affect my evaluation of the paper, but I trust that the authors will address this feedback (I will check the camera ready).
In the end, all reviewers agreed that this is a solid piece of work. However, there were also some doubts regarding the relevance of the block diagonal design and the underlying assumptions about the p/n ratio. The majority of the reviewers, on the other hand, had the impression that the positive aspects dominate the potential problems, and I also share this viewpoint. However, I d like to encourage the authors to carefully address the points of criticism raised by the reviewers in their final version.
This paper tackles the problem of regret minimization in a multi agent bandit problem, where distributed learning bandit algorithms collaborate in order to minimize their total regret. More specifically, the work focuses on efficient communication protocols and the regret corresponds to the communication cost. The goal is therefore to design protocols with little communication cost. The authors first establish lower bounds on the communication cost, and then introduce an algorithm with provable near optimal regret.  The only concern with the paper is that ICLR may not be the appropriate venue given that this work lacks representation learning contributions. However, all reviewers being otherwise positive about the quality and contributions of this work, I would recommend acceptance.
The paper considers learning classifiers under a fairness constraint which enforces the loss to be equal on certain subgroups. Reviewers found the work to be well motivated, but raised concerns on the lack of discussion and comparison to relevant prior work. Notable examples in the fairness literature are Donini et al., "Empirical Risk Minimization under Fairness Constraints", Celis et al., "Classification with Fairness Constraints: A Meta Algorithm with Provable Guarantees", while in the more broader constrained optimization literature, Kumar et al. "Implicit Rate Constrained Optimization of Non decomposable Objectives". The authors are encouraged to incorporate reviewers  detailed comments for a revised version of this work.
This is an interesting paper which further extends the duality theory of deep networks.  Unfortunately, reviewers had many concerns about, presentation, technical details, and missing prior work.  I will add that a large volume of relevant implicit bias work (e.g., in the setting of deep linear networks, mirroring Proposition 2) is completely uncited (e.g., works by Arora et al., Soudry et al., Ji et al.), despite being earlier than many of the works which are currently cited.  As such, I urge the authors to continue in their valuable line of work, taking into consideration all of these points and also the reviewer comments.  Separately, I note that there is a violation of the blind policy in the current revision: grant information was included.  The PC decide this was a minor violation and should not affect the review process, however their decision could have easily been otherwise.  I urge the authors to be exceptionally careful with such issues in the future.
This paper proposes an approach for imitation learning from video data. The problem is important and the contribution is timely. The reviewers brought up several concerns regarding the clarity of the paper and the lack of sufficient comparisons. The authors have improved the paper significantly, adding several new comparisons and improving the presentation. However, concerns still remain regarding the description of the method and the presentation of the results. Hence, the reviewers agree that the paper does not meet the bar for publication. 
This paper proposes a method for conditional inference with arbitrary conditioning by creating composed flows. The paper provides a hardness result for arbitrary conditional queries. Motivated by the fact that conditional inference is hard the paper therefore suggests a novel relaxation where the *conditioning* is relaxed.  There were various concerns from the reviewers regarding notation, comparison algorithms, and how the hardness result motivates the smoothing operation introduced. After careful study of the paper and all the comments I find that I am most concerned about the hardness result and how it motivates the smoothing operation that is done. Novel computational complexity results *as such* are not really in the scope of ICLR. There s nothing wrong with having such a result in a paper, of course, but a paper like this should be evaluated on the basis of the algorithm proposed.  Like R4, I do not follow how this hardness result is meant to motivate the smoothing that s applied. The paper is unambiguous that the goal is to do conditional inference. A hardness result is presented for conditional inference, and so a relaxed surrogate is presented. This has a minor problem that it s not clear the relaxed problem avoids the complexity boundary of the original one. There s a larger problem, though. The hardness result has not been sidestepped! The goal is still to solve conditional inference. The algorithm that s presented is still an approximate algorithm for conditional inference. R4 suggests that other approximation algorithms should be compared to. The authors responded to this point, but I am not able to understand the response. For the same reason, I think it is valid to ask for comparison to other approximate inference algorithms (e.g. without smoothing)  None of the above is to say that the smoothing approach is bad. It may very well be. However, I think that either the existing argument should be clarified or a different argument should be given.  Finally here are two minor points (These weren t raised by reviewers and aren t significant for acceptance of the paper. I m just bringing them up in case they are useful.)  Is Eq. 3 (proof in Appendix B.1) not just an example of the invariance of the KL divergence under diffeomorphisms?  Proof in appendix B.2 appears to just a special case of the standard chain rule of KL divergence (e.g. as covered in Cover and Thomas)
All three reviewers are consistently negative on this paper. Thus a reject is recommended.
There is consensus in the reviews that this paper convincingly demonstrates strong acceleration of policy learning using differentiable simulation in tasks involving contact rich dynamics. The authors are encouraged to explore where the smoothness assumptions made in the simulator actually transfer to real robots. The paper may be further strengthened through more complex benchmarks involving contact rich manipulation.
This paper presents a meta learning framework to learn novel visual concepts with few examples. The proposed FALCON model uses an embedding prediction module to infer novel concept embeddings. This is done via paired image and text data as well as supplementary sentences. The resulting systems shows improvements on a series of datasets with synthetic and real images. The reviewers were supportive of this submission and praised the novelty, central ideas and experimental setups.  Concerns included:\ (a) [2P5Z] Justifying the formulations in this paper and situating it with past work   "Why is this an ecologically valid problem formulation?", "why a meta learning approach is the best formulation to tackle the problem?", "Why the box embedding space?"\ (b) [W3YC] More details required about the dataset and approach.\ (c) [98FU] Failure patterns  The authors provided detailed responses to these concerns. Concern (a), (b) and (c) were well addressed in the rebuttal and paper, and led to in increase in the reviewers rating.  Given the above, I recommend acceptance. But I do urge the authors to add the details provided in the rebuttal into the main paper. In particular, the concerns/suggestions by reviewer 2P5Z can hugely help in improving the paper and informing the reader.
The focus of the submission is the estimation of the Shannon differential entropy (DE). The authors propose a differentiable DE estimator referred to as KNIFE (Kernelized Neural diFFerential Estimator): it is a plug in method (5) using KDE (kernel density estimation; (4)). KNIFE has parameters including the locations (a), weights (w) and covariances (A) in KDE, which are tuned according to the upper bound heuristic in (6). The approach is illustrated on toy examples and in the context of training neural networks.  Estimating information theoretical quantities is a current topic of machine learning. Unfortunately, as assessed by the reviewers 1) the submission lacks context and comparison to available entropy estimators, 2) the estimator closely follows Schraudolph (2004); the technical novelty is quite limited.  More work and major revision are required.
This paper proposes a meta learning approach for inferring the Hamiltonian governing the dynamics of physical systems from observational data, and using it to adapt to new systems from the same class of dynamics quickly. The paper does this by effectively combining the previously published Hamiltonian Neural Networks and MAML/ANIL. The reviewers agree that the paper is well written, and the experiments are comprehensive, however, they also have reservations about the technical novelty of the proposed solution, given that it appears to be combination of pre existing models. Saying this, the authors were able to  address a lot of the reviewers  concerns during the discussion period, hence I recommend this paper for acceptance.
The paper proposes to learn a "virtual user" while learning a "recommender" model, to improve the performance of the recommender system. A reinforcement learning algorithm is used for address the problem the authors defined. Multiple reviewers raised several concerns regarding its technical details including the feedback signal F, but the authors have not responded to any of the concerns raised by the reviewers. The lack of authors involvement in the discussion suggest that this paper is not at the stage to be published.
This paper proposes a specific architecture for training an ensemble of separate policies on a family of easier tasks with the goal of obtaining a single policy that can perform well on a harder task. There are significant similarities to the recently published Distral algorithm, but I am convinced that this work offers a meaningful contribution beyond that work. Moreover, the authors performed a thorough comparison between their method and Distral and found that DnC performs better.
AR1 finds that extension of the previously presented ICLR 18 paper are interesting and sufficient due to the provided analysis of universality and depth efficiency. AR2 is concerned with the lack of any concrete toy example between the proposed architecture and RNNs. Kindly make an effort to add such a basic step by step illustration for a simple chosen architecture e.g. in the supplementary material. AR3 is the most critical (the analysis TT RNN based on the product non linearity done before, particular case of rectifier non linearity is used, etc.)  Despite the authors cannot guarantee the existence of corresponding weight tensor W in less trivial cases, the overall analysis is very interesting and it is the starting point for further modeling. Thus, AC advocates acceptance of this paper. The review scores do not indicate this can be an oral paper, e.g. it currently is unlikely to be in top few percent of accepted papers. Nonetheless, this is a valuable and solid work.  Moreover, for the camera ready paper, kindly refresh your list of citations as a mere 1 page of citations feels rather too conservative. This makes the background of the paper and related work obscure to average reader unfamiliar with this topic, tensors, tensor outer products etc. There are numerous works on tensor decompositions that can be acknowledged:   Multilinear Analysis of Image Ensembles: TensorFaces by Vasilescou et al.   Multilinear Projection for Face Recognition via Canonical Decomposition by Vasilescou et al.   Tensor decompositions for learning latent variable models by Anandkumar et al.   Fast and guaranteed tensor decomposition via sketching by Anandkumar et al.  One good example of the use of the outer product (sums over rank one outer products of higher order) is paper from 2013. They perform higher order pooling on encoded feature vectors (although this seems to be the shallow setting) similar to Eq. 2 and 3 (this submission):   Higher order occurrence pooling on mid and low level features: Visual concept detection by Koniusz et al. (e.g. equations equations 49 and 50 or 1, 16 and 17 realize Eq. 3 and 13 in this submission)   Higher Order Occurrence Pooling for Bags of Words: Visual Concept Detection (similar follow up work)  Other related papers include:   Long term Forecasting using Tensor Train RNNs by Anandkumar et al.   Tensor Regression Networks with various Low Rank Tensor Approximations by Cao et al.  Of course, the authors are encouraged to cite even more related works.
The paper proposes a novel GAN formulation where the discriminator outputs discrete distributions instead of a scalar. The objective uses two "anchor" distributions that correspond to real and fake data. There were some concerns about the choice of these distributions but authors have addressed it in their response. The empirical results are impressive and the method will be of interest to the wide generative models community. 
This paper addresses the problem of super resolution of coarse physical simulations into fine grained video by satisfying some physical properties. The method uses generative models for sequences of images (conditional GANs with spatial and temporal discriminators), that take into account both the multiplicity of realisations given the same conditioning (via adversarial losses) and the spatial frequencies of the modeled surfaces using an evaluation in the Fourier domain, and that also allow training without paired low  and high resolution samples by relying on a generative losses applied to the downsampled reconstruction. As demonstrated on synthetic 2D data or on 3D frames showing particle simulations, the proposed method can generate images with high frequency content.  Reviewers have praised the intuition of using GANs for modeling high frequency features in physical simulation reconstruction. Weaknesses included: * insufficient analysis and explanation of the Fourier domain supervision * limited novelty w.r.t. TempoGAN (with the exception of frequency based evaluation) and lack of evaluation on more general tasks * unconvincing results with marginal improvement over plain MSE loss * missing ablation studies * toy dataset evaluation only, which makes this work seem preliminary  This paper comes out as 6/7 in my AC stack and I will unfortunately recommend to reject it, hoping that the authors will resubmit an improved version, taking into account the reviewers’ suggestions, for an upcoming conference or for a journal venue. 
The authors propose a method called Hybrid Memoised Wake Sleep (HMWS) for training models with both discrete and continuous latent variables efficiently using amortized inference. They extend Memoised Wake Sleep (MWS), which can only handle discrete latent variables, to discrete continuous systems by using importance sampling to approximately marginalize out the continuous variables and then applying MWS to the discrete variables.  This is well motivated and well written paper. The method is novel, clearly described, and evaluated in two fairly different interesting settings. However, while the empirical evaluation was considerably strengthened by the ablation studies and other experiments included in response to the reviewers, it is still on the weak side. The main issues are the relatively low dimensional latent spaces and insufficiently tuned baselines. As one reviewer pointed out, importance sampling is unlikely to scale well to high dimensions, so exploring this aspect experimentally would strengthen the paper. The use of default Adam parameters for all methods and models substantially undermines the results and might at least in part explain the underwhelming baseline performance. For example, VIMCO has been successfully used to train a discrete/continuous model of seemingly comparable complexity in [1] and yet here it seems to fail completely. To shed some light on this, the authors might want to explain how they used VIMCO and whether their approach was substantially different from the one from [1].  Finally, it at least somewhat misleading to claim, as is done e.g. in Sec. 4, that HMWS is more memory efficient than the baselines, when unlike them, it needs to store M discrete latent configurations per training case. Please make this claim more precise to avoid potential confusion.  [1] Variational Memory Addressing in Generative Models, Bornschein et al., NIPS 2017
This paper presents a new distillation method with theoretical and empirical supports.  Given reviewers  comments and AC s reading, the novelty/significance and application scope shown in the paper can be arguably limited. However, the authors extensively verified and compared the proposed methods and existing ones by showing significant improvements under comprehensive experiments. As the distillation method can enjoy a broader usage, I think the propose method in this paper can be influential in the future works.  Hence, I think this is a borderlines paper toward acceptance.
The authors tackle the problem of cost sensitive hierarchical classification. They decompose the problem into level wise learning to abstain sub problems, and apply the distributionally robust learning (DRL) technique to minimize the abstaining loss. The proposed approach is compared with a few competitors on several data sets. The reviewers find the key idea in the work, namely leveraging DRL as the key technique to solve the decomposed problem, to be somewhat interesting and new. Some of the reviewers find the motivations clear, while others believe that the paper could use a better positioning to connect the motivation with the significance of the technical contributions.   While the authors have extended the discussions on related works and added some additional experiment results during the rebuttal, the reviewers generally agree that the improvements were not sufficient to warrant acceptance. Most importantly, the few baseline competitors and the small scale data sets make it hard to convince the readers about the validity of the proposed approach. In particular, the scalability of the approach to larger scale data sets remains questionable, and the spectrum of baseline competitors, both in terms of breadth and recency, is not sufficient. Some reviewers suggest the authors include time/efficiency/convergence analysis of the proposed approach. Furthermore, the authors are encouraged to clarify the significance of contributions, explain the choice of DRL, and deepen the discussions on related works in future revisions.
This paper proposes a dedicated deep models for analysis of multiplexed ion beam imaging by time of flight (MIBI TOF).  The reviewers appreciated the contributions of the paper but not quite enough to make the cut.  Rejection is recommended. 
All reviewers eventually agreed on rejection. The highest scoring reviewer agreed their interpretation of the framing of the paper caused their initial high score, where as the other reviewers took a totally different view on the papers contribution. The authors agreed that the text of the paper was not clear in this regard. And the high scoring reviewer downgraded their score and suggested a different pitch.  Much of the reviews focused on how the paper includes a single handcrafted environment for empirical evaluation, and missing related work on reward shaping. In the AC s view (and several of the reviewers said this too) the simple observation "non obvious shaped rewards bias language" indeed begs of a broader study across a variety of environments.  Whether more experiments are needed or if this work can be reshaped such that one existence proof experiment is enough does not need to be resolved here; the paper in its current form needs significant changes.
Dear authors,  The reviewers all appreciated the interest of studying properties of the latent representations rather than of the weights. The impact of the rank on the robustness to adversarial attacks is also of interest.  There were, however, two main issues raised. Due to the lack of confidence of some reviewers, I reviewed the paper myself and found the same issues:   Clarity could be improved. Some models are mentioned before being described (N LR) and some important details are missing. In particular, we sometimes lose track of the goal of the experiments. For instance, there are quite a few experiments on the further reduction of the rank of the representation but it is not clear what to extract from them.   More importantly, there are several important gaps in the analysis. In particular: a/ As many reviewers have pointed out, low rank constraints on the weight matrices induce low rank representations if the activation function is linear. As it is not, this might not be true but deserves a discussion. b/ You state that the rank constraint has little effect given that the actual rank is much less than the constraint. However, one would expect the resulting rank to be a smooth function of the rank of the constraint. Since there is a discrepancy between ResNet N LR and ResNet 1 LR, this should be investigated. c/ For the robustness to black box adversarial attacks, these attacks are constructed using the N LR models. Is is thus not too surprising that those models do not perform as well.  Thus, despite the lack of confidence of one reviewer (the question about the N LR models might stem from the fact that it is used before being introduced), I strongly encourage you to take their comments into account for a future submission.
This paper presents an interesting approach to image compression, as recognized by all reviewers. However, important concerns about evaluating the contribution remains: as noted by reviewers, evaluating the contribution requires disentangling what part of the improvement is due to the proposed approach and what part is due to the loss chosen and evaluation methods. While authors have done a valuable effort adding experiments to incorporate reviewers suggestions with ablation studies, it does not convincingly show that the proposed approach truly improves over existing ones like Balle et al. Authors are encouraged to strengthen their work for future submission by putting particular emphasis on those questions.
This paper exposes a modification of self play (in a non cooperative setup) where agents expose their internal states to each other, and adds a "truthfulness" mechanism to ensure the agents do not hide information to each other.  The reviewers generally agree that the ideas presented, in particular the imaginary rewards, is interesting and should be explored further. The experimental results are good.  However, reviewers point out many problems related to clarity, writing and notations, and they note many typos. The motivation of this work also needs to be explained further. Reviewers also find that the related work section needs to be remade, as it both lacks citations, and cites works that are not really relevant. This work positioning with respect to these previous works also needs to be expanded. The reviewers also point out that the experimental results should be discussed further.  Generally, the paper cannot be published in its current state, and unfortunately the authors have not submitted an updated version or answers to the reviews. I therefore recommend rejection for this paper.
The paper sets up a complex algorithm for out of distribution generalization. The algorithm requires first, a generalization of identification results for variational autoencoders, the followed by second, a causal discovery subroutine, and third, learning an invariant predictor using the discovered causes. The procedure reads sound, and the results on common benchmarks look good, though I do not know how practical the approach would be in general.
This paper presents a way to aggregate and precompute node features on a graph to enable fast parallel training of neural models on massive graphs for various node prediction tasks.  We have seen quite a few papers in this line of work (precompute node features without training, and then treat the nodes as independent during training) recently and this is a continuation in this trend.  Most reviewers lean toward rejection.  The main concern is the lack of novelty and the marginally better results reported in the experiments.  In some sense, the proposed method could be thought of as replacing the concatenation operation of node features across multiple hops used in the SIGN paper with a sequence model, either conv + pool, or attention.  Given this, the novelty of this paper is indeed a bit limited.  Additionally, it is unclear why this sequence model perspective is better than concatenation, which should be in principle more expressive and in practice faster and more efficient (as also reported in Table 6).  I recommend rejecting this paper, but do encourage the authors to position their work better with respect to prior work and really consider what’s the defining advantage of their approach compared to alternatives, like SIGN.
 The paper proposes the novel task of detecting hallucinated tokens in sequence generation, and a strategy to train such models using artificially generated samples. The methods show reasonable correlation with human judgements.  The expert reviewers are unanimous in their lack of enthusiasm about this work, with overall borderline assessments. The reviewers provided some suggestions for improvement, and it is worth remarking that the authors provided an impressive amount of work in the revised version, addressing the suggestions. Specifically, they added baselines that validate that the task is non trivial, and the case study on improving machine translation.  In the discussion period, the reviewers appreciated the additions, and some increased their rating, but the overall assessment remains borderline.  The reviewers find the work lacks the expected amount of depth.  Some concerns emphasized in the discussion period involve insufficient empirical analysis (e.g., more NMT datasets and and analysis); understandable as this work was added after submission, but still important.  A reviewer stresses concerns about the definition of the task itself, which I agree is vague ("... cannot be entailed by the sentence") and does not match the synthetic data generation entirely, leading to unfortunate edge cases involving synonyms or   worse   slight narrowing that technically would still be entailment but maybe should be considered unfaithful.  This casts doubt on the human evaluations and on considering the task itself a main contribution, therefore leading to the empirical framing that the reviewers perceive and expect.  It also seems to me that there is a incremental, cat and mouse spirit to predicting automatically generated hallucinations. In short, it seems like this paper is caught in between trying to be a significant empirical contribution and a linguistically well motivated task and annotation project, and I understand that the reviewers would prefer committing to one of these directions.  While I encourage the authors to pursue this direction more deeply, in light of the borderline reviews, I do not recommend acceptance.
This paper proposes a network architecture which labels object with an identifier that it is trained to retain across subsequent instances of that same object.  After discussion, the reviewers agree that the approach is interesting, well motivated and written, and novel. However, there was unanimous concern about the experimental evaluation, so the paper does not appear to be ready for publication just yet, and I am recommending rejection.
The reviewers  evaluation of this paper are borderline/negative.  The AC considered the reviews, rebuttal, and the paper itself, and concurs with the reviewers.  The AC found that the paper is an extension of previous work DM GAN (DM GAN: Dynamic Memory Generative Adversarial Networks for Text to Image Synthesis, CVPR 2019, https://arxiv.org/pdf/1904.01310.pdf). This work uses the word features in addition to sentence features at the first stage of generation, while DM GAN and other previous work don’t use word features in the first stage, but use them in the later stages when the feature resolution is higher. The authors improve the dynamic memory in DM GAN into spatial dynamic memory, and also change the image refinement process in DM GAN into an iterative refinement. The proposed multi tailed word level initial generation, spatial dynamic memory, and iterative refinement are incremental changes to DM GAN. Moreover, the proposed structure almost doubles the parameter size of DM GAN (shown in Table 2), yet the evaluation results on COCO are similar to DM GAN with only minor improvements. It is not clear whether the performance improvement comes from  the increased number of parameters or the architecture design. Especially on the CUB dataset with limited number of images, the model can easily overfit with a larger number of parameters.  The proposed method shares the similar network structure and dynamic memory blocks as DM GAN, except for a few changes.  Overall, the AC finds this paper not suitable for acceptance at ICLR in present form.
  This paper proposes two mechanisms, SelfNorm (used during training and inference) leveraging an attention based recalibration of mean and standard deviation for instance normalization, and CrossNorm which performs cross channel swapping of mean/stdev. Is is shown that the combination (often combined with AugMix) performs well across several datasets in terms of model robustness. Overall the paper has strength in the fact that the method is interesting, simple to implement, and modular. However, reviewers brought up a number of issues including the overstated motivation/writing, lack of clarity, and most importantly need for clear experimental results (comparing to uniform/standard baselines) and identification of the separate mechanisms. It is especially uncertain why it is necessary that they are used *together* (often with AuxMix as well) to obtain the strong performance. As a result, the score for this paper is borderline, tending towards a weak acceptance.   It is appreciated that the authors provided a lengthy rebuttal, including new results in a different domain (NLP); however, the reviewers agreed that not all of the concerns were addressed.  After a lengthy discussion, all of the reviewers agree that while the method is simple, modular, and effective when combined (hence the positive scores from some reviewers), the authors fail to describe the underlying reason for the method s gains, especially with respect to the individual parts (SelfNorm vs. CrossNorm) and why the results only come when these rather independently derived modules are used together. The exposition of the experimental results, with differing baselines/conditions that make it very hard to understand where the effect is coming from, exacerbates this issue.   As a result of these concerns, I recommend rejection of this paper. However, the method is interesting and results promising, so I hope that the authors can clarify the writing and improve the presentation of the results (specifically separating out the effects of SelfNorm and CrossNorm, as well as analyzing how they interact together to improve results) and submit to a future venue. 
This paper proposes performs an empirical study to evaluate CNN based object classifier for the case where the object of interest is very small relative to the size of the image. Two synthetic databases are used to conduct the experiments, through which the authors made a number of observations and conclusions. The reviewers concern that the databases used are too structured or artificial, and one of the two databases is very small as well. On top of that, only one network architecture is used for evaluation. Furthermore, the conclusion from two databases seem inconsistent as well. The authors provided detailed responses to the reviewers  comments but were not able to change the overall rating of the paper. Given these concerns, as well as no methodological contribution, there are general concerns from all reviewers that the contributions of this work is not sufficient for ICLR. The ACs concur the concerns and the paper can not be accepted at its current state.
The paper proposes to combine RL and Imitation Learning. It defines a regularized reward function that minimizes the KL distance between the policy and the expert action. The formulation is similar to the KL regularized MDPs, but with the difference that an additional indicator function based on the support of the expert’s distribution is multiplied to the regularized term.  Several issues have been brought up by the reviewers, including: * Comparison with pre deep learning literature on the combination of RL and imitation learning * Similarity to regularized MDP framework * Assumption 1 requiring a stochastic expert policy, contradicting the policy invariance claim * Difficulty of learning the indicator function of the support of the expert’s data distribution  Some of these issues have been addressed, but at the end of the day, one of the expert reviewers was not convinced that the problem of learning an indicator function is going to be easy at all. The reviewer believes that learning such a function requires "learning a harsh approximation of the density of visits of the expert for every state which is a quite hard task, especially in stochastic environments.”   Another issue is related to the policy invariance under the optimal expert policy. In most MDPs, the optimal policy is not stochastic and does not satisfy Assumption 1, so the optimal policy invariance proof seems to contradict Assumption 1.  Overall, it seems that even though this might become a good paper, it requires some improvements. I encourage the authors to address the reviewers’ comments as much as possible.
This paper presents a dataset and method for training a model to infer, from a visual scene, the program that would generate/describe it. In doing so, it produces abstract disentangled representations of the scene which could be used by agents, models, and other ML methods to reason about the scene.  This is yet another paper where the reviewers disappointingly did not interact. The first round of reviews were mediocre to acceptable. The authors, I think, did a good job of responding to the concerns raised by the reviewers and edited their paper accordingly. Unfortunately, not one of the reviewers took the time to consider author responses.  In light of my reading of the responses and the revisions in the paper, I am leaning towards treating this as a paper where the review process has failed the authors, and recommending acceptance. The paper presents a novel method and dataset, and the experiments are reasonably convincing. The paper has flaws and the authors are advised to carefully take into account the concerns flagged by reviewers—many of which they have responded to—in producing their final manuscript.
The proposed algorithm is found to be a straightforward extension of the previous work, which is not sufficient to warrant publication in ICLR2020.
This paper presents a reinforcement learning approach to hierarchical text classification.  Pros: A potentially interesting idea to drive the search process over a hierachical set of labels using reinforcement learning.  Cons: The major concensus among all reviewers was that there were various concerns about experimental results, e.g., apple to apple comparisons against prior art (R1), proper tuning of hyper parameters (R1, R2), the label space is too small (539) to have practical significance compared to tens of thousands of labels that have been used in other related work (R3), and other missing baselines (R3). In addition, even after the rebuttal, some of the technical clarity issues have not been fully resolved, e.g., what the proposed method is actually doing (optimizing F1 metric vs the ability to fix inconsistent labeling problem).  Verdict:  Reject. While authors came back with many detailed responses, they were not enough to address the major concerns reviewers had about the empirical significance of this work.
All three reviewers, even after the rebuttal, agreed that the paper did not meet with bar for acceptance. A common complaint was lack of clarity being a major problem. Unfortunately, the paper cannot be accepted in its current form. The authors are encouraged to improve the presentation of their approach  and resubmit to a new venue.  
This paper introduces a new architecture for sparse coding.  The reviewers gave long and constructive feedback that the authors in turn responded at length on. There is consensus among the reviewers that despite contributions this paper in its current form is not ready for acceptance.  Rejection is therefore recommended with encouragement to make updated version for next conference.    
Motivated by addressing the problem of lacking parallel training data for supervised code translation, this paper proposed to construct noisy parallel source code datasets using a document similarity based approach, and empirically evaluated its effectiveness for code translation tasks.   The paper is in general well written, easy to follow, and the method is simple and empirical results look positive. Some major concern by reviewers is that while the proposed method is simple and may be easy to use, the overall technical novelty/contribution is limited, e.g., there generally lacks of more thorough discussions on how to deal with the critical noise issue in a more robust or sophisticated way. In addition, there were also other concerns about the experimental issues, such as datasets, metrics, ablation analysis, usability, etc.   Overall, the paper presents some preliminary positive results for an interesting research problem, but the overall technical novelty and contributions are incremental and the paper is not strong enough for the acceptance by this conference. Nonetheless, this work could be potentially valuable for the niche area of code translation research, and authors are encouraged to continue to improve this research with more thorough investigation for a future venue.
The paper proposes to explain the representation for layer aware neural sequence encoders with multi order graph (MoG). Based on the MoG explanation, it further proposes Graph Transformer as a graph based self attention network empowered Transformer. As commented by the authors, a main purpose of Graph Transformer is to show an example application of the MoG explanation.  During the discussion period, after reading the paper and checking the code, the AC had raised a serious concern: There is a big gap between the MoG motivation and the actual implementation. The AC had urged the referees to take a careful look at the implementation details, in particular, Lines 524 561 in the attached code: "supplement/fairseq 0.6.2_halfdim_gate⁩ ▸ ⁨fairseq⁩ ▸ ⁨models⁩ ▸transformer.py". The AC had made the following comments to the referees: "Whether the performance gain of Graph Transformer over Transformer is due to the MoG explanation is highly unclear. There is no direct evidence, such as appropriate visualization, to support that. In a high level description, instead of using a usual skip connection that would combine beforex and x, the actual implementation is to 1) define increamental_x   x   beforex, 2) let increamental_x attend on beforex to produce x1, let beforex attend on increamental_x to produce x2, and let increamental_x attend on increamental_x to produce x3, 3) combine beforex, x1, x2, x3 in a certain way to produce the layer output."  Reviewer 2 responded to the AC s concern: "After examining the transformer.py and Section 2 & 3, we cannot understand why the output of self attentions could be regarded as MoG subgraphs? The authors did not explain the connection. In their code, the graph transformer seems to just utilize 3 multi head attentions (line 539 541) in their encoder. Using MoG to interpret the outputs of three attentions (line 539 541) is not very convincing. The link is weak. We agree with your comments."  To summarize, the link between the actual implementation in the code and all the MoG explanations is quite weak, and the technical novelty of the actual implementation is not strong enough for an ICLR publication. Therefore, the AC recommends Reject.     
The paper proposes the challenge of rapid task solving in unfamiliar environments and presents an approach to achieve this called Episodic Planning Networks   a non parametric memory based on the transformer architecture to learn tasks that require planning from previously experienced tasks, following a form of meta RL.  The problem and approach are compelling, with strong empirical results.  The paper is well written and is an exciting contribution.  This is a clear accept.  In response to the initial reviews, the authors updated their paper to improve the formalization and address other concerns in the reviews, which were viewed favorably by the reviewers as a good improvement. Based on the reviewer discussions, the work could still be placed better in context with respect to other literature.
This paper presents a dataset, created using a combination of existing resources, crowdsourcing, and model based filtering, that aims to tests models  understanding of typical progressions of events in everyday situations. The dataset represents a challenge for a range of state of the art models for NLP and commonsense reasoning, and also can be used productively as a training task in transfer learning.  After some discussion, reviewers came to a consensus that this represents an interesting contribution and a potentially valuable resource. There were some concerns—not fully resolved—about the implications of using model based filtering during data creation, but these were not so serious as to invalidate the primary contributions of the paper.  While the thematic fit with ICLR is a bit weak—the primary contribution of the paper appears to be a dataset and task definition, rather than anything specific to representation learning—there are relevant secondary contributions, and I think that this work will be practically of interest to a reasonable fraction of the ICLR audience. 
The paper contributes to the literature on federated learning by introducing a hybrid local SGD (HL SGD) method. HL SGD is motivated by the setups where edge devices are grouped into clusters with fast connections within the cluster, but slower connection between the devices and the server. HL SGD uses hybrid updates: decentralized updates within the clusters and federated averaging steps between the clusters and the server.   Initially, the reviews expressed concerns regarding comparison to prior work, empirical results, and privacy of the proposed scheme. However, the authors adequately addressed all of the concerns, added relevant discussions and results to the paper, and a consensus was reached that the paper should be accepted.
After discussion with the reviewers, it seems that a. without fine tuning the result is close to being trivial (as noted also by two reviewers) b. with fine tuning results are lower c. The setup of just a linear classification layer is less common (but exists) d. The cases where extraction succeeds the performance is low such that BERT would not even be used.  In response, the authors offer many interesting directions: a. Propose a new hybrid approach that combines learning based and extraction based methods b. Run experiments to try and support the claim that their setup of one linear layer with frozen layers is practical.  These proposed modifications are interesting and show that there is potential in this paper, but it deviates substantially from the original paper and still, caveats remain, so my recommendation is to re submit after further pursuing the new directions proposed in the response.
This paper proposes to use deep ensembles to parameterize a variational Gaussian process posterior, and uses an additional L2 penalty on parameters of the neural networks, and an (MC) NN GP prior (although the prior is a free design choice). Reviewers appreciated aspects of the paper, finding there to be a minor improvement in uncertainty calibration over regularized deep ensembles, and nice results for the contextual bandit experiments. Ultimately, however, after a healthy and active exchange between reviewers and authors, four out of five reviewers are voting to reject the paper. There is a belief that the paper can be substantially improved from its current form, by carefully accommodating reviewer feedback, but it is not currently at a stage ready for publication.  There were common themes in the concerns expressed by several reviewers. Many reviewers found the technical contributions incremental. Parametrizing a GP using deep ensembles, or adding L2 regularization, is not itself a major technical contribution, and the variational framework leans heavily on Sun et. al (2019) and work that came before it from Titsias (2009). Similarly, the theoretical contributions were found to be incremental.   These concerns about the technical contributions may have been counterbalanced if the experimental results had been outstanding or the framing of the paper perceived to be very clear and well justified. However, the experimental results had a mixed reception, with several reviewers noting accuracy was not in fact much better than the simpler regularized deep ensembles, despite some improvements in uncertainty calibration. One reviewer liked the bandit experiments, but wished there was a deeper exploration of this application domain. The current experimental results do not seem to warrant the relative complexity of the approach over simple regularized deep ensembles.   Additionally, several reviewers found the framing and presentation of the paper needing significant work. The introduction of the L2 regularization terms, for example, was perceived to be overly complex, involving several steps that were not well motivated.    Several reviewers also found the motivation about making deep ensembles Bayesian unconvincing. A procedure being sensitive to initialization, or unreliable in certain settings, does not mean it does not perform approximate Bayesian inference. For example, variational methods and Laplace approximations can depend on initialization, and could get stuck in poor local optima. Quoting papers referring to deep ensembles as non Bayesian is also not an argument in itself. The blog post linked by a reviewer is clearly pushing back against these claims, and does address points raised in the discussion, such as unimodal approximations and theoretical guarantees. As reviewers have also noted, several papers have now provided plain deep ensembles with a Bayesian justification, and these papers should be acknowledged. It could be reasonable to argue that your paper makes deep ensembles _more_ Bayesian, and you could potentially try to measure this claim in a concrete way. Or you could simply argue that your approach helps reduce sensitivity to initialization, and represents solutions with lower posterior density, which can be helpful practical contributions and don t need to be tied to claims about the method being Bayesian.  Please thoughtfully reflect on the reviewer comments in updated versions of the paper. The reviewers put a lot of effort into providing feedback and engaging during the rebuttal period. While the paper has some nice features, there is significant room for improvement on several fronts: technical innovation, experimental investigation, and framing. Improving the framing will help, but working further to also address other concerns will likely be needed to sway reviewers.
This paper introduces a technique called TOMA to learn abstract graph representations of MDPs. Such an approach is said to be more efficient both in terms of memory and computation. Despite this being an interesting research topic, the reviewers unanimously recommend rejecting the paper. They all agree that the writing needs to be improved for clarity, that some of the algorithmic choices seem arbitrary, and that there are relevant baselines missing. Moreover, the authors didn’t submit a response to most reviewers. 
This work presents a novel and clever experiment for interpretable vision.  Reviewers all agreed that it tackles an important and interesting research question via a user study design.  There are some concerns around the generalization and transfer to large scale real world settings, as well as dataset construction. With the authors’ responses and discussion, I think the pros seem to outweigh the cons of this work a bit.
The paper proposes interesting  deep learning based spectral clustering techniques. The use of functional embeddings for enabling spectral clustering to have an out of sample extension has of course been explored earlier (e.g., see Manifold Regularization work of Belkin et al, JMLR 2006). For polynomials or kernel based spectral clustering, the orthogonality of the outputs can be exactly handled via a generalized eigenvector problem, while here the arguments are statistically flavored and not made very clear in the original draft. Some crucial comparisons, e.g., against large scale versions of vanilla spectral clustering and against other methods that generalize to new samples is missing or not thorough enough. See reviews for more precise description of issues. As such the paper will benefit from a revision. 
This paper collects a variety of results that cast straight through estimators as arising as principled methods that make a linearization assumption on the loss for functions with binary arguments. R1 & R3 recommended against acceptance, citing clarity concerns and a lack of novelty. R2 & R4 recommended acceptance, but had low confidence. This paper had uncharacteristically low confidence on behalf of the reviewers, and this is my fault. I apologize to the authors for this.   I have read the paper myself. I believe that this paper contains many interesting ideas, but I agree with R1 & R3 that the paper suffers from clarity issues. Unfortunately, these issues persist in the recent revisions, despite having been asked by R1 & R3 to improve the clarity. The authors asked for concrete reference points. Here are some:    "proxy function" is not well defined, despite being critical to the arguments.   deterministic ST is not defined clearly before it is discussed.    The section structure of Sec 2 could be improved. At the moment it seems to flow from the loss function to the standard ST algorithm through to a disjointed list of questions addressed in the paper.   The section titles are not particularly informative.   It is difficult to know which results are known and which results are new.  In general, I believe this work could benefit from a significant restructuring. It would be best to delineate preceding work in its own section, then lay out the new results, making sure that all of the important concepts are clearly defined. I think many of these results are valuable for the community, but the current draft makes it challenging for these great ideas to reach their full potential. 
This paper looks at  building new density estimation methods and new methods for tranformations and autoregressive models. The request from reviewers for comparison improves the paper. These models have seen a wide range of applications and have been highly successful, needing the added benefits shown and their potential impact to be expanded further.
This is a borderline paper with 2 marginally above and a marginally below acceptance recommendations. While the authors provided valid responses to some of the criticism, I still find some of the motivation and assumptions not sufficiently clear, theoretical and practical issues are mixed, and the validation on only synthetic data raises practical questions.
The paper investigates the use of the subset scanning to the detection of anomalous patterns in the input to a neural network. The paper has received mixed reviews (one positive and two negatives). The reviewers agree that the idea is interesting, has novelty, and is worth investigating. At the same time they raise issues about the clarity and the lack of comparisons with baselines. Despite a very detailed rebuttal, both of the negative reviewers still feel that addressing their concerns through paper revision would be needed for acceptance.
This paper studies the properties of regions where a DNN with piecewise linear activations behaves linearly. They develop a variety of techniques to chracterize properties and show how these properties correlate with various parameters of the network architecture and training method.  The reviewers were in consensus on the quality of the paper: The paper is well written and contains a number of insights that would be of broad interest to the deep learning community.  I therefore recommend acceptance.
While the paper has merits, I generally agree with negative reviewers. Among other issues, there were concerns about the theoretical contribution overlaps with prior work. While the authors argued the current work is not an extension, but rather designing ADML is. If this is the case, the paper should be rewritten to deemphasize the less novel contribution and focus more on what the authors believe to be the novel contribution. I don t believe in the practice of putting different messages (some novel and some not) into a paper with the hope that this makes the overall result "more novel". I d suggest the author rewrite the paper and more clear about the message.
The authors propose a method to guarantee the stability of a learnt continuous controller by optimizing the objective through a Lyapunov critic. The method is demonstrated on low dimensional continuous control problems such as cart pole.   The reviewers were mixed in their opinion of the paper, especially after the authors  rebuttal. The concerns center around some of the authors  claims regarding theoretical results, in particular that stability guarantees can be asserted for a model free controller. This claim seems to be incorrect especially on novel data where stability cannot be guaranteed, thus indicating that  robust controller  might be a better description. There are also concerns about the novelty and the contributions of the paper. Overall, the method is promising but the claims need to be carefully written. The recommendation is to reject the paper at this time.
This paper presents a model for dynamical systems with multiple interacting components. Each component is modeled as an RNN, and the interactions between components are functions of their distance in a learned embedding space. It s an interesting idea and well motivated inductive bias. The results were made more compelling with the addition of "ablation" studies during the discussion phase, which showed how various aspects of the model combined to yield the best performance.  Overall, this paper should be of interest to many in the ICLR community working on complex, multi agent systems.
The paper proposes to improve (generalized) zero shot learning, by training a generator jointly with the classification task, such that it generates samples that reduce the classification loss.  To achieve this, they use a zero shot model that has a (differentiable) closed form solution (ESZSL), so the full model can be optimized end to end. The approach is evaluated on the standard benchmarks of GZSL.   Reviewers had some concerns regarding novelty compared with previous work and quality of experiments and evaluations. The authors answered most of these concerns in their rebuttal including discussion with previous work and additional evaluations.  As a result, the paper would be interesting for the ICLR audience.
The submission proposes an approach to accelerate network training by modifying the precision of individual weights, allowing a substantial speed up without a decrease in model accuracy. The magnitude of the activations determines whether it will be computed at a high or low bitwidth.  The reviewers agreed that the paper should be published given the strong results, though there were some salient concerns which the authors should address in their final revision, such as how the method could be implemented on GPU and what savings could be achieved.  Recommendation is to accept.
This paper proposes a new approach to online 3D bin packing with deep reinforcement learning. It received mixed reviews. AC finds that the responses from authors have addressed the concerns satisfactorily.
This work proposes to analyse convergence of episodic memory based continual learning methods by looking at this problem through the lense of nonconvex optimisation. Based on the analysis a method is proposed to scale learning rates such that the bounds on the convergence rate are improved.  Pros:   I agree with the reviewers that this is an interesting and novel perspective on continual learning  Cons:   Reviewers point out concerns/issues with the clarity of the manuscript with respect to several parts:    reviewers raise concerns with respect to the significance of the evaluation    reviewers point out that the theoretical analysis itself is somewhat standard and not novel in itself, and 2 reviewers raise concerns with respect to the analysis made  Unfortunately the authors seem to have missed the upload of the revised version. The reviewers have nevertheless considered the rebuttal by the authors and the consensus is that this manuscript is not ready yet in it s current form. 
The paper studies benchmarking of bias mitigation methods. The authors propose a synthetic dataset of images (alike colored MNIST) that enables a controlled setup over different types of correlations between a binary sensitive attribute, dataset features, and a binary outcome label. The authors have evaluated 2K models that are the variants of three recently proposed debiasing methods using fair representation learning across various settings.  While the reviewers acknowledged the importance of benchmarking fair learning methods in a systematic controlled setting, they have raised several concerns:  (1) the proposed benchmark is too abstract/unrealistic (R4, R2, R3); it is not clear whether the findings from this benchmark can be generalized to real world data with real sensitive features, (2) the proposed benchmark is limited to pseudo sensitive attributes (R1) that are binary (R1, R2), (3) the paper lacks in depth analysis on why certain methods work under certain conditions (R3). Among these, (2) did not have a major impact on the decision, but would be helpful to address in a subsequent revision, (3) was partially addressed in the rebuttal. However, (1) makes it very difficult to assess the benefits of the proposed benchmark, and was viewed by AC as a critical issue.  The authors provided a detailed rebuttal addressing multiple of the reviewers’ concerns. AC can confirm that all four reviewers have read the author responses and have contributed to the discussion. A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. See R1 post rebuttal encouragement and suggestions how to strengthen the work. We hope the detailed reviews are useful for improving the paper. 
I thank the authors for their submission and active participation in the discussions. The majority of reviewers have concers with this paper, in particular, regarding the motivation of the method [dgHr], clarity [Mgm9], and theorethical support [4ENc]. I side with reviewers 4ENc, dgHr and fFaW, and recommend rejection of this paper. I want to encourage the authors to use the feedback by the reviewers to improve their paper.
This work studies the impact of distribution shift via a collection of datasets MetaShift. Reviewers all agreed that this work is simple, effective, and well motivated, and has key implications, and will be quite useful to the community. There were some concerns about the lack of analysis of MetaShift, and the binary classification setting, which was addressed by the authors’ responses. Thus, I recommend an acceptance.
All reviewers consistently agree on the high quality of the research presented in this paper, such that it the paper clearly is significantly above the acceptance threshold of ICLR.
*Summary:* Study isolated orientations of weights for networks with small initialization depending on multiplicity of activation functions.   *Strengths:*    Interesting analysis of properties in early stages depending on activations.   *Weaknesses:*    Reviewers found the settings limited.    Reviewers found experiments limited.    *Discussion:*  In response to ejGJ authors reiterate scope of covered cases and submit to consideration that their experiments should be adequate for basic research. Reviewer acknowledges the response, but maintains their assessment (limited scope of theory, limited experiments). KucV found the experimental part limited in scope, the settings unclear (notion of early stage, compatibility with theory), and review of previous works lacking. KucV’s sincerely acknowledged authors for their efforts to address their comments and improving the manuscript, and raised their score, but maintained the experimental analysis is not fully convincing and unclear, and the comparison with prior work insufficient. zuZq also expressed concerns with the experiments and the notions and settings under consideration. They also raised questions about the comparison with standard initialization. Authors made efforts to address zuZq concerns. zuZq acknowledged this but maintained initial position that the article is just marginally above threshold. jDJ5 found the paper well written and the conclusion insightful. However, also raised concerns about the experiments the settings under consideration. Authors made efforts to address jDJ5’s concerns, who appreciated this but was not convinced to raise their score.   *Conclusion:*   Two reviewers consider this article marginally above and two more marginally below the acceptance threshold. I find the article draws an interesting connection pertaining an interesting topic. However, the reviews and discussion conclude that the article lacks in several regards that in my view still could and should be improved. Therefore I am recommending reject at this time. I encourage the authors to revise and resubmit.
Well, this paper has achieved something remarkable in this review process:  The initial scores came in at fairly low scores (4, 5, 3, 6).  However, as the discussions / rebuttals went back and forth, the reviewers were able understand and see the merits of the proposed methodology.  Namely, the setting of L2E (Learning to Exploit), which makes use of a novel method called Opponent Strategy Generation, to quickly generate very different types of opponents to play against.  One more pertinent component is the use of MMD (maximum mean discrepancy regularization) which can remove the necessity of dealing with task distributions, and does a better job in creating diverse opponents.  Having understood the technical approach, three of the reviewers decided to substantially increase their scores. R4 increased 4 >6, R5 increased 5 >6, R3 increased 3 >4, while R2 held steady with a score of 6. It was also good to see empirical favorable results compared to other baseline methods: L2E had the best return against unclear opponents, such as Rocks opponent and Nash opponent.  Without any reviewer arguing strongly for acceptance, the program committee decided that the paper in its current form does not quite meet the bar, and also that it would benefit from another revision. 
This paper introduces a novel approach for out of distribution detection that generates scores from a trained DNN model by using the Fisher Rao distance between the feature distributions of a given input sample at the logit layer and the lower layers of the model and the  corresponding mean feature distributions over the training data.  The use of Fisher Rao distance is novel in the context of OOD, and the empirical evaluations are extensive.  The main concerns of the reviewers were the limitations of the Gaussianity assumption used in computing the Fisher Rao distance and the use of the sum of the Fisher Rao distances to the class conditional distributions of the target classes rather than the minimum distance. These concerns were addressed satisfactorily in a revision. In terms of technical novelty, experimental evaluation and novelty, the paper is above the bar of acceptance.
Three reviewers recommend rejecting or weak reject. The studied problem is interesting, but as one reviewer pointed out, it is not that clear how this work changes our theoretical understanding of those methods or what they imply for applications. Overall, I feel this work is on the borderline (probably it deserves higher score than the current score), but probably below the acceptance bar at the current form. 
The average score of the reviewers is 6. There are various pros and cons pointed out by the reviewers. Unfortunately, the AC and SAC found that the merit could be outweighed by the limitations of the work, and would like to recommend rejection. For example, a central concern raised by the reviewers is the lack of theoretical justification the measurement that the paper proposes does seem to show an interesting empirical phenomenon, but as a few reviewers pointed out, it s unclear how the interesting phenomenon directly links to the generalization mechanism, and it s unclear whether the new interesting phenomenon is caused by the change of measurement of the gradient coherence or it is something fundamental. The arguments related to these are found to be generally vague and hand wavey by the reviewers.  The AC would like to encourage the authors to address the reviewers  concerns thoroughly, especially those regarding the difference and similarity with prior works, the interpretation, and limitations of the results, etc., and consider adding more rigorous analysis to justify the proposed measurement of gradient alignment.  
The paper adapts a previously proposed modular deep network architecture (SHDL) for supervised learning in a continual learning setting.  One problem in this setting is catastrophic forgetting.  The proposed solution replays a small fraction of the data from old tasks to avoid forgetting, on top of a modular architecture that facilitates fast transfer when new tasks are added.  The method is developed for image inputs and evaluated experimentally on CIFAR 100.  The reviews were in agreement that this paper is not ready for publication.  All the reviews had concerns about the lack of explanation of the proposed solution and the experimental methods.  The reviewers were concerned about the choice of metrics not being comparable or justified: Reviewer4 wanted an apples to apples comparison, Reviewer1 suggested the paper follow the evaluation paradigm used in earlier papers, and Reviewer2 described the absence of an explained baseline value.  Two reviewers (Reviewer4 and Reviewer2) described the lack of details on the parameters, architecture, and training regime used for the experiments.  The paper did not not justify which aspects of the modular system contributed to the observed performance (Reviewer4 and Reviewer1).   Several additional concerns were also raised.   The authors did not respond to any of the concerns raised by the reviewers.  
The main goal of the submission is to figure out a way to produce less "noisy" saliency maps. The RectGrad method uses some thresholding during backprop, like Guided Backprop. The visuals of the proposed method are good, but the reviewers rightfully point out that evaluating whether the proposed method is any good is not obvious. The ROAR/KAR results are perhaps not telling the whole story (and the authors claim that RectGrad is not expected to get a high ROAR score, but I would like to see this developed more in a further version of this work).  Generally, I feel like there was a healthy back and forth between authors and R3 on the main concerns of this work. I agree that the mathematical justification for RectGrad seems not fully developed. Given all of these concerns, at this point I cannot support acceptance of this work at ICLR.
This paper analyzes some design choices for neural processes, paying particular attention to their small data performance, uncertainty, and posterior contraction.  This is certainly a worthwhile project, and R3 found the analysis interesting, giving the paper a score of 8.  However, R1, R2, and R4 found the experimental validation to be incomplete and insufficient to support the paper s broader recommendations.  As the paper is investigating the various combinations of implementations, I tend to agree with R1, R2, and R4 that this paper while having some interesting ideas needs a bit more precision and breadth to its experiments.
We thank the authors for their detailed responses to reviewers, and for engaging in a constructive discussions.  As explained by the reviewers, the paper is clearly written and the method is novel. However, the novelty is to combine existing ideas and techniques to define an objective function that allows to incorporate cluster assignment constraints, which was considered incremental. Regarding quality, the discussion highlighted some possible improvements that the authors propose to do in a future version of the paper, and we encourage them to follow that direction. Regarding significance, although the experimental results are promising there were some concerns that the improvement over existing techniques is marginal, and that more experiments leading to a clearer message would be useful.  In summary, this is not a bad paper, but it is below the standards of ICLR in its current form.
The paper proposes a new technique to handle oversquashing in GNNs by introducing a novel rewiring technique. The reviewers are quite positive about the paper and the rebuttal phase greatly helped clarify the method and it s impact.
This paper generated a large amount of discussion.  Three reviewers were marginally above and one marginally below.  The paper presents an intriguing relationship between self supervised learning and topic model inference that extends earlier work of Tosh.  The result seems to be subtle because there was considerable discussion with the authors wrapping up with a reminder of what the main goal is:  SSL can achieve the state of the art performance for topic inference problem, moreover (main goal) SSL can be oblivious to the specific topic model.  This is indeed intriguing.  But with all the discussion, and one persistent negative reviewer, I feel the paper needs to be polished.  Given the theorem gives a testable statement, I don t see why experimental results cannot be done for 4 different real data sets, to give us more confidence.
This paper proposes a new online contextualized few shot learning setting, with two associated datasets (notably, including one obtained from trajectories within the real world Matterport3D reconstructions). A simple recurrent contextualized extension of Prototypical Networks is also proposed as a stronger baseline, demonstrating the need for incorporating such context. The reviewers all agreed that this is an interesting setting combining continual and few shot learning, offering a more realistic problem that mirrors those that might be encountered by embodied agents. The authors provided very detailed rebuttals, answering some of the questions and concerns raised by the reviewers. In the end, all reviewers agreed that this paper would contribute a significant novel setting, and so I recommend acceptance. I encourage the others to include modifications related to some of the comments, such as strengthening/clarifying the setting including metrics, details of the method, etc.
In this paper, the authors propose a MCM aware twin least squares GAN (MTGAN) model for hyperspectral anomaly detection. The proposed method is somewhat novel, and the efficacy of the proposed method is validated through experiments. However, the clarity of the paper is low, and the explanation of some formulas is not clear enough. Therefore, the quality of the current version is below the acceptance threshold.  I encourage authors to update the paper based on the reviewer s comments and resubmit it to a future venue.
The manuscript presents a promising new algorithm for learning geometrically inspired embeddings for learning hierarchies, partial orders, and lattice structures. The manuscript builds on the build on the box lattice model, extending prior work by relaxing the box embeddings via Gaussian convolutions. This is shown to be particularly effective for non overlapping boxes, where the previous method fail.  The primary weakness identified by reviewers was the writing, which was thought to be lacking some context, and may be difficult to approach for the non domain expert. This can be improved by including an additional general introduction. Otherwise, the manuscript was well written.  Overall, reviewers and AC agree that the general problem statement is timely and interesting, and well executed. In our opinion, this paper is a clear accept.
The problem of introducing interpretability into sepsis prediction frameworks is one that I find a very important contribution, and I personally like the ideas presented in this paper. However, there are two reviewers, who have experience at the boundary of ML and HC, who are flagging this paper as currently not focusing on the technical novelty, and explaining the HC application enough to be appreciated by the ICLR audience. As such my recommendation is to edit the exposition so that it more appropriate for a general ML audience, or to submit it to an ML for HC meeting. Great work, and I hope it finds the right audience/focus soon. 
The consensus among the reviewers is that this paper is not quite ready for publication for reasons I will summarize in more detail below. However, I think there are some things that are really nice about this approach, and worth calling out:  PROS:  1.  the idea of tackling tasks broadly all the way from perception through symbolic reasoning is an important direction.  2. It certainly would be useful to have a "plug and play" framework in which various knowledge sources or skills can be assembled behind a simple interface designed by the ML practioner to solve a given problem or class of problems.  3. Clearly finding ways to increase sample efficiency   especially in a deep net approach   is of great importance practically.  4. The writing  is good.  CONS:  1. The comparison to feedforward networks needs to be made fair in order to disentangle the benefit of the architecture from the benefit of pre training the modules.  2. Using the very limited 2x2 grid was too low a bar for the reviewers.  The authors aim at a  more general, efficient architecture useful for a variety of tasks, and perhaps you didn t want to devote too much time to this particular task, but I think having a slam dunk example of the power of the approach is really necessary to be convincing.  3. Given the similarity, I think more has to be done to show the intellectual contribution over Zaremba et al, the difference in motivation notwithstanding.  One way to do this is to really prove out the increased sample efficiency claim.
The authors propose two linguistic verifiers for improving extractive question answering when the question is answerable. The first replaces the interrogative in the question with candidate answers and evaluates the result both in isolation and in combination with the answer containing sentence to do answer verification. The second jointly encodes individual sentences and spans with questions in a hierarchical manner to improve use of context in answer prediction performance.  The reviews for this paper are roughly on the cusp: 2 reviewers rate the paper a bit below the acceptance threshold, 1 a bit above, and then 1 now rates the paper as a solid Accept.   Pros    The main strength of the paper, certainly as emphasized by the most positive reviewer is the strong empirical results. Especially on SQuAD v2, the method here seems to roughly equal the current leading system on the leaderboard.   The paper also proposes two methods for improving question answering that make sense, are relatively simple, and work  Cons   The writing and presentation of the paper is not that great. Even at the level of the introduction, the writing just is not very focused: The first page has a lot of background and tutorial information on MRC that just doesn t get to the point of where this paper is situated and what it contributes.   Neither of the proposed systems are that novel (though it is interesting to see that they still have value even in the age of large contextual language models)   The paper lacks ML novelty   The methods appear to be significantly more expensive to run   Some empirical comparisons appear to be lacking  As well as the missing comparisons mentioned by some reviewers, I think that there are a number of other missing relevant datapoints. While not denying that gathering the available results for NewsQA/TriviaQA is much less straightforward than with that nice leaderboard for SQuAD, aren t there are lot of systems with better results on TriviaQA that aren t mentioned in the paper. These include: RoBERTa and SpanBERT (mandarjoshi); BigBird ETC see https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9 Paper.pdf; Longformer; SLQA see https://www.aclweb.org/anthology/P18 1158.pdf .  But, overall, I think the decision on this paper comes down to focus and contributions. Not withstanding the growing size of ICLR, I would like to think that it is not just another ML and ML applications conference, but it is a conference centered on representation learning. The present paper, no matter its quality and strong results, just isn t a contribution to representation learning. It is a much better fit to an NLP conference where it would be a strong contribution to question answering, showing the continuing value of linguistic methods like question rewriting in answer validation. But this just isn t a contribution within the focus of representation learning. Just as R4 does, I encourage the authors to clean up the presentation of the paper a bit and to submit it to an NLP conference, where it would be a strong contribution, for the reasons that R3 emphasizes. 
The authors propose a novel MIL method that uses a novel approach to normalize the instance weights. The majority of reviewers found the paper lacking in novelty and sufficient experimental performance evidence.
The paper proposes to do unsupervised discovery of 3D physical objects. The core idea is to decompose the scene into primitives that contain: (a) a segment; (b) 3D position and dynamics; and (c) appearance. These are combined with a physics model and renderer to discover objects/primitives by watching videos; the core supervisory signal used is that one should be able to reconstruct future scenes and that objects/primitives ought to be physically consistent. The system is tested on synthetic data as well as real videos of blocks.   The reviewers were positive about many aspects but, at the time of submission had a number of concerns. These were, in view of of many of the four reviewers, largely addressed. These are as follows:   One overarching concern (R3, R4) was the experiments that the paper’s title and motivation focused heavily on 3D but the experiments lacked a 3D experiment of any variety. The authors addressed this by adding 3D IOU and recall. While numbers are low for IOU, this is a challenging area and the AC appreciates this as did R3 and R4.   Another concern is the data itself (R4,R1). R4 in particular cites the synthetic nature of it as a stumbling block; R1 is similarly concerned about the difficulty of the backgrounds (and the rigidity of the objects). The AC thinks that the data is sufficient for this paper given the overall paper focus, methodological contributions, and particular set of claims. However, the AC is highly sympathetic to R4’s arguments and thinks more realistic real data (beyond the additional data of towers of blocks in front of a white sheet) would substantially improve the impact of the paper and the direction of research.   The last content focused concern was disagreement that the system is unsupervised (R2,R4). The authors have addressed this with experiments using a hard coded system that uses a heuristic based on the bottom coordinate, which obtains good results as well. All reviewers with this concern seem satisfied although the AC would note this assumes a single ground plane, which ties into concerns about the data (although this is a small nitpick).   R2 had substantial concerns about the legibility and reproducibility of the paper. These have been largely addressed in the revision, as far as the AC can tell.  The paper is an good contribution on a challenging and important problem. While the AC shares some of R4’s concerns about the data (and indeed how data difficulty and method interact), the AC finds the revised paper compelling and recommends acceptance.
This paper proposes a set of synthetic tasks to study and discover the inductive biases of seq2seq models.  Authors did a great job in convincing all the reviewers except R5 in their rebuttal. I do not find any serious concerns from R5 s review. I personally think this is a very useful analysis paper.
Summary: This paper casts the problem of step size tuning in the Runge Kutta method as a meta learning problem. The paper gives a review of the existing approaches to step size control in RK method. Deriving knowledge from these approaches the paper reasons about appropriate features and loss functions to use in the meta learning update. The paper shows that the proposed approach is able to generalize sufficiently enough to obtain better performance than a baseline.    The paper was lacking in advocates for its merits, and needs better comparisons with other baselines before it is ready to be published.
This paper presents a new method to decrease the supervision cost for learning spurious attributes using worst group loss minimization. Their method uses samples both with and without spurious attribute annotations to train a model to predict the spurious attribute, then use the pseudo attribute predicted by the trained model as supervision on the spurious attribute to train a new robust model having minimal worst group loss. The experiments show promising results in this domain for reducing annotation cost.   The reviewers vote to accept the paper, and some of them increased their scores during the discussions since the authors have addressed their concerns.
The authors study "robustness curves" which are plots of the robust error versus the radius used in the corresponding l_p ball threat model.  Pro: I completely agree with the authors that the current evaluation purely based on evaluation for a single radius is insufficient and one should report the complete curve.   Con: The authors are overclaiming that they have come up with robustness curves. Very early papers e.g. even in the adversarial training paper of Madry there are plots of robust accuracy versus chosen threshold. Moreover, I agree with one of the reviewers that using PGD for the purpose of a robustness curve is inaccurate and in particular inefficient as several attacks for different radii have to be done. There have been several attacks developed which aim to find the adversarial sample with minimum norm and thus compute the robustness curve in one run.  The additional insights e.g. intersection of robustness curves are partially to be expected and I don t find them sufficient to move the paper over the bar for ICLR.  As these insights are additionally  only shown for relatively small models which seem far away from the state of the art, it is unclear if they generalize. However, I encourage to follow some of the reviewer s suggestions to improve the paper.
While the results are promising, several concerns were raised in the reviews, leading to the reject recommendation at this time.  There is an agreement among all reviewers that the paper would benefit from a revision.  Most reviewers felt that the paper lacks a rigorous and compelling theoretical justification for the proposed algorithm, making suggestions for what would make the paper stronger.  AnonReviewer4 would like to additionally convey the following message:  I would like to thank the authors to revise the statement of the assumptions according to my suggestions. However, the wording "with high probability" has rigorous mathematically meaning, so should be used with care. Their Figures still don t justify the current statement of the assumptions in my opinion. Their experiment in Appendix E, which only contains one single example, is still too simplistic and cannot fully justify their claim. I would encourage the authors to test on more examples.
This paper proposes asymmetry learning for learning counterfactual classifiers, i.e. classifiers which are invariant to certain symmetry transformations w.r.t. hidden variables that differ between the training and test sets.  The reviewers universally agreed that the proposed setting, and theoretical contribution, were interesting and novel. They also praised the writing quality, but had some quibbles about the quality of the experiments, and discussion of prior work. Neither of these concerns were considered significant enough to be a barrier to acceptance, but the authors should try to improve them, if possible.
This paper proposes a new generation technique for multi category marked temporal point processes.  The paper was reviewed by three expert reviewers who expressed concerns for limited novel contributions, theoretical justification, and empirical evidence. The authors are encouraged to continue research, taking into consideration the detailed comments provided by the reviewers.
The reviewers are unanimous in their opinion that this paper offers a novel approach to causal learning.  I concur.
The paper considers an important problem of investigating the effects different statistical characteristics of representations (hidden unit activations) , such as sparsity, low correlation, etc, have on the neural network performance; while all reviewers agree that this is clearly a very important topic, there is also a consensus that perhaps the authors must strengthen and emphasize their contribution more clearly.  
This paper proposes a hybrid algorithm that combines RL and population based search. The work is interesting and well written. But, the contribution of the work is very limited, in comparison with the state of the art. 
The paper presents an algorithm for real time auto ML based on zero shot learning, which matches an ML pipeline to a dataset via the meta features of the pipeline and the dataset.  It aims to address an important problem, and the idea of the proposed solution seems interesting. However, there are several issues with the current draft:   (1) A central question is how to justify the complexity of the proposed solution, given that simple alternatives, such as Random Forest, can achieve similar or better results than the proposed method.   (2) The proposed solution lacks in novelty contribution of methodology and theoretical justification   (3) Various technical details are missing in the current draft.  The authors  feedback did not fully address the issues above. We hope that reviews can help the authors improve the draft for a strong publication in the future. 
This paper proposes benchmark tasks for offline policy evaluation.  The proposed benchmark tasks evaluate the policy with batch data with respect to three metrics, including the standard mean squared error. The paper also evaluate several baseline offline reinforcement learning methods with the benchmark tasks, which will serve as standard baselines.  All of the reviewers are in favor of the paper.
This paper tackles the challenging problem of learning against an opponent that may or may not be simultaneously learning as well. The key contribution of this paper is a learning algorithm that accounts for how the opponents may update their policies from past interactions. The proposed algorithm, MBOM, relies on the environment model to model a hierarchy of opponents using different depths of recursive reasoning (from non learning agents to deep recursive agents). It is agreed that this papers studies an important problem and shows promise. However, the current results aren t convincing enough. In particular, since there is no theoretical analysis, more empirical validation of the method is expected. The current experiments only considers a single opponent, and it is unclear how well the method works given accumulated errors through the recursion. Future submissions would benefit from additional empirical analysis (e.g., ablations) to help understand when and why MBOM works.
The paper performs an ablation analysis on LSTM, showing that the gating component is the most important. There is little novelty in the analysis, and in its current form, its impact is rather limited.
This paper considers the problem of reinforcement learning with goal conditioned agents where the agents do not have access to the ground truth state.  The paper builds on the ideas in hindsight experience replay (HER), a method that relabels past trajectories with a goal set in hindsight.  This hindsight mechanism enables indicator reward functions to be useful even with image inputs.  Two technical contributions are reward balancing (balancing positive and negative experience) and reward filtering (a heuristic for removing false negatives).  The method is tested on multiple tasks including a novel RopePush task in simulation.   The reviewers discussed strengths and limitations of the paper.  One strength was that the writing was clear for the reviewers. One limitation was the paper s novelty, as most of these ideas are already present in HER with the exception of reward filtering.  Another major concern was that the experiments were not sufficiently informative.  The simulation tasks did not adequately distinguish the proposed method from the baseline (in two of the three tasks) and the third task (RopePush) was simplified substantially (using invisible robot arms).  The real world task did not require the pixel observations.  The analysis of the method was also found to be somewhat limited by the reviewers, though this was partially addressed by the authors.  This paper is not yet ready for publication since the proposed method has insufficient supporting evidence.  A more thorough experiment could provide stronger evidence by showing a regime where the proposed method performs better than alternatives.
This paper proposes a new link prediction algorithm based on a pooling scheme called WalkPool. The main idea is to jointly encode node representations and graph topology information into node features and conduct the learning end to end. The paper shows the superiority of the method against the baselines.  Strength * The paper is generally clearly written. * A new method is proposed, which is technically sound. * Many experiments are conducted to verify the effectiveness of the proposed method.  Weakness * The novelty of the work might not be so significant.  There is a similarity with the SEAL algorithm.  The authors have addressed most of the problems pointed out by the reviewers. They have also conducted additional experiments.
The paper proposes a nice approach to massively multi label problems with rare labels which may only have a limited number of positive examples; the approach uses Bayes nets to exploit the relationships among the labels in the output layer of a neural nets. The paper is clearly written and the approach seems promising, however, the reviewers would like to see even more convincing empirical results. 
This work proposes a hybrid system for large scale distributed and federated training of commonly used deep networks. This problem is of broad interest and these methods have the potential to be significantly impactful, as is attested by the active and interesting discussion on this work. At first there were questions about the originality of this study, but it seems that the authors have now added extra references and comparisons.  Reviewers were split about the clarity of the paper itself. One notes that "on the whole clearly presented", but another finds it too dense, disorganized and needing of more clear explanation. Reviewers were also concerned that methods were a bit heuristic and could benefit from more details. There were also many questions about these details in the discussion forum, these should make it into the next version.  The main stellar aspect of the work were the experimental results, and reviewers call them "thorough" and note they are convincing. 
The authors develop a memory based method for continual learning that stores gradient information from past tasks. This memory is then used by a proposed task aware optimizer that, based on the task relatedness, aims at preserving knowledge learned in previous tasks.  The initial reviews were reasonable but indicated that this paper was not yet ready to be published. In particular, the reviewers seemed to agree on the somewhat limited methodological novelty of the paper given prior work (such as LA MAML and OGD in terms of method and GEM in terms of task similarity comparison).  In their response, the authors do seem to agree to a certain extent with some of the criticisms, but also point to clear differences with respect to previous work (and other distinguishing aspects such as a smaller memory footprint than OGD). The authors also carefully responded to reviewer comments and provided additional results when possible.  In the end, the main criticism from the reviewers remained (Reviewer 95tf also suggests that the authors should compare their method to others in terms of memory consumption (which the authors partly did) and compare to replay based methods) and this paper was a borderline one. Three, out of the four, reviewers suggest that it is not ready to be published. One reviewer did give it a high score (8) but also understood the limitations raised by the other reviewers. As a result, my recommendation is that this paper falls below the acceptance threshold.   I am sorry that for this recommendation and I strongly suggest the authors consider the reviewer s suggestions in preparing the next version of this work. In particular, it seems like providing a full study of the memory usage of your approach vs. others as well as providing more insights about the "trajectory" (see the comment from ZR5n) might go a long way toward improving the paper.
This paper proposes a new method for graph representation in sequence to sequence models and validates its results on several tasks. The overall results are relatively strong.  Overall, the reviewers thought this was a reasonable contribution if somewhat incremental. In addition, while the experimental comparison has greatly improved from the original version, there are still a couple of less satisfying points: notably the size of the training data is somewhat small. In addition, as far as I can tell all comparisons with other graph based baselines actually aren t implemented in the same toolkit with the same hyperparameters, so it s a bit difficult to tell whether the gains are coming from the proposed method itself or from other auxiliary differences.  I think this paper is very reasonable, and definitely on the borderline for acceptance, but given the limited number of slots available at ICLR this year I am leaning in favor of the other very good papers in my area.
This paper observes the similarity between the universality in renormalization group and the lottery ticket hypothesis and proposes that the iterative magnitude pruning, which is used to find the winning tickets, could be a renormalization group scheme. The authors also provide some evidence on their theory on vision model of ResNet families. While it is interesting to try a theoretical explanation of the transferability of lottery ticket used in similar tasks using the theory from statistical physics, the paper does not provide enough experimental results to show how to use such an explanation to improve iterative magnitude pruning or determine the best architecture that can be transferred for different tasks. Therefore the work is more like working in the progress report and not ready for publication yet.
The paper presents a linear classifier based on a concatenation of two types of features for protein function prediction. The two features are constructed using methods from previous papers, based on peptide sequence and protein protein interactions.   All the reviewers agree that the problem is an important one, but the paper as it is presented does not provide any methodological advance, and weak empirical evidence of better protein function prediction. Therefore the paper would require a major revision before being suitable for ICLR. 
The paper proposes a method for SLAM like dense 3D mapping (colored occupancy grid) based on differentiable rendering with a possibility to provide a probabilistic generative predictive distribution, evaluated on UAVs.  Initially this paper has a wide spread of reviews, with ratings between 4 and 9. Reviewers appreciated the elegant and principled formulation and the interest of the predictive distribution. On the downside, several issues were raised on the incremental nature wrt to DVBF LM; presentation and writing being very dense and difficult to follow; positioning wrt to prior art; performance with respect to known visual SLAM SOTA baselines; limited evaluations.  The authors provided responses to many of this questions and also updates to the paper, which convinced several reviewers, who unanimously recommended acceptance after discussion.  The AC concurs.
This paper presents OmniNet, an architecture based on the popular transformer for learning on data from multiple modalities and predicting on multiple tasks.  The reviewers found the paper well written, technically sound and empirically thorough.  However, overall the scores fell below the bar for acceptance and none of the reviewers felt strongly enough to  champion  the paper for acceptance.  The primary concern cited by the reviewers was a lack of strong baselines, i.e. comparison to other methods for multi task learning.  Unfortunately, as such the recommendation is to reject.  However, adding a thorough comparison to existing literature empirically and in the related work would make this a much stronger submission to a future conference.
This submission proposes a method for detecting adversarial attacks using saliency maps.  Strengths:  The experimental results are encouraging.  Weaknesses:  The novelty is minor.  Experimental validation of some claims (e.g. robustness to white box attacks) is lacking.  These weaknesses were not sufficiently addressed in the discussion phase. AC agrees with the majority recommendation to reject. 