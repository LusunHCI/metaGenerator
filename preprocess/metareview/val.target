This work analyses the impact of mini batch size on the variance of the gradients during SGD, in the context of linear models. It shows an inverse relationship between the variance of the gradient and the batch size for such models, under certain assumptions. Reviewers generally agree that the work is theoretically sound. However, all reviewers believe that the contributions of this work are limited. This concern was not adequately addressed during the discussion phase and led to the ultimate decison to reject.
This paper introduces a set of techniques that can be used to obtain smaller models on downstream tasks, when fine tuning large pre trained models such as BERT. Some reviewers have noted the limited technical novelty of the paper, which can be seen more as a combination of existing methods. This should not be a reason for rejection alone, but unfortunately, the results in the experimental section are also a bit weak (eg. see [1 4]), there are not very insightful analysis and it is hard to compare to existing work. For these reasons, I believe that the paper should be rejected.   [1] DynaBERT: Dynamic BERT with Adaptive Width and Depth  [2] Training with quantization noise for extreme model compression  [3] MobileBERT: a Compact Task Agnostic BERT for Resource Limited Devices  [4] SqueezeBERT: What can computer vision teach NLP about efficient neural networks?
Thank you for your submission to ICLR.  The reviewers unanimously felt that there were substantial issues with this work, owing to the fact that both the techniques and applications have been considered in a great deal of previous work.  Furthermore, the manuscript itself needs substantial amounts of revision before being suitable for publication.  As there was no response to these points during the rebuttal period, it seems clear that the paper can t be accepted in its current form.
The paper proposes a method to identify informative latent variables by thresholding based on the conditional generative model. While the exposition of the paper has substantially improved during the discussion period, some major concerns remain after the discussion among the reviewers. In particular, the problem considered in the paper has a very limited scope. Moreover, the evaluation of the methods needs to be improved. The paper could benefit from discussing how it situates in the broader context.
The reviews are a bit mixed. While all the reviewers feel that the paper proposed an interesting mechanism to train conditional generators from a single image and demonstrated good image editing results in the experiments, there are also common concerns about the practicality of the proposed method for interactive image editing. All the reviewers asked for the computation time, and some expressed the concerns about technical contributions. While these concerns were (somewhat) addressed in the rebuttal, the AC feels that it’s a hard sell to bet on the dramatic increase of computational capacity to make the computing time from an hour to realtime. Concerns about novelty also remained. Given the drawbacks, the final decision was to not accept. However, this work is promising and can be made stronger for publication in a later venue.
A majority of the reviewers find the paper lacks novelty and provides an insufficient discussion of the state of the art in knowledge distillation and student teacher training to warrant publication. The approach is quite narrow to the application domain and the paper does not provide novel insights on how to chose a good network. A subset of the experiments performed on an internal data set with random train test splits do not evaluate a realistic transfer setting.  
This work is likely to lead to more connections between machine learning and neuroscience at a fine grained level where ML methods can help explain and understand neural circuits.  To encourage this, it would be helpful if authors described the biology of the PN KC APL network and the known constraints over possible formalizations of that network. The authors present one formalization, but little discussion is given toward the design space for such models. Are there other possible ways to describe the PN KC APL network? Are all alternate ways to do so equivalent to the model presented here? What properties are unknown and how could they affect the formalization presented here?  Overall, reviewers agree this is a good submission.
Three reviewers provided negative reviews and the authors wrote detailed feedback. During the later discussion stages, the reviewers acknowledged that some concerns are alleviated (e.g. R1 raised score from 4 to 5), but two concerns still remain: i) the novelty is less clear to the reviewers; ii) the advantage over existing approaches is not strong enough. I personally think the first concern is somewhat subjective; for the second concern, the authors indeed added a few experiments on "comparison to Top K, SignSGD, EF SignSGD, and PowerSGD", but R1 still maintained that the comparison is "not well demonstrated", which I guess is because R1 views the two works mentioned in R1 s review as "concurrent or later" while R1 pointed out in the discussion that those two works appear before Oct 2019.   R2 pointed out that the paper could have shown the advantage in one of two aspects (see below) but did not.  See the discussions of the three reviewers below.  R1 "The advantage of the proposed compression method over existing approaches is not well demonstrated." R2: "To justify the significance of the contribution, I think the paper should show at least 1 of the followings: When approaching a very high compression ratio, the natural compressor has significantly better testing accuracy compared to the previous work, i.e., the proposed work could push the compression ratio higher than the previous work with nearly no accuracy loss. The authors could use the new compressor alone, or combine it with some other compressors such as top k, as the authors mentioned in the answers. (However, according to the extra experiments, the testing accuracy is slightly better than top k, and slightly worse than power k. None of them makes a significant difference.) The natural compressors can achieve similar accuracy compared to the previous work with the same compression ratio, but with much less computation overhead. (I think this one may work for this paper, as the authors mentioned in their answers. However, to justify such a contribution, we will need to check the training time compared to EF SGD with top k and power k, which seems not included in the extra experiments.)" R4:  "I stand with my point that the novelty is limited."  Overall, I think the paper might have presented an interesting idea, but unfortunately can not be accepted in the current form.  
This paper proposes a self supervised learning method for learning representations for graph structured data, with both local and global objectives. The local objective aims to maximize the mutual information between two correlated graphs generated with attribute masking [Hu et al. 19], with the InfoNCE loss [van den Oord et al. 18], and the global objective aims to cluster the graphs using the RPCL [Xu et al. 93] objective, which pulls the sample toward the closest cluster while pushing it away from the rival clusters. The proposed method is validated on standard graph classification benchmarks by training a linear classifier on top of the GNN pre trained with it, and the results show that it largely outperforms existing graph pre training methods.   This paper fell into a borderline case, receiving split reviews with two of the reviewers learning toward rejection, and two others proposing to accept. The reviewers in general agreed that the experimental validation is thorough (except for one reviewer), and some of the reviewers mentioned that the proposed idea of performing self supervised learning at both local and global level makes sense. However, the negative reviewers were concerned with the limited novelty of the proposed method, since the proposed method seems like a simple combination of two objectives each of which are based on existing ideas (although the latter has not been explored for GNN pre training). The reviewers had interactive discussions with the authors, and the authors provided detailed feedback. Yet, the reviewers were not convinced that the method has sufficient novelty to warrant publication even after the internal discussion period, and decided to keep their negative ratings.  I believe that this is a simple yet effective pre training method for GNNs on graph structured data. The proposed method of combining the local and global objective seems like a promising solution to learn a metric space that well captures the graph level similarity and also is well separated for discriminative classification, and it may have some practical impact given its good performance on benchmark datasets. However, as the two negative reviewers mentioned, the paper in its current form is presented as a simple combination of existing approaches. The local objective is a slight modification of attribute masking strategy of [Hu et al. 19], and the global objective of clustering has been explored in self supervised learning of CNNs for image data [Asano et al. 20]. Thus, I lean toward rejecting the paper, considering its relative novelty and quality.   However, I find the proposed work highly promising, and encourage the authors to further develop the method while also improving on the paper writing. I suggest the authors to focus more on the main idea of learning with both local and global objectives, without specifically tying each objective to any of the existing methods. The authors may consider various techniques for both local and global objectives (such as hinge loss based contrastive loss with k means clustering as shown in the response to R3), and suggest the proposed work as a more general framework.    [Asano et al. 20] Self Labeling via Simultaneous Clustering and Representation Learning, ICLR 2020
This paper introduces the idea of cascading decision trees.  The reviewers agree that this is a potentially novel and valuable idea, but they also agree that the paper fall short in execution.  The paper would be substantially strengthened with more theoretical analysis, more discussion of why cascading decision trees are useful, and most importantly substantially more empirical evaluation, especially with more data sets and more baselines for comparison.
This article proposes a novel weakly supervised segmentation method that unifies several annotation types using contractive/metric learning. This method clearly outperforms the current SOTA. While the unified framework itself is not novel enough, the reviewers agree that the contrastive loss formulation is interesting and the extensive experiments show its effectiveness. Overall, I consider that this unified framework is well engineered, the formulations are insightful, and the results advance the SOTA of weakly supervised segmentation. Accordingly, I propose to accept this paper at ICLR 2021.
This paper considers the problem of (biological) sequence design and optimization. The authors made an interesting yet important case that in certain sequence design tasks, a simple evolutionary greedy algorithm could be competitive with the increasingly complex contemporary black box optimization models.  Most reviewers appreciate the design of the open source simulation environment in benchmarking AdaLead (and other competing algorithms) in a number of biological sequence design tasks (e.g. TF binding, RNA, and protein). However, there is a common concern that the experimental results are not fine grained enough to explain the outperforming results of the proposed algorithm. There are also unresolved comments on missing important BO baselines in the empirical study. As a purely empirical work, these appear to be important concerns. While these results appear to be useful for the domain of biological sequence design, the reviewers are unconvinced that the proposed algorithm is significantly novel, or the results are sufficiently compelling to merit an acceptance to this venue. 
The submission proposes a transductive few shot classification method on the basis of the simple Conditional Neural Adaptive Processes (CNAPS) introduced by Bateni et al. The paper received two borderline accept and two borderline reject reviews, indicating that the paper may not be yet ready for a publication. The meta reviewer recommends rejection based on the observations below.  All reviewers indicated that the paper is well motivated, clearly written and neatly organized. However, all four reviewers agree that the novelty of the paper compared to the CNAPS paper is limited. The main novelty of the method being transductive CNAPS extends the task encoder of CNAPS to incorporate both a support set embedding and a query set embedding through Long Short Term memory (LSTM) network. Similarly, the classifier in CNAPS has been modified to operate in the transduction setting, i.e. it is extended to include the unlabeled examples in the query set.  The reviewers indicate that extension of the task encoder via LSTM may not be enough technical novelty for such a competitive venue. Additionally, in terms of experimental evaluation, although R1 found the experimental evaluation adequate, R3 indicated some concerns about the unexpected behaviour of the method and R4&R2 found the benchmark evaluations limited. 
The authors address the important task of improving dialogue summarization using conversation structure and factual knowledge.  Pros: 1) Clearly written and well motivated (as acknowledge by all reviewers) 2) Technically sound (the proposed architecture is clearly in line with the problem that the authors are trying to solve) 3) Significant upgrades to the paper after the reviewer comments (in particular the authors have added detailed ablation studies and results on non dialogue datasets)  Cons: 1) There is a significant difference between the results in the ablation studies in the original version and in the new version. Originally, the differences between KGEDCg and KGEDCg GE and KGEDCg FKG were very minor, but now the margins are as large as 7+ pts. I would request the authors to explain this in the final version  The reviewing team felt that while many Qs were sufficiently addressed by the authors, the large difference in the numbers reported for the ablation study in the initial and final version of the paper raises some new Qs which need to be addressed before the paper can be accepted. 
This is a solid paper that proposes a new method for approximating softmax attention in transformer architectures that scales linearly with the size of the sequence. Even though linear architectures have been proposed before using a similar idea (Katharopoulos et al 2020), this paper provides a better solution along with theoretical analysis and makes a rigorous empirical comparison against other methods. All reviewers agree that this is a strong paper that should be accepted. I suggest citing the recent paper https://arxiv.org/abs/2011.04006 (Long Range Arena, mentioned in the discussion) which provides further comparisons on long range benchmarks, including the method presented in this paper and Katharopoulos et al 2020, along with a detailed discussion of the differences between the two methods.
The paper proposes a discretization of Wasserstein gradient flow with an euler scheme, and propose a way to estimate each step of the euler scheme using ratio estimators from samples regularized with gradient penalties. Statistical bounds are given to bound the estimated flows from the wasserstein flow.    Reviewers have raised concerns regarding the assumptions under which results present in the paper hold, this was clarified by the authors (goedesic lambda convexity, log sobolev constant for the target density . lipchitizity of velocity fields).  The paper needs a revision to incorporate that feedback and to be in shape for publication.   Other concern were on earlier claims in the paper regarding the monge ampere equation and approximation of the optimal mapping this was addressed by the rebuttal.   Other concerns were also on explaining the relation of the work to score based models and energy based models.   Overall the paper needs to state in a clearer way the assumptions for the theoretical results and to acknowledge the limitations of those assumptions in analyzing the euler scheme.   
This paper advances the idea that recent “influence estimation” methods for supervised learning cannot be trivially applied to GANs. Based on Hara et al.’s method, the authors propose a novel influence estimation for GANs, and an evaluation scheme based on popular GAN evaluation methods, exploiting the fact that they are differentiable with respect to their input data. The paper demonstrates empirically that the proposed influence estimation method correlates to true influence. It also shows that removing “harmful” instances using the average log likelihood, Inception Score, and Frechet Inception Distance versions of the proposed metric improves the quality of generated examples.  All reviewers were positive about the paper. R2 pointed out that it was well written and appreciated the detailed analysis. They thought it thoroughly explained the similarities between it and the most closely related recent work (Hara et al. and Koh & Liang). Concerns expressed by the reviewer were: the amount of samples needed to be removed to obtain a statistically significant result, lack of qualitative results, and an outdated baseline for anomaly detection. The reviewer also stated that they had some concerns with practical applicability and would like to see more GAN metrics, like Precision & Recall. The authors added qualitative results to the paper which partially satisfied the reviewer.  R1 also thought that the paper was well written and contributed to the interpretability of GAN training. Like R2, they pointed out the lack of visual examples (addressed in rebuttal), and asked for more insight into what kind of characteristics make a data point influential. They also requested that the authors add a metric that trades fidelity and diversity like P&R. The reviewer originally felt that the paper was below the bar, because it was “like a story without a satisfying conclusion”. However, the authors responded with additional analysis which satisfied the reviewer, and they upgraded their score by two points.  R3 also found the paper well written and interesting, like the other reviewers. The reviewer raised some similar concerns as the other reviewers (e.g. qualitative results), as well as the scalability of the method to relevant architectures, which I thought was surprising that the other reviewers didn’t mention. The authors responded that they believe their method succeeded in improving diversity of the generated samples but not their visual quality. This is an important point. The additions in Appendix D have addressed the main concerns of R1 and R2, as well as R3’s concern about lack of visual analysis. R1 seems quite convinced now, and R2, though not changing their score, was already in favour of acceptance. It is an interesting finding that “harmful” instances seem to come from regions of distributional mismatch.  I would like to see a fidelity diversity tradeoff like P&R added to a paper, and a discussion of this work in relation to DeVries et. al “Instance Selection” that appears to be similarly motivated though executed differently. I think one major thing holding back this paper is the scale of the experimental analysis (Gaussians & MNIST); I hope the authors can scale the method in future work.
Reviewers were all on the positive side for this paper. Multiple reviewers liked the new and interesting task that this paper presents, found that the proposed method works well, is sufficiently compared to alternative approaches, and could serve as a solid baseline for future work in this area. The main limitation that reviewers noted was that results were shown only on the synthetic ABC dataset using dense point clouds with very little noise. The authors wrote *very* thorough responses to all reviewer questions. One reviewer noted that these responses answered all of their questions.  It is worth noting that a paper that solves a similar problem was recently published at NeurIPS ("PIE NET: Parametric Inference of Point Cloud Edges"). This paper was not published as of the ICLR submission deadline, so it was judged as  contemporary work  which the authors have no obligation to compare against. Nevertheless, they did attempt to make a comparison in their responses. I would ask that the authors include some discussion of this comparison in their final version of the paper.
The paper presents a generalization bounds for l1 regularized networks.  The reviewers thought the results were clear and sound, but on the other hand rely on rather standard technical tools, and their impact is limited. One question is why this particular regularization is related to practical learning of neural nets where explicit regularization is not used. The authors may want to relate their results to e.g., Theorem 1 in https://arxiv.org/pdf/1412.6614.pdf.  
This paper proposes an anonymization method for federated learning based on the Indian buffet process. The reviewers found the idea interesting, but raised the following main concerns (please see the reviews for more details): * Motivation and terminology needs clarification * Better comparison with secure aggregation methods * Missing privacy guarantees Overall the reviewers of this paper are borderline. I hope the authors will take the reviewers  feedback into account when revising the paper. 
Most reviewers did not feel that this paper was ready for publication. I thank the authors for answering all the concerns of the reviewers, running new experiments and submitting a revised version, however, this was not not enough to alleviate the reviewers  concerns, notably relating to the handling of the ethical consideration in the writing of the manuscript.
The idea of combining instance level contrastive loss and deep clustering is a promising direction in recent unsupervised/self supervised visual representation learning studies. However, authors did a poor literature review and did not cite and compare with quite a few recent popular work exploring the similar direction. The proposed methodology is not particular novel and the experimental results are also not convincing. Overall, the paper explored a promising research direction, but the paper quality is clearly below acceptance bar. 
In this paper, the authors studied a robust method for detecting out of distribution (OOD) instances. OOD instance detection is an important practical problem, and multiple reviewers recognized the proposed approach is interesting. However, it was the common opinion of several reviewers that the main theoretical analysis was imported from existing studies, and the novelty is not sufficiently high. It was also observed that the relationship between the proposed method and closely related studies was not properly discussed. Although this point has been improved in the revision, a reviewer and area chair still concern that enough evidence is not provided for some of the points the authors claim as advantages over existing studies. Although the proposed method is interesting and could be an important contribution to the ICLR community, the current paper needs non trivial revision before publication.
This work proposes a novel reparameterization of batch normalization that is hypothesized to give a better inductive bias for learning several tasks, including neural architecture search, conditional image generation, adversarial robustness and neural style transfer. The reviewers indicate that this is useful and is of interest to the ICLR audience, but they are not satisfied with the analysis offered in the paper. Specifically, the reviewers request that the authors provide a more detailed analysis of why the proposed reparameterization improves results, given that it does not change the expressive power of the model class. Additionally, the reviewers have some concerns about the structure of the paper. I therefore recommend rejecting the paper at this time.
The reviews were a bit mixed, and there was some concern on the usefulness and actual novelty of this work. On one hand, the authors did a nice job in visualizing their findings and conducting a wealth of interesting experiments. On the other hand, the submission suffers severely from hand waving definitions and arguments. Many terms were not precisely defined, various hyperparameters were not thoroughly investigated, and yet conclusions were made based on indirect experimental results. The AC agrees with the reviewers that it is not very clear how this work would impact the field. For exploratory work like this one, there is also a great danger that one may simply overfit the observations and squeeze conclusions from thin air. It would be more convincing if the authors could largely quantify their definitions and results. For example: what do we mean by human aligned? robust / nonrobust feature? (this definition depends on the perturbation size hence needs more elaboration.) Is there any way to quantify the results in Fig 2, including the impact of epsilon? Should these adversarial examples be called universal if their ASR falls below what threshold? Are (some of) the conclusions (e.g. translation invariance, semantic) a direct consequence of the perturbation being universal? At this stage this work would be an excellent workshop paper but a bit more rigor would be needed for publishing at the ICLR conference. 
The discussion with the expert reviewers reached the consensus that the paper lacks in novel *technical* contributions, and as such it does not meet the bar for a theory oriented paper at ICLR.
This paper studies the following model: The input to our classifier is the instance X which determines the label Z and we observe a noisy version of this label Y. The key assumption is that the label noise is independent of the instance, and the goal is to learn the channel from Z to Y. The main motivation is that generally algorithms that can handle instance independent noise need to know the noise model. Thus the main contribution of this paper is to decouple the problem of learning the noise channel and the problem of learning a high accuracy classifier. In particular they inject their own label noise and design a discriminator to test if the noise on the labels has maximum entropy. They show that their method is statistically consistent. Finally they complement this with synthetic experiments on CIFAR to show that their algorithm works.   While the reviewers all found the ideas promising, they brought up a few deficiencies in this work which they hope could be improved in later versions. First, the writing is at times unclear and imprecise. For example, there are many places that could benefit from further discussion, particularly in terms of justifying why the assumptions are "mild" or not. Second, the experiments would be more compelling if there were an application where learning the noise model actually led to improved performance on some downstream application. Third, the approach crucially relies on having a separable map, which seems like a rather strong assumption. 
This paper studies the relationship between test error as a function of training set size and various design choices of neural network training. Overall all of the reviewers are excited about the prospect of relating error curves to neural network design choices, but different reviewers complain about the rigor of empirical evaluation and the accuracy of conclusions given limited data points. I agree with reviewers on both points, i.e., the paper studies different design choices, but does not do a thorough job studying those design choices. Moreover, it is not clear what aspects of the study are directly related to error curves vs. a standard correlation study done in prior work, e.g. in "Do better ImageNet models transfer better?" for usefulness of ImageNet pre training. So, overall, I believe not only the empirical evaluation needs improvement, but also the story needs refinement. I am looking forward to seeing this paper published in other ML venues.
The authors study the problem of augmenting embedding based entity alignment in knowledge graphs (KG) through the use of joint alignment with deduced neural ontologies (more specifically, alignment of the KG  neural  axioms). Motivated by the observation that the representation between two potentially aligned entities must be bound by a minimal margin, which can be problematic when there are many potential alignments, they propose aligning neural axioms by Wasserstein distance based loss between learned entity embeddings conditioned on the relation embeddings. Experiments are conducted on OpenEA against multiple strong baselines   showing that adding the ontology alignment to these baselines improves the results.    Pros   + The addition of aligning (conditional) ontologies is ostensibly novel. + For KGs with sufficient entity/relation overlap, the proposed NeoEA method is applicable.  + NeoEA has been shown empirically to improve many SoTA methods.    Cons      While the theoretical justification is a welcome motivation, the reviewers did not find the theoretical arguments significant nor convincing.   Overall, the narrative needs work to make the paper more self contained and approachable for a broader range of readers. The reviewers (and myself) found many concepts and statements somewhat confusing and needing clearly context and contrast with existing works.  Evaluating along the requested dimensions:   Quality: Conceptually, the core idea is interesting, well motivated, original, and ostensibly effective. Empirically, NeoEA is shown able to improve upon several strong baseline (underlying) methods.  I believe that all of the reviewers find the work is interesting and promising. However, there were continuing concerns the strength/value of the described theory; it isn t clear if stronger theory isn t possible or if this just hasn t been fleshed out.    Clarity: Most of the reviewers (and myself) found the paper difficult to follow as a self contained work in terms of concepts, clear definitions (e.g., \mathcal T isn t defined early on) and the actual applicability of the theory. The figures help, but even these need some work. A related work section (or more structured presentation of related work) might be clarifying along with running examples and a more unifying math presentation that captures existing and proposed work. After thinking about this more, it is actually a relative simple (in a good way) and clever idea. However, it took several readings and readings of related work to get there. Additionally, the fact that all of the reviewers were concerned about different limitations is concerning wrt clarity. Appendix B helps a bit and I believe can also be put into the main paper.   Originality: As best as the reviewers and I can tell, we haven t seen this method applied to entity alignment despite this being a relatively mature subfield.   Significance: The consensus seems to be that the approach could be a notable contribution to an important area. However, it also appears that most of the reviewers don t feel the paper is ready for publication at a top tier venue yet.  As stated throughout this meta review, there are several aspects to like about this work including the originality of the idea, strong motivation, and good empirical results. However, we all agreed that the paper isn t quite ready in its current form   thus, I presently recommend reject for this submission.
This paper proposes a benchmark suite of offline model based optimization problems. This benchmark includes diverse and realistic tasks derived from real world problems in biology, material science, and robotics contains a wide variety of domains, and it covers both continuous and discrete, low and high dimensional design spaces. The authors provide a comprehensive evaluation of existing methods under identical assumptions and get several interesting takeaways from the results. They found there exists surprising efficacy of simple baselines such as naive gradient ascent, which suggests the need for careful tuning and standardization of methods in this area, and provides a test bed for algorithms that try to solve this challenge. However, most reviewers agreed that a more in depth analysis and insightful explorations for the RL experiment results will help readers understand why their method has superiority even without trajectory data, and  that the paper needs another revision before being accepted. Therefore, I recommend rejection although all reviewers agreed that the tasks is very interesting and a good start.
The paper studies a novel problem setting of automatically grading interactive programming exercises. Grading such interactive programs is challenging because they require dynamic user inputs. The paper s main strengths lie in formally introducing this problem, proposing an initial solution using reinforcement learning, and curating a large dataset from code.org. All reviewers generally appreciated the importance of the research problem studied and the potential of the work. Even though the reviewers found the work interesting, there was a clear consensus that the work is still immature and not yet ready for publication. I appreciate the authors  engagement with the reviewers during the discussion phase. Overall, the reviewers have provided very detailed and constructive feedback to the authors. We hope that the authors can incorporate this feedback when preparing future revisions of the paper. 
All reviewers tend towards accepting the paper, and I agree.
The paper proposes a new variant of capsule networks, where iterative routing is replaced by an attention based procedure inspired by Induced Set Attention from Set Transformers. The method is competitive on several classification benchmarks and improves generalization to unseen views on SmallNORB.  The reviewers note that the method is presented well (R2, R3, R4), is more scalable than other capsules variants (R3, R4), and the results are good (R1, R2, R3, R4). However, the reviewers also point out missing relevant baselines (R2, R3, R4), limited amount of generalization experiments (R3), and issues with the positioning of the method and the details of the formulation (R1). In particular, R1 did a very thorough job at reading the paper and discussing with the authors.  The issue with missing baselines has been satisfactorily addressed in the updated version of the paper.  Considering all this feedback and after reading the paper myself, I would summarize the pros and cons of the paper as follows.  Pros: 1. Good presentation 2. The method is more scalable than prior capsule based models 3. Competitive results on several small  to mid scale classification datasets 4. Good results on viewpoint generalization on SmallNORB   Cons: 1. Classification results on all datasets are worse than non capsules models (SE ResNet, AA ResNet). I could not find a discussion of this fact either in the paper, or in the authors’ responses. Given this fact, superior generalization (or some other nice properties) would be a potential advantage of the proposed model. Which leads to the next point. 2. Generalization results on SmallNORB are encouraging, but it is just a single dataset. If these results are key to showing the benefit of the method (as argued in the previous point), it is crucial to demonstrate this generalization in more settings, e.g. at least on MultiMNIST and AffNIST, as suggested by R3. 3. Scalability of the method is only studied in limited detail (I do appreciate Figure 2). The best indication in the direction of scalability is that the model can be trained on ImageNet (which is great), but it performs worse than the ResNet 50 used as a backbone and it is not explained why (even after one of the reviewers asked about it) and how expensive computationally the model is. 4. I share the concerns of R1 regarding the use of the term “MoG”. It is a mathematical term, so one would expect mathematical precision when using it.   4a. It is unclear how the mixing probabilities \phi are learned (IIUC they get no gradient, as described by R1) and if they are in some way actually learned, it is unclear how it is guaranteed that they sum to one.   4b. MoG usually comes with the standard procedure of fitting it to data (EM), which IIUC the authors are not following here. This should be clearly explained.  5. A relatively more minor concern: again, as pointed out by R1, the use of “self ” in “self attention” does not seem accurate. Self attention assumes inputs to the attention procedure attend to themselves in some sense. As one consequence, the output sequence has the same length as the input sequence. ISAB from Set Transformer can be seen as a factorized version of self attention where first inducing points attend to the inputs and then the inputs attend to the inducing points, so the output of the whole block is still the same length as the input. But in the proposed model this second step of going back to the inputs is absent and the length of the output sequence is generally different from the length of the input sequence.  Note: I partially share the doubts R1 raised on the positioning of the method as “capsules” as opposed to “attention”, but I believe it is not the authors’ fault that the definition of what capsules are is historically vague and that this term has been used in many different ways in the past. I would strongly recommend to discuss this point in the updated version of the paper and I hope the capsules community manages to get more clarity on what exactly capsules are. But I do not count this point as a weakness here.  Based on all this evidence, I recommend rejection at this point. The paper has its merit, but it has unfortunate gaps both on the experimental and the presentation sides, as listed above. Some of these have been mentioned during the discussion phase, but the authors have not quite addressed them. There is no mechanism to ensure these are fixed in the final version, so resubmission to a different venue is the only option. 
This paper has been evaluated by four expert reviewers resulting in two rejections one marginal score and one acceptance recommendation. The authors provided rebuttals to the critiques, but they did not sway the reviewers  assessments. The prevailing impression is that the work is interesting but perhaps not yet mature nor organized enough to benefit the ICLR audience in its current form. There is also some vagueness left at the conceptual level, e.g. regarding the actual objectives   some reviewers pointed out confusing entanglement of the concepts of simplicity and interpretability. Nonetheless, the paper presents an interesting work that will benefit from incorporating the constructive feedback received here 
This paper got mixed reviews. One for reject and three for acceptance. The reviewers and authors have extensive discussion. Authors also provided additional experiments for further clarifying some questions from the reviewers. The paper has some clarify issue in the theoretical justification part as pointed out by AR1. Authors should extensively improve this part or revise the statement. However, the method proposed in this paper is simple and the results are indeed good. This paper is valuable and should be shared within the community to advance research on ZSL. Therefore, AC recommends acceptance. 
This paper proposes OpenCos for semi supervised learning that can leverage unsupervised information in open set scenarios where samples can be out of class.  They first pre train by learning an unsupervised representation using SimCLR on both the labeled and unlabeled data.  Then, they detect out of class samples in the unlabeled set based on similarity measures on the representation learned in the previous step.  The unlabeled data can now be split into in class and out of class.  OpenCos optimizes (8) which combines a semi supervised loss for in class unlabeled data and an auxiliary cross entropy loss with soft labels for the out of class samples.  Finally, they perform an auxiliary batch normalization.  The paper is easy to read and clearly structured.  It also places the work well with respect to related work.  The proposed approach makes sense; however, as pointed out by the reviewers, the novelty is marginal.  The technical innovation seems to be an extension of SimCLR and the auxiliary batch norm of Xie et al. 
The paper proposed locally free weight sharing strategy (CafeNet) for searching optimal network width. The proposal is a nice tradeoff between manually fixed weight sharing pattern (too small search space) and completely free weight sharing pattern (too large search space). The *originality* and *significance* are clearly above the bar. The paper is related to the general interests of deep learning research and its *applicability* deserves a spotlight presentation.  It seems the *clarity* can still be improved, so please carefully revise the paper following the reviews. BTW, I am very curious, why "locally free weight sharing strategy" goes to a short name CafeNet? I went over the paper but I didn t find the answer. Perhaps the name of the proposal should also be explained...
The paper compares transfer learning with fine tuning and joint training and then proposes a new approach (Merlin). Reviewers have pointed to the fact that Merlin works in a setting that is different from normal transfer learning settings (it assumes some target domain data is available during training). The authors acknowledge this and think it can still be a reasonable setting, but of course it makes comparisons more difficult. Overall, while there are interesting analysis and results, the paper remains borderline and more work should be done to make it a good contribution, including significantly improving the presentation to make clear the distinction in settings. I therefore recommend to reject the paper.
This paper received 2 borderline accepts, 1 accept, and 1 reject.  This paper was discussed on the forum and no consensus was reached. The two reviewers who rated the paper as borderline accept emphasized that the biological claims are overblown, that the intellectual contributions (the initialization scheme and partial training) are incremental from a statistical learning perspective, and that the potential applications for the future (like alternate learning rules) are too speculative. I agree with both of these reviewers (and the negative reviewer) that the biological rationale is problematic and the approach is not credible as a model of biology. It is not evaluated as a computer vision model either. And I completely agree with the point raised by several reviewers that there is simply no data about how many synaptic updates to target. Hence, statements regarding % of total synaptic updates and % of brain matches seem empty without a precise target. For all these reasons, I recommend this paper be rejected.
The paper builds on the prior work by Miryoosefi et al. (2019) that finds a feasible mixed policy under convex constraints through distance minimization over a simplex set. Instead of the primal dual approach used in Miryoosefi et al. (2019), this paper proposes to apply Frank Wolfe type algorithm (particularly, the minimum norm point algorithm) to promote sparsity of the mixed policy, while achieving the same complexity.   Despite the improvement on sparsity, the AC and some reviewers share two main concerns: (1) incremental novelty of the algorithm/theory, which basically follows from existing optimization work, (2) lack of (theoretical and numerical) justification of the significance of sparsity (especially given that the main computation costs come from projection and RL oracle).   Unfortunately, the paper lands just below borderline and cannot be accepted this time.  
This paper explores the use of partial rejection control (PRC) for improved SMC based variational bounds. While an unbiased SMC variant with PRC has been previously introduced by Kudlicka et al. (2020), this work introduces innovations that can help apply such ideas to variational inference. These bounds result in improvements in empirical performance.   This paper was heavily discussed, with significant engagement by both the authors and the reviewers. Most reviewers recommended acceptance of this paper, with one reviewer (R4) recommending against acceptance. R4 s central concerns regard the novelty of the proposed approach and its positioning relative to the existing SMC literature. The authors argued vigorously in the comments that this paper should be judged as a contribution to the VI literature and not the SMC literature.  Unfortunately, I will recommend that this paper is rejected. It is my opinion that R4 s concerns were not fully addressed.  On the one hand, I agree with the authors that there is significant value to be had in exploring variants of SMC for VI. Indeed, some prior art, like FIVO and IWAE, contributed little to the Monte Carlo literature. I believe that these were good contributions.  On the other hand, I am concerned that the current draft does not clearly circumscribe its contributions. I read the sections that disuss the works of Schmon et al. (2019) and Kudlicka et al. (2020), and the writing did not leave me with a clear enough sense of the differences. I also read the abstract and introduction of the paper. The introduction of the paper positions this work clearly within the VI literature, but does not clearly discuss prior SMC art, e.g., it does not cite Kudlicka et al. (2020). Despite citing rejection control for SMC, the writing of the abstract and introduction left me with the impression that this work was the first to introduce *unbiased, partial* rejection control for SMC. I believe that impressions matter and that the machine learning community should be generous to adjacent communities when assigning credit.  I realize that my decision is a matter of taste. I also want to say that I am confident that the authors have a clear sense of where their contribution sits, and I suspect that it is a valuable contribution. However, I cannot recommend the draft in its current form. If this is a contribution to the VI literature, as the authors argue, then the authors should not hesitate to give full credit to prior SMC art. My reading of the current draft still leaves me confused about which aspects of the SMC estimator are actual contributions.
This paper received mostly positive reviews. The reviewers praised the strong performance when compared with previous work. Also, the evaluation clearly shows the benefit of the proposed contributions in terms of performance. Most concerns raised by reviewers were properly addressed in the rebuttal.  Lack of comparison to several previous works has been noted in a comment, but the authors clarified this concern, stating that the current work is a “large deviation from prior works”. The authors promised to include the missing references into the comparison.  Given the reviews, comments, and author s answers, I suggest acceptance.
The reviewers brought up many important concerns about this paper. On the positive side, the understanding of data augmentation is an important topic in deep learning, having good theoretical results is interesting here , and the experiments seem to do an okay job of backing up the theory. On the negative side, presentational issues make the paper difficult to follow and mischaracterize the results. A major issue is that some of the assumptions are hidden in the appendix and are not stated formally, and other assumptions are stated in a much weaker form, then made suddenly stronger when the theorems are stated. For example, Assumption 2 as stated holds trivially for any dataset as long as the possible data augmented versions any two different examples are disjoint (just choose the discrete metric on the images of the examples under the data augmentation function); however, in every theorem that uses A2, the distance chosen is restricted to be the L1 norm. Other Assumptions are stated strangely: for example, A1 says "i.e., for any $a_1(), a_2() \in A$, $a_1(x_1) ⫫ a_1(x_2)$ for any $x_1, x_2 \in X$ that $x_1 \ne x_2$. But what is the point of introducing $a_2$ if it s never used in the formula? And what is the meaning of the symbol ⫫? Normally, this is used for conditional independence, but there aren t any random variables in this formula ($a_1$ and $a_2$ are defined as just functions, not random functions, and $x_1$ and $x_2$ are just examples and aren t random variables either). This paper will be much stronger with these presentational issues cleared up.
The paper proposed a new in processing approach to train fair predictors under several notions of statistical fairness. Tho this end, the author rely  on  the Exponential Renyi Mutual Information (ERMI) between sensitive attributes and the target variable as notion f fairness, and show that it is a strong notion of fairness that provides guarantees on several previously discussed fairness metrics.   The paper is overall well written and interesting, but as with many other papers on this area, I wonder even after rebuttal whether the paper indeed constitute a step forward in the field. I find the concern raised by the reviewers about the tightness of the bound important and, while the authors properly addressed this point in the rebuttal period, I still believe this is an open question which probably does not have a better answer. On the positive side, the experimental evaluation support the theoretical results. However, comparisons to previous methods are only performed on the Adult and the German dataset, which makes me wonder if the advantages of the proposed approach generalize beyond these two well studied datasets. As a consequence, the paper remains borderline, as it is an interesting paper but its impact and significance remain limited.    Moreover, I believe that there are some missing recent related works, that I believe the authors should also compare to. For example, see the recent Neurips 2020 paper, "A Fair Classifier Using Kernel Density Estimation" by Cho et al.  Also, as a side note, previous approached have already considered non binary (although most of the times categorical) sensitive features, see e.g., [42]. Finally, the author may want to consider complementing their italic comment on the second paragraph of the Intro with existing works that already discussed biased in the labels, due to e.g., the selective labeling problem (see [1 3] below).     [1] Lakkaraju, Himabindu, et al. "The selective labels problem: Evaluating algorithmic predictions in the presence of unobservables." Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2017. [2] Kilbertus, Niki, et al. "Fair decisions despite imperfect predictions." International Conference on Artificial Intelligence and Statistics. PMLR, 2020. [3] Bechavod, Yahav, et al. "Equal opportunity in online classification with partial feedback." Advances in Neural Information Processing Systems. 2019.     
The paper studies the Lipschitz properties of neural networks — in particular, two layer neural networks that interpolate generic datasets. It conjectures a “size robustness tradeoff”: in this setting, the number of neurons required to interpolate with an O(1) Lipschitz function is proportional to the number of data points n, while the number of neurons required for interpolation alone is proportional to n/d, where d is the data dimension. More precisely, the conjecture is that the best achievable Lipschitz constant is proportional to $\sqrt{n/k}$, where $k$ is the number of neurons. The paper proves weaker versions of both sides of this conjecture: it proves that a spectral upper bound on the Lipschitz constant is lower bounded by $\sqrt{n/k}$ and that there exist networks achieving this Lipschitz constant when $k ~ n/d$ and $k ~n$. The paper also provides experiments supporting its claims, with the caveat that the actual Lipschitz constant is a worst case quantity that cannot be directly observed.   Pros and cons:  [+] The paper identifies a novel (conjectured) phenomenon involving the dependence of the Lipschitz constant of an interpolating network on the degree of overparameterization. In words, Lipschitz interpolation requires significantly more neurons than mere interpolation. This observation seems likely to stimulate future work.   [+] The paper provides relatively simple and rigorous proofs of simplified versions of its conjectures (both upper and lower bounds on the achievable Lipschitz constant).   [+] The exposition is technically clean, and the paper is clear on the limitations of its analyses.   [ ] The setting of the paper’s analysis seems somewhat mismatched with the practice of deep learning. The data are assumed to be generic, where neural networks excel in fitting structured data. Several reviewers noted this mismatch and raised concerns about whether this conjectured/proved tradeoff on generic data carries over to structured datasets.   [ ] A technical limitation is the shallowness of the network: controlling the Lipschitz properties of deep networks is much more challenging at a technical level, because one needs to argue that for the worst input, features propagate in a “generic” fashion. It is technically challenging to avoid exponential dependence on depth.   [ ] The paper obtains only partial progress towards proving its conjectures — for example, it shows that it is possible to interpolate with a Lipschitz constant of $n \log n / k$, where the conjectured bound is $\sqrt{n/k}$.   [ ] Comparing to kernel methods would help to better contextualize the results, since in a similar setting, kernel methods could also potentially be analyzed via localization arguments.   Overall, the paper conjectures a novel phenomenon around size/robustness tradeoffs in interpolating neural networks. While the paper s conjectures have the potential to stimulate further empirical and theoretical work, the reviewers (in particular R1 and R2) note a number of significant limitations to the paper’s analysis. In light of these issues, the paper falls below the par for acceptance. 
The paper proposes the challenge of rapid task solving in unfamiliar environments and presents an approach to achieve this called Episodic Planning Networks   a non parametric memory based on the transformer architecture to learn tasks that require planning from previously experienced tasks, following a form of meta RL.  The problem and approach are compelling, with strong empirical results.  The paper is well written and is an exciting contribution.  This is a clear accept.  In response to the initial reviews, the authors updated their paper to improve the formalization and address other concerns in the reviews, which were viewed favorably by the reviewers as a good improvement. Based on the reviewer discussions, the work could still be placed better in context with respect to other literature.
This paper investigates the topic of nondeterminism and instability in neural network optimization. The reviewers found the results on different sources of nondeterminism particularly interesting and relevant. The experiments are carried on both language and also vision, which strengthens the findings. Concerns were raised about the use of smaller non standard models, which were somewhat mitigated by the addition of Resnet 18 experiments on CIFAR. The reviewers also noted that the measures used in the experimental protocol were already present in the literature, and that the proposed mitigation strategy is from another work. Furthermore, R2 also found that the optimization instability section should be more developed. The paper should be resubmitted with an improved discussion of related works and more developed section on instability as suggested by the reviewers.
This paper studies n step returns in off policy RL and introduces a novel algorithm which adapts the return’s horizon n in function of a notion of policy’s age. Overall, the reviewers found that the paper presents interesting observations and promising experimental results. However, they also raised concerns in their initial reviews, regarding the clarity of the paper, its theoretical foundations and its positioning (notably regarding the bias/variance tradeoff of uncorrected n step returns) and parts of the experimental results.  In the absence of rebuttal or revised manuscript from the authors, not much discussion was triggered. Based on the initial reviews, the AC cannot recommend accepting this paper, but the authors are encouraged to pursue this interesting research direction. 
The submitted paper contains interesting theoretical insights into common approaches for exploration and proposes a new way for deriving intrinsic rewards for exploration which is evaluated in several benchmark environments. While all reviewers appreciate these aspects, there are concerns about whether the paper is ready for publication. In particular, the authors’ response did not clarify all open questions and concerns (although the authors already improved the paper a lot by updating the submitted paper according to recommendations/questions of the reviewers). After discussions and author feedback, 3 knowledgable reviewers suggest (weak) rejection of the paper and 1 reviewer suggested acceptance of the paper. Considering this, I recommend to reject the paper but I would like to encourage the authors to consider the comments of the reviewers to revise their paper accordingly, as I expect the paper to then turn into a strong and impactful one.
This paper extends the Majorization Minimization principle, particularly the MISO method, to problems where the surrogate of randomly selected batch functions are intractable, such as in formulations rising from Variational Inference. There is a large gap between the reviewers  evaluations even after the author rebuttal and discussions. While the strength of the proposed method looks to be its generality, the main criticism from the reviews are the limited applicability and less convincing arguments and empirical evidences against alternatives such as Monte Carlo versions of popular adaptive stochastic optimization methods. Weighing these considerations and considering strengths of other submissions on similar topics, I have to recommend rejection of the paper at the current form.
The paper presents an analysis of the spectral impact of non linearities in a neural network, using harmonic distortion analysis as a means to quantify the effect they have in the spectral domain, linking a blue shift phenomenon to architectural choices. This is an interesting analysis, that could be strengthened by a more thorough exploration of how this analysis relates to other properties, such as generalization, as well as through the impact of the blueshift effect through the training process.
This paper presents a way to use a translation memory (TM) to improve neural machine translation.  Basically the proposed model uses a n gram retrieve matching sentence （or pieces） and takes advantage of the useful parts using gated attention and copying mechanism.  Although the idea of leveraging TM in the context of NMT is not new,  this work seems to be a fair contribution. My major concerns are the following 1. The retrieval part  is not clearly presented, raising questions about  complexities and the noise brought by the common words. The authors should give a better exposition on the ranking mechanism.  2. The experiments are not convincing enough since the proposed model is not compared to the SOTA and the competitive models described in the prior work.  In conclusion I would suggest to reject this paper.
This paper proposes \alphaVIL, a method for weighting the task specific losses in a multi task setting in order to optimize the performance on a particular target task. The idea is to first collect gradient updates for the model based on all the separate tasks, and then re weight those updates in order to optimize the loss on a held out development set for the target task. In practice, this meta optimization is performed with gradient descent. Experiments on multi MNIST and several tasks that are part of GLUE and SuperGLUE show that \alphaVIL is close in performance to a baseline multitask method and discriminative importance weighting.  Strengths:   The idea is intuitively appealing. Directly reweighting tasks as a meta optimization step is straightforward and appears to not be proposed previously in the literature.   The paper is clear in its presentation.  Weaknesses:   The reviewers agree that the main weakness is that the experimental results do not show that \alphaVIL offers any substantial benefits over existing methods. On the multi MNIST task, while \alphaVIL tends to have the highest mean performance, the difference is small (less than a standard deviation). On the GLUE/SuperGLUE tasks, it outperforms other methods on only 1 out of 10 experiments. There are also no confidence intervals/standard deviations provided to assess the significance of the results.
In this paper, the authors study the behavior of the Lookahead dynamics of Zhang et al. (2019) in bilinear zero sum games. These dynamics work as follows: given a base algorithm for solving the game (such as gradient descent ascent or extra gradient), the Lookahead dynamics perform $k$ iterations of the base algorithm followed by an exponential moving average step with weight $\alpha$. The authors then provide a range of sufficient conditions for the eigenvalues of the matrix defining the game under which the Lookahead dynamics become more stable and converge faster than the base method.  This paper received four reviews and generated a very lively discussion between the authors and reviewers. Reviewer 4 was enthusiastic about the paper; the other three initially recommended rejection. During the discussion phase, the authors revised their paper extensively, and Reviewer 3 increased their score to an "accept" recommendation as a result. In the end, the reviewers were evenly split, and I also struggled a lot to reach a recommendation decision.  On the plus side, the paper treats an interesting problem: prior empirical evidence suggests that the Lookahead dynamics can improve the training of some adversarial machine learning models, so a theoretical study is very welcome and of clear value. On the other hand, the setting treated by the paper (bilinear min max games) is somewhat restrictive, and the authors  theoretical conclusions do not always admit as clear an interpretation as one would like.  The issues that ended up playing the most important role in my recommendation were as follows: 1. The Lookahead dynamics with period $k$ involve $k$ gradient evaluations, so their rate of convergence should be compared at a $k:1$ ratio to GD and EG (with an additional $2:1$ ratio between GD and EG to put things on an even scale). To a certain degree, this $k:1$ ratio is present in the last part of Lemma 3; however, the exact acceleration achieved by the "shrinkage" of the spectral radius is not clear. This can also be seen in the semi log plots provided by the authors, where the corresponding slopes of GD/EGD methods should be multiplied by $k$ when compared to the respective LA variants. In this regard, a comparison with the values of $k$ provided in Appendix D reveal that the performance of the Lookahead variants in terms of gradient queries is very similar (if not worse) to the non LA variants. This is a cause of concern because, if LA does not accelerate convergence in simple bilinear games, it is not credible to expect faster convergence in more complicated problems. During the AC/reviewer discussion of this point, Reviewer 3 pointed out that this might be due to a suboptimal tuning of $\alpha$ (i.e., that it was not chosen "small enough"), and went out to note that this echoes the arguments of other reviewers that the characterization of acceleration may be problematic and not significant (even if it takes place). 2. Another major concern has to do with the stabilization provided by the Lookahead dynamics: using a benchmark game proposed in a recent paper by Hsieh et al. (2020), the authors showed that the Lookahead dynamics converge to a point which is unstable under GDA/EG (and hence avoided). This is fully consistent with the authors  theoretical analysis, but it also highlights an important problem with the Lookahead optimizer: if $k$ and $\alpha$ are tuned to suitable values for stabilization, the algorithm converges to a non desirable critical point (a max min instead of a min max solution). This is a major cause of concern because it shows that the algorithm may, in general, converge to highly suboptimal states.  The above create an inconsistency in the main story of the paper. In fact, it seems to me that the authors  results form more of a "cautionary tale in hiding": even in very simple bilinear problems, the lookahead step may not provide acceleration and, even worse, it could converge to highly undesirable critical points. I find this "negative" contribution quite valuable from a theoretical standpoint, and I believe that a thoroughly revised paper along these lines would be of interest in the top venues of the community (though a more theoretical outlet like COLT might be more appropriate). However, this would require a drastic rewrite of the paper, to the extent that it should be treated as a new submission.  In view of all this, I am recommending a rejection at this stage. I insist however that this should not be seen as a critique for the mathematical analysis of the authors (which was appreciated by the reviewers), but as a recommendation to reframe the paper s narrative to bring it in line with the algorithm s observed behavior. I strongly encourage the authors to resubmit at the next top tier opportunity.
This paper is an intriguing study of agents that can give explanations (contrastive) of their actions via symbolic representation such as dialog.  Agents can also allow users to argue against the agents  decisions. I am extremely impressed by the quality of the reviewer comments and discussions.  It is also interesting that the reviewers have formed two camps of thought on the paper: One camp consists of R3 and R5 who are in agreement in vociferously criticizing the weak points in the paper.  The other camp consists of R1, R2, and R4, who champion the merits of what they see as strong points.  Notably, all reviewers have fairly high confidence values   only one confidence score of 3 and all others are 4.  It was a borderline case and not an easy decision. In the end the program committee decided that the paper in its current form does not quite meet the bar, and would benefit from another revision (see e.g., R4 comments).  We think that the work is interesting, and encourage the authors to address the reviewers  comments and resubmit the work to another venue. 
This paper empirically investigates the gradient dynamic of two layer network nets with ReLU activations on synthetic datasets under $L^2$ loss. The empirical results show that for a specific type of initialization and less overparametrized neural nets, the gradient dynamics experience two phases: a phase that follows the random features model where all the neurons are *quenched* and another phase where there are a few *activated* neurons. As pointed out by Reviewer 1, this paper lacks mathematical support and did not distinguish between *random features model* and *neural tangent model*. Reviewer 3 and Reviewer 4 also complained that the paper is purely experimental. Therefore, this paper may benefit from proposing an at least heuristic or high level conjecture/interpretation/argument that tries to explain the empirical results.  
The paper introduces an augmentation technique that, given an image with a detected object, keeps the object and removes the background.  The reviewers expressed numerous valid concerns about the paper s novelty, the setting (assumption that there s a single object), the scalability of the approach and the experimental setup, including the baselines used.  The authors have not addressed these concerns.
This paper proposes an interesting unified framework for meta learning with commentaries, which contains information helpful for learning about new tasks or new data points. The authors present three kinds of different instantiations, i.e., example weighting, example blending, and attention mask, and show the effectiveness with the extensive experiments. The proposed method has a potential to be used for a wide variety of tasks.
This paper proposes an interesting collaborative multi head attention (MHA) method to enable heads to share projections, which can reduce parameters and FLOPs of transformer based models without hurting performance on En De translation tasks. For pre trained language models, a tensor decomposition method is used to easily covert the original MHA to its collaborative version without retraining.   This paper receives 3 weak reject and 1 weak accept recommendations. On one hand, all the reviewers agree that the paper is well motivated and the proposed idea is interesting. On the other hand, all the reviewers also commented that the current empirical results and comparisons are weak, which are not enough to support the paper s main claim. From the current results, it is difficult to draw a conclusion that collaborative MHA is better.   Specifically, (i) From Table 2, it can be seen that the proposed method is not effective for pre trained models, i.e., even if the model size is not reduced much, the performance can be dropped significantly. (ii) More experiments, such as QA, more translation/generated tasks will make this paper more convincing. (iii) More rigorous experiments are needed to justify the practical value of the proposed method. If the authors try to emphasize that they go beyond practical realm, then probably a careful re positioning of the paper is needed, which may not be a trivial task.   The rebuttal unfortunately did not fully address the reviewers  main concerns. Therefore, the AC regrets that the paper cannot be recommended for acceptance at this time. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
This paper proposes a new task domain for learning based AI agents, HALMA, a game that is designed to bring together multiple areas of research in AI. Perception, in the form of recognition of MNIST digits, learning mathematics   in the form of arithmetic operations on the natural numbers, and navigation and planning. When combined into a game, these elements are argued to require various important properties of human cognition, such as abstraction, analogy and affordance.   I commend the ambitious goals of this work, and its multidisciplinary motivations. I believe the benchmark can indeed eventually be an important challenge for the community. I think that the dynamic testing aspect is particularly interesting, where the environment produces trials designed to go beyond the agent s experience to that point. However, having considered the views of the reviewers and read the (main body) paper entirely myself, I unfortunately cannot recommend acceptance in its current form.   The main reason for the decision is simply that it is prohibitively challenging for me to grasp exactly how the game actually works after a thorough reading and considerable thought. The authors spend two pages motivating the approach with (arguably excessively grandiose) allusions to Marr s levels of analysis, Gibson s affordances, Holyoak s analogy and various other famous works from the history of AI and philosophy of mind; as well as to the board game HALMA. But as a reader I can t myself start to make any of these connections because the game proposed by the authors has not been explained to me! It is finally introduced on the fourth page   with reference to Figure 2 which is too small to consult and very hard to interpret. After consulting the appendix (where the idea is a bit clearer) I was able to decipher the way in which the numbers related to the maze itself, but was (and am still) unclear on the actions available to the agent. These are explained as follows:   ""The direction set is t , , , u. The primitive action set, in terms of the numberof moves, is t , , , u; this design of primitive numbers with a maximum of three aligns withthe doctrine of core knowledge in developmental psychology (Feigenson & Carey, 2003; Dehaene,2011). If an option is selected, consecutive hops as in Halma are simulated; all observations fromintermediate states will be skipped, and only the observation of the final state is provided. A movewould fail if a wall stops the agent, leaving the agent’s position unchanged; failure moves bringpenalties to the agent. The agent would receive a positive reward when reaching the goal""  From reading this I am left with the following questions:    Are directions primitive actions?    How can a primitive action also be a number of moves?    What does it mean to select an option?    Can I select an option and an action at the same timestep?   Most importantly, I still don t really know how the game works.   This example is intended to illustrate the difficulty faced by readers of this paper in general.    I note that the reviewers awarded this work scores that place it on the borderline for acceptance, but with consistently low confidence. On consulting with the reviewers it is clear that this is not because they lack expertise but because they too did not understand the full details of how this domain/task works. This is also clear from the lack of detail in their reviews; only reviewer 3 engaged with any of the details of the task itself.   To summarise, I think there is potentially a very interesting and important contribution in this dataset. However, the work will only have impact in the community if it can be understood and adopted after a single read of the paper. I therefore recommend that the authors resubmit this work to a different venue taking account of the following:     Explain how the game works *then* connect it to the literature on human learning *not* vice versa (from the concrete to the abstract)   Be very concrete, perhaps guide the reader through a single particular episode explaining the observations available to the agent and the options open to it at each important point   Get to the point of your contribution. Tenuous connections to cogsci etc can go in the discussion   Make all diagrams and illustrations extremely simple to interpret and large enough to easily read   Avoid use of subjective adjectives, and particularly describing one s own contributions as "ingenious solutions" and "impeccable"   Avoid rhetorical flourishes and latin   Submission to a journal may allow the authors greater space to draw the desired connections to disparate fields without compromising on readability or exposition of their methods
The paper presents an online algorithm for dynamic tensor rematerialization.  The theoretic analysis on the tensor operation and memory budget bound of the proposed method, as well as on the relationship between the proposed method and optimal static analysis method is novel and interesting.  It covers a pretty comprehensive study across theory, simulation and system implementation. In addition, the paper is well written. 
The paper gives an extension of the transformer model that is suited to computing representations of source code. The main difference from transformers is that the model takes in a program s abstract syntax tree (AST) in addition to its sequence representation, and utilizes several pairwise distance measures between AST nodes in the self attention operation. The model is evaluated on the task of code summarization for 5 different languages and shown to beat two state of the art models. One interesting observation is that a model trained on data from all languages outperforms the monolingual version of the model.  The reviewers generally liked the paper. The technical idea is simple, but the evaluation is substantial and makes a convincing case about setting a new state of the art. The observation about multilingual models is also interesting. While there were a few concerns, many of these were addressed in the authors  responses, and the ones that remain seem minor. Given this, I am recommending acceptance as a poster. Please incorporate the reviewers  comments in the final version. 
This paper presents an empirical study of different efficient ways to estimate the performance of architectures in NAS, focussing on weight sharing and performance prediction methods. Most reviewers appreciated the paper s goal of performing a careful, controlled study of different factors that can affect the ranking of architectures. However, all reviewers also had substantial concerns. Many of these could in principle be fixed by additional experiments, but the short time window of the author response period did not allow for this.  As a result, all reviewers voted for rejection, and I will follow that recommendation. Nevertheless, I would like to encourage the authors to continue this work, as I believe that the NAS community needs more careful controlled studies of this type. For their next version, I encourage the authors to take into account the many points mentioned by the reviews.
The reviewers generally liked the paper but had several concerns. The rebuttal and revision of the paper could mitigate most concerns and the reviewers are now mostly positive towards the paper. Remaining concerns are mostly about the presentation of the paper which indeed has room for improvements but overall is good enough to accept the paper.  
The paper tackles a major problem of supervised ML, that of the minimisation of the risk of a set of classifiers. This problem has received attention in numerous work over the past decades, much of which spans the formal aspects of the problem. The paper tackles the problem from a “diversity” standpoint. My main concern is, for such a problem and exhaustive formal and experimental SOTA, one cannot just evacuate any formal understanding of a contribution to future work (Authors’ reply to R2). The argument is then a victim of its own content, ending up in a sloppy vocabulary where “speculation” and “intuition” are called forward as justification to the calls for “rigorous” (R1) and “theoretical” understanding (R2 + answer to R2). I am confident the authors can find formal merit to their contribution, but this needs to be addressed. R1 + R4 hint on avenues to understand the contribution.  
This paper presents an algorithm for distributed optimization in that aims to be "Byzantine robust", in the sense that it learns successfully when some of the workers send arbitrary messages.  The goal in this work is to remain robust when each worker samples data from a different distribution.  While reviewers found the work interesting, issues about the theoretical development arose during the discussion period, and it appears that the paper cannot be accepted in its current form.  The most serious issue was with Proposition I, which appears to be incorrect.  In its putative proof, the authors write that each gradient is sampled at most $s$ times.  This naturally leads to the conclusion that, in Algorithm 2, when the Break statement is reached,  $g_{j_i}$ is not used to compute $\bar{g}_t$.  Given this interpretation of Algorithm 2, it seems that Proposition I cannot be true. For example, if $s 1$, once $t \geq n/2$, fairly often, gradients will be sampled that had previously been sampled. In such cases, would be zero, so that, on average, $\bar{g}_t$ would be biased toward zero in later rounds.  Their putative proof of Proposition I refers to a whole chapter of a statistics text. We couldn t find anything in that chapter that implies what they claim about Algorithm 2 (or that treats a sampling scheme like Algorithm 2 at all).  Throughout the paper, when the authors took expectations, it was not always clear what was random and what was fixed.  After some discussion, disagreement remained about how to interpret some of the assumptions.  This was true in particular about the assumption in the first displayed equation on page two.  
The reviewers appreciate the steps taken to combine continual learning with few shot learning, this is an interesting intersection with many potential applications. However, the reviewers generally outlined a number of concerns with the benchmark and paper in its current form. They largely feel that this benchmark doesn’t differentiate itself well enough from other incremental learning benchmarks, nor does it experiment with a wide enough variety of settings (additional episode configurations, more FSL/CL approaches). As such, it is difficult to determine at this point what major insights can be gained from this benchmark. I understand that there is a tradeoff: computation is limited, and there is merit in keeping things simple. However, the general consensus is that more work needs to be done in order to fully realize the potential of this benchmark. 
This paper introduces a new algorithm to solve game, more or less similar (in the general idea, yet differences are interesting) than CFR. The concept is to sample from past policies to generate trajectories and update sequentially (via regret matching).  The three reviewers gave rather lukewarm reviews, with possible suggestions of improvements (that were more or less declined by the authors for those proposed by Rev3 and Rev4; the added material focuses more on the clarity of the text than on the content itself).  I have also read the paper, and find it quite difficult to assess. At the end, it is not clear to a reader whether ARMAC is the new state of the art, or just a "variant" of CFR that will be soon forgotten. The performances do not seem astonishing (at least against NSFP) and even though DREAM might not be satisfactory to the authors (EDIT POST DISCUSSION: actually, DREAM is a valid competitor and must be included in the comparative study), it would have been nice to provide some comparison. Maybe the issue is the writing of the paper that could and should be improved so that it is clearer what are the different building blocks of ARMAC (and their respective importance).  If ARMAC is the new state of the art, then I am sure the authors will be able to clearly illustrate it in a forthcoming revision (maybe with more experiments, as suggested by Rev2). Unfortunately, for the moment, I do not think this paper is mature enough for ICLR.
This paper explores the robustness of one class classifiers to geometric transformations at test time. The authors observe that some existing methods fail to detect novel images from the same class when they have undergone specific transformations at test time i.e. in plane rotations. In contrast, it is suggested that humans have no difficulty in ignoring the impact of these types of transformations. To address this issue, the authors propose to take the maximum prediction over the set of rotated versions of a given test image.   The current consensus from reviewers, and this meta reviewer agrees with this view, is that the paper, while not without some merit, is too narrow in focus to be of general interest in its current form. The main contribution is limited to one family of transformations, and it is not immediately clear how to generalize this to others when the entire transformation space is not easily enumerated. There are also legitimate concerns regarding if the specific issue outlined is likely to be a problem in practice (see R1 s comments). The authors allude to some interesting negative results related to data augmentation in their response to R4 (R2 also had questions about this). The authors should consider adding these results to a future revision of the paper as it will strengthen the central message.  In conclusion given the limited support, this AC also agrees that the paper is not yet ready for publication at ICLR.  
Three reviewers recommended accept or weak accept. There are some concerns on the novelty of this approach since this work mainly validates that lottery tickets can be found on GANs, which seems like applying an existing idea to a new problem. Nevertheless, there are two reasons that such an effort is interesting: first, the implementation of this idea may be harder than it seems; second, it was not known a priori whether lottery tickets would exist for GANs due to the significantly different optimization problem (game instead of minimization). I think this work will be of interest to the community, and recommend acceptance. 
This paper investigates the training dynamics of simple neural attention mechanisms, in a controlled setting with clear (but rather strict) assumptions. Some reviewers expressed caution about the applicability of the assumptions in practice, but nevertheless there is agreement that the results deepen our understanding and enrich our toolkit for reasoning about attention. In support of this, in the discussion period, it was emphasized that the work uses different techniques than most current work in this direction. I am therefore confident that the paper will be useful, and recommend acceptance.  I strongly encourage the authors to improve the clarity of the work and thorough citation, as suggested by the reviewers.
The reviewers appreciate the simplicity of the approach, but found the exposition lacking. There were also concerns about strong similarities to CascadeRCNN, which were not resolved in the rebuttal. In the end all reviewers recommend rejection. The AC sees no reason to overturn this recommendation.
This paper is an extension of Monotone Operator Equilibrium Networks (MON). It first tries to address a key issue in MON: whether the activation function $\sigma$ can be represented by a proximal operator of some function $f$. Then it derives the constraints on the weight $W$. Connections to neural ODEs and convex optimization are also built.   Pros: 1. Very nice theory, reads exciting, and provides great insights. 2. Experiments seem to validate part of the theoretical analysis.  Cons: Besides the issue of weak experiments raised by the reviewers (and the authors admitted "quite rudimentary empirical results"), the AC regretted that the proofs have some key flaws.  1. The proof of Proposition 1, which is the cornerstone of the paper, is wrong. Although the AC believed that Proposition 1 is likely to be true, the current proof of "only if" is unfortunately incorrect. Please notice the claim at the first line of the "only if" proof, which writes "$\sigma$ is a non decreasing and piece wise differentiable function". Although it is known in real analysis that monotone functions are almost everywhere differentiable, this does not necessarily mean that monotone functions can be piece wise differentiable functions as the non differentiable points can be dense, though of zero measure.  2. Proposition 7, which claims that LBEN parameterization (8) contains all feedforward networks, is wrong. From the proof on page 19, the AC doubted whether the lower diagonal blocks of $ 2H_D^{ 1}  H_L$ can be $\{W_i\}$. This is because by the definition of $H$ given at 5 lines below (40), $H$ must be positive definite. If the lower diagonal blocks of $ 2H_D^{ 1} H_L$ can be $\{W_i\}$, then $H_i \Lambda_i W_i$.  It is not apparent whether for any choice of $\{W_i\}$ there exist $\{\Lambda_i\} \in D^+$, such that $H$ is positive definite. Moreover, the lower diagonal block of $V^TV$, i.e., $\Gamma_{j+1}V_j\Phi_j$ (corrected from the typo in the minor issue 2 below), must equal to $\Lambda_j W_j$. However, it is obvious that for some $W_j$ (e.g., it has more columns than rows), there cannot exist $\Gamma_{j+1}$, $V_j$, $\Phi_j$ and $\Lambda_j$, such that $\Gamma_{j+1}V_j\Phi_j \Lambda_j W_j$, due to the structures of these matrices. Moreover, due to the existence of $(...)/(2\gamma)$ in the definition of $H$, the diagonal blocks of $H$ cannot be diagonal by the choice of $V$. 3. Proposition 7 again. The choice of parameters in (44) does not match the feedforward network in (15). Please observe that by (44) $z_1 \sigma(U_0 x + b_0)$, rather than $z_1 U_0 x + b_0$ in (15).  Minor issues: 1. The proof on page 19 is for claiming that (9), rather than (8), contains all feedforward networks. 2. In the choice of $V$, $\Psi_i V_i$ should be $V_i\Phi_i$.  Although the reviewers and the AC liked the paper, due to the above flaws and limited acceptance rate, the AC deemed that the flaws should be fixed prior to acceptance.
This paper proposes a new sampling method named Random Coordinate LMC (RC LMC), which integrates the idea of randomized coordinate descent and Langenvine dynamic. The authors prove the total complexity of RC LMC for log concave probability distributions, which are better than that of LMC under different settings. The idea of this paper is very neat and the reviewers are in general positive about it. However, as pointed out by one of the reviewers and seconded by the other reviewers, the proof in the original submission is flawed, and the fix needs some substantial work. The new version needs to be carefully checked before publication, which is far beyond the review process of ICLR. Therefore, I encourage the authors to carefully revise the paper and submit it to the next conference. 
The paper generated a lot of discussion. After reviewing all of the opinions, and my own reading of the paper, we have concluded that the theoretical innovation is too incremental for ICLR. It is possible that the idea of "residual feedback" could be helpful, but for this to be demonstrated effectively one would need to consider concrete models where the assumptions are verified.
For meta learning with variable shot, this paper proposes a method for adapting the learning rate by a function of the number of training examples. The functional form is theoretically derived, and the method is simple and effective. However, meta learning methods that adapt learning rates have been proposed, and the novelty is not high enough.
The reviewers enjoyed reading about an interesting take on lifelong learning, encapsulating an EM methodology for selecting a transfer configuration and then optimizing the parameters. R3 made valid concerns regarding comparison with previous, recent work. R2 also would prefer to see more thorough experiments (ideally in settings where multiple tasks exist, as also commented by R4). During the rebuttal phase the authors made a good effort to run additional experiments which cover the related work aspect better. These experiments and the overall paper were discussed extensively among reviewers after the rebuttal phase.  In the discussions, the reviewers agreed that an interesting idea can be publishable even if it does not achieve SOTA results in all scenarios, as long as it brings new perspectives and shows at least comparable results. However, in the particular case of this paper, there exist remaining concerns regarding the usefulness and applicability of the method. Specifically, the paper could benefit from a more convincing demonstration about how the method can scale (e.g. R3 and R4’s comments), especially since training time and model capacity are important factors to consider for practical continual learning scenarios. Furthermore, it is not clear how the proposed method can be used in combination with other machine learning tools within a continual learning application, for example by leveraging modern deep architectures or by complementing existing adaptive knowledge approaches (as discussed by R3).   Although the opinions of the reviewers are not fully aligned, this borderline paper seemed to lack an enthusiastic endorsement by a reviewer to compensate for the concerns discussed above and the relatively weak experimental results. Therefore I recommend rejection.  
The paper presents a framework for modeling dynamical systems by combining prior knowledge available as ODE and implemented via a differentiable solver, with statistical modules. This is a key problem consisting in complementing available partial knowledge on a physical system with information extracted from available data with agnostic statistical methods. In their framework, both the ODE parameters and the residual model parameters are learned. Experiments are performed on synthetic and on a simplified but realistic problem.   All the reviewers do agree that the topic is important and that the paper has merits and brings an interesting contribution. They highlight some weaknesses in the presentation and more importantly in the experimental assessment. Overall, this is a good paper that should still be somewhat improved for publication. The authors are encouraged to investigate further the analysis of their framework in different settings and to bring more experimental evidence.
The authors propose a recurrent model of self position, with a handcrafted expression of the rotational structure in terms of a matrix Lie group. As noted by the reviewers, this work strongly builds upon Gao et al (ICLR 2019). This really is mentioned too late and not prominently enough in the manuscript, and furthermore, the difference to this work is not clearly explored in the paper (there are just two sentences immediately prior to the conclusion and no experimental comparison). The reviewers pointed out that the phenomena observed here are handcrafted into the structure of the model, rather than being emergent. The reviewers raised concerns that it is not clear what conclusion to draw from this work. For these reasons, I recommend rejection this stage.
The paper is proposing Risk Extrapolation (REX) as a domain generalization algorithm. Authors extends the distributionally robust learning to affine mixture of distributions from convex mixture. Authors later uses variances instead of this extension and demonstrate various empirical and theoretical properties. The paper is reviewed by four expert reviewers and the reviewers did not reach to a consensus. Hence, I also read the paper in detailed and reviewed it. In summary, reviewers argue the following:    R#2: Main argument is the lack of justification of the claim "Rex could deal with both covariate and concept shift together". Authors try to address this in their response. Moreover, reviewer also argues in the private discussion that manuscript is not updated and authors did not address any of the issues during the discussion period.   R#3: Argues that (similar to R#2), dealing with covariate shift is not explained properly. Reviewer is not persuaded that REX results in invariant prediction.   R#1 and R#4: Largely positive about the paper. In the mean time, argue that organization of the paper is lacking and some of the material in the supplement is relevant and should be moved to the main text. R#1 decreases their score due to the lack of re organization during the discussion.  The value of the paper is clear to me, the joint treatment of minimax perspective, domain generalization and invariances is definitely interesting and valuable. Hence, the paper has merit to be published. However, the presentation is lacking significantly.  The main contribution of the paper lies in Table 1 but the invariant prediction property is not justified at all in the main text. Hence, Table 1 is not justified properly. Authors discuss Thm 1&2 in their response but they both are in the supplement. From reading only the main text, confusion of the reviewers are well justified. ICLR guidelines clearly states that "...Note that reviewers are encouraged, but not required to review supplementary material during the review process..." It is authors  responsibility to make the main paper self contained. Even more worrisome is the fact that authors dismiss this concern in their response to R#1 which eventually leads to R#1 decreasing their score. Hence, I decided to reject the paper since the presentation is subpar and authors did not persuaded reviewers that they can fix this presentation issue by the camera ready deadline. On the other hand, I think the paper can be really influential if it was written clearly. I suggest authors to revise the claims more precisely, extended the discussion on the claims and move the theorems to the main paper.
The reviewers have a strong consensus towards rejection here, and I agree with this consensus , although I think some of the reviewers  concerns are misplaced. For example, the paper does not appear to use a magnitude upper bound that would be vacuous together with a strong convexity assumption (although variance bounds + strong convexity do cover only a small fraction of strongly convex learning tasks, these assumptions aren t vacuous).  Some feedback I have that perhaps was not covered by the reviewers:  Pros:     Studying the setting where the number of bits varies dynamically is very interesting (although, as Reviewer 3 points out, not entirely novel). There is significant possibility for improvement from this method, and your theory seems to back this up.  Cons:     The experimental setup is weak, and is measuring the wrong thing. When we run SGD to train a model, what we really care about is when the training finishes: the total wall clock time to train on some system. For compression methods with fixed compression rates, it s fine to use the number of bits transmitted as a proxy, because (when the number of bits transmitted is uniform over time) this will be monotonic in the wall clock time. However, when the bits transmitted per iteration can change over time, this can have a difficult to predict effect on the wall clock time, because of the potential for overlap between communication and computation (where below a certain number of bits sent, the system is not communication bound). Wall clock time experiments comparing against other more modern compression methods would significantly improve this paper.
The paper introduces a notion of distributional generalization, which aims at characterizing aspects of underlying distribution that are learned by a trained predictor. Authors make several interesting conjectures and support them with empirical evidence. Reviewers agreed on the novelty of the ideas; however, the work seems to be preliminary in its current form. Unfortunately, I cannot recommend acceptance at this time. 
Inspired by biological agents that have developed mechanisms like attention as an information bottleneck to help function more effectively under various constraints of life, this paper looks at an approach of learning a hard attention scheme by leveraging off the prediction errors of an internal world model. They demonstrate their approach via a simple but easy to understand 2D pixel multi agent game, a gridworld env, and also PhysEnv, to show the effectiveness of the learned hard attention, and go on to discuss interesting aspects such as curiosity attention.  Overall, I thought more highly of the paper than the reviewers, and might have proposed a score of 6 if I were a reviewer, but I also read each review and respect the points given by all four reviewers, and also agree with much of their feedback in the end. I think if this work was submitted to ALife (Journal or Conference), it might have been accepted. Not that those venues are easier, if anything they can often be more selective, but I think what ICLR (and similar venues like ICML) tends to expect is a bit different than what this paper offers.  To improve this work, I recommend following some of the reviewers  advice (especially R4), particularly on experimental design. Reviewers suggest that the current experiments are small and simple, but while true to some extent, I think more importantly missing are clear baseline methods to compare your approach against. What can your approach do that existing popular approaches in RL will totally fail at doing? It can be worthwhile to try your approach on a larger task domain, such as Atari (but perhaps modified) or ProcGen [1] to show the benefits of hard attention compared to existing approaches. For instance, some recent work [2] demonstrated that hard attention can help agents generalize to out of training domain tasks the agent has not seen before during training   something that traditional approaches without attention tend to fail at doing.  In the current state, the work will be a great workshop paper. But I recommend the authors to continue improving the work in the direction that can help the idea gain acceptance by the broader community.  [1] https://openai.com/blog/procgen benchmark/ [2] https://arxiv.org/abs/2003.08165 
This paper presents an uncertainty quantification method that is conceptually interesting and practical. All reviewers are in consensus regarding the quality and significance of this manuscript. 
The paper has received 4 positive reviews all supporting the acceptance of the paper. The authors have provided a strong rebuttal and have addressed the reviewers  concerns. Please make sure to include all reviewer feedbacks in the camera ready version. 
I think this is a very solid and good work in the topic of "Practical Massive Parallel MCTS."   I think it will be good to open up perspectives among ICLR s audience going beyond just Deep Learning and Machine Learning. I also noted a lot of positive comments during the evaluation and discussion period.  Still, it was a borderline case and not an easy decision (primarily because of the concerns raised by R3 towards the end of the discussion period). In the end the program committee decided that the paper does meet the bar.  We think that the work is interesting and original, though not without weaknesses. 
The average score of the reviewers is 6. There are various pros and cons pointed out by the reviewers. Unfortunately, the AC and SAC found that the merit could be outweighed by the limitations of the work, and would like to recommend rejection. For example, a central concern raised by the reviewers is the lack of theoretical justification the measurement that the paper proposes does seem to show an interesting empirical phenomenon, but as a few reviewers pointed out, it s unclear how the interesting phenomenon directly links to the generalization mechanism, and it s unclear whether the new interesting phenomenon is caused by the change of measurement of the gradient coherence or it is something fundamental. The arguments related to these are found to be generally vague and hand wavey by the reviewers.  The AC would like to encourage the authors to address the reviewers  concerns thoroughly, especially those regarding the difference and similarity with prior works, the interpretation, and limitations of the results, etc., and consider adding more rigorous analysis to justify the proposed measurement of gradient alignment.  
This paper proposes an extension of the monotonic policy improvement approach to the average reward case. Although the reviewers acknowledge that this work has merits (well written, clearly organized, well motivated, technically sound) the reviewers have raised several concerns, which have been only partially addressed by the authors  responses. In particular, Reviewer4 is still concerned about the discrepancy between the theorem and the implemented algorithm, and the proposed simplification used in the implementation boils down to an algorithm that is very similar to TRPO, thus making the contribution quite incremental as also stressed by Reviewer1. Furthermore, I share the concerns raised about the fairness of comparing algorithms that optimize different objective functions. I suggest the authors take into serious consideration the suggestions provided by the reviewers in order to produce an improved version of their work. The paper is borderline and I think that it needs another round of fresh reviews before being ready for publication. 
This paper proposes an attention endowed architecture for deep image based RL. While some positive points were raised by the reviewers, most comments were on the negative side. The reviewers noted marginal/incremental advances in terms of empirical results and low novelty and significance. Moreover, the provided baselines seem weak. Because of this, the present submission unfortunately does not meet the publication bar. I recommend the authors take into account the constructive feedback from reviews and discussion and submit an improved version to another venue.
The authors model point processes with equivariant normalizing flows. Reviewers agreed that the paper is well written and addresses a problem of interest to the ICLR community, some reviewers considered the contribution to be incremental.  Perhaps the biggest contribution is a closed form expression for the trace that needs to be computed as part of the normalizing flow, which is valuable but not particularly emphasized. The authors combine this trace formulation with an equivariant normalizing flow to model the conditional density of point locations given cardinality. (As an aside, it was unclear to me if and how those conditional distributions share parameters; in some contexts, the conditional density could look very different depending on the number of points in the set.) Overall, the paper is interesting but needs a little more to lift it over the bar.
This work provides evidence against the hypothesis that ResNets implement iterative inference, or that iterative convergent computation is a good inductive bias to have in these models. The reviewers indicate that they think this hypothesis is interesting and relevant to the ICLR community, but they do not find the current work sufficiently convincing. Both theoretically and experimentally the paper does not fully demonstrate the claim that iterative inference is not useful in ResNets, and the reviewers are unanimous in their recommendation to reject the paper until the evidence for this claim is strengthened. 
In this paper, the authors proposed a method to handle the problem of LUMP GNN architecture. This problem is indeed important and the proposed method has some merits. However, the proposed approach is only applicable to node classification.  Moreover, the proposed approach shows the similar theoretical results of Sato et al 2020. In the paper, it can be applicable to any GNN tasks because it only adds random features to each node. Therefore, the novelty of the proposed method is limited.  I encourage authors to revise the paper based on the reviewer s comments and resubmit it to a future venue. 
The authors propose an approach for the task of categorizing competencies in terms of worker skillsets. This is a potentially useful (if somewhat niche) task, and one strength here is a resource to support further research on the topic. However, the contribution here is limited: The methods considered are not new, and while the problem has some practical importance it does not seem likely to be of particular interest to the broader ICLR community. 
Thank you for your submission to ICLR.  Overall the reviewers and I think that this paper presents some nice contributions to the adversarial attacks literature, demonstrating a low sample complexity, "physically realizable" attack in a domain of clear importance and interest in machine learning.  The move to considering more "in the loop" adversarial examples is particularly compelling, and the threat model and improvement over BO methods are both compelling here.  The main downside of this paper, of course, is the fact that the "physical adversarial examples" are of course nothing of the sort: they are simulated.  Rather, they are just simulated in a manner that may plausibly be slightly more amenable to real world deployment. The authors claim that they don t carry out an evaluation on a real system because it is "dangerous" is a bit overly dramatic: the tests could easily be carried out in a controlled environment, and demonstration on an actual physical system (even, e.g., and RC car) would vastly improve the impact of this work.  As it is, the paper is borderline, but ultimately slightly below the high bar set by ICLR publications.  I would strongly encourage the authors to reconsider the inclusion of the word "physical" in the title, as it honestly sets expectations high for a promise that the paper cannot deliver on, or (even better) to run real experiments on even a small physical system, demonstrating the transferability there.  The paper ultimately has the potential for a high impact in this field, if these issues are addressed.
The paper considers the problem of abstention in robust classification. A number of issues were identified in the formal framework and the writing was also not up to scratch. The authors should take into regard the very many constructive suggestions made by the reviewers in preparing a revision.
The reviews were a bit mixed, with some concerns on the incremental nature of this work, which the AC concurs (after independently going through both the submission and Xie et al 2020). In a nutshell, the main contribution on the authors  side appears to be a simple linear interpolation of two masks so that it is possible to leverage attacks with varying strengths. Other claimed contributions are not substantiated. In particular:   (a) Fig 1 and its conclusion are a bit disturbing. It is suggested that the authors back up their claim with more empirical and theoretical evidence. For example, why can one conclude from the same mean and variance that there is no distribution mismatch between clean and adversarial examples (as claimed in Xie et al)? If the two sources have similar distribution, why is there a sharp difference in gamma for the two? When one claims different results from previous work, due diligence is required. For instance, did the authors reproduce the mean and variance on the same architecture and dataset of Xie et al? How about other BN layers (in addition to the first one)? How to explain the difference in gamma? The fact that you are using different masks for different epsilon is an indication that their distributions are probably different. The authors mentioned the joint effect between gamma and relu activation, which could be potentially insightful. However, this is a bit speculative in its current presentation. How about an ablation study with leaky relu or tanh/sigmoid? Without these careful comparisons, these claimed contributions are not appropriate to publish in their current form.  (b) As the reviewers pointed out, why BN, after all for models that do not use BN they still suffer from adversarial examples? Even when we restrict to models that use BN, why replicating BN for different sources helps generalization? Here, an excellent experiment point is to compare to fine tuning or replicating other layers in the network. During rebuttal, the authors only tried to fine tune ONE convolution layer and quickly concluded its ineffectiveness. Note that in contrast the authors fine tuned ALL BN layers. How about all convolution layers, some pooling layers, the last softmax layer? These experiments could help us understand if there is some magic in BN. Or maybe it is just more convenient to fine tune BN because of its small number of parameters? In any case these experiments would largely strengthen the findings of this work.  (c) As pointed out by the reviewers, Xie et al hinted at the advantage of using multiple BN masks and the authors proposed to linearly interpolate the masks. In the ablation study, what if we increase the number of BN masks in Xie et al, say we discretize p into 11 values p   {0, 0.1, ..., 0.9, 1} and have 1 mask for each value? Here an interesting experiment is to compare K   11 (basically AdaProp) with smaller K (such as 2 or 5). The authors seemed to suggest that a larger K does not seem to help, which would be clarified through the preceding experiment (and perhaps more). Note that using a uniformly random p is equivalent as adversarial training with a weaker (and varied) attack, and the better tradeoffs shown in the experimental section (e.g. Table 3) are perhaps expected.  (d) As pointed out by the reviewers, a head to head comparison against AdaProp (preferably with more masks) is desirable. The authors mentioned some difficulty in conducting this experiment fully. If it is only the software side, maybe check the sources here:  https://paperswithcode.com/paper/adversarial examples improve image  (e) Finally, a minor point: Algorithm 1 with k 2 and p 1 does not reduce to AdaProp as one will only train on adversarial examples and ignore all clean samples?   In the end this submission appears to be a bit incremental. However, the authors are strongly suggested to follow the reviewers  comments to further polish their work and address the concerns above. With proper revision this work can eventually become a solid contribution on top of AdaProp.
This paper investigates some variants of the double Q learning algorithm and develops theoretical guarantees. In particular, it focuses on how to reduce the correlation between the two trajectories employed in the double Q learning strategy, in the hope of rigorously addressing the overestimation bias issue that arises due to the max operator in Q learning. However, the reviewers point out that the proofs are hard to parse (and often hand waving with important details omitted). The experimental results are also not convincing enough.     
Dear authors,  The reviewers appreciated the insights provided by your paper and the strong results. Congratulations. I encourage you to address the other points raised to make your final submission as complete as possible.
The paper presents novel model stealing attacks against BERT API. The attacks are split in two phases. In the first phase, the black box BERT model is recovered by submission of specially crafted data. In the second phase, the inferred model can be used for identifying sensitive attributes or to generate adversarial examples against the basic BERT model.  Despite the novelty of presented attacks against BERT models, the current version of the paper has some problems with clarity and motivation. The presentation of attacks is very short, and some technical details are not adequately covered. The practical motivation of adversarial example transfer attacks is not very clear, and the authors  response on this issue did not provide a convincing clarifications. Furthermore, creation of surrogate models for generation of adversarial examples is a well known technique and the difference of the proposed AET attack from this conceptual approach is not clear.  Overall, the paper reveals a solid and interesting work but a substantial revision would be necessary to make it suitable for the ACLR audience.  
The paper is proposing a novel representation of the GradNorm. GradNorm is presented as a Stackelberg game and its theory is used to understand and improve the convergence of the GradNorm. Moreover, in addition to the magnitude normalization, a direction normalization objective is added to the leader and a rotation matrix and a translation is used for this alignment. The paper is reviewed by three knowledgable reviewers and they unanimously agree on the rejection. Here are the major issues raised by the reviewers and the are chair:   The motivation behind the rotation matrix layers is not clear. It should be motivated in more detail and explained better with additional illustrations and analyses.   Empirical study is weak. More state of the art approaches from MTL should be included and more realistic datasets should be included.   The proposed method is not properly explained with respect to existing methods. There are MTL methods beyond GradNorm like PCGrad and MGDA (MTL as MOO). These methods also fix directions. Hence, it is not clear what is the relationship of the proposed method with these ones.  I strongly recommend authors to improve their paper by fixing these major issues and submit to the next venue.
The consensus of the reviews is to accept the paper. I agree.  Reviewers highlighted many strengths, including a compelling main idea: * R5: "The paper presents an interesting and motivating case for Bayesian inference in probabilistic generative models: a problem that has inherent uncertainty along with the ability to incorporate domain knowledge that can reduce the inference complexity." * R3: "Overall, the idea is interesting and supported by correct mathematical derivations and experimental proofs of concept." * R4: "the generative approach is novel. Adding domain knowledge is relevant and significant when dealing with real world applications"  As well as compelling experiments, substantially improved in the discussion period: * R1: "The authors have shown some promising results in modeling particle dynamics." * R5: "The addition of Appendix H, in my opinion, considerably strengthens the paper s story and case for acceptance. [... T]he authors have addressed most of my major concerns."  And clear writing: * R5: "In general, the paper is well written (apart from some higher level structural issues discussed below) and the notation is clear and unambiguous." * R4: "The paper is very well written, clear"  The main weaknesses highlighted were in experiments (lacking good baselines, as well as ablations), and in discussing some choices in the model s construction. These were effectively addressed in the discussion (though R5 still points to some places that could be improved).
The reviewers of this paper unanimously agreed that this paper adds an interesting theoretical and practical discussion to discrete flows. The paper has improved from the first version to the final one, in which the comments and suggestions by the reviewers have been followed.   The paper is still incremental with respect to the previous paper and the reviewers all recommended a poster presentation.
This paper proposes a temporal module for video representation learning, which is a combination of temporal attention and temporal convolution.  The reviewers  opinions diverge. R2 does not find any major flaws of the paper, while R1 expressed concerns in terms of experimental details, ablations, and missing comparison to the state of the arts. R4 expressed a similar concern, while favoring the paper a bit more.  The AC agrees more with the senior reviewers (R1 and R4) that the paper misses its experimental comparison to the state of the arts. In particular, the AC supports the statement from R4 that "Some SOTA performances are ignored selectively" to favor the proposed approach. The missing SOTA includes Slow Fast as pointed out by R4, X3D as pointed out by R1, and more. For instance, X3D is able to obtain 80.4% top 1 accuracy on Kinetics 400, which is superior to 76.9% of this paper, but is being ignored in the paper. X3D that uses a different strategy to abstract temporal information than this paper is also superior in terms of the FLOPS: X3D gets 79.1% while using almost half of the computation the proposed approach is using. The authors responded by "X3D uses network architecture search strategy to discover the optimal setting for 3D CNNs", but this is a misleading statement as X3D does not use any neural architecture search method. The authors argue that their proposed approach might be able to also benefit X3D, but this has not been confirmed and we cannot judge since no quantitative results are provided. In addition, as mentioned, several other standard baselines such as Non Local R101 (with 77.7% accuracy) and ip CSN 152 (with 77.8% accuracy) performing better than the proposed approach are missing.  Overall, we find the experimental section of the submitted paper incomplete.
The paper provides variance reduction techniques for GCN training. When training a GCN it is common to sample nodes as in SGD, but also subsample the nodes’ neighbors, due to computational reasons. The entire mechanism introduces both bias and variance to the gradient estimation. The authors decompose the gradient estimate into its variance and bias error, allowing them to apply more targeted variance (and bias) reduction techniques.  The results and improvement over existing GCN methods seem to be solid. The main weakness of the paper is its novelty. As pointed out in the reviews the techniques seem to be quite close to papers [5],[11] (referring to the authors posted list). It therefore boils down to the question of whether the authors simply applied existing techniques, achieving a better implementation than previous art, or did they develop a truly new algorithm that will encourage further research and deepen the understanding of GCNs. Given the decisive opinions of reviewers 1 and 4, that remained after taking the response into account, I tend to believe that the improvement provided here is either too incremental or not stated in a crisp enough manner in order to be published in its current form 
Dear authors,  Improving the theoretical understanding of powerful algorithms is an important contribution to our field. Nevertheless, most of the reviewers are inclined to reject the paper. I somehow have to agree with them as e.g., adding more restrictive assumptions can allow deriving better bounds, but the question then is how useful this result will be to the ICLR community. I would encourage you to chose maybe another venue.  Thanks
The paper proposes an architecture of a model for multiple correlated time series that has application in anomaly detection.  The main idea is to use attention both along time to capture trends, seasonality, etc and along series types to capture their correlation.  Training loss attempts to reconstruct masked ranges of values, and thus the discrepancy between predicted and actual values can be used to flag anomalies.  While the ideas proposed in the paper are useful and lead to a well engineered model while combining current architectural elements to best suit their task, the paper fails to impress as a research contribution.   The writing requires improvement, and the experiments on two datasets are not entirely convincing.  The authors have realized these limitations, as can be seen in the author feedback.  However, the changes entailed are too extensive to  revise the ratings across all reviewers. I hope these will help the authors submit a better next version to another conference.
This paper introduces a new model, called Memformer, that combines the strength of transformer networks and recurrent neural networks. While the reviewers found the idea interesting, they also raised issues regarding the experimental section. In particular, they found the results unconvincing, because of weak baselines, non standard experimental settings (eg. using reporting perplexity results on BPE tokens), or evaluating on only one dataset. These concerns were not well addressed by the rebuttal. For these reasons, I recommend to reject the paper.
I would like to thank the authors for the their time and effort on this work. The paper is proposing an activation function that combines RELU like piecewise activation functions and a primitive attention mechanism. Then, they show that their proposed method works better in transfer settings.  I think the approach authors taking here is more akin to a gating mechanism rather than an attention. So I would recommend the authors to change the name perhaps Gated Rectified Linear Units. The paper is interesting, but I agree with AnonReviewer4, that the experiments are not very convincing focusing on small scaled experiments in the supervised learning setting only. I would recommend the authors to compare their approach against other results from the literature as well. As it is right now it is not clear how significant the results in this paper are. I don t think the transfer and meta learning experiments are very well motivated in this paper. I would recommend the authors to better motivate those results.  After considering my suggestions above and the comments from the reviewers I would recommend the authors to consider resubmitting to another conference.   
The paper proposes the use of contrastive learning to learn patient specific representations from medical data. The authors show how their method can be used to find similar patients within and across datasets.   The paper has some issues, as indicated by the reviewers:   similarity to past work; in the response to R1, the authors specify differences to related papers; however, experimental comparisons should still be performed against CLOCS and DROPS   the evaluation is not fully convincing (the follow up comments of Reviewer 3), including the retrieval of similar patients
The paper studies three aspects of the representational capabilities of normalizing flows, with a particular focus on affine coupling layers. Normalizing flows are valuable generative modelling tools, so advancing our understanding of their theoretical properties is an important research direction.  Reviewers #2 and #4 found the contribution of the paper significant without expressing major concerns, and so recommended acceptance.  Reviewer #3 reviewed the paper very thoroughly, and expressed some concerns mainly about the experimental evaluation. Most of their concerns were addressed in the rebuttal, so they recommended weak acceptance, recognizing the merits of the paper but also pointing out the potential for improvement.  Reviewer #1 was the most critical: they expressed major concerns regarding the significance of the contributions and the overall clarity of the exposition. Despite a long exchange between the reviewer and the authors, a consensus was not reached, so the concerns remain.  The discussion so far has led me to believe that there are potentially valuable theoretical contributions in the paper, however it s clear that there is significant room for improvement in getting the contributions across. Given the strong concerns expressed, the lack of consensus, and the clear potential for improvement, I m unable to recommend acceptance of the paper in its current form. However, I do believe that the work has potential, and I hope that the discussion here will help improve the paper for a future submission.
The paper presents a new Bayesian optimization method based on the Gaussian process bandits framework for black box adversarial attacks. The method achieves good performance in the experiments, which was appreciated by all the reviewers.  At the same time, the presentation of the method is quite confusing, which currently precludes acceptance of the paper. In particular, during the discussion phase the reviewers were not able to decipher the algorithm based on the description presented in the paper. It is not clear how the problem is modeled as a bandit problem, what the loss function $\ell$ is minimized and why minimizing it makes sense (assuming, e.g., that $\ell$ it the hinge loss as suggested and the initial prediction is good with a large margin, that is, the loss is zero, equation 6 never changes $x_t$ when the procedure is started from $x$). This connection, since it is the fundamental contribution of the paper, should be much better explained. Once the problem is set up to estimate (maximize?) the reward, it is changed to calculating the difference in the minimization (cf. equation 11), which is again unmotivated. (Other standard aspects of the algorithm should also be explained properly, e.g., the stopping condition of Algorithm 1)  Unfortunately, the paper is written in a mathematically very imprecise manner. As an example, consider equation (6), where $B_p$ and the projection operator are not defined, and while these can be guessed, a projection of the argmin seems to be missing as well in the end (otherwise nothing guarantees that $x_T$, which is the final outcome of the algorithm, remains in the $L_p$ ball). Another example is the $Discrete\ Approximate\ CorrAttack_{Flip}$ paragraph which requires that every coordinate of $x$ should be changed by  $\pm\epsilon$. It is also not clear what "dividing the image into several blocks" means in Section 4.1 (e.g., are these overlapping, do they cover the whole image, etc., not to mention that previously $x$ was a general input, not necessarily an image). It is also unlikely that the stopping condition in Algorithm 1 would use the exact same $\epsilon$ for the acquisition function as the perturbation radius for adversarial examples, etc. While some of these inaccuracies and unclear definitions are also mentioned in the reviews, unfortunately there are more in the paper.  The authors are encouraged to resubmit the paper to the next venue after significantly improving and cleaning up the presentation.  
The paper works towards analysis to understand the difference   and primarily the lack thereof   between different pruning methods. The central observation is that the convolutional filters in a layer are not strongly correlated and   if the weights of the layer are taken as a matrix   then the covariance matrix is block diagonal.   Extending this objective the regime of a large number of filters, then the matrix is approximately diagonal and all weights are   approximately Gaussian and i.i.d. The point of this analysis is that under this assumption, norm based metrics, particluarly $\ell_1$ and $\ell_2$, behave quite similarly.  The pros of this paper are the extensive evaluation and   after revisions   relatively clear text. The core analysis is nice to have elaborated in detail in the community.  The primary con of this paper is, as the reviewers point out, that there are limited conclusions to take away from this work. Specifically, a plausible default hypothesis is that different pruning criteria result in different pruning decisions. From the results in this paper, that still seems to hold with   exception of the norm based metrics. So, while this work does demonstrate that these norm based metrics are relatively similar   a nice clarification to see in the community   the work offers limited comment on the broader space of pruning metrics.  My recommendation is Reject. Despite the strong empirical evaluation, the ultimate results offer limited clarification on the similarity of pruning metrics. 
All reviewers agree that the current approach is very similar to traditional uncertainty based active learning, and that the empirical results are inconclusive, so at this point the paper is not ready for publication.
This paper is very pleasant to read. The reviewers also like the key idea discussed and find the targeted application interesting and practical. However, after reading the indeed interesting motivation, all four reviewers expected to see more from the evaluation section, including more challenging and realistic set ups and clearer gains over standard methods. The reviewers also discuss how both the navigation problem as well as the GP constraint problem have been tackled in the past, often in combination (e.g. reference [1] by R1). Therefore, it would be needed to see additional experimental evaluation in line with those previous works.
The paper focuses on adversarial attacks for RL, which is an exciting understudied research direction, and can be of interest to the community. All the reviewers are (mildly) positive about the paper and the author competently replied to the concerns expressed by the reviewers. 
This paper proposes benchmark tasks for offline policy evaluation.  The proposed benchmark tasks evaluate the policy with batch data with respect to three metrics, including the standard mean squared error. The paper also evaluate several baseline offline reinforcement learning methods with the benchmark tasks, which will serve as standard baselines.  All of the reviewers are in favor of the paper.
The paper presents a new GNN+ architecture and provide interesting theoretical observations about the architecture. The paper is quite promising and has several interesting insights. However, most of the reviewers believe that the paper is not ready for publication and can be significantly improved by: a) more formal and precise statements, b) clarifying the key points of the paper, c) more thorough experimental validation of the framework on real world datasets.   
All four reviewers unanimously recommended for an acceptance (four 7s). They generally appreciated that the proposed idea is novel and experiments are convincing. I think the paper tackles an important problem of evaluating GANs, and the idea of using self supervised representations, as opposed to the conventional ImageNet based representations, would lead to interesting discussions and follow ups. 
The paper aims to address several challenges in learning neural network based optimization algorithms by increasing the #unrolled steps, increasing the #training tasks, and exploring new parameterizations for the learning optimizer. The authors demonstrated the effectiveness of applying persisted Evolution Stratergies and backdrop through over 10,000 inner loop steps can improve the performance of the learned optimizer. Empirical experiments showcased incorporating LSTM to the previous state of the art improve their training performance.   There are a lot of interesting ideas in the paper. However, packaging them together and only glance over each idea briefly unfortunately dilutes the contribution and the novelty of the work. There are still some major concerns echoed among the reviewer:  1) The proposed hierarchical optimizer seems interesting. It is one of the major contributions of the paper. But, its architecture was only briefly mentioned in Sec 3.3. Its motivation, implementation and the corresponding engineering choices remain unclear by just reading the main text. Some of the details were discussed in the appendix but it would be of great interest if authors could give some intuition on which subset of the tasks the proposed architecture gives the most improvement / failure among the 6000 tasks.  2) Training the optimizer on a diverse set of tasks is crucial for the learned optimizer to generalize. One of the paper s contributions is to further expand the task dataset from the prior work Metz et al., (2020). The authors have conducted very thorough experiments on this new dataset, which is amazing. I would argue there are even enough results for another standalone paper. However, there is surprisingly little detail on how the newly proposed dataset differs from the prior TaskSet dataset. What are the new optimization problems? How are they different from the family of tasks in TaskSet? A TSNE plot of the tasks similar to Figure 1 from Metz et al. (2020) could provide more intuition for the reader and highlight the contribution.   Overall, if the authors could provide more insight into their experiments and the proposed methods, it would help the readers greatly to see the novelty and the contribution of the paper. The current version of the paper will need additional development and non trivial modifications to be broadly appreciated by the community.   
As discussed by several reviewers the paper is an application of classical ML approaches for a very relevant problem of calibration of radio interferometers. The application is interesting but lacks novelty in terms of ML methodology and the experiments do not provide a meaningful comparison between the state of the art and the proposed approach or justification for the choice of predictors and their parameters. This paper in clearly not a good fit for a general ML conference.
All the reviewers are positive about the paper; R2 and R3 voted for clear accept. Overall, all the reviewers feel that evolution is comprehensive and the results are decent. There is a novel objective formulation that controls for motion diversity, disentanglement and content matching, outperforming existing methods across multiple datasets. High res videos at 1024x1024 are generated and there is cross domain video generation. Many good questions were raised by the reviewers, and they were addressed in details in the rebuttal. In particular, the question about subtle motion and short video sequences was raised (which was the concern that the AC had). The AC agrees with the reviewers that the paper warrants a publication. Please address the questions raised by the reviewers in the final version. 
The paper proposes a trick for stabilizing GAN training and reports experiment results on spectrogram synthesis. All the reviewers rate the paper below the bar, citing various concerns, including a lack of clarity and unconvincing results. Several reviewers suggest conducting evaluations in the image domain as most of the GAN training techniques are proposed in the image domain. After consolidating the reviews and rebuttal, the area chair finds the reviewer s argument convincing and would not recommend acceptance of the paper.
This paper deals with a problem of feature compatible learning, where the features produced by new model should be compatible with old features. As pointed out by the reviewers, there are several weaknesses with this paper: (a) the novelty is not strong enough, (b) the experimental results should be better explained and be more thorough, (c) the formulation is not well motivated.
This is a well written paper with good experimentation.  The paper builds on the work of FedDF and does ablation studies to demonstrate its improvements.  The key original idea is the use of a common pool of unlabeled data which is used in transmitting partial results between local and global servers.  The results seem pretty good.  From a practical viewpoint, the unlabelled common data will, in most cases, need to be generated/artificial data since it will need to be public (to the other servers at least).  This option should be tested to demonstrate feasability.  AnonReviewer2 was concerned about whether it was fair to provide additional unlabelled data.  The authors tested this out and showed it was OK.  Regardless, the different servers could easily generate artificial data for this purpose.  AnonReviewer1 had a number of issues which the authors largely addressed. The other two reviewers appreciated the paper.  All reviewers gave constructive suggestions. 
While the paper studies an interesting and important problem, namely the language generation, it is poorly written, which makes it difficult to judge its value. The reviewers also expressed concern over the scope of the evaluation and the lack of comparison to SOTA.
The paper proposes an approach to sparse CCA with deep neural nets, performing simultaneous feature selection with stochastic gating and canonical correlation maximization.  The reviewers think that there is merit in defining an objective function that optimizes the goals jointly throughout the networks. However, the paper has not clearly presented the novelty in methodology. In particular, the reviewers agree that the paper needs to clearly distinguish itself from the two building blocks (Andrew et al. 2013 and Louizos et al. 2017), and demonstrate the significance of combining the two techniques theoretically and/or experimentally. Also, there is a large literature in sparsifying classical method. Sufficient discussions and comparisons with prior work can better position the current work in the literature.
A line of work since 2016 has investigated learning NN based optimisers, which produce optimisation updates by processing loss/gradient info with neural networks. This paper tries to understand the learned dynamics of these NN based optimisers by linear approximation to the learned non linear dynamics. Visualisation of these approximations are shown on 3 optimisation problems: linear regression, Rosenbrock function, and a toy neural network classification problem, with the hope of covering different types of objective landscapes.   Reviewers agreed that the paper studies an important research question, which would interest researchers working on meta learning learning algorithms. However there are several major concerns raised by the reviewers: (1) the example optimisation problems are toyish, and (2) the paper does not explain very well the link between the visualised behaviour and the better optimisation results, i.e. it is unclear to the reviewers why the learned dynamics lead to better optimisation results.   While I am not too concerned about issue (1), I think issue (2) is a significant one, flagging that the clarity of the paper needs to be improved. Ultimately, the paper is motivated by the question "How is a learned optimizer able to outperform a well tuned baseline?", so a reader would expect some clear explanation towards answering this question. Also some reviewers are concerned about the fact that only the RNN based optimiser in Andrychowicz et al. (2016) is analysed; since there exists other forms of learned optimisers, focusing on studying only one type of them might lead to early conclusions that are not so accurate.
The paper received four borderline reviews.  Overall, the manuscript has improved after the rebuttal (in particular, an issue in the convergence proof has been fixed), and a reviewer has increased his score to borderline accept. Yet, the paper did not convince the reviewers that the contribution was significant enough and none of the reviewer got enthusiastic about the paper. The main issue with the paper seems to be the unclear positioning between the optimization literature for stochastic composite optimization, the literature on support identification (e.g., Nutini, 2019), and the (more empirical) deep learning literature.  The paper postulates that the group sparsity regularization is crucial for deep neural networks, which seems to be the main motivation of the paper. Yet, the experiments do not demonstrate any concrete consequence of better group sparsity, wether it is in terms of accuracy or interpretability.  If positioned in this literature, a comparison should be made with classical pruning approaches, where pruning occurs as an iterative procedure that is distinct from optimization. If positioned instead in the stochastic optimization literature, better analysis of the convergence rates should be provided; if positioned in the support identification literature, the paper should explain how the results compare to those of the literature (e.g., Nutini, 2019 and others). In other words, any point of view requires clarifications and additional discussions.   Besides,       the theoretical assumptions need to be discussed:  does the Lipschitz assumption holds for multilayer neural networks ? Certainly not for ReLu networks, but what can we say something useful, even with smooth activation functions?      the experimental setup needs more details. Reproducing the experiments with the current paper seems difficult; in particular, the choice of hyper parameters is not crystal clear.  For these reasons, the area chair recommends to reject the paper, but encourages the authors to resubmit to a future venue while taking into account the previous comments.
This paper proposes a controllable text generation model conditioned on desired structures, converting a text into structure information such as part of speech (POS) and participial construction (PC). It proposes a “Structure Aware Transformer” (SAT) to generate text and claims better  PPL and BLEU compared with GPT 2. Reviewers pointed out that limited novelty of this paper   SAT is essentially a transformer run on multiple sequences of structure information, with sums of structure embeddings as input embeddings    the proposed method essentially infuses structure information as features, rather than “controlling” text generation. Some references are also missing, most prominently:   1. Zhang X, Yang Y, Yuan S, et al. Syntax infused variational autoencoder for text generation[J]. arXiv preprint arXiv:1906.02181, 2019. 2. Casas N, Fonollosa J A R, Costa jussà M R. Syntax driven Iterative Expansion Language Models for Controllable Text Generation[J]. arXiv preprint arXiv:2004.02211, 2020. 3. Wu S, Zhou M, Zhang D. Improved Neural Machine Translation with Source Syntax[C]//IJCAI. 2017: 4179 4185.  Unfortunately, no answers are provided by the authors to the questions asked by the reviewers, which makes me recommend rejection.
Adversarial training is usually done on the image space by directly optimizing the pixels. This paper suggests the adversarial training over intermediate feature spaces in the neural network. The idea is very simple. The authors have done extensive experiments to justify its performance. But the performance gain though this idea seems to be marginal. Further, the layer to conduct the adversarial training can be optimized within the framework, which aligns with the general autoML idea. The new version L ALFA has been well introduced, but unfortunately, the practical result can be very straightforward, that is just to select the final layer. The more important ALFA hyperparameters that would most benefit from automatic tuning are not sufficiently treated.  There have been extensive discussions between the authors and the reviewers. After incorporating the reviewers  comments, the paper will have a good chance to be accepted at another venue.   
In this paper, the authors propose a test for subgroup treatment effects in settings where data is obtained online, via a method they call SUBTLE.  The authors adopt a semi parametric (generalized linear model) approach to modeling nuisance functions.  The authors derive the form of the distribution of their test statistic in (12), which is based on asymptotic normality the influence function based estimator.  The authors evaluate their methods via simulation studies, and on a dataset of user clicks from Yahoo!  The opinion of the reviewers was somewhat split on this paper.  One reviewer felt the paper was out of scope for ICLR, although this did not influence the overall evaluation of the paper   since ICLR s scope has now broadened and solicits work on all areas of machine learning and related areas of data science (as the ICLR website makes clear).  However, reviewers raised a number of concerns about the paper (in particular, see reviewer 2) that on balance did not persuade them that the paper is ready for publication in the current state.
The paper proposes a novel off policy meta RL algorithm able to achieve efficient exploration in meta training able to perform a fast task identification. Although the reviewers agree that this paper has merits (relevant topic, interesting idea, nice experimental analysis), they have raised several concerns about the clarity, the novelty, and the soundness of the proposed approach, which make the paper not ready for publication. However, the authors are encouraged in improving their approach since the direction is considered promising by the reviewers.
This paper is motivated by figuring out what regularization do popular neural network reconstruction techniques correspond to. In particular, this paper studies a convex duality framework that characterizes the global optima of a two layer fully convolutional ReLU denoising network via convex optimization. The authors use this regularization to interpret the obtained training results. The reviewers raised a variety of concerns regarding the tractability of the optimization problem (seems to be exponential in number of constraints), the utility for interpretation etc, significance of the results compared to existing literature. Some of these concerns were alleviated but not fully resolved. One reviewer had concerns about the correctness of the proof that was resolved based on the authors’ response. I share many of the above concerns. However, I do think having a computationally feasible way to figure out the exact regularization in these simple settings (at least with small dimensions) could provide some insights to guide further theoretical development.  Therefore I am recommending acceptance. However, I strongly urge the authors to further revise the paper based on the above comments.
This paper presents an interesting mix of new theoretical and empirical results showing how learning temporally extended primitive behaviors can help improve offline (batch) RL.  Although 2/3 reviewers initially raised concerns regarding the motivation of the approach and some of the choices that were made, the authors did an excellent job at addressing these concerns in detail, and there is now a consensus towards acceptance.  I consider that this work is a meaningful contribution towards better offline RL, which is definitely a very important use case in practice. The authors have given convincing explanations to motivate their approach, and made several improvements to the paper. As a result, I am recommending it for acceptance, as a poster.
Paper was reviewed by four expert reviewers. Unfortunately all reviewers, uniformly felt that paper fell marginally bellow bar and argue for rejection. A number of concerns have been identified by the reviewers in the review phase.  Those included: (1) lack of novelty [Reviewer3, Reviewer4], (2) lack of various ablations [Reviewer 2], (3)  issues with experimental setup [Reviewer4], and (4)  lack of significant improvements in performance [Reviewer 1, Reviewer 3]. While authors addressed some of the concerns with provided experiments and ablations during the rebuttal, Reviewers remained unconvinced on the main concerns of novelty and significance. As such, the reviewers are unanimous in their assessment and AC does not see a reason to overturn this consensus. 
in this submission, the authors propose a sophisticated pretraining strategy for neural machine translation based on the paradigm of self supervised learning. despite some interesting and potentially significant improvement in various machine translation settings, the reviewers as well as i myself could not determine where specifically those improvements come from. is it their particular strategy of pretraining or is it just self supervised learning in general? in order for this question, which i believe is a key question to be answered, more thorough ablative experiments and/or comparison to other self supervised learning based pretraining algorithms, such as MASS & BART which were discussed as similar and motivational in the submission, must be done. when these are done, the submission will be much stronger and attract much more interest.
This paper provides a simple approach to incorporate temporal information in RL algorithms. AC agrees with authors that simplicity is a virtue. As reviewers point out that experimentally the approach is not conclusively better (given that environments might be hand chosen). Even R3 believes some reported improvements is within variance. Given the discussions, AC agrees that results do not seem convincing enough.
This submission received reviews with a very wide range of scores (initially 3,5,5,9; then 5,5,5,9). In the discussion, all reviewers maintained their general position (although a private message by the reviewer giving a score of 9 said he/she would consider going down to an 8).  Because of the high variance, I read the paper in detail myself. I agree with all reviewers that NAS is a very important field of study, that the experiments are interesting, and that purely empirical papers studying what works and what doesn t work (rather than introducing a new method) are definitely needed in the NAS community. But overall, for this particular paper, I agree with the 3 rejecting reviewers. The paper presents a lot of experiments, but I am missing novel deep insights or lasting overarching take aways. The papers reads a bit like a log book of all the experiments the authors did, before having gone through the next iteration in the process to consolidate findings and gain lasting insight.  In a bit more detail, half the results in Section 4 use medium sized super networks, which seem broken to me, yielding much worse performance than small super networks. I did not find any motivation for studying these medium sized networks, no reason given for them to perform poorly, and none stating why the results are still interesting when the networks perform so poorly (apologies if I overlooked these). The poor performance may be due to using a training pipeline that works poorly for these larger networks, but this is hard to know exactly without further experiments. I would either try to fix these networks  performance or drop them from the paper entirely, as I do not see any insights that can be reliable gained from the current results. As is, I believe these results (accounting for half the plots in the paper) only muddy the water and are preventing a crisp presentation of insightful results.  Another factor that I find unfortunate about the paper is that it only uses NAS Bench 201 for its empirical study, and even for that dataset, mostly only the CIFAR 10 part. After getting rid of isomorphic graphs from the original 15625 architectures, NAS Bench 201 only has 6466 unique architectures (see Appendix A of NAS Bench 201), while, e.g., NAS Bench 101 has 423k unique architectures. As the authors indicate themselves in their section "Grains of Salt", it is unclear whether insights gained on the very small NAS Bench 201 space generalize to larger spaces. I therefore believe that there should also be some experiments on another, larger space, to study how well some of the findings generalize. An additional benchmark that the authors could have directly used without performing additional experiments themselves is the NAS benchmark NAS Bench 1shot1 (ICLR 2020: https://openreview.net/forum?id SJx9ngStPH), which studies 3 different subsets of NAS Bench 101, and which was created to allow one shot methods to use the larger space of evaluated architectures in NAS Bench 101.   Minor comments:   It reads as if the authors performed 5 runs, computed averages of the outcomes, and then computed correlation coefficients. That would be a suboptimal experimental setup, though; in practical applications, only one run of the super network would be run, and therefore, in order to assess performance reliably, one should compute correlation coefficients for one run at a time, and then obtain a measurement of reliability of these correlation coefficients across the 5 runs.   The y axis in Figure 2 appears to be broken: for example, in the left column it goes from 99.978 to 99.994, and the caption says these should be accuracy predictions of NAS Bench201. However, even the best architectures in NAS Bench201 only achieve around 95% accuracy.   Overall, I recommend rejection for the current version of the paper. Going forward, I encourage the authors to continue this line of work and recommend that they iterate over their experiments and extract crisp insights from their experiments. I also recommend performing experiments with a much larger search space than that of NAS Bench 201 to assess whether the findings generalize.
The paper proposes a method  to generate conversations for evaluate dialog systems using counterfactual generation.   Pros:   The reviewers agree that the paper makes a good contribution towards evaluation of DST models.    The paper adds to a growing body of work on robust evaluation of NLP models  Cons:   One reviewer has commented on the lack of novelty. However, I believe that the authors have adequately addressed it. In particular, I do not see any harm in using heuristics/templates to generate counterfactuals as long as the final goal of robust evaluation is achieved.    It would have been good to evaluate the method on other datasets. However, I agree with the authors  rebuttal that this is indeed the most popular dataset for the task and most SOTA methods evaluate on this dataset.   The authors have adequately addressed all reviewer concerns and have clearly highlighted their contributions and novelty.  I think of this as a valuable contribution and would like to see the paper accepted. 
This paper proposes a set of synthetic tasks to study and discover the inductive biases of seq2seq models.  Authors did a great job in convincing all the reviewers except R5 in their rebuttal. I do not find any serious concerns from R5 s review. I personally think this is a very useful analysis paper.
This paper introduces an approach based on filter transform for designing networks equivariant to different transformation groups. Especially, the authors rely on the haramonic analysis view of steerable CNNs given in Weiler & Cesa (2019) to design an equivariant filter bank by computing simple transforms over base filters.   The reviewers finds the paper technically solid but difficult to read and with a limited contribution.  The AC carefully reads the paper and discussions. Although the connection between steerable CNNs and filter transform are interesting, the AC considers that the main contributions of the paper should be consolidated, especially the positioning with respect to Weiler & Cesa (2019). \ Therefore, the AC recommends rejection.
The reviewers appreciated the author replies, the additional experiments (more runs but also more ablations/baselines), and the updated paper. Also R2 is now largely satisfied (but seems to have been too late to post a public reply or to raise the score of the review).  The paper provides important insights in model based RL and its connections to planning, by studying MuZero with systematic ablations. Hence a valuable contribution to the community. All (major) cons have been addressed in the revision.
This paper proposes a new paradigm for learning to perform cooperative tasks with partners, which factors the problem into two components: how to perform the task and how to coordinate with the partner according to conventions. The setting is new and the reviewers are excited about the paper. A clear accept.
Multiple reviewers point out the interesting improvement to mix attention maps at different layers via convolution based prediction modules. This module is sufficient to show improvements only on encoder side while comparing to concurrent work Synthesizer. However, the novelty of the work is limited as compared to other papers and the results though improved did not convince the reviewers fully to gain a strong accept. 
The paper proposes an end to end architecture, Net DNF,  for handling tabular data. This is a novel approach in the relatively under explored domain of application of neural networks; the paper also presents justification of the design choices via ablation studies. The paper is clearly written, and empirical results are convincing. 
This paper presents an interesting connection between learning theory and local explainability. The reviewers have reacted to each others  thoughts, as well as the authors  comments; they are largely in favor of acceptance. I think the ICLR community will enjoy discussing this paper at the conference.
The paper proposes matching the distribution of biases for an LSTM to estimates of long range mutual information from analyzing the statistics of languages. The authors shows empirical evidence that LSTMs seem indeed to be following such a distribution, using natural language and Dyck 2 grammar. They show that explicitly enforcing the distribution of biases in learning can actually help LSTM language models. The reviewers had slight concern about some of the baseline numbers reported, but the authors took the time to address those concerns. Overall, it was an interesting and thought provoking paper that can provide a useful angle to consider when building recurrent models for a problem   namely that of matching the properties / inductive bias of the model to that of the data.
This paper presents a novel approach for integrating time into deep neural network models based on the Gaussian process limit view of a neural network model. Specifically, the approach augments an a temporal neural network designed to process a single time point with a temporal kernel that relates data points across time. The composition of the a temporal neural network kernel with with the temporal kernel is accomplished efficiently using a random features representation of the temporal kernel. The authors propose to represent the temporal kernel via its spectral decomposition, which makes the approach quite flexible. Learning leverages re parameterization. While random features have been used to approximate temporal kernels in prior work [1], the approach in this paper is significantly more general in that it can be composed with any a temporal deep architecture and the authors show results for RNNs, CNNs, and attention based models. The predictive performance of the approach also appears to be consistently better than baselines and it works particularly well on the challenging case of irregularly sampled data.  In terms of weaknesses, the reviewers had a number of questions about the paper. The authors updated the paper to include some more recent models including ODE RNNs. This material is currently presented in the appendices and needs to be moved into the main paper. Several of the reviewers also had technical questions questions that are in fact addressed in the manuscript; however, the authors are relying heavily on the appendices to present many important details and the paper is currently over 30 pages long. The frequent references to the appendix for additional details makes the paper a challenging read. The authors have already done some work to address clarity by adding a new figure, but should prioritize moving additional key details into the main body of the paper to improve readability.   [1] http://auai.org/uai2015/proceedings/papers/41.pdf
This paper shows that transformer models can be used to learn certain advanced mathematical concepts such as the local stability of differential equations. Reviewers found this surprising and useful for engineers, and the evaluation was adequate. They also felt that it opens the doors to similar studies on other aspects of mathematics.
This paper presents a method for unrolling the iterative expectation maximizing steps in the EM algorithm for a Gaussian mixture model into layers in a neural network. Then, the proposed method is applied in the latent space of an autoencoder to allow deep clustering using autoencoder features. The reviewers raised concerns regarding the novelty, lack of systematic ablation experiments, and the efficacy of mimicking iterative optimization steps. Moreover, the training objective does not *exactly* correspond to the variational lower bound that EM often follows.   The additional experimental results in the revised version address some of the concerns regarding the empirical studies. However, the relationship between the proposed method and incremental EM algorithms and top down generative models is not properly discussed. Moreover, backpropagating through unrolled iterations does not seem elegant compared to principled approaches such as variational/amortized inference that are commonly used. For example, increasing the number of unrolled steps is observed to hurt the performance due to "vanishing gradients" whereas in the EM algorithm increasing the quality of optimization in the M steps should provide a better fit to training data.   Given the current presentation, I believe that the paper is not ready to be presented at ICLR. However, I encourage the authors to take the review feedback into consideration to improve the paper.
This work is well written and accurately covers the context and recent related work. It s a good example of how to apply self supervised training to the event sequence domain. However, the combination of a lack of technical originality (composing a set of previously explored ideas) and significant improvements in results (results with CoLES overlap in error bars with RTD results) limits the impact of this paper.  Pros:   Well written.   Extensive evaluation.   Well formulated problem.  Cons:   Lack of technical novelty. The method appears to be general to all sequences rather than specialized for event sequences so the motivation for this design is not crystal clear.   Minor improvement in results from using the method despite written claims that the method  significantly outperforms .   Limited analysis that shows the periodicity and repeatability in the data.
This meta review is written after considering the reviews, the authors’ responses, the discussion, and the paper itself.  The paper proposes a training scheme for autoencoders, involving data augmentation and interpolation, that results in autoencoders for which interpolations in the latent space lead to meaningful interpolations in the image space. The paper notices that this property carries over reasonably well to datasets different from the training one.  The reviewers point out that the idea is interesting (R1, R2, R4) and simple (R2), but the experiments are substandard (R2, R4) and presentation is at times suboptimal (R1). Overall consensus is towards rejection. Authors addressed some of the concerns in their responses, but failed to convince the reviewers to change their evaluations.  I agree with the reviewers and recommend rejection at this point. The idea is indeed interesting and could be publishable if presented and evaluated well, but in the current manuscript the presentation is at times unclear or somewhat misleading (e.g. presenting the method as a general image generation method, not an interpolation method) and the experiments are reasonable, but not quite convincing, mainly because the architectures and the baselines are outdated (as also pointed out by R1 and R4). I encourage the authors to further improve the paper and resubmit to a different venue.  
The paper receives a mixed rating, with R3 rates the paper above the bar, R1 and R2 rates marginally above the bar, and R4 recommends rejection. The cited positive points include 1) decomposing image generation into first synthesizing segmentation masks and then converting segmentation masks to images, and 2) good results comparing to Progressive GAN and BigGAN. R4 raises several concerns, including the novelty concern and unconvincing experimental validation. After analyzing the papers, the reviews, and the rebuttal, the AC finds the arguments made by R4 more convincing. Decomposing image generation to a two step approach has been illustrated in the prior work [Wang & Gupta ECCV 2016, Hong, Yang, Choi, Lee CVPR 2018]. The proposed method does not provide additional insights. The provided experimental results are not very convincing, either. As the proposed setting assuming the availability of segmentation masks, it is not surprising that it outperforms the unconditional baselines. Overall, the AC believes the paper does not have enough novelty to justify its acceptance and would recommend rejection of the paper.
This paper exposes a modification of self play (in a non cooperative setup) where agents expose their internal states to each other, and adds a "truthfulness" mechanism to ensure the agents do not hide information to each other.  The reviewers generally agree that the ideas presented, in particular the imaginary rewards, is interesting and should be explored further. The experimental results are good.  However, reviewers point out many problems related to clarity, writing and notations, and they note many typos. The motivation of this work also needs to be explained further. Reviewers also find that the related work section needs to be remade, as it both lacks citations, and cites works that are not really relevant. This work positioning with respect to these previous works also needs to be expanded. The reviewers also point out that the experimental results should be discussed further.  Generally, the paper cannot be published in its current state, and unfortunately the authors have not submitted an updated version or answers to the reviews. I therefore recommend rejection for this paper.
The paper considers sample generation in high dimensional bayesian inference and proposes a multi scale procedure that performs coarse to fine multi stage training and enables interpretability of intermediate activations at coarse scales.  The method is simple, elegant and addresses a very important bottleneck of high dimensional bayesian inference. The clarity of the paper has been greatly improved based on the reviewers suggestions.   However some concerns remain regarding the evaluation that are needed to clearly demonstrate the value of the approach. In particular, it would be important to assess the impact of the number of levels, and how quickly the dimensions grow from one level to the next. The number of forward simulations does not provide a sufficient picture of the computational cost of the approaches. It would be also important to provide wall clock time.  Figure 2a should also provide the best of 3  independent experiments,  or better, more experiments should be run, and curves with shaded areas should be provided so one could visualize variability w.r.t runs. 
This paper studies the link between generalization behavior and "flatness" of the loss landscape in deep networks. Specifically, the authors study two measures of flatness (local entropy and local energy), and show that these two measurements are strongly correlated with one another. Moreover they show via a careful set of numerical experiments that two previously proposed algorithms (entropy SGD and replica SGD) that optimize for local entropy tend to both find flatter minima as well as provide better generalization.  Despite the fact that the paper proposes no new models or algorithms, the experiments are compelling and provide non trivial insights into predicting generalization behavior of deep networks, as well as solid evidence on the benefits of entropy regularization in SGD. The authors also seem to have satisfactorily answered the (numerous) initial concerns raised by the authors. Overall, I recommend an accept.
This paper studies two techniques for handling high dimensional action spaces in deep RL, namely selecting action components independently or selecting components sequentially in an autoregressive approach.  The methods are developed for two deep RL algorithms (PPO and SAC) and tested on multiple domains.  The reviewers recognized the significance of this research topic but found significant problems in the presentation.  The reviewers raised concerns on the relationship to prior work in the literature (R2), baseline comparisons that are missing in the experiments (R2, R4), and a lack of clarity in the intended message of the experiments (R3).  The authors responded favorably to the reviews, answered clarification questions, and acknowledged the limitations of the submitted work.  The authors expressed their intent to release a stronger paper sometime in the future.  The reviewers acknowledged the author response and were in consensus that the submission needs more work.  Three reviewers have indicated reject for the reasons described above.  The paper is therefore rejected.
This is a fairly technical paper bridging deep learning with uncertainty propagation in computations (i.e. probabilistic numerics). It is well structured, but it could benefit from further improvements in readability given that there are only very few researchers that are experts in all sub domains associated with this work. Given the above, as well as low overall confidence by the reviewers, I attempted a more thorough reading of the paper (even if not an expert myself), and I was also happy to see that the discussion clarified important points. Overall, the idea is novel, convincing and seems well executed, with good results. The technical advancements needed to make the idea work are fairly complicated and are appreciated as contributions, because they are expected to be useful in other applications too (beyond irregular sampled data) where uncertainty propagation matters.
The reviewers, including me, agreed that considering sampling diversity is interesting and reasonable when designing GNNs. However, the proposed method is too heuristic and empirical. Without the authors  feedback, I tend to reject this work.
This paper proposed an MCMC sampler that combines HMC and neural network based proposal distribution. It is an improvement over L2HMC and [Titsias & Dellaportas, 2019], with the major innovation being that, the proposed normalizing flow based proposal is engineered such that the density of the proposal $q(x |x)$ is tractable. Experiments are conducted on synthetic distributions, Bayesian logistic regression and deep energy based model training.  While reviewers are overall happy about the novelty of the approach, some clarity issues have been raised in some of the reviewers  initial reviews. Also concerns on the evaluation settings, including the missing evaluation metric such as ESS/second, are also raised by the reviewers. The revision addressed some of the clarity issues, but some experimental evaluation issues still exist (e.g. comparing with L2HMC in terms of ESS/second), and the replaced MALA baseline results make the improvement of the proposed approach less clear.  I personally find the proposed approach as a very interesting concept. However I also agree with the reviewers that more experimental studies need to be done in order to understand the real gain of the approach. 
This paper is borderline, as evidenced by all of the reviewer s scores.  The pros are:   important and relevant topic    IMPORT is a reasonable, technically sound approach   paper is relatively clear  The cons all lie in the experimental evaluation, and whether the experiments sufficiently back the claim that IMPORT can learn sophisticated exploration strategies and validate IMPORT s merits compared to prior algorithms. In particular:   The choice of benchmarks does not sufficiently test the ability to explore in a sophisticated manner   Lack of comparisons to PEARL and MANGA, which can readily be applied to the online setting   The empirical improvements are relatively modest.  Overall, the cons slightly outweigh the pros of the paper. Indeed, no reviewer was willing to champion the paper s acceptance.
This paper studies the problem of adversarial training for graph neural networks. The proposed method is build on the free training approach, and more specifically FreeLB, with some additional tricks including bias perturbation (for node classification) and unbounded attacks.  While these additions are potentially useful, there are only limited investigation into their effect.  Putting aside the technical distinctions of the method with prior work, this paper can also be viewed as an empirical study of adversarial training techniques on graph data with various GNN architectures.  It is worth noting that overall the conclusions on "adversarial training" are positive, we do see consistent improvement over a variety of architectures and tasks.   The issues however, is that it is unclear whether these improvements can be similarly achieved using prior technique like FreeLB (oblation is only done on one single task, where biased perturbation is shown to lead to minor improvement).   The paper also provides some results showing the effect of depth of the network as well as different training strategies such as batch norm, dropout with general adversarial training.  These results are interesting to see but do seem to be limited in both scope and depth.  It appears that the authors have two goals in mind, one is to propose FLAG and demonstrate its usefulness, and the other is to provide a better understanding of how adversarial training works for GNNs in general.  Given the limited novelty of FLAG compared to prior methods, the main contribution actually comes from the later part, which unfortunately is somewhat underdeveloped.   
All reviewers agree that this paper is very solid work, that presents great progress in no press diplomacy. The method and presented experiments are of very good quality and the work merits to be presented at ICLR.
Addressing the initialization issue in DNNs is an important topic, and the proposed approach is found by the reviewers to be interesting. However, the reviewers feel that to clearly promote this research beyond the  proof of concept  phase, deeper investigation in multi layer architectures would be required. This would raise the significance of the paper. Besides extending the study to deeper networks, the paper could also benefit from elaborate experiments to increase convincingness, in particular by addressing R4 s concerns regarding robustness of performance e.g. on small dataset sizes. Finally, the methodology is sound and the authors clarify the significance of the ReLU associated covariance; however, overall the paper does not offer significant technical advancements that could make up for the shortcomings in the areas discussed above. 
This paper proposes a model agnostic FL method called FedKT that performs only one communication round and reduces the communication complexity of federated learning. The reviewers have the following concerns about the paper: * Limited novelty because the proposed method is directly based on PATE * Insufficient experiments  The authors did a great job of responding to the reviewers  comments and also added some new experimental results in the updated version. But the reviewers still recommend significant revision of the paper and resubmission to a future venue. I hope the authors will find their constructive and detailed comments below helpful!
First as a procedural point, the paper got 7, 7, 5, 5. AnonReviewer3 gave it a 5, but seemed satisfied by the discussion and promised to raise their score. They did not do so, but I must interpret their last messages as indicating they now support the paper. AnonReviewer2, the other 5, had some concerns that other reviewers seem to have helped address during rebuttal. They did not update their score, but were happy to leave their certainty low and defer to other reviewers  recommendation. As such, although the average score looks low in the system, the paper is of an acceptable standard according to reviews.  The paper adapts a method from tabular RL to Deep RL, allowing (as the title aptly says), agents to learn What to do by simulating the past. Reviewers speaking in support of the paper found that the paper was clear and sound in its evaluation, providing interesting results and a useful and reusable method. It is my feeling that after discussion, the case for the paper has been clearly made, and in the absence of any strong objections from the reviewers, I am happy to go with the consensus and recommend acceptance.
The authors propose to provide fast convergence results for the OGDA and OMWU algorithms based on a reinterpretation of the metric subregularity in the saddle point problem setting. During the rebuttal period, the paper improved significantly, not only due to the diligence of the authors but also due to reactive reviewers that provided extremely constructive comments. The technical developments are quite nice: Lemma 2 allows constant step size parameter as compared to Daskalakis and Panageas, followed by Theorem 3, which establishes the first linear rate under the saddle point metric subregularity. The numerical demonstrations are also helpful in driving the point home. Although it is not surprising that the shape of the polytope matters, it is still impactful to see the linear rate.    ps. The authors should consider including a related work comparison to the reflected FB algorithm in [1] since it reduces to the FoRB and it also provides convergence analysis for the sequence in the general monotone inclusions.   [1] Cevher and Vu, "A reflected forward backward splitting method for monotone inclusions involving Lipschitzian operators,   https://arxiv.org/pdf/1908.05912.pdf
The paper proposes a method for SLAM like dense 3D mapping (colored occupancy grid) based on differentiable rendering with a possibility to provide a probabilistic generative predictive distribution, evaluated on UAVs.  Initially this paper has a wide spread of reviews, with ratings between 4 and 9. Reviewers appreciated the elegant and principled formulation and the interest of the predictive distribution. On the downside, several issues were raised on the incremental nature wrt to DVBF LM; presentation and writing being very dense and difficult to follow; positioning wrt to prior art; performance with respect to known visual SLAM SOTA baselines; limited evaluations.  The authors provided responses to many of this questions and also updates to the paper, which convinced several reviewers, who unanimously recommended acceptance after discussion.  The AC concurs.
The papers studies machine learning tasks in the presence of adversarially corrupt data (during training). In particular, it is assumed that the labels of a small constant fraction of the datapoints are arbitrarily corrupted.  The paper proposes a natural method to solve this problem and evaluates it on various datasets. As pointed out by the reviewers, the theoretical contributions of this paper are subsumed by a number of prior works (which were not initially cited). The experimental results of the paper are interesting. However, the method proposed  and evaluated is not particularly novel. In my opinion, the problems studied in this submission are important (in particular, the memory/space consideration in the context of robustness). However, this work still needs work and is not ready for publication.
Summary: The authors built on existing work of GP vine copula models. Some modifications are made, to conditional marginals and mixing. Applications to mutual information estimation are discussed and evaluated, and the approach is applied to joint neural/behavioral data.   Discussion: Strengths mentioned in the reviews are that the application is (from a neuroscience perspective) interesting, that estimating mutual information is an important problem, and that the paper is very well written. Weaknesses are the limited novelty (from a machine learning perspective), and weak empirical validation.   The authors have responded in detail, and were able to clarify a number of unclear points. Clearly, however, the main criticisms noted above are hard to address in discussion.  Despite the paper being overall clearly written, I agree with reviewers that it is hard to tell from abstract and introduction where the paper is going (even after modifications made by the authors in the course of the discussion); of the fairly long abstract, just about half a sentence relates to where the proposed model differs from previous work.    Recommendation: I recommend rejection. Despite some clearly positive aspects, the two main criticisms voiced by reviewers are serious: Weak validation and minimal  novelty from a machine learning perspective. I agree that the neuroscience application may be interesting, but requires more validation.  If the authors want to pursue this work further, I would suggest to perhaps consider first where to position the paper s focus. Estimation of mutual information is a problem that is both hard and important. Any progress here would be welcome, and simple usefulness could offset any lack of model novelty, but it would have to be carefully and comprehensively evaluated. On the other hand, a focus on neuroscience applications would require more emphasis on, and presumably more space in the paper for, relevant experiments.  
This paper proposes a modification to MCTS in which a sequence of nodes (obtained by following the policy prior) are added to the search tree per simulation, rather than just a single node. This encourages deeper searches that what is typically attained by vanilla MCTS. STS results in slightly improved performance in Sokoban and much larger improvements Google Research Football.  R4 and R1 both liked the simplicity of the idea, with R1 also praising the paper for the thoroughness of its evaluation. I agree that the idea is interesting and worth exploring, and am impressed by the scope of the experiments in the paper as well as the additional ones linked to in the rebuttal. However, R1 and R5 explicitly noted they had many points of confusion, and across the reviews there seemed to be many questions regarding the difference between STS and other variants of MCTS. I also needed to read parts of the paper multiple times to fully understand the approach. If this many experts on planning and MCTS are confused, then I think readers who are less familiar with the area will definitely struggle to understand the main takeaways. While I do think the clarifications and new experiments provided in the rebuttal help, my overall sense is that the paper at this stage is not written clearly enough to be ready for publication at ICLR. I would encourage the authors to try to synthesize their results and organize them more succinctly in future versions of the paper.  One comment about a point of confusion that I had: I noticed the PUCT exploration parameter was set to zero for Sokoban, and one for GRF (with an explanation given that many values were tried, though these values are unspecified). As the exploration parameter is normally considered to be the thing that controls whether MCTS acts more like BFS ($c   \infty$) or DFS ($c   0.0$), I would encourage the authors to more explicitly report which values they tried and to be clearer about the advantage of STS s multi step expansions over low values of the exploration parameter.
This paper addresses a method for unsupervised meta learning where a VAE with Gaussian mixture prior is used and set level inference, taking episode specific dataset as input, is performed to calculate its posterior. In the meta testing phase, semi supervised learning with the learned VAE is used to fast adapt to few show learning. Reviewers are satisfied with the author responses, agreeing that the method is a principled way to tackle unsupervised meta learning.  
This paper presents a systematic breakdown and evaluation of several assumptions and algorithmic choices for pruning algorithms. As covered in the reviews, the evaluation and its conclusion offers a timely contribution to the broader community.  In particular, this paper uncovers the observation that precisely modeling the loss (and hence minimizing the drop in loss after pruning) may not in fact yield improvements in pruning. This is an important observation as the community continues to propose new techniques with the justification that their improved performance results from improved loss modeling.  A significant concern on the part of the reviewers is the limited practical prescription offered by the paper. Specifically, the paper does not propose a new algorithm. It also doesn’t necessarily identify why this interesting phenomenon emerges.  For example, to the latter, it doesn t articulate what features of the network or loss landscape is indicative of this property.  Ultimately, the decision for this paper is very challenging given the reviews. Whether or not a phenomena is interesting is an inherently subjective consideration. Moreover, without a clear technical prescription or path forward that can be evaluated on its merits, the reviews fall into two categories of either 1) those that   by my estimation   felt personally inspired by the work and 2) those that could not intuit the impact of the observation.    A significant complication is that the narrative of the paper includes claims around addressing locality and convergence which, if not read with the understanding that contributions here are simply a synthesis of current work, appear as claims to novelty (when these techniques have no or limited novelty). This is a source of contention in at least one review.  Given this partition, my recommendation is Reject.  For future versions of this paper, I recommend that the authors narrow the claimed contributions to exclusively focus on the final observation that modeling the loss may not be as important as thought.   The work in this paper on developing the ideas around convergence and locality can, instead, be cast as efforts to provide best available baselines for the topline claim.  I believe these changes will eliminate a significant source of distraction, enabling readers (and reviewers) to avoid any attempt to evaluate the novelty of  the locality and convergence narratives, which have indeed been considered in other work in various ways.  An additional step that I highly recommend for this paper to unambiguously clear the bar is to identify with what the performance of pruning does correlate. Appendix C.4 provides an evaluation of two recent gradient preservation methods. Unfortunately, the paper did not present if, instead, the preservation of the gradient correlated with additional performance.   In essence, the paper need not solve the mystery by providing a SoTA algorithm that exploits the right features of the problem for pruning. However, it would be valuable to provide a roadmap for future directions along with an articulation of the challenges down those directions. 
This paper addresses some of the well documented instabilities that can arise from fine tuning BERT on a dataset with few samples. Through a thorough investigation, they highlight various bizarre behaviors that have a negative impact on stability: First, that BERT inexplicably uses an unusual variant of Adam that, in fact, harms behavior; and second, that people tend to undertrain BERT on some downstream tasks. Separately, they find that reinitializing some of the final layers in BERT can be helpful. Since fine tuning BERT has become such a common way to attack NLP problems, these practical recommendations will be quite welcome to the community. These findings address issues raised by recent work, so the paper is timely and relevant. The paper has through empirical analysis and is clear to read. There is a concurrent ICLR submission with similar findings, and this paper stands on its own. Reviewers all agreed that this paper should be published.
This paper presents a novel approach to grammar induction. Like older work by Klein and Manning, the paper finds benefit in jointly inducing both constituency and dependency structure. However, unlike most approaches to grammar induction, the model is not generative   rather, it is a transformer based architecture that is trained to optimize a masked language modeling objective. The resulting parses appear to beat non trivial baselines, but direct comparisons with several relevant state of the art systems are not drawn. Reviewers overall found the approach interesting and novel. However, nearly all reviewers raised serious concerns about experimental comparisons with related work and brought up several missing state of the art baselines that, like the proposed system, do not require gold POS. Reviewers also pointed out issues with clarity in several sections. In rebuttal, authors provide a substantial update to the original draft. So substantial that all reviewers mentioned in discussion that the new draft would effectively require an entirely new review. While I applaud authors for the substantial revisions, and while ICLR guidelines do not explicitly limit the amount change to a draft allowed in rebuttal, in this case the revisions are sufficiently drastic that I agree with reviewers that a new review process is required. Thus, I recommend rejection but strongly encourage authors to resubmit. 
This paper improves calibration of neural networks by investing its connection to adversarial robustness. Two reviewers suggested acceptance, and two did rejection. As the authors and some reviewers highlighted, AC also agreed that the correlation between adversarial robustness and calibration is interesting to explore. However, as R1 pointed out, AC also thinks that the experimental results are not strong enough to meet the high standard of ICLR, e.g., Mixup often outperforms the proposed method (without further post processing) and the proposed method does not outperform the deep ensemble (although deep ensemble is expensive and both method can be combined). Due to this, AC doubts whether adversarial robustness is indeed the best way to improve calibration (it can be useful though). Hence, AC recommends rejection.
The paper advocates empowerment for stabilising dynamical systems, the dynamics of which is estimated with Gaussian channels. Baseline comparisons have improved and that makes the experimental section good. While initial versions of the paper were problematic, all reviewer issues have been addressed and acceptance is almost unanimous.
This paper is certainly on the way to be a solid contribution: it s an interesting research question, and we need more understanding papers (rather than yet another algorithmic trick paper).  The reviewers thought the paper was not yet ready. The reviewers suggested: (1) more motivation of why the proposed metrics were of interest, (2) clearer discussion and evidence of how the analysis better articulates the performance of PER, (3) missing empirical details like methodology for setting hyper parameters, why these 9 Atari games, undefined errorbars, unspecified number of runs, and (4) conclusions not supported by evidence in Atari: with missing experiment details, likely too few runs, and overlapping errorbars in most games few scientific conclusions can be drawn.  The work might be strengthen by developing the first part of the paper (and focussing on the reviewer s suggestions) and deemphasizing the novel algorithmic contribution part.
The paper shows empirically that training unstructured sparse networks from random initialization performs poorly as sparse NNs have poor gradient flow at initialization. Besides, the authors argue that sparse NNs have poor gradient flow during training. They show that DST based methods achieving the best generalization have improved gradient flow. Moreover, they find the LTs do not improve gradient flow, rather their success lies in re learning the pruning solution they are derived from. I read the paper and the reviewers discussed the rebuttal. Although all the reviewers found the rebuttal helpful and they all agree that the paper is decently well written and has some clear value, the majority believes that further observations are required for making the paper and its hypothesis convincing. There are also some recent related work on initialization of pruned networks, e.g. by rescaling their weights at initialization. I believe, adding the discussion of such related techniques and making the connection to existing work will greatly strengthen the paper and provides more evidence to support its claims.   
The paper proposes learning of 3D object representation from images. The pretraining used assumes it can generate implicit 3D models for the objects, and then objects are detected in multi object scenes without further supervision. Reviewers raised concerns regarding experiments being conducted only on synthetic data. Authors are encouraged to try out their approach on real data, to demonstrate the benefits of their solution.
The paper proposed an active search algorithm for efficiently identifying rare concepts among heavily imbalanced datasets. Reviewers find the paper very well motivated and addressing an important real world challenge in active learning. All reviewers appreciate the extensive demonstration of the effectiveness of the proposed algorithm on real world tasks, in particular in industrial settings where the scale of problems goes far beyond common academic datasets.  In the meantime, there are shared concerns among several reviewers in the technical depth of the proposed algorithm. Although the authors provided intuitive explanations of the nearest neighbor based approach, the results reported are restricted mostly to several final performance metrics on the search performance. As a purely empirical work, the paper would benefit from more fine grained experimental analyses and ablation studies (e.g., by breaking down to analyses at intermediate levels). 
The reviewers agree that the EM perspective of Federated Learning is novel and interesting. However, a common criticism is that the connection made is rather shallow and not sufficiently developed. There look to be quite interesting potentials of the proposed framework and the specific FedSparse method, but I agree with the reviewers that both aspects need further development before they are in publishable form.
This paper proposed a new experience replay approach, applicable to deep RL methods. Two reviewers suggested acceptance and two did rejection. The first negative reviewer R1 raised a concern on continuous vs. discrete issue, but AC thinks that the authors  response is not fully convincing enough.  The second negative reviewer R2 pointed out that the reported performance of SAC is poor compared to the existing implementation (although authors claim a different set of hyperparameters is used), which AC thinks is a critical weakness to judge the value of the experiments. Two other positive reviewers (even R4) shows mixed opinions. Overall, AC thinks this is a borderline paper, a bit toward rejection.
Pros:   The authors propose a novel method to perform MCMC in the condition where there is a distribution over models describing the data, rather than just a distribution over the parameters of a single consistent model (ie, in the theme of reversible jump MCMC). Sampling from the posterior of mixtures of parametric models is something current MCMC algorithms are very bad at, so this addresses an important need.   Reviewers believed the proposed technique was novel and technically correct.   The paper appears to build a bridge between the fields of system identification and Bayesian sampling techniques  Cons:   Major concerns were raised about lack of clarity     From a *lightweight* read, I also had difficulty understanding what the paper was proposing, or even the precise problem it was tackling   Experimental validation was limited, and missing baselines   During discussion, the paper had no strong advocate
I agree with the reviewers that said that this paper has valuable insights. However, all reviewers ultimately recommended rejection. I think the main reason was that the reviewers did not feel these insights don t accumulate together to a message that would justify a paper. I hope the authors can address these concerns and resubmit. There were additional concerns, like the fact a very simplistic toy model is being used, but I agree with the authors that it makes sense to first explore such phenomena in the simplest model that produces them.
This paper provides a novel method for calibrating probabilistic regression models without requiring a held out calibration set. The technical advances are interesting, and the experimental results look promising. The authors made a number of improvements based on the reviews, and the authors have done a good job with reproducibility of the experiments. Nevertheless, it ultimately does not meet the bar for acceptance. In revising the article for a future submission, I would encourage the authors to emphasize the observation in Section 4.3 that overall we want models that are both "well calibrated and sharp". This helps motivate the method and will help the reader interpret the results. 
This work explores the distillation of language models using MixUp for data augmentation. Distillation with MixUp seems to be novel in the narrow context of distilling language models, although it has been used before in different contexts as the reviewers point out. The results of the experimental validation are encouraging, and the application is valuable and of wide interest to the ICLR audience. I therefore recommend accepting this paper for a poster presentation.
This paper analyzes the expressive power of NTK corresponding to deep neural network. It is shown that the depth hardly affects the behavior of the spectrum of the corresponding integral operator, which indicates that depth separation does not occur as long as NTK is considered. The analysis is novel and gives a significant insight to the NTK research literature. The theoretical framework considered in this paper is considerably broad and potentially can be applied to several types of activation functions (while only ReLU is analyzed as a concrete example in the paper). Moreover, some numerical experiments are conducted that support the validity of the theoretical analysis.  All reviewers are positive on this paper. I agree with their evaluations. For these reasons, I think this paper is worth acceptance.
The authors propose an approach for pre training that involves "taking notes on the fly" for rare words. The paper stirred a lively discussion on the reasons for the reported results, which the authors followed up with new experiments and findings that convinced the reviewers that indeed their approach is valid and interesting. Thus, I am recommending acceptance.
The paper focuses on the problem of high quality video generation. It approaches the problem by extending VQ VAE to videos, where a GPT is used to model the low dimensional representation of the VAE. As agreed upon by the authors and the reviewers, the proposed method is simple and produces interesting results.   Based on all the reviews and the subsequent discussions, it seems that the reviewers  comments were mostly not addressed and they maintain their stance with regards to the paper s technical novelty, empirical justification of the paper s claims (specifically the claim on computational efficiency), and the rigorous comparison with prior art. The authors themselves make it clear that technical novelty was not the main driving force in this paper. However, in this case, it would be expected that the major claims of the paper be very clearly justified (especially with experiments and analysis) and comparison with other methods be more thorough. It seems that these latter two points remain in the latest revision of this paper. Since the paper shows promise, the authors are recommended to take the reviewers  comments and suggestions into consideration to produce a stronger and more thorough submission in the future.
The paper present an approach for defending for, and search for,  modularity  in neural networks, as a step to better interpretations of their functional structure. This is an interesting, and highly original approach, as recognised by the reviewers. However,  there was also some discussion about what exactly can be learned from the derived clusters/modules, and if and how they will lead to a better understanding of neural networks, or provide concrete ways of improving them.  While the authors addressed some issues during the review process, and provided additional results, the consensus (of all three reviewers) was finally that the paper did not reach the quality standards required by ICLR. I share this view  the paper provides a refreshing perspective, but I still am not convinced that I see a clear, compelling  use case  for their approach.  
The paper investigates the order of Transformer modules and its effect on performance. The proposed approach IOT, consists of several pre defined encoder and decoders with different orderings (and weight sharing), along with a light predictor which is trained to choose the best configuration per instance.    Most reviewers found the general idea of predicting the order of Transformer modules at instance level quite intriguing. Other strengths included wide range of evaluation tasks, major empirical gains, and novelty.     R1 and R4 raise valid and important concerns on validity of results when the model size and training time are controlled.   However, after carefully reading the author response and the revised paper, I feel that this issue is resolved.   The authors provide comparison with larger models, ensemble models, and models trained longer, and in all cases the gains are still obvious.    Overall, I feel that the general idea behind this paper is very exciting and could inspire more research in this direction. Therefore I recommend accept. 
This work describes a series of strategies for optimizing the training speed of word embeddings (as in word2vec and fasttext).  All reviewers appreciate the convincing empirical results, which are without a doubt impressive.  Reviewers also mostly agree that speeding up embedding training is important, and there is no doubt that this type of paper is appropriate for ICLR (as clearly highlighted in the CfP.)  However, the specific optimization strategies deployed and described here are deemed not to bring novel insight, useful in itself to the community, beyond the software contribution described. The paper seems to mostly serve as documentation of the implementation, limiting its value and impact to further research. The pedagogic value is also limited, as the paper tackles multiple different, eclectic optimizations, a narrative strategy that does not leave room to describe a single one more generally, helping the community find other places to apply it. All in all this leads to a borderline negative assessment, and I cannot recommend acceptance.
Dear Authors,  Thank you very much for your very detailed feedback and also updating the manuscript in the rebuttal phase. Your effort has highly contributed to clarifying some of the concerns raised by the reviewers and improving our understanding of your work.   On the other hand, we still think that the current work has rather limited novelty, and motivation and theoretical justification need to be further enhanced to be accepted for ICLR.  For these reasons, I suggest rejection of this paper, in comparison with many other strong submissions. The reviewers added further comments after receiving your feedback. I hope their comments are useful for improving your work for future publication.
The reviewers recognized that the proposed method is interesting and seems to be useful in some cases, and the authors provided sufficient empirical results to support their claim. In addition, some comments have already been clarified. However, some reviewers still concerned that the proposed defence method will be defeated under some conditions, and still have the major concern regarding the issue of adopting some attack strategies to find adversarial examples near the safe spots, even though the authors clarified some critical points of the proposed method.  These drawbacks led to the decision to not accept. However, this paper has some merit and can be made into a stronger contribution in the future. 
The reviewers noted that this is an important, interesting but difficult topic. They appreciated that the authors clarified their assumptions in the theorem statements. Nevertheless, they recommend the authors to detail in depth when the method work better than the method where only the covariates are adjusted. They still think that the paper would require major modifications to be considered for publication hence the decision is rejection the paper.  
The paper proposes a deep learning approach to blind image denoising based on deep unrolling. In particular, the proposed network is derived from convolutional sparse coding algorithms, which are unrolled, untied across layers and learned from data. The paper proposes a frequency domain regularization scheme in which the filters consist of a single analytically defined low pass filter and a large collection of filters which are constrained to reside in the mid to high frequency ranges. It also proposes to tie the thresholds in the soft thresholding stages of the learned network to estimates of the noise variance, making the proposed scheme more robust to variations in the noise level.   Pros and Cons:  [+] Having a single low pass dictionary atom reduces redundancy (and potentially coherence) in the learned dictionary. This type of regularization may also reduce the time/data required to learn.   [+/ ] Using noise estimators and a noise adaptive threshold renders the model more robust to variations in the noise level. This is important, since in most denoising applications the noise level is not known a priori. As the reviewers note, the idea of tuning thresholds in an unrolled sparse coding method based on the noise level is not a novelty of the paper; the novelty here is coupling this with a wavelet based estimate of the noise level.   [ ] All three reviewers raise concerns regarding the novelty of the work compared to existing convolutional sparse coding based neural networks. The structure of the network is similar; the main difference is the frequency restriction for learned atoms, which is enforced by prefiltering the learned atoms with a high pass filter.   [ ] The paper is not entirely clear in its motivation and argumentation. Reducing the coherence of the learned dictionary makes sense from the perspective of certain worst case results from sparse approximation. However, the coherence is a worst case quantity; moreover, certain approaches to coherence control (e.g., using large stride) control coherence at the expense of the expressiveness of the dictionary, and hence may not actually improve its ability to provide sparse reconstructions of natural signals. The proposed frequency domain regularization is a sensible approach to controlling coherence, since low frequency atoms will tend to be highly coherent, but would benefit from a crisper analytical motivation.   [ ] Reviewers found the experiments lacking in some regards. In particular, the paper only evaluates its proposals on synthetic experiments with Gaussian noise. While this is in line with some previous work on deep learning based denoising, more extensive and realistic experiments would have bolstered the paper s argument.   Overall, the paper makes a sensible proposal regarding the adaptivity to unknown noise levels, and introduces a potentially useful frequency domain restriction on the learned filters in a CSC network. However, the reviewers did not find that the paper made a clear argument for the significance of these proposals, and raised other concerns regarding the clarity and experiments. The consensus of the reviewers is to recommend rejection. 
All four reviewers expressed significant concerns on this submission during review. None of them is willing to change their evaluations and supports this work during discussions. Thus a reject is recommended.
I think we did learn something new from this paper, and I think the reviewers all seem to agree with this. The observation you make about the objective seems correct and interesting (though reviewers and ACs do sometime miss errors), but I have the following complaints that keep me from recommending acceptance:  1. The theory seems right, but in practice, all sorts of GANs with all sorts of objective functions experience "mode collapse",  so it doesn t seem like the issue you point out w/ the NS GAN objective can be the whole story.  However, we generally don t ask of a paper that it tells the whole story all in one go...  2. I do think the experiments are somewhat poorly done (compared to those for say the median paper about GANs that gets accepted to one of these conferences). Moreover, many people have made similar experimental claims to the ones that are in this paper that haven t held up on more complicated data sets, so I tend to apply more scrutiny to such claims when they re only evaluated on smaller tasks.  3. There have, as R3 points out, been a huge number of papers proposing tricks for training GANs, and some of them work really well.  What I m missing from this paper is an exploration of the relationship between your observation and those (mostly ad hoc) tricks.  Does your observation explain why those tricks are necessary?  Does it explain why some existing trick works as well as it does? If your observation is totally orthogonal to existing tricks, can you get much better performance on a challenging data set by using it? I don t feel like I got satisfactory answers to those questions.  All this being said, the paper was borderline, and I think if you dealt with some of the complaints above you would have a pretty good shot of getting a revised version accepted at another major machine learning conference.
In this paper, the authors work on improving the interpretability of CNNs following distillation methods. The paper is written in such a convoluted way and with many changes in the notation that makes it hard to understand what they are proposing and what for. This is a major impediment for the paper going forward. But also, the reviewers point out some clear aspects of the paper and the interpretability that it provides for CNNs, for example, the entropy analysis and its variability or referring to the discrimination part of the CNN or mentioning an explainable alternative to CNNs, while still relying on the convolutional filters. The authors need to dedicate more time to explain this paper carefully before it can be properly reviewed and published at any major conference.
The paper proposes an algorithm for training flow models by minimizing the KL divergence in the latent space Z. The paper addresses an important problem in training flow models. However, some major concerns remain after the discussion among the reviewers. The scale of the experiments and the scalability of the approach appear limited in the current version of the paper. Moreover, the applicability of the current theoretical analysis to general distributions is quite limited. 
The authors study co ordination in multi agent systems. Specifically they propose a scheme where agents model future trajectories through the environment dynamics and other agents  actions, they then use this to form a plan which forms the agents  intention which is then communicated to the other agents.  The major concerns raised by the reviewers were around novelty, lack of ablations and significance of results as improvements were modest. During the rebuttal, the authors have extended their work with ablations and have conducted a statistical test. While it is true the current results present a small improvement, i think this is an interesting contribution in the field of emergent communication
Three knowledgeable referees rate this paper ok but not good enough or borderline positive (4,4,6), and one fairly confident referee rates it borderline positive 6. The referees discussed the authors  responses and, while they considered the idea and some of the theory good, they remain concerned, in particular about the experimental part and generalization discussion. The scores remained unchanged after the discussion. Hence I must reject the article. 
This paper presents a new idea and approach for self supervised video representation learning.  The reviewers  opinions diverge. R1 suggests that the paper is (marginally) above the threshold. R2 supports the paper, saying that he/she likes the idea behind the paper. R3 explicitly mentioned that he/she would like to provide a borderline rating (but cannot due to the system). R4 is not in favor of the paper, even after the rebuttal.  The AC’s opinion is more aligned with R3 and R4, who are the more senior reviewers among the four. There are two main concerns with the paper: technical contribution and experimental comparison. In terms of the contribution, both the reviewers find that the paper is lacking: "What I m not entirely sure about is how much this method manages to push the boundary of SSL." (by R3). "Overall, the paper presents yet another method to design the pretext task for SSVRL. But my major concern is it lacks enough insights for inspiring future research for this topic." (by R4). The authors argue the difficulties of being in academia in doing this research with limited computation resources and argue that the reviewers should focus more on novelty and contribution, but even after the rebuttal, R4 is not convinced and R3 is still mildly concerned whether the proposed approach really brings something new to the field as the paper fails to show "clear superiority over existing methods".  In addition, as pointed out by the reviewers, there are several state of the art self supervised video representation learning works that the paper misses to cite, or compare against. In addition to Pace and SpeedNet R3 mentioned, below are approaches reporting results on UCF101 and HMDB with the standard self supervised classification task setting (Table 1):  AVTS 89.0, 61.6 CVRL 92.1, 65.4 ELo 93.8, 67.4 XDC 94.2, 67.4 GDT 95.2, 72.8  We note that all these results are much superior to the best results reported by the proposed approach, 79.5 and 50.9 on UCF101 and HMDB. The authors mention in the rebuttal that these superior approaches not included in the paper use stronger backbones (and are thus omitted), but we believe a more academically proper attitude is to include all these numbers and explicitly describe why the proposed approach is not performing better, instead of completely omitting their results.  The AC also questions whether the R2D3D 34 backbone used in this paper really is computationally lighter compared to the backbones used in previous approaches like R(2+1)D 18, which alternates 2D residual modules and 2+1D residual modules (using much fewer parameters and compute than 3D modules) and also has fewer layers. XDC using R(2+1)D 18 backbone reports 86.8/52.6 (UCF/HMDB) accuracies with Kinetics 400 unlabeled data. AVTS also reports 84.1/52.5 (UCF/HMDB) accuracies using MC3 18 backbone. Similarly, GDT uses R(2+1)D 18 backbone and reports 89.3/60.0 (UCF/HMDB) accuracies using unlabeled Kinetics 400.  Even MemDPC reports 86.1/54.5 using R 2D3D backbone when optical flow feature is added. All these are far superior to the results being reported in the paper.  Overall, we view the experimental section of this paper as incomplete, and we cannot convince ourselves that the paper reaches the quality of ICLR.   [AVTS] Korbar, B., Tran, D., Torresani, L.: Cooperative learning of audio and video models from self supervised synchronization. In: NeurIPS (2018)  [ELo] A. Piergiovanni, A. Angelova, and M. S. Ryoo. Evolving losses for unsupervised video representation learning. In Proc. CVPR, 2020  [XDC] H. Alwassel, D. Mahajan, L. Torresani, B. Ghanem, and D. Tran. Self supervised learning by cross modal audio video clustering. arXiv preprint arXiv:1911.12667, 2019  [GDT] M. Patrick, Y. M. Asano, R. Fong, J. F. Henriques, G. Zweig, and A. Vedaldi. Multi modal self supervision from generalized data transformations. arXiv preprint arXiv:2003.04298, 2020  [CVRL] R. Qian, T. Meng, B. Gong, M. H. Yang, H. Wang, S. Belongie, and Y. Cui. Spatiotemporal contrastive video representation learning. arXiv preprint arXiv:2008.03800, 2020
In this paper, the authors propose a simple yet interesting new graph sampling method for graph neural networks.  It addresses the two main problems that GNNs have not previously been extended to deep GNNs: expressivity and computational cost.  Through experiments, the authors show the effectiveness of the proposed algorithm.   Overall, the proposed approach is interesting.  However, the reviewers were still not convinced by the response, and the paper is still below the acceptance threshold.  I encourage the authors to revise the paper based on the reviewer s comments and resubmit it to a future venue. 
This paper analyzes BatchNorm through a series of experiments and shows that BatchNorm improves training and generalization by preventing excessive growth of the activations of the previous to the last layer. The paper received mixed reviews. Two reviewers find that the paper brings clear and important new contributions to the understanding of BatchNorm, and appreciate the experimental evaluation, though some suggestions for improving the experiments were also provided. The other reviewer appreciated the contribution of regularizing against explosive growth in the previous to the last layer, but did not find the results to be very convincing as they can limit the learning rate. The authors responded that, contrary to the reviewer s claim, the learning rate is not limited. Overall, while there are some aspects that need improvement, and the authors should address those in the final version, this is a solid paper that brings an interesting contribution to ICLR.
This work is attempting to develop a new way to train models that are robust to (l_p bounded) adversarial perturbations and to do so in a way that departs from the tools successfully used for this purpose in the past. This is a worthwhile aspiration, however, as pointed out in the comments/reviews, there are significant problems with the methodology of evaluating the proposed approach (and some well founded skepticism that this approach is indeed successful). As such, this paper cannot be accepted in its current form.  
Based on the paper, reviewers  comments and discussions, and the responses, the meta reviewer would like to suggest the authors to improve the paper and resubmit.
This is a clear reject. None of the reviewers supports publication of this work. The concerns of the reviewers are largely valid.
The main idea of the paper is to  use image data to guide radar data acquisition by focusing on the blocks where the object has appeared. Four reviewers have relatively consistent rating: 3 of them rated “Ok but not good enough   rejection”, while 1 rated “clear rejection”. The main concerns include ad hoc choices of algorithm design, lack of algorithm novelty, not adequate experiments in illustrating the performance, etc. During the rebuttal, the authors made efforts to response to all reviewers’ comments. However, the major concerns remain, and the rating were not changed. While the motivation is clear and the work has merits, the ACs agree with the reviewers’ concerns and this paper can not be accepted at its current state. 
This paper proposes a new criterion for neural architecture search that does not require the expensive step of training the model. The reviewers found the proposed approach of relying on gradient statistics promising. However, the reviewers found that the clarity of the paper needs to be improved and that the empirical evidence is too limited to support some of the claims.
The paper presents an approach for weakly supervised pre training for videos using textual information provided with web videos on Youtube and Instagram.  ## Strength * The work shows strong results with relative small dataset and computational resources compared to other work in the area of self/weakly supervised learning for videos. * Interesting ablations  ## Main Concerns * The authors don t discuss and compare to the weakly supervised work [Ghadiyaram et al. CVPR 19] adequately. Furthermore, the authors characterize the work incorrectly in their author response as detailed by R2. I agree to R2 here and like to highlight the concern is not that the method of [Ghadiyaram et al. CVPR 19] being similar to this work but the level/type of supervision. * Limited novelty over prior work.  ## Further Concerns * Some unclarities * The authors did not provide an updated revision of the pdf  Overall the paper received reject and borderline scores after author response and discussion (With the strongest score 6 from R1) due to the concerns concerns listed above apart from the ability to work with small number of data. I think the missing comparison to  [Ghadiyaram et al. CVPR 19] which operates in a similar setting weights strongly and I recommend reject.
The paper proposes a Bayesian neural network model for tensor factorization, with particular focus on streaming data. The key contribution is the streaming posterior inference of the deep TF models.  The combinations of online tensor factorization, Bayesian NN with sparsity priors, posterior inference is new and interesting.  However, there are many approximation steps, and the quality of the approximation and convergence of algorithm are not well justified.   
The paper investigates the tendency of image recognition models to depend on image backgrounds, and propose a suite of datasets to study this phenomenon.  All the reviewers agree that the paper investigates an important problem, is well written and contains several interesting insights that should be of interest to the community. I recommend acceptance. 
This paper proposes a method for  interpretable  graph neural networks. The idea is intuitively well motivated: after training the model, discard spurious edges that are not critical to making predictions in the graph, and only retain salient edges. Experiments on synthetic and real datasets show that the proposed method is effective at dropping only the edges that are not useful for the task at hand;  while leading to  only small performance degradation.  The paper is well written. Overall, the paper brings together prior ideas in a useful way, and is well executed.
The paper initially received mixed ratings, with one reviewer strongly supporting the paper given that the idea of combining unrolled algorithms and NAS is new and interesting, and one reviewer not convinced by the significance of the results. His/her main concern was the use of synthetic data only, which is not realistic. This was a legitimate concern as the performance of sparse estimation algorithms can change drastically when there is correlation in the design matrix. See for instance, the benchmarks in  F. Bach, R. Jenatton, J. Mairal and G. Obozinski. Optimization with Sparsity Inducing Penalties.   The rebuttal addresses this concern in a satisfactory manner and the area chair is happy to recommend an accept.
The paper proposes hybrid discriminative + generative training of energy based models (HDGE) building on JEM. By connecting contrastive loss functions to generative loss, HDGE proposes an alternative loss function that reduces computational cost of training EBMs.  The reviewers agree that this is an interesting idea and that the empirical results look promising. However, multiple reviewers raised concerns that the theoretical justification was incomplete and felt that some of the claims about the equivalence between the two, as well as some of the practical approximations introduced, need more justification.   I encourage the authors to revise the paper and resubmit to a different venue. 
We also had some discussions about the paper that are not visible to the authors. To summarize: the reviewers appreciated the efforts the authors put into the replies and updates. While those clarifies quite a few points, the paper unfortunately is still not publishable in its current form at ICLR.  Overall the paper tackles a very relevant and important question, proposing a tool that could be extremely useful for research on RL. On the downside the paper is mainly descriptive, outlining WHAT the tool can do. Multiple reviewers pointed out that deeper, new insights are missing, e.g., WHY certain features were included and whether the tool actually is helpful for practitioners. A user study has been commenced, which is an excellent step in this direction.
All reviewers agree that the contributions of this paper are not significant, and the paper does not compare well with many of the existing works. Authors did not respond.
This paper presents a theoretical characterization of the impact of noise in causal and non causal features on model generalization, through the lens of counterfactual data augmentations with toy data and models, and demonstrates that the predictions of this characterization bear out in several experiments on language counterfactually augmented language data with substantial models.  Pros:   Spurious features and their relationship out of domain generalization are a practically issue in modern applied ML, and this work helps to coalesce our understanding of this area.   Fairly extensive experimental work.  Cons:   Reviewers didn t find the connection between the theoretical analysis, which focused on a simplified setting, and the experimental work, to be especially clear. In particular, reviewers worried that the predictions that were tested experimentally were fairly intuitive ones that could reasonably be derived from a number of starting assumptions, so it s not clear that they offer strong support for the specific account given here.   Reviewers found the presentation, especially of the empirical work, confusing.  Overall, this paper makes a legitimate and sound contribution to an important research area. That contribution is small, and somewhat easy to misinterpret, but after some discussion, reviewers agreed that the paper should still be a worthwhile net positive for the field.  
This paper proposes to learn an ensemble of weights given a set of base weights from some point late in normal training. The authors apply this approach to a number of configurations and find modest performance improvements for normal test settings and larger improvements for out of distribution settings. While reviewers had some concerns about the size of the improvement relative to baselines, all reviewers agreed that the proposed method is interesting and will likely impact future work, especially given the new experiments provided by the authors. I recommend that the paper be accepted. 
The reviewers have not supported the acceptance of this paper where the key weakness is that the study of the proposal neglect effect is not sufficient (see the reviews for the details). I agree with the assessment of the reviewers and recommend rejecting the paper in its current form.
The paper seeks to understand how training over parametrized models (e.g., those based on neural networks) to zero training accuracy even when the test error is small (i.e., benign overfitting) can introduce vulnerabilities in the form of adversarial examples and how to remedy the situation. The paper implicates label noise as one of the causes of adversarial robustness, and suboptimal representations learned as part of the training as another. The claims are supported both theoretically and empirically. A good paper overall, accept! 
This work develops an approach to embed random graphs (some even with dependent edges, hence going beyond classical models such as Erdos Renyi G(n,p)) using GNNs, and uses these to develop approximation algorithms for solving NP hard scheduling problems, which typically involve some notion of minimizing weighted completion time (or equivalently, the reward incentivizes early completion, where the age of a job is a linear function of time). This is then used to schedule multiple identical robots  to solve a given set of spatially distributed tasks. The problems considered Multi Robot Reward Collection (MRRC) model vehicle routing, rideshare etc., and are well motivated.   This paper takes as motivation earlier work on “structure2vec” by Dai et al. (2016) that uses GNNs to (approximately) solve other NP hard graph problems: specifically, the random structure2vec developed here is used for an RL approach that learns near optimal solutions for the MRRC problems considered.   While the paper’s contributions were appreciated in general, its clarity, the fact that the (1 – 1/e) bound of Theorem 2 follows from classical work of Nemhauser et al. (1978), and the fact that real life examples were not considered, were considered weaknesses. The authors are encouraged to work on these aspects of the paper.  
This paper studies how two layer neural networks can learn DNFs. The paper provides some theoretical analysis together with empirical evidence.  The direction of analyzing how neural networks learn certain concept classes is definitely extremely important, and the authors do make some progress towards this direction. However, there are some major concerns about the paper:   In the main result, the authors seem to only able to prove that the learning process converges with exponentially many neurons (exponential in the input dimension, see 6.1 setup). With this many neurons, it is unclear whether the result is directly covered by existing works such as:    (a). "Learning and generalization in overparameterized neural networks, going beyond two layers". (b). "Fine grained analysis of optimization and generalization for overparameterized two layer neural networks"   These two works provide efficient optimization and generalization bounds w.r.t the complexity of the function. However, these works are still in the NTK regime. It would be nice if the authors can distinguish the current technique from NTKs by providing some theoretical guarantee that their main result is indeed more efficient than kernels (as they argue in the intro), the author can refer to:    (a) "What can ResNet efficiently learn, going beyond kernels" .  (b) "When Do Neural Networks Outperform Kernel Methods?"   With the current form of the draft, it is unclear how the result is better than existing approaches. The authors should address that in the next version of the paper.   Missing reference of NTKs: "A convergence theory for deep learning via over parameterization"   
This work proposes a method to discover neighboring local optima around an existing one. Reviewers all found the idea interesting but argued that the paper needed more work. In particular, some of the claims are too informal or not sufficiently supported and the reviewers found the key section were difficult to follow. The paper should be resubmitted after improving the presentation of the results.
This paper first investigates the behavior (e.g., catastrophic overfitting) of fast adversarial training (FastAdv) through experiments. It finds that the key to its success is the ability to recover from overfitting to weak attacks. Then, it presents a simple fix (FastAdv+) that incorporates PGD adversarial training when catastrophic overfitting is observed. The resulting method is shown to be able to train for a large number of epochs. It also presents a version (FastAdvW) that use the improved fast adversarial training as a warmup of PDG adversarial training, similar as in previous work. Overall, the analysis is useful and the ideas are valid. The empirical results also show promise. However, the main weakness of such empirical analysis is that it may be sensitive to the settings (e.g., # of epochs, splitting of datasets, …). The authors’ rebuttal also reflected such potential concerns.
This paper proposes "HyperDynamics" a framework that takes into account the history of an agents recent interactions with the environment to predict physical parameters such as mass and friction. These parameters are fed into a forward dynamics model, represented as a neural network, that is used for control.  Pros:   addresses an important problem (adapting dynamics models to "new" environments) and provides strong baselines   well written and authors have improved clarity even further based on reviewers comments  Cons:   I agree with the reviewer that it is currently unclear how well this will transfer to the real world   The idea of predicting physical parameters from a history of environment interactions is not not novel in itself (although the proposed framework is, as far as I know). The authors should include related work along the lines of (1) (this is just one paper that comes to mind, others exist)  (1) Preparing for the Unknown: Learning a Universal Policy with Online System Identification
All three reviewers recommend rejection, based on multiple (mostly shared) concerns. While the authors address the concerns in their rebuttal, the unanimously negative scores remain. I don t see basis to accept the paper.
This paper proposed a new family of losses for GANs and showed that this family is quite general and encompasses a number of existing losses as well as some new loss functions. The paper compared experimentally the existing losses and the new proposed losses. But the benefit of this family is not clear theoretically, and this work did not also provide the very helpful insights for the practical application of GANs. 
This paper focuses on the problem of performing imitation learning from trajectory level data that includes optimal as well as suboptimal demonstrations.  The authors wish to avoid the requirement of a separate filtering process that would throw away the bad trajectories.  The authors propose a clever innovation that allows for leveraging the policy that is itself being learned to reweight the samples for a next round of weighted behavioral cloning.  The paper is also somewhat theoretically rigorous and provides insight into the problem.   The reviewers pointed out some initial issues related to clarity and the authors did a good job of addressing reviewer concerns.  Ultimately all reviewers agreed that the core innovation of the paper was interesting and empirically worked reasonably well.    One older line of work that I think is quite relevant, but which is not discussed, is the empirically observed "clean up effect", described by Michie and colleagues in the 90s (e.g. "Learning to fly" Sammut et al 1992).  This clean up effect is intuitive and reportedly achieved for free in settings where the learning objective is mode seeking and the dataset is large, insofar as the mean value of the resulting policy *should* produce actions that corresponds to the average action produced by demonstrators in the same situation.  I think it would be worth discussing how the analysis of this paper relates to this empirical phenomenon. In particular, it would be worth clarifying in what regimes the suboptimality of training from a dataset with noisy examples arises and how likely this is to effect the mean value of the learned policy (for context, it is fairly common in practice to evaluate the student policy in BC settings by only using the mean action value; perhaps this point was present in the paper, and I missed it). From a certain perspective, the innovation of this paper is to accentuate the clean up effect.  As noted by a reviewer, and subsequently incorporated into the paper, the actual algorithm has some similarities to versions of recent "offline RL" algorithms (though of course it does not leverage rewards).  In particular, the motif of performing a weighted regression could perhaps be a bit more thoroughly contextualized by connecting it to other weighting factors (e.g. see Critic Regularized Regression).  That said, I leave this entirely to the discretion of the authors.  The final scores were 8, 7, & 6.  I see this as a strong paper and will endorse it for a spotlight.
This paper deals with a particular model structure selection problem: inferring the order of a given sequence of latent variables. This problem is closely related to the matching problem that involves discrete optimization. The authors propose to cast the problem into a one step Markov Decision problem and optimize it using the policy gradient.  The proposal here is using Variational Order Inference (VOI) using and using a Gumbel Sinkhorn distribution to construct a proposal over approximate permutations. The approach is mathematically sound and novel.  Empirical results on image caption and code generation show promising results: method outperforms the previous Transformer InDIGO and other baselines (Random, L2R, Common, Rare). This paper further analyzes the learned orders globally and locally, and conducts ablations.  The reviewers were overall very enthusiastic.  
The reviewers acknowledge that the paper has some promising experiments. However, they think that the theoretical contributions are not rigorous, specifically the assumption in Theorem 2. It is true that the main part of the proof relies on this assumption. The main question is whether this assumption holds or not. The way that the authors provide some numerical verification is not convincing and does not necessarily help in this situation:   1) MNIST data set with LeNet is definitely not enough to represent all situations.  2) This assumption depends on the algorithm so the parameter choices are also very important.   Some reviewers and I agree that this assumption may hold in some scenarios, but assuming it (without proving it) would significantly reduce the contribution of the paper.   The following is the suggestion to improve the paper. Since this assumption is not standard and hard to verify, the authors should verify more experiments with various data sets and network architectures with difference choices of the algorithm parameters to have some sense whether this assumption may be true or not. Next, please try to show that this assumption holds or come up with different analysis with more reasonable assumptions.   The authors should consider to improve the theory to strengthen the paper and resubmit this paper in the future venues.  
The paper proposes a new defense against adversarial attacks on graphs using a reweighting scheme based on Ricci flow. Reviewers highlighted that the paper introduces interesting ideas and that the use of Ricci curvature/flow is a novel and promising contribution. Reviewers also recognized that the paper has significantly improved after rebuttal and clarified some aspects of their initial reviews.  However, there exist still concerns around the current version of the manuscript. In particular, important aspects of the method and algorithm, as well as some design choices are currently unclear. This includes evaluating and discussing robustness, training method, and practicality/improvements in real world scenarios. I agree with the majority of the reviewers that the current version requires an additional revision to iron out the aforementioned issues. However, I also agree with the reviewers that the overall idea is promising and I d encourage the authors to revise and resubmit their work with considering the feedback from this round of reviews.
The decision for this paper is quite difficult: the methodological ideas are interesting, such as learning the generators directly, but the experimental results are relatively weak. Perhaps learning the generators is too unconstrained. Moreover, the proposed  L conv  builds heavily on prior work such as the  LieConv  model of Finzi et. al, which is not made very transparent in the current narrative. And LieConv does provide better performance than L Conv   this should also be made clear in the text. This was a very difficult call, and after extensive discussion, the decision is intended to be in the long term best interests of the paper. The ideas are interesting and warmly appreciated, the reviewers appreciated aspects of the response, and the project is sufficiently promising that it was felt that their impact would be much greater if the experimental execution were strengthened, such that the project largely terminating at this point would do it a disservice in the long run. There was a general feeling that this is still a  work in a progress  and was somewhat rush written. The authors are strongly encouraged to continue pursuing this work, strengthening the experiments and narrative,  as above.
The paper proposes an approach for solving constrained optimization problems using deep learning. The key idea is to separate equality and inequality constraints and "solve" for the equality constraints separately. Empirical results are given for convex QPs and for a non convex problem that arises in AC optimal power flow. There was much discussion of this paper between the reviewers and the area chair. THe key question was whether the empirical evaluation is sufficient to convince that the method is more effective than existing solvers. The current experiments do not show that the method achieves better solutions than existing solvers. For the convex case this is to be expected since solvers are optimal. But in the non convex case, it would have been nice to see that the method indeed can find better solutions. This leaves the advantage of the method in its speedup over existing methods. However, as the authors acknowledge, it is possible that this speedup is due to better use of parallelization than the methods they compare to. It is true that deep learning is particularly easy to parallelize, but this is not impossible for other methods (e.g., for linear algebra operations etc). Thus, taken together the empirical support for the current method is somewhat limited. The method itself does make sense, and this was indeed appreciated by the reviewers.   
This paper extends the recent theoretical understanding on geometric properties for word embeddings to relations and entities of knowledge graph. It categorizes relations into different types and derive requirements for their representations. Empirically they experiment several graph embedding approaches and show that when the loss function is aligned with the requirement of the relation type, we can achieve better performance.  The reviewers generally find the paper to be solid, well executed and provides useful insights. The authors are encouraged to strengthen the discussion of the motivation of this work, and improve the presentation based on reviewers  comments. 
The paper proposed a *novel* methodology for protecting personal data from unauthorized exploitation for training commercial models. The proposal is conceptually *intuitive* and technically *motivated*. It goes to the opposite direction of adversarial training: by adding certain error minimizing noise (rather than error maximizing noise) to the data, the model is fooled and believes there is nothing to learn from the data, and thus this can protect the data from being used for training. The paper is of not only *high quality* but also *broad interest* given the current social concerns about personal data privacy. I think its potential impact should get it a spotlight presentation.
After the rebuttal phase, all scores are borderline (6) or negative (4). Among the most confident reviewers (confidence 5), one gives 6 and one gives 4. The reviewer with confidence 4 gives overall score 6 but states they cannot support the paper. There were several concerns about the novelty of the task and method, the challenge of the experimental settings, missing comparisons to recent prior work in the original paper, etc. While the reviewers see merit, the paper can benefit from another revision before being accepted, including to better position the novelty of its method and perhaps reduce claims of novelty of the task. 
This paper tackles an important problem and includes experiments on a new domain (Russian documents vs English documents). Unfortunately, all reviewers agree that this paper lacks novelty for publication in its current state. Additional details and clarifications to the proposed approach, notably through a more thorough performance analysis, would improve the significance of the paper.
The paper is very clear.  It provides a good overview of the problem, making it easy to follow even for researchers outside the area.    This work provides a novel approach for extrapolating the expected accuracy on a larger set of classes from a training set with smaller number of classes with a creative, simple and elegant solution through reversed ROC.  Such an approach will be useful for extreme classification settings.  In real world settings, classifiers are often trained on a pilot set of data, and then deployed where the classes are much larger.  It is useful to have a mechanism to estimate how the classification performance will change with larger number of classes.    The reviewers all agree that this work provides a novel contribution to predicting classification accuracy.  The authors have satisfactorily addressed the reviewers’ comments and provided sufficient clarification to the questions.  We also appreciate the edits that the authors have made.   
The paper focuses proposes a new framework for low resource video domain adaptation leveraging synthetic data with supervised disentangled learning for tackling keystroke inference attacks.  The paper received contrasting reviews, 2 positive and 2 negative, and the overall confidence of the reviewers is not so high. Overall, it is recognized that the work has some merit, but also some problems, which the rebuttal has not fully fixed, and I mainly refer to R2, R3 and R4 remarks.   The first issue is the level of novelty, which is not much high as compared to the former work of (Moiitan et al. 2017). Besides, the questions raised by some reviewers, also discussed in the rebuttal, also denote a certain lack of clarity, despite the paper is considered well organised in general.   The other main issue regards the experimental evaluation. To start with, the application addressed is very specific and it is not clear how this approach can be extended to other problems too, since no evidence is provided in this sense. The reported comparative analysis wrt baselines are in fact quite "simple" (e.g., ADDA is a work dated back to 2017, so as CycleGAN). Moreover, although the considered dataset is the only one in this scenario, its significance is a bit limited since only 3 subjects were considered, and this likely raised the comment of one reviewer questioning if this paper was not better suited to an application oriented, security conference. A discussion for the setting of the lambda parameters is also missing.  Overall, given the above issues, I consider the paper not yet ready for publication in ICLR 2021. 
Contributions of this type are very important for the community. There is a great deal of confusion among practitioners about how to pick optimizers. Perhaps worse, there is confusion among optimization researchers about how to demonstrate the effectiveness of their novel algorithms on deep learning tasks. I applaud this paper as one of the best attempts to make sense of this confusion.  Unfortunately, I am recommending that it is rejected. This was an extremely difficult decision. This paper was very thoroughly discussed by reviewers, both with the authors and after the feedback phase. I agree with R4 that this paper is exemplary in terms of its breadth of optimizer choices. I also agree with R3 that this paper s choices regarding hyperparameter search spaces and seed fixing significantly diminish the contribution of the paper at hand. The key issue that persuaded my decision centered on whether the paper s evidence supported its conclusions.  The two key conclusions that I want to highlight are:  1. *evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer*  2. *different optimizers exhibit a surprisingly similar performance distribution compared to a single method that is re tuned or simply re run with different random seeds*  These conclusions can only be supported if optimizers are well tuned. Based on R3 s remarks and a quick reading of the paper, I am concerned that the use of fixed search spaces means that these optimizers cannot be considered well tuned. This concern splits into two sub concerns.  1. I appreciate the author s desire to encode "no prior knowledge about well working hyperparameter values". Unfortunately, I don t think this is realistic or possible. The learning rate range used in this paper did not include 1e100 for good reasons, all of which depend on the prior knowledge of our community. This isn t just a glib concern, the apparently neutral search spaces may bias the conclusions towards well known methods whose hyperparameters are well understood.  2. I am also skeptical of the choice to use the same range for hyperparameters with "similar naming". The reason is that these hyperparameters *may have been misnamed by the inventors* and may, in fact, play very different roles in the dynamics of optimization.  Top line conclusions have a way of becoming memes in our community. Therefore, it is critical that conclusions, as stated, are actually supported by the experimental design and the empirical evidence. Unfortunately, I am not confident that this is is the case for the paper at hand.  It is clear that this paper represents a heroic effort by the authors. I am aware of the challenges involved in getting this type of paper published and of the urgent need for them. I hope that the authors address the concerns that I expressed and the concerns of the reviewers in a future submission.
The paper proposes a method for data augmentation by cross modal data generation. While the reviewers agree that the paper addresses a relevant and important problem in medical imaging, they also agree on that the paper has limited novelty over the state of the art. Also the setup of experimental validation to comparison methods is questioned.
This paper proposes to use an ensemble of VAEs to learn better disentangled representations by aligning their representations through additional losses. This training method is based on recent work by Rolinek et al (2019) and Duan et al (2020), which suggests that VAEs tend to approximate PCA like behaviour when they are trained to disentangle. The method is well justified from the theoretical perspective, and the quantitative results are good. Saying this, the reviewers raised concerns about the qualitative nature of the learnt representations, which do not look as disentangled as the quantitative measures might suggest. There was a large range of scores given to this paper by the reviewers, which has generated a long discussion. I have also personally looked at the paper. Unfortunately I have to agree that the latent traversal plots do not look as disentangled as the metric scores would suggest, and as one might hope to see on such toy datasets as dSprites. The traversals are certainly subpar to even the most basic approaches to disentanglement, like beta VAE. For this reason, and given the reviewer scores, I unfortunately have to recommend to reject the paper this time around, however I hope that the authors are able to address the reviewers  concerns and find the source of disagreement between their qualitative and quantitative results for the future revisions of this work.
The paper introduces some good ideas, but I don t think it is quite there in terms of a method to be recommended for publications. I think it is mostly reasonably written (I do not agree with the comment of a  complete rewrite ) but there are indeed some passages for improvement (for instance, an equation as y   σ−1[Q0(t, x)] + εH(t, x)], Section 2, needs comments, as the left hand side is discrete and the right hand side is continuous, unbounded).  My main concern is the disregard for identification. Some citations are unclear (the second to last paragraph in Section 4 cites a few papers in identification that have little to do with the problem here, which is proxy based. The papers cited don t even mention latent variables at all). As stated, the split in three sets of variables as suggested by Figure 1 is just an idealization: there is no reason at all they can be identified, and actually the theory where just Zc is considered impose a lot of restrictions on when we can possible identify Zc (see e.g., Miao et al. 2018, Biometrika, https://arxiv.org/pdf/1609.08816.pdf ). I know that some papers like Louizos et al. play fast and loose with identification too, but at least their Z_c structure they aim at has been studied elsewhere (like the Miao et al. paper), while here, like the Zhang et al. paper cited, may be leading researchers to an unfruitful path. This, combined with the relative modesty of the novelty, is the primary reason for my recommendation. I do think the paper can be improved in a productive way by investigating it from the point of view of either i) the theoretical justification for identification; ii) or from a more empirical direction with much experimentation on the different ways the structured latent space is capturing confounding (the target learning aspect of it is pretty much orthogonal to this).
Reviewers agree that the idea of layer wise regularization is interesting and is in line with many efforts in the optimization realm to specialize in the training procedure and the learning rate to each layer.  Given the depth of some state of the art neural networks, efficiency is at stake and the idea brought up in this paper naturally falls into that.  While the theoretical result in Theorem1 is sound and clear, an extended result on the impact of such « merge » and « layer skipping » on the overall predictions of the algorithm can be well appreciated. The overall goal of network compression should remain to reduce drastically the network size, and thus the training time (energy consumption etc...), while keeping a relatively good prediction accuracy (at least of the same order). Being able to back this with theory (and of course experiments) is crucial.   Reviewers also pointed out that the empirical evaluations were not sufficient for ICLR. For example, there are no enough comparisons with existing algorithms and there should be more experimental results based on real datasets. Although the rebuttals did help clarify some of the issues raised by the reviewers, overall this paper does not seem to meet the bar to be accepted.   
This paper use Group convolutional neural networks in both generators and discriminator of GANs, and demonstrates advantages of this approach when training with a relatively small sample size. While the novelty is limited in the work  as it simply applies G CNN for GANs , I believe this application is interesting and  the authors have applied it to many GAN image synthesis applications (conditional generation , pix2pix) on various benchmarks, which gives  evidence of  the potential of GCNNs in generative modeling. Accept
In this paper, the authors theoretically analyzed the attacking mechanisms of the two kinds of adversarial examples in the Gaussian mixture data model case and proved that adversarial robustness can be disentangled in directions of the data manifold. The reviewers commonly felt that the idea and theoretical analysis in this paper are interesting, but experiments are not satisfactory.  At the current status, they still have a main concern regarding the correctness of comparison between the results of Theorem 4 and Corollary 3 (which is the heart of their theoretical claims, the main message of the paper and the main motivation for experiments).  As a whole, this paper has some merits but the authors still cannot clarify some concerns raised by some reviewers.  
The reviewer acknowledged that the proposed method is simple and seems to work well on the chosen benchmarks. Yet the expressed several concerns that were not fully addressed by the authors in their responses. The major concern is about the experimental setup. The chosen tasks have been judged too simple and quite different from those where the baselines were tested initially (e.g. DQfD was demonstrated on a diverse set of Atari games).   The clarity of the paper should also be improved. For instance, the way the number of trajectories that are added to the replay buffer increases with time is not well explained and it seems to be crucial for the algorithm to outperform the baselines. The authors also seemed to select the experiments so that the "results are more persuasive", discarding experiments where you can be unlucky. This looks very much like cherry picking and didn t convince the reviewers. 
This paper proposes a meta learning based few shot federated learning approach to reduce the communication overhead incurred in aggregating model updates. The use of meta learning also gives some generalization benefits. The reviewers think that the paper has the following main issues (see reviews for more details): * Limited technical novelty   the paper seems to simply combine meta learning with federated learning * Not clear whether the communication overhead is actually reduced because the meta learning phase can require significant communication and computation. * The experimental evaluation, in particular, the data distribution, could have been more realistic.  I hope that the authors can use the reviewers  feedback to improve the paper and resubmit to a future venue.  
The paper considers the OPE problem under the contextual bandit model with continuous action.  They studied the model of a piecewise constant value function according to the actions.   The assumption is new, though still somewhat restrictive as it requires the piecewise constant partitions to be the same for all x.  The proposed algorithm estimates the partitions, and then used it to build a doubly robust estimator with stratified importance sampling (fitting an MLP for each partition separately).   The reviewers have mixed views about the paper.  The following is the AC s evaluation based on reading the paper and consolidating the reviewers  comments and the authors  responses.  Pros:     The algorithm is new and it makes sense for the new problem setup  (though computationally intractable)   The experimental results outperform the baseline and reinforces the theory. But it s a toy example at best.  Cons:    The method is called "Q learning" but it is somewhat disappointing to see that it actually applies only to the contextual bandit model (without dynamics).  There is quite a bit of branding issues here. I suggest the authors to revise it to reflect the actual problem setup.     The estimator is assumed to be arg min, but the objective function is non convex and cannot be solved efficiently in general, e.g., (3) involves searching over all partitions... and (4) involves solving neural network partitions.  In other words, the result applies to a hypothetical minimizer that the practical solvers may or may not obtain (the authors cited Scikit Learn for the optimization algorithm and claims that the optimization problem can be solved, which is not the case ...  the SGD algorithm can be applied to solve it, but it does not necessarily find you the solution).     The theory is completely asymptotic and generic. There is no rate of convergence specified, and no dependence on the number of jumps |D_0| at all in Theorem 1.       Theorem 3 is obnoxiously sloppy. The assumptions are not made explicit (do you need Assumption 1 and 2, what is the choice of \rho? ) The notion of "minimax rate" is not defined at all.   Usually the minimax rate is the property of problem setting,  i.e., Min over all algorithms, and Max over all problems with in a family.  However, in the way the authors described the results in Theorem 3,  it says the "the minimax convergence rate of kernel based estimator is Op(n^{−1/3})."  which seems to be restricting the algorithms instead.  Such non typical choices require clear definitions and justification.    Based on what is stated, it really appears that the authors are just comparing upper bounds of the two methods.  I looked at the appendix and while there is a "lower bound analysis", the bound is not information theoretical, but rather a fixed example where an unspecified family of algorithms (I think it is a specific kernel smoothing method with a arbitrary choice of the bandwidth parameter h) will fail.   Suggestions to the authors:     Instead of a piecewise constant (and uniformly bounded) function, why not consider the total variation class, which is strictly more general and comes with the same rate?    For formalizing the lower bound, I suggest the authors to look into classical lower bounds for linear smoother, e.g., Donoho, Liu, MacGibbon (1990); which clearly illustrates that kernel smoothing type methods do not achieve the minimax rates; and that wavelets based approaches, locally adaptive regression splines, and fused lasso (You can think about the  Haar Wavelets as a basis function of piecewise linear functions ) do.   The authors can improve the paper by ensuring that the theoretical parts are clearly and rigorously presented; and perhaps to iron out the more useful finite sample analysis that depends on model parameters of interest.
The paper extends previous work on asymmetric self play by introducing a novel behavior cloning loss (referred to as ABC). The zero shot results are impressive and demonstrate that the proposed curriculum learning approach pushes the state of the art. The reviewers acknowledge these contributions. The pros of the paper are well summarized by R2,     The experimental results are very encouraging.   The analysis of the method helps to understand which components are important.   The evaluation on the hold out tasks is very impressive and pushes the state of the art.   The paper is well written and very easy to follow, the illustrations are informative and appealing.   Although this approach is based on previous work on asymmetric self play, the authors clearly describe the contributions of this work (training clearly from self play, zero shot generalization).  R1, R2, R5 recommend accepting the paper with scores of 7, 7, 6. R1 expressed that he is not confident about the paper. R4 recommends acceptance with a score of 6. However, R4 also expresses the concern for real world applicability, "the sim real gap with respect to applicability to robotics is still a major concern IMO. I have updated my score accordingly." The sim to real gap is a concern due to knowledge of perfect state information and the assumption of resets.   Based on confident reviews of R2, R4, and R5, and the impressive zero shot results, ordinarily, I would recommend the paper to be accepted. However, unfortunately, both the authors and reviewers missed a comparison to prior work, which I detail below. While the current paper makes a good case for zero shot generalization, it does not compare to previous approaches that also exhibits zero shot generalization. For instance, Li et al. ICRA 2020 (https://arxiv.org/abs/1912.11032) show that using a simple curriculum that depends on the number of manipulated objects + Graph Neural Networks can generalize very well to unseen tasks. E.g., the results reported in the paper demonstrate that a policy trained on 2/3 blocks generalizes and can stack many more blocks. Their policy learns to stack 6 7 blocks, whereas the paper under review can only stack up to 3 blocks (Figure 8).   The authors mention in Section 5.2 of their paper, "The curriculum:distribution baseline, which slowly increases the proportion of pick and place and stacking goals in the training distribution, fails to acquire any skill. The curriculum:full baseline incorporates all hand designed curricula yet still cannot learn how to pick up or stack blocks. We have spent a decent amount of time iterating and improving these baselines but found it especially difficult to develop a scheme good enough to compete with asymmetric self play."  This is at odds with results in Li et al.   This reason for dissonance is that good generalization can be achieved by improving two separate components   the neural network architecture or the learning curriculum. This paper shows good generalization with weak neural net architectures + a good curriculum learning method. It is unclear to me how critical the self play method would be with a stronger architecture such as a graph network which is arguable more appropriate for the set of tasks presented in the paper. I would like to see if the curriculum is necessary (i.e., complements a stronger architecture) or is it just a replacement for alternate neural network architecture. Without such a study, this paper should not be accepted, because it will add to more noise rather than advancing the field of robotic manipulation. 
The paper provides a simple approach to explaining GNN predictions for each node by greedily selecting nodes or features in each computation graph so as to increase the fidelity score. The fidelity score is based on comparing the original GNN output to what is obtained with noisy versions of the masked nodes/features. While simple, the approach seems somewhat inefficient (efficiency should be assessed/characterized). Also, several improvements to the evaluation expressed in the reviews/discussion (e.g., human evaluation, practical utility, comparison to gradient based methods) would make the submission somewhat stronger.  
Paper was reviewed by four expert reviewers who identified the following pros/cons for the approach:  > Pros:   Paper addresses an important problem [R3]    Formulation is simple, elegant an easily adoptable [R2, R3, R4]   Experimental results are compelling (ECCV challenge winner) [R2, R3, R4]  > Cons:   Experiments  are using "novel" evaluation metric with little justification [R1, R3]   Details of the approach are unclear [R1]   Lack of simpler baselines in evaluation [R4]    No comparison with handlabelling counterparts [R4]  Authors have addressed a number of comments in the rebuttal. With [R2] and [R4], generally, being convinced about accepting the paper.  However, [R1] and [R3] actually became less positive about the paper as a function of the rebuttal. [R3] mentioned reducing the score to a 3 and [R1] to a 6 in the discussion. It is unclear why these changes are not reflected in the publicly facing reviews. However, the fact that two of the four reviewers were disappointed with author responses and became LESS convinced about the paper is problematic in AC s opinion.   AC also agrees that standard performance metrics should have been included. It is obviously reasonable to propose new metrics, but doing so should be accompanied by (1) reporting of performance with the original standard metric(s) and (2) justification for where prior metrics are problematic or faulty. While the proposed metric is reasonable for the specific use case outlined in Appendix C, it doesn t preclude standard evaluation metrics nor points out why they would be inappropriate or faulty.   Overall, AC likes the paper and agrees that it presents a valuable approach that should be published. Unfortunately, the unjustified use of non standard metrics without accompanied evidence, as noted above, is problematic and needs to be addressed in AC s opinion before that can happen. Since this issue was not addressed by authors in the rebuttal, AC sees no recourse but to reject the paper at this time, with a strong encouragement to address the aforementioned issue and to resubmit to CVPR or another top tier upcoming venue.   
The reviewers have supported the acceptance of this paper (R3 and R5 were particularly excited) so I recommend to accept this paper.
This paper studies the *last iterate* convergence of the projected Heavy ball method (and an adaptive variant) for convex problems, and propose a specific coefficient schedule. All reviewers thought that looking at the last iterate convergence of the HB method was interesting and that the proofs, while simple, were interestingly novel. Several concerns were raised by the quality of the writing. Several were addressed in a revision and the rebuttal. While R1 did not update their score, the AC thinks that the rebuttal has addressed appropriately their initial concerns. The AC recommends the paper for acceptance, *but* it is important that the authors make an appropriate careful pass over their paper for the camera ready version.  ### comments about the write up    The paper still contains many typos (e.g. missing $1/t$ term in the average after equation (2); many misspelled words, etc.), please carefully proofread your paper again.   The AC agrees with R1 that the quality of presentation still needs improvement. $\beta_{1t}$ is still used in the introduction without being defined   please define it properly first e.g.    The word "optimal" and "optimality" is usually misused in the manuscript. To refer to the convergence rate of an optimization algorithm, the standard terminology is to talk about the "suboptimality" or the "error" (e.g. see the terminology used by the cited [Harvey et al. 2019, Jain et al. 2019] papers). For example, one would say that the error or suboptimality of SGD has a $O(1/\sqrt{t})$ convergence rate. Saying "optimality of" or "optimal individual convergence rate" is quite confusing, and should be corrected. The adjective "optimal" (when talking about a convergence rate) should be restricted to when a matching lower bound exists.   Finally, the text introducing the experimental section should be fixed to clarify the actual results and motivation. Specifically, the "validate the correctness of our convergence analysis" only applies in the convex setting. I recommend that a high level description of the convex experiment and the main message of the results is moved from the appendix to the main paper there (there is space). And then, the deep learning experiments can be introduced as just investigating the practical performance of the suggested coefficient schedule for HB.
Due to uniformly unfavourable reviews and lack of author engagement in the discussion period, this paper is rejected.
The manuscript presents a training method for Spiking Neural Networks (SNN). The method jointly optimizes input spike encoding parameters, spiking neuron parameters (membrane leak and voltage threshold), and weights in an end to end fashion using gradient descent. SNNs are very interesting for energy efficient implementations of neural networks. Their energy efficiency strongly depends on inference latency (SNNs compute in time, unlike feed forward ANNs) and activation sparsity.   All reviewers acknowledged that the approach directly improves inference latency and activation sparsity on large convolutional models at very good performance levels.  The main concern of all reviewers was the limited conceptual novelty. The paper combines some known techniques (hybrid SNN training, direct input encoding, training of neuron parameters like leak time constant and threshold) and scale the setup up to large networks and datasets (e.g. ImageNet).  In summary, the paper presents impressive results, but the conceptual innovation is missing.    
The paper studies robust learning in the presence of noisy labels and proposes a new loss function called the golden symmetric loss (GSL) combining both regular cross entropy and reverse cross entropy and leveraging an estimate of the corruption matrix. The paper appears to be well written.  Pros:   Good range of application domains (both vision and text).   Learning with noisy labels is an important practical problem.   Theoretical guarantees for the procedure using framework from recent work.  Cons:   Limited novelty as the method appears to be a weighted combination of two existing ideas.    Concerns about the baselines used: why are the same baselines not being used throughout?   Having at least a few trials with mean and standard deviation for the experiments would make the conclusions stronger.  Overall, the limited novelty combined with the concerns about the empirical analysis was a key reason for rejection. 
The paper deals with cross domain few shot learning in the case of large source target domain shifts.  The paper received mostly below threshold reviews, with one exception (R3) whose review is addressing more general aspects, but still with some concern, especially in relation to the experimental part (to which authors did not answer). R1 s review is not of much help.  Clarity of the presentation and missing details seem to be recurrent issues all over the reviewers, together with remarks concerning the experimental validation, which would have required a deep revision and improvement, in particular regarding the use of more backbones, better ablation (Hebbian learner contribution, unclear initialization), processing times/computational complexity, significant comparative analysis re robust baselines.   The rebuttal clarifies some of the raised remarks but there are still issues, especially regarding Hebbian learning rule and ensemble learning strategies, and about results too, so not all reviewers were convinced to raise their ratings.  Overall, given the above issues, I consider the paper not yet ready for publication in ICLR 2021. 
This paper proposes a mathematical framework to theoretically understand and quantify the benefit of self supervision on the downstream tasks. The theoretical analyses in this paper are concrete and the authors conducted experiments to support their claims. However, the current version still has the following weaknesses.      This paper would benefit from incorporating the reviewers  comments (which was complained by multiple reviewers) and the authors  responses if any.   The authors need to make it clear in the abstract and introduction that (1) this paper only considers the *reconstruction based* SSL, instead of the general SSL, and (2) addresses the discrepancy between the practice and the proposed framework.     In the post rebuttal phase, Reviewer 2 pointed out "Lemma 3 seems much more meaningful after the clarifications. I also notice that R5 concerned about the discrepancy in downstream task setup and may doubt the mathematical framework in this paper. In fact, I agree with R5. There is indeed a gap between the proposed mathematical model and the practical SSL algorithms. There is still some work need the authors to complete, i.e., $\mathbb{E}[Y|X_1]\approx \mathbb{E}[Y|X_1,X_2]$."
This paper proposes a method for out of distribution (OOD) detection by introducing a K+1 abstention class for outliers, in addition to the in distribution classes. While the method has shown promising performance compared to the Outlier Exposure (OE), the novelty is limited given the idea is almost identical to an AAAI 20 paper (Mohseni et al. 2020). In addition, several reviewers have raised concerns regarding the lack of fairness in the experimental setting. Authors are encouraged to address them in a future submission.   The AC believes an interesting and valuable contribution to the community would be showing conceptual and theoretical reasoning for the pros and cons of using K+1 class vs. entropy regularization. Currently, the tradeoff between these two types of training objectives is not well understood.   Overall, three knowledgeable reviewers in this area have indicated rejections. The AC discounted R2 s rating due to the less familiarity in this area and lack of participation in the post rebuttal discussion among reviewers.   Lastly, the AC would like to greatly thank R1, R3, and R4 for the active engagement and participation in the paper discussion period. It was very helpful for the decision making process.      
The paper studies federated learning in what they call ``` single sided trust  scenario, i.e. there is no dedicated server and the trust relationship is asymmetric.  This paper was a trickier case to decide on, and more borderline, in our opinion, than the reviewers  scores suggest, primarily, because the reviewers  recommendations are based on more subjective notions of novelty and importance/appropriateness of the studied setting, rather than identifying specific flaws in theoretical analysis or experiments. Ultimately, it boils down to three reviewers being (rather) negative about the paper, and the only (very) positive reviewer not stepping in to champion it. The negative reviewers believed that the novelty is not substantial enough to meet the high ICLR acceptance bar  (see details in reviews by R1 and R2 re similarity to ) and also have questioned the general motivation  (R1) and/or the online learning setting (R3).  While this assessment may be too harsh (esp. R1)   I think that the paper has merit   I share their feeling that in its current form it does not have a strong enough contribution.    
The authors propose a particle based entropy estimate for intrinsic motivation for pre training an RL agent to then perform in an environment with rewards. As the reviewers discussed, and also mentioned in their reviews, this paper bears stark similarity to work of 5 months ago, presented at the ICML 2020 Lifelong ML workshop, namely, "A Policy Gradient Method for Task Agnostic Exploration", Mutti et al, 2020 MEPOL. What is novel here is the adaptation of this entropy estimate to form an intrinsic reward via a contrastive representation and the subsequent demonstration on standardized RL environments.  The authors have added a comparison to MEPOL, and in these experiments, APT outperforms this method, sometimes by some margin. Unfortunately this work does not meet the bar for acceptance relative to other submissions. 
This paper proposes a semi supervised graph classification technique that unifies feature and label propagation techniques. The resulting algorithm is a simple extension that attains strong performance. Reviewers were divided on this submission. Some reviewers felt the proposed algorithm did not constitute a sufficient technical contribution given that it was a simple combination of existing techniques. I tend to agree with other reviewers that the simplicity is a benefit. However, despite the methods simplicity there was significant confusion about the details of the method and multiple reviewers flagged that the paper was difficult to read and understand. It further could benefit from additional discussion and some clarification/cleanup of the experimental results. Finally, multiple reviewers asked for better situating of the proposed method with respect to prior work. Given these concerns, I do not think the paper is ready for publication. I would recommend the reviewers do a thorough re write of the paper to address these concerns and consider resubmitting.
Reviews are somewhat mixed, but all are below the acceptance threshold. Reviewers praise the overall application and the presentation (though there is some variance in response to this aspect), but have concerns about lack of certain comparisons and technical novelty.
This paper presents a compelling mechanism for reducing the neural architecture search process based on accumulated experience  that the reviewers found compelling with significant improvements in performance.  This is an intriguing idea. However, there were concerns about clarity that need to be addressed, and more concerning, the paper lacked technical depth or details in several aspects described in the reviews.  The authors subsequent response and revisions have somewhat addressed these issues.  The reviewer discussion had mixed opinions, with some for weak acceptance and others for weak rejection.  There were compelling points that the contribution is significant, but overall this paper would benefit from thoroughly addressing the shortcomings mentioned in the reviews before it is ready for publication. 
Two reviewers are very positive about this paper and recommend acceptance, one indicates rejection and one is on the fence. Although all referees appreciate the extensive experiments and analysis presented in the paper, their main concerns are related to the limited superiority of the method wrt state of the art [R1], seemingly arbitrary choices and questionable assumptions [R4]. The rebuttal adequately addresses R1 s concerns by highlighting statistical significance of the results, and partially covers R4 s concerns. Although the proposed approach may be perceived as incremental [R1, R2, R3, R4], the authors argue that introducing self supervision to graph attention is not trivial, and emphasize their findings on how/when this is beneficial. Moreover, R2 and R3 acknowledge that the contribution of the paper holds promise, is worth exploring, and may be useful to the research community. Most reviewers are satisfied with the answers in the rebuttal. After discussion, three referees lean towards acceptance and the fourth reviewer does not oppose the decision. I agree with their assessment and therefore recommend acceptance. Please do include your comments regarding the choice of average degree and homophily in the final version of paper.
The reviewers agree that the idea of introducing structural biases in the attention mechanism is interesting but the results and presentation right now is not convincing. Improvements are seen on only some datasets and the comparisons are not exact. A reject.
The paper considers the problem of 2D point goal navigation in novel environments given access to an abstract occupancy grid map of the environment, together with knowledge of the agent s state and the goal location typical of point goal navigation. The paper proposes learning a navigation policy in a model based fashion, whereby the architecture predicts the parameters of the transition function and then uses this learned transition function to plan the agent s actions. The authors also describe a model free approach that extends a version of DQN to reason over the 2D maps.  The paper was reviewed by four knowledgeable referees, who read the author response. The general problem of learning to navigate a priori unknown environments to reach a desired goal is an interesting problem that has received significant attention of late in the learning community. In its current form, however, the paper does not adequately convey why this is a difficult problem that can not be solved using existing planning techniques or why it benefits from learning, particularly given access to an abstract map. These concerns apply more generally to point goal navigation, namely the assumption that the pose of the agent and goal are fully known throughout (or the agent relative pose of the goal) and that there is no uncertainty in the agent s motion. The practicality of these assumptions is unclear, and they are inconsistent with decades of research in robotics and robot learning, which addresses the more realistic setting in which there is uncertainty in pose and motion. The author response helps to clarify some of these questions, but it is still not fully clear why existing methods are insufficient for this task, whether they use traditional planning methods or are learned. Revisiting the discussion of why this is a hard problem would strengthen the paper, as would a more thorough evaluation that compares against other baselines.
In this paper, the authors propose a new GNN architecture based on Generalized PageRank to handle two weaknesses in some existing GNNs. The novelty of this approach is that it works well for both homophilic and heterophilic graphs (due to the use of GPR).  Overall the paper is interesting and well written. Moreover,  the authors addressed the concerns of reviewers during the rebuttal period. Thus, I vote for acceptance.
This work presented a broad set of interesting applications of model information toward understanding task difficulty, domain similarity, and more. However, reviewers were concerned around the validity and rigor of the conclusions. Going into more depth in a subset of the areas presented would strengthen the paper, as would further discussions and experiments around the limitations of model information with regards to specific models and dataset sizes (as you have begun to discuss in Section 8). Additionally, reviewers found the updated paper with connections to Kolmogorov complexity interesting, but reviewers wanted a more formal treatment and analysis of the relationship. 
This paper proposes an attention based technique to focus on relevant entities in multi agent reinforcement learning.  While the effectiveness of the proposed method is demonstrated on some tasks, there remain major concerns including the following: 1. It is not sufficiently convincing that the proposed method performs well in more complex domains 2. Novelty over Agarwal et al. and MAAC is rather minor
This paper received 1 weak accept, 1 accept, and 1 weak reject.  All reviewers questioned the motivation for continuous space/time with respect to biological vision. Obviously, discrete approximations used in machine vision are approximations but it is not clear from the paper or the authors’ response that this severely limits the ability of deep nets to predict neural data in ways that their continuous nets would not.   In addition, I have to confess that I did not really understand the argument made by the authors in their revision. In any case, the burden should be on the authors to go beyond general statements and to really demonstrate that the proposed models provide actual insights for neuroscience since the performance in terms of machine vision on CIFAR10 is underwhelming (the authors have to find a low data regime and even then the reviewers stated that the baselines used are not strong baselines, the reduction in the number of parameters is quite small relative to methods for actually reducing the number of parameters).    Clearly, the work has potential as noted by the reviewers. The authors suggest that “DCNs can be used to model the temporal profiles of neuronal responses, which are known to not be constant even when the experimental stimuli are static images: for example, spatial frequency responses change over time in response to stationary gratings (Frazor et al., 2004). Similar observations are made for the contrast response function (Albrecht et al., 2002). Such temporal profiles cannot be simulated in conventional CNNs. “ This sounds like an interesting set of neuroscience data that the authors could be indeed leveraging to demonstrate the benefit of their approach. My recommendation would be to add those in a revision of this paper which will significantly strengthen the work. I would add that the concepts of temporal and spatial continuity are independent and the authors should consider studying them separately to provide more in depth analyses and convincing results.   As it stands, the paper has clear potential but it does not make a sufficient contribution to either ML or neuroscience and hence, I recommend the paper to be rejected. 
This paper proposes a new sparsity inducing activation function, and demonstrates its benefits on continual learning and reinforcement learning tasks.  After the discussion period, all reviewers agree that this is a solid paper, and so do I. I am thus recommending it for acceptance as a poster. Hopefully, such visibility (combined with the open source release of the code) will encourage other researchers to try this new technique, and we will see more evidence confirming its usefulness in more varied settings and versus stronger baselines (that remain somewhat limited in the current work: this is the main weakness of the paper).
The authors address the problem of self supervised monocular depth estimation via training with only monocular videos. They propose to use additional information extracted from semantic segmentation at training time to (i) provide additional “semantic context” supervision and (ii) to improve depth estimation at discontinuities through an edge guided point sampling based approach. Results are presented on the KITTI and Cityscapes datasets.    One of the main concerns is related to the utility of the semantic supervision given the relative cost required to obtain semantic training data in the first place. The authors state that "the pixel wise local depth information can not be well represented by current depth network". However, Guizilini 2020a can generate detailed depth edges and they do NOT require any semantic information during training. The authors also state that "the required labeled semantic dataset only accounts for a very tiny proportion, which indicates a relatively lower cost." This is a bit misleading. The proposed method uses per pixel semantic ground truth from three datasets (Mapillary Vistas, Cityscapes, and KITTI). It takes a lot of effort to provide this ground truth compared to self supervised methods which do not require any ground truth depth. It is encouraging that dataset specific semantic finetuning does not seem to have a large impact (Table 3), but this still requires access to a large enough initial semantic training set. Finally, the quantitative results are not much better than methods that don t require any semantics e.g. Guizilini 2020a, Johnston and Carneiro. Clearly, methods that do not require semantics are much more scalable, especially when adapting to new types of scenes.    Regarding the specific contributions of the paper, the SEEM module is the most novel component of the model. However, the addition of the SEEM module does not improve quantitative performance by much (see Table 2). In addition, the qualitative improvement it provides is also very subtle. This can be seen by comparing the last two rows of Fig 7 i.e. without and with. The authors need to make a stronger case, either quantitatively or qualitatively, as to why this is valuable.    Finally, but only a minor concern, the following relevant reference is missing: Jiao et al. Look Deeper into Depth: Monocular Depth Estimation with Semantic Booster and Attention Driven Loss, ECCV 2018   In conclusion, there were mixed views from the reviewers   with some supportive of the paper (R2&3) others not as enthusiastic (R1&4). The authors should be commended for their detailed responses and changes already made based on reviewer comments and suggestions. Unfortunately, this did not change the mind of the reviewers. It is the opinion of this AC that there is still more work required to fully show the utility of the proposed approach, especially considering the non trivial effort that is required to obtain semantic supervision in novel domains.  
The rationality of the proposed method, especially its implementation detail, is challenged by the reviewers. Additionally, the experimental part and the writing of the paper should be improved. According to the feedback of the reviewers, I don t think this work is qualified enough at its current status. 
The paper had three borderline reviews. While the idea of posterior sampling of a neural network is potentially useful and Langevin dynamics are a way to attempt to address that, the reviewers did not appear convinced by the experiments and what the MCMC sampling was doing wasn t really front and center there.
**Overview** This paper provides a way to combine SVRG and greedy GQ to improve the algorithm performance. In particular, the finite iteration complexity is improved from $\epsilon^{ 3}$ to $\epsilon^{ 2}$.  **Pros** The paper is well written. Reviewers believe this is a solid theoretical work on advancing value based algorithms for off policy optimal control. It has sufficient theoretical advancement and experiments demonstrations of the methods.   **Cons** Some reviewers are concerned that SVRG is not SOTA. SVRG is not used in practice. The techniques appear to be similar to some existing works.   **Recommendation** The meta reviewer believes that the paper has solid theoretical contributions. SVRG is a component in the new algorithm to improve the complexity. It does not need to be "useful" or "SOTA". The paper is also well written. Hence the recommendation is accept.
The paper investigates the relationship between data augmentations used during training and their effect on the accuracy when evaluated on unseen corruptions at test time. The paper proposes a metric called minimal sample distance (MSD) to measure the similarity between augmentations during training time and corruptions at test time.  The reviewers agree that the paper aims to solve an important problem and the paper has some interesting findings. However, the current version has a few shortcomings:   Some of the claims about “overfitting” are confusing, especially for data augmentations that use ops similar to those in ImageNet C. This is already known and which is why some papers uses a subset of operations (e.g. AugMix uses a subset of AutoAugment operations).   The main take home message and novelty is unclear: The initial version titled (“Is Robustness Robust?“) seemed to argue that we may be overfitting to Imagenet C, but the rebuttal and the updated version revised some of the claims (see response to R3 and R4).  In light of the revision, I’m not sure how the main take home messages differ from existing papers such as Yin et al. 2019 or “Many faces of robustness”.    One of the main differences is quantification of the distribution similarity, however, as pointed out by R2, this analysis does not explain when stylized corruptions would help, so the current version of the paper feels a bit incomplete to me.  I recommend the authors to revise the draft based on reviewer feedback and resubmit the paper to another venue. 
This paper is about learning the output noise variance of a VAE and its effect on the generated image quality as measured by FID. The paper argues that the output variance parameter plays an important role and proposes a simple procedure, where a maximum likelihood estimate of the noise variance is estimated. Experiments on some standard datasets are provided. Overall, the paper is well written and has been perceived positively by the reviewers. However, the effect of observation variance has been in detail analysed by earlier work, in particular Dai and Wipf 2019. The novelty of the current paper is somewhat limited in scope. The paper is somewhat borderline in these respect; a much stronger experimental section would have been helpful.  One key contribution of the work is empirical comparison of alternative parametrizations of the output noise. Overall, the paper would be stronger if this aspect is analysed more in detail, possibly with careful comparisons with competing methods. Inclusion of controlled experiments (e.g. by adding extra noise to data) to show how precise the noise variance estimation and how the procedure influences the convergence of other parameters would have made the paper much more impactful. 
The focus of the submission is shape constrained regression, particularly the goal is to learn monotonic,  reasonably rich  functions. In order to tackle this task, the authors extend the monotonic regression framework (Gupta et al., 2016) which scales less benignly in the input dimension. They propose to use lattice functions with parameters having Kronecker product structure (, and their ensembles). The resulting function class can be (i) stored and evaluated in linear time (Proposition 1), (ii) characterized / checked from monotonicity perspective (Proposition 2). The efficiency of the approach is demonstrated in three real world examples.  Shape constrained regression is a central topic in machine learning and statistics. The authors propose a parametric family to learn monotonically constrained functions. The storage and the evaluation of the resulting functions are both fast (linear), and the numerical experiments are encouraging. The submission can be of definite interest to the ICLR community. 
All reviewers agreed that the novelty of the method was not at the level expected for publication, and also raised a number of technical concerns regarding the approach. There was no response from the authors on these issues, hence the reviewer consensus is that the paper is not ready for publication at this time.
The paper received diverging review feedback. While reviewers found merits in the work, they also raise serious concerns over experimental validation, comparison with the existing methods, and practicality of the proposed method. It appears that the paper can benefit from better writing and more experimental validations clarifying all these points. 
The authors propose an algorithm to perform backprop in a feed forward neural network without the need to backpropagate errors. They hence claim that this algorithm is a biologically plausible variant of Backprop.  After a forward propagation phase, the method introduces a relaxation phase and they remark that at the equilibrium of this phase, the activity is equal to the derivatives. Some related algorithms have been proposed previously (predictive coding, equilibrium prop, target propagation). Advantages of the proposed algorithm are that it does not need multiple distinct backwards phases and that it only utilizes a single type of neuron instead of separate populiations (such as in predictive coding).  Their method is tested on MNIST and fashion MNIST using a 4 layer fully connected network. In a revised version after the initial reviews, the authors added preliminary results on CIFAR 10 with a 4 layer CNN.  The authors then study the impact of some unbiological constraints such as symmetric weights.  While the reviewers agreed that the work is interesting, there was some disagreement on the significance of the model. In particular, it was noted that while the learning rules are indeed local in space, they are not local in time (the network has to remember variables from the forward phase until the update at the relaxation phase), which was deemed questionable from the biological perspective.  In addition, it was criticized that the simulations are no sufficient to support the claims of the paper. The datasets are relatively simple, networks are shallow and performances of baseline models are not state of the art.  In a revised version after the initial reviews, the authors added preliminary results on CIFAR 10 with a 4 layer CNN. However, these results do not seem conclusive, as the baselines are far below SOTA and networks are still quite shallow (a study by Lillicrap found that biological approximations to backprop struggle especially when applied to deep networks).  In summary, the work looks promising but some questions remain about the locality of learning and its applicability to more demanding tasks.  I add that one reviewer gave a very good rating with a poor review and did not respond to any questions about the justification. Therefore I had to neglect this review.
This paper brings interesting ideas (decentralized setting, auto distillation) but it does not meet the very high requirements that a publication at ICLR requires.  Three main reasons for that:  1/ Motivation & justification: Ultimately the paper is advocating for a pure decentralized approach "which encodes each entity from and only from the embeddings of its neighbors" with the main motivation being to represent better on unseen entities at training. This is quite radical and leads to a complex model and training procedure for a benefit and justification that are not very clear. Are there that many unseen entities in general? What would periodically retrain the whole model do? The computational cost associated to DecentRL should be discussed with regards to that. Some implementation details in appendix A.2 seems rather critical and are not motivated.  2/ Missing comparisons and references: as noted by several reviewers, it would be helpful to have comparisons of other methods that are dealing with missing entities. Some much simpler heuristics could be tried for instance (retraining the model, averaging neighbors, etc.). A discussion with DeepWalk, that is really an adaptation of CBOW for KG should also be added.  3/ Clarity could be improved. Thanks to reviewers  comments, the clarity has increased but could still be worked on as noted by several reviewers. For instance, the analogy  with CBOW right in the intro is confusing: in the 2nd paragraph, CBOW is used as a common manner for methods that are limited, but in the 3rd paragraph, CBOW is also used as an intuition for DecentRL. Some content from supplementary material like the description in A.1 would add a lot of clarity if added earlier.  We encourage the authors to use the many comments from the reviewers to improve further the paper. 
The paper identifies the phenomenon of oversquashing in GNNs and relate it to bottleneck. While this phenomenon has been previously observed, the analysis is new and insightful. The authors conclude that standard message passing may be inefficient in cases where the graphs exhibit an exponentially growing number of neighbors and long range dependencies, and propose a solution in the form of a fully adjacent layer. While the paper does not offer much methodologically, it is the observation of bottleneck that is of importance.   We therefore believe that the criticism raised by some reviewers of the observation not being novel and the solution "too simple" rather unsubstantiated. The authors have well addressed these issues in their rebuttal. The AC recommends accepting the paper. 
The paper presents an attempt to learn interaction based representations by taking advantage of body part movements and gaze attention. Video representations are learned by benefiting from additional supervisory signals, which are not the ones commonly used, making the paper more interesting.  R3 expresses a concern that the supervisory signal does not come "for free" and that the paper is misleading. The ACs do agree with R3 that the paper benefits from additional signals and is not a pure self supervised learning paper, strictly speaking. The authors also agreed to this in their response to the R3’s comment. R1 also mentioned (after the rebuttal phase) that the proposed approach is not a practical self supervised learning solution and that it does not perform as effectively as conventional self supervised learning methods like InfoNCE on Moco.  Simultaneously, the AC and the majority of the reviewers believe that the paper itself has a value as a multi modal learning paper. We strongly suggest the authors revise the paper to remove the  self supervision  claim. As mentioned above, the paper is not a self supervised learning paper and the authors are asked to correct the details of the paper to reflect this. We also recommend adding analysis on each body signal qualitatively in the final manuscript, as suggested by R4.  It will be great if the authors can consider this as a "conditional accept". In particular, the  self supervision  claim in the current version of the paper is misleading and this must be corrected in the final version. Note that this was also pointed out by the Program Chairs.
The authors study empirically and theoretically the behavior of neural networks under $l_\infty$ perturbations on the weight matrix. For this purpose they first derive bounds on the logit layer of the neural networks under perturbuations of a single or all layers. Then they propose to merge this bound (which depends on the product of the weight matrices) into a margin based loss function/cross entropy loss and suggest to optimize this bound while simultaneously penalizing the 1,infty norm of the weight matrices (which appears in the bound). Furthermore they derive a generalization bound for the robust error under weight perturbations.  There was a discussion among the reviewers over this paper. While several ones appreciated the setting, there was concern about that the bound is potentially vacuous and that the theoretical results are "messy". One reviewer criticized heavily the bound as not very useful and overly pessimistic.  This paper is in my point of view borderline but I argue for rejection. There are several reasons for this   the motivation for this paper remains unclear. While the authors argue that a network could be attacked by changing logical values, this seems at the moment unrealistic as if the attacker has already hacked into the system much more harm can be caused in a much easier way e.g. by directly changing the output of the network. But even if one considers adversarial bit error attacks to be realistic, then the threat model would be completely different from $l_\infty$ and would rather be like $l_0$ (with potential unbounded changes if the network is not quantized). If the target is to study that more flat minima generalize better, then the target would not be a bound on the robust error but a bound on the normal test error which integrates an upper bound which measures "flatness" of the function. As the derived Theorem 4 contains a term where one has to take the supremum over the product of matrices over all functions in the derived function class, this is not true for the derived bound. Thus I don t see why this bound is related to either of these two motivating topics.    the bound is incomplete in the sense that it contains the sup_f of the product of 1,infty norms of the weight matrices. Why did the authors not try to upper bound this term over the chosen function class? Even better would clearly to derive a bound on the Rademacher complexity in terms of the norms which are actually appearing in the bound. In the current way the terms in the bound and the chosen function class are mis aligned.    From a practical perspective the resulting loss is not useful for training deeper models (in the experiments a four layer network is used) as the product of the norm of the weight matrices grows exponentially in depth. Moreover, as pointed out the IBP bounds of Weng et al (2020) are much tighter than the bounds derived in this paper (and even IBP bounds are loose)     The experiments suggests that one gets a minor improvement in robustness while having to suffer from a significant drop in test accuracy (Figure 1b). Regarding the achieved robustness for epsilon 0.01 (it remains completely unclear how this noise model relates to the normal size of the model weights) one gets a robust error of around 30% on MNIST. This is much worse than what has been achieved for MNIST with much larger input perturbations (epsilon 0.3)    The authors are missing an assumption on the activation function. With just non negativity, monotonicity and 1 Lipschitz the upper bound in A.2.1 (original version) cannot be derived. I guess you are implicitly assuming that rho(0) 0 but this assumption is not stated. There are other typos in the main text. In fact in the original version Theorem 4 did not contain the term depending on b_h and s_j   this term was added in the revised version but not highlighted as all other changes.  In total there are too many open issues here. While I appreciate the hard work the authors put into the author rebuttal, I think that this paper needs a major revision before it can be published.  
This work targets an important problem: susceptibility of ML models to adversarial perturbations that make them completely misclassify an input, as opposed to "just" fail to get the right fine grained class while getting the correct coarse grained one. This natural question did not receive enough attention so far, so having this work look into it is a definite plus.  However, as the reviewers point out, this study has a number of issues in terms of the methodology of the experiments. For example, it is unclear whether the proposed (natural) variant of training the robust model is particularly beneficial for the stated goal. As such, it seems that the paper is not ready for publication and the authors are strongly advised to revise the article and submit it again.
# Paper Summary  This paper considers calibrating the output of a multiclass classifier in such a way that the output probabilities approximately are approximately "correct". They observe that if such a method is able to re order the logits, then it will change the accuracy of the classifier. Therefore, if they use a calibrator that is constrained to be monotonic in the input logits, then they can train it to optimize any metric they choose, without impacting the accuracy.  They propose minimizing ECE (expected calibration error, which is essentially a binned approximation to the L1 distance between the model s confidence in its top label, and the probability that the top label is correct). Borrowing an idea from local temperature scaling, they also allow their calibrator to see the input features, by also taking the top hidden layer (the layer before the logits) as an input.  Their experiments are, with one glaring exception, comprehensive: they have a good selection of datasets, a reasonable choice of metrics, and they dig pretty deep into what the results mean. Reviewer 2, however, believes that the baselines are far from state of the art, and two of the other reviewers (and I) agree.  # Pros  1. Well organized and well written (not exceptionally so, but above the bar) 1. Good insight overall. In particular, the observation that imposing a monotonicity constraint enables one to optimize any metric, including ECE, was considered both original and significant by the reviewers 1. Well thought out experiments. They were generally praised, aside from the (unfortunately crucial) question of whether the baselines are state of the art  # Cons  1. The paper seems to mainly discuss related work coming from the temperature scaling "tradition". The reviewers would like to see a more comprehensive discussion of other calibration approaches (Reviewer 2 provided a number of references) 1. There are some misstatements (e.g. that LTS is "state of the art"), and incorrect implications (e.g. that temperature scaling like methods are dominant). Reviewer 2 listed several of these, all of which are fairly minor, but more care should be taken 1. The TS and LTS baselines are not state of the art. The reviewers were generally impressed with the experiments, but the lack of a strong baseline is a fatal flaw  # Conclusion  This was a paper that initially received mostly positive reviews, but Reviewer 2 raised several concerns that were not adequately addressed in the author response, causing two other reviewers to lower their scores. Ultimately, three of the four reviewers recommended rejection.  The general consensus is that this is a well written paper, with good insight, well thought out experiments (except for the baselines), and that it overall makes a worthwhile contribution. The main issues, all of which were raised by Reviewer 2, are eminently fixable: (i) adding a more thorough discussion of related work, especially work unrelated to temperature scaling, (ii) being more careful to avoid misstatements, or to seem to imply incorrect statements (e.g. that temperature scaling like methods are dominant), and (iii) adding a couple of new state of the art baselines to the experiments.
The paper describes a framework for multi agent reinforcement learning that uses Markov Random Fields. Unfortunately, the paper is not clearly written and would benefit from significant revisions that improve its structure and make the model and approximations more explicit.  In particular, the paper says a graph says which agents $i,j$ communicate. This is typically called the "coordination graph" in this setting, see "Collaborative Multiagent Reinforcement Learning by Payoff Propagation", Kok and Vlassis, 2006. Note that within that paper they provide Q function decomposition, which can only serve to approximate the optimal policy.  The authors of this submission claim that an MRF is sufficient for optimal policies. I fail to see how this is true. In particular, Proposition 1 has to be checked more carefully. I tried to go through it, but it did not seem to make sense to me. Why is there an exp() term in the definitoin of the optimal trajectory probability? Why would minimising the KL divergence be enough to obtain an optimal policy? Perhaps it gives an optimal policy within the class of MRF policies, but that s not the same thing as the globally optimal policy.  Overall, I find the lack of clarity and in depth discussion of early related work disturbing, particularly with respect to the theoretical claims in the paper. 
This paper aims at answering an interesting question that puzzles the whole community of deep learning: why CNNs perform better than FCNs? The authors show that CNNs can solve the k pattern problem much more efficiently than FCNs, which partially contributes to the answer of the question.  Pros: 1. Studies an interesting question on DNNs. 2. Constructs a specific problem, the k pattern problem, so that CNNs can solve much more efficiently than FCNs.  Cons: 1. The analysis is only a very limited answer to the question. It only shows that CNNs are more efficient than FCNs on a very specific problem, which is of little interest to the community. On the one hand, people want to see the advantage of CNNs on more common problems, perhaps the image recognition problem (The AC understands that analyzing this problem is nearly impossible. It is just for hinting the choice of problems to analyze)? On the other hand, maybe others can find another specific problem that FCNs can solve much more efficiently than CNNs. If so, the value of this paper will be totally gone. The authors did not exclude such a possibility (Nonetheless, it is still a computational "separation" between CNNs and FCNs :)). 2. Reviewer #4 pointed out an issue in the proof. The response from the authors, though looked promising, did not fully convince the reviewer (in the confidential comment). Reviewer #3 also raised a question on the bounded stepsize. The authors should address both issues.  Overall, since the problem studied is of great interest to the community and the analysis is mostly sound, the AC recommended acceptance.
This paper proposes a transferable adversarial attack method for object detection by using the relevance map. Four reviewers provided detailed reviews: 2 of them rated “Ok but not good enough   rejection”, 1 rated “Marginally below” and 1 rated “Marginally above”. While reviewers consider the paper well written and using relevance map novel, a number of concerns are raised, including limited novelty, the lack of theoretical results, no use of the proposed dataset, insufficient ablation, etc. During the rebuttal, the authors made efforts to response to all reviewers’ comments. However, the major concerns remain, and the rating were not changed. The ACs concur these major concerns and agree that the paper can not be accepted at its current state.
The paper gives an elegant and efficient closed form solution for steering directions in the latent space of a pretrained GAN to to produce transformations in the image domain such as scaling and rotation etc,  this also extended to attribute transfer. The new method leads to "speed up, analytical transformation end points, and better disentanglement"  w.r.t to competitive methods.  All reviewers agreed on the merits of this work, and the good qualitative and quantitative results . The rebuttal addressed reviewers questions and concerns regarding the structure of the paper and its coherence.  Accept
This paper aims at learning disentangled representation at different level without the supervision signal of group information. To achieve this, the proposed UG VAE model uses both global variable $\beta$ to represent common information shared across all data, as well as a mixture of Gaussian prior for the local latent variable $p(z)   \int p(z|d)p(d)d$ where $d$ represents the assignment of the group for a particular datapoint. Experiments considered evaluation on unsupervised global factor learning, domain alignment and a downstream application task on batch classification.  Reviewers agreed that the proposed model seems interesting and novel, however some reviewers raised clarity concerns on how to interpret the learned representation by UG VAE. Revision has addressed this clarity issue to some extent, although some doubts from some reviewers still exists. Also reviewers raised concerns on less competitive experimental results, and the authors have updated the manuscript with improved results.   To me the main issues of the experimental section are (1) no quantitative result is provided regarding global factor learning and domain alignment, and (2) there is no other benchmark being studied in the experimental section. In my view, at least some other VAE representation learning baselines can be included in the batch classification section in order to demonstrate the real benefit of learning global factor based representations in downstream tasks. 
The proposed approach is interesting and is differentiated enough from the recent body of work on Neural Network Granger causal modeling as it offers a mechanism for detecting signs of causality.   The authors have satisfactorily addressed the points raised in the reviews. In particular relationship with prior work and novelty of the contributions are now clearly articulated. The added discussion on the superiority of TCDF on simulated fMRI experiments is insightful. Though prediction error is only a proxy for the task at hand, the readers will appreciate the added evaluation.   The proposed approach to stability evaluation leveraging the time reversal trick is novel and particularly pertinent, and could motivate some interesting follow up work on this topic. It is also important that the authors have characterized the computational advantage of the approach. 
The paper introduces a procedure that uses low attention areas to de noise temporal prediction. The paper appears to focus on  temporal noise  as opposed to constant noise present in the video (it may handle shifting shadows, but not background noise).   The idea is certainly interesting, however, the experimental protocol suffers from the issues pointed out by reviewer 3:   maintaining the same protocol as prior methods to ensure a direct comparison of the results against reported scores by the sota   in the context of attention, alignment (or lack or it) is extremely important; assuming perfect alignment is not very realistic (if the alignment is perfect, one might try a simple method such as taking all readings at a point over time and considering the mode, then correcting any outliers in the off attention areas)  These specific issues were not fully addressed during the review period.  The questions raised by reviewer 2 were addressed to a satisfactory degree in the rebuttal.
In this paper, the authors propose an RL based method for learning DAGs based on searching over causal orders instead of graphs. Order search for learning DAGs is a well studied problem, and it is well known that this can relieve some of the burden of searching through the space of DAGs. Several reviewers raised legitimate concerns regarding the experiments,  and without identifiability or theoretical results to advance the state of the art, the contribution of this work is limited.
This work tackles sparse or delayed reward problem in reinforcement learning. The key idea is to build a classifier to detect states that will lead to high rewards in the future and provide a bonus to those states. All the reviewers liked the idea but had issues with the execution of empirical results. The approach is evaluated only in a few Atari games skipping many sparse reward games and missing comparison to many exploration baselines. Furthermore, many reviewers found the writing confusing and hard to follow. The authors provided the rebuttal and addressed some of the concerns. However, upon discussion post rebuttal, the reviewers decided to maintain their score. Reviewers believe that the paper will immensely benefit with improved writing, evaluation on all atari games, and comparison to exploration baselines. Please refer to the reviews for final feedback and suggestions to strengthen the future submission.
There is definite consensus on this paper, with all reviewers expressing very favorable opinions. The author responses are very well articulated and address the main concerns expressed by the reviewers. The paper is very well written and the ablation study well executed. Some recent related work was missed in the original submission, but this was adequately addressed in rebuttal. The proposed approach is novel technique for feature representation learning. The clarifications to the manuscript and the new analyses are especially appreciated. 
This paper presents a zero shot generation approach by disentangling representations into swappable components (each component corresponding to an attribute) and then conditioning on any desired combination of attributes to do zero shot synthesis of samples containing those attributes.  There were some concerns raised in the original reviews which the authors have addressed in the rebuttal and the revised submission. Post the discussion phase, all reviewers see merit in the proposed ideas and unanimously recommend acceptance. Based on my own reading of the paper and the reviews/author responses, I agree with the assessment. 
This paper suggests a NAS approach for quantization which focuses on expanding the number of channels in problematic layers, given some uniform quantization level for all the layers. The reviewers were initially all negative, but the authors added more experiments and the scores changed to borderline (6/6/5). I think that though the reviewers appreciated the effort made by the authors and clarifying many of the issues. Yet, I think two concerns still remained. First, the method is demonstrated convincingly in only one large scale case (ImageNet)   on ResNet50 W2A2. The other case (ResNet18) is less impressive since it uses more parameters and has a small gap from EdMIPS. I think using more architectures (and possibly also precision levels) is important to convincingly demonstrate the utility of the method (e.g. EdMIPS used 5 architectures). Second, the novelty of the method in comparison to the previous method was not completely clear. Though the authors added more experiments on CIFAR to compare with previous methods, the significance of the results is not clear (small differences in the figures, and no error bars), and also the explanation of why the results of NCE should be better than TAS.
This paper proposes a method to improve the convergence time of PSRO. The paper was well received by all reviewers and is likely to be of interest to a similar sub community within ICLR, but may be of less relevance to the wider community not focused on multi agent learning.   A number of issues were raised by reviewers regarding the clarity of the originally submitted version of the paper. I encourage the authors to consider all constructive feedback given and revise the paper to maximise its impact. This will be of particular help in reaching a wider audience than those with pre existing experience with the methods this work builds on.
All Reviewers and myself believe that ICLR may not be the right venue for this paper. Hence, my recommendation is to REJECT it. As a brief summary, I highlight below some pros and cons that arose during the review and meta review processes.  Pros:   Important domain, but out of scope of ICLR.   Collection of sensor and environmental data, which may be potentially hard to collect.  Cons:   Not the appropriate venue.   Lack of machine learning novelty.   Potential lack of generalization of the proposed approach.   Experimental part is hard to follow.   Not very informative figures.
This paper proposed two algorithms for curriculum learning, one based on the the knowledge of a good solution (e.g. a local minima or a solution found by SGD) and another one proposed for natural image datasets based on entropy and standard deviation over pixels.   Reviewers seem to like the ideas behind the proposed algorithms and their simplicity. However, there are several major concerns that are shared among reviewers: 1  One of the algorithms needs knowledge of a good solution (e.g. a local minima or a solution found by SGD) which makes it impractical and the other one doesn t use any information about the mapping between input and the label. 2  Discussing previous work on curriculum learning, explaining how proposed algorithms are different than previous work and empirical comparison to other curriculum learning methods are lacking or need a significant improvement. 3  The experiment section needs improvement both in terms of experimental methodology and having more tasks/datasets.  Reviewers have done a great job at pointing to specific areas that need improvement. I hope authors would use reviewers  comments to improve their work.  Given the above major concerns, I recommend rejecting this paper.  
Dear Authors,  Thank you very much for your very detailed feedback to the reviewers. They have highly contributed to clarifying some of the concerns raised by the reviewers and improved their understanding of this paper.  Overall, all the reviewers acknowledge the merit of this paper and thus I suggest acceptance of this paper. However, as Reviewer #4 pointed out, there are conceptual and theoretical issues that need to be more carefully addressed. Please clarify these issues in the final version of the paper.
This paper proposes routing strategies for multilingual NMT. The motivation is to train a single mixture model that can serve the training and prediction of multiple models. Several strategies are proposed: token level, sentence level and task level. This is a simple and straightforward approach (which is fine). The main concerns from the reviewers regard novelty and missing comparisons. In their updated draft, the authors added comparisons to bilingual models and they added a discussion wrt related work. However, the author’s response did not address enough some of other reviewers’ concerns regarding comparison with other approaches, and the lack of novelty persists (mixture models for multi task learning have been previously proposed in the literature), which makes me lean towards rejection. I suggest the authors address these aspects in future iterations of their work. 
This paper aims to present a new representation learning framework for supervised learning based on finding a representation such that the input is conditionally independent given the representation, the components of the representations are independent and the representation is rotation invariant. While there were both positive and negative assessments of this paper by the reviewers, there are 3 major concerns that lead me to recommend rejecting this paper: 1. Most importantly, experiments do not seem to be conclusive as they do not properly ablate the specific aspects of this method. More specifically, the authors compare their deep learning based approach with non deep learning approaches but do not compare against deep learning baselines. This makes it impossible to assess the merit of the proposed approach (which also appears to be complicated) over much simpler baselines. 2. The required properties of the representations do not seem to be properly motivated.  3. The paper refers to their produced representations as disentangled representations. As pointed out by AnonReviewer4, this appears not to be consistent with prior uses of that word in the community. 
With reviewer scores of (7, 7, 9, 7), and with only one low confidence score (R5 s score of 7 with confidence of 2) it is obvious that the paper should be accepted.  
This is interesting work, but not yet sufficiently mature for publication.  Although the authors propose an novel algorithm and provide an analysis, the reviewers raised several criticisms about the comparison to previous work, the lack of any empirical evaluation, the strength and unnaturalness of the assumptions used to establish convergence.  After discussion, the reviewers remained largely unsatisfied with the author responses to these questions, and none recommended accepting this paper.
The consensus among the reviewers is that this is a borderline paper: its main idea is sensible and natural. Unfortunately, while the reviewers appreciated the authors  responses to their comments, they felt that the paper failed to demonstrate the usefulness of the idea beyond toy datasets. The latter would considerably strengthen this paper.
The paper provides a functional approximation of the error of ResNets and VGGs pruned with IMP and SynFlow on CIFAR 10 and ImageNet, showing that it is predictable in terms of an invariant tying width, depth, and pruning level.  In particular, it formulates the test error as a function of the density of the network after pruning and identifies a low density high error plateau, a high density low error plateau, and a power law behavior for intermediate density. It further demonstrates that networks of different sparsities are freely interchangeable. The paper provides an interesting insight on the power law structure of the error as networks are pruned, however the results are very limited to specific types of networks (ResNets and VGGs), pruning methods (IMP and SynFlow) and datasets (CIFAR 10, ImageNet). Hence, it s not clear if the proposed functional approximation generalizes to other network families, pruning methods, and datasets. I understand that adding a new architecture or dataset is expensive, but fitting the proposed scaling law (the five parameters) requires pruning only a small number of networks, as mentioned by the authors. Comparing the calculated error and the actual error of the pruned network for different architectures and datasets can help verify the findings in the paper, and significantly widens its scope.
The authors proposed to train an energy based model with a hierachical variational approximations. The entropy can be tricky in hierarchical variational approximations.  The authors suggest using the auxillary samples to guide an importance samples to compute the gradient of the entropy. They evaluate their approach on a slew of models. The idea is straightfoward and could potentially be applied to other hierarchical variational models out side of the energy based model setting.  The authors were responsive and clarified many agressive questions. I d ask the authors to clean up two things    Equation 8 would be easier to follow if it kept the expectation from   equation 6 thereby making z_0 feel like it materialize out of thin   air     A more detailed discusion of when the proposal is good and what could   be missed out	when relying on the generating z to center the proposal
This paper propose an approach to efficient Bayesian deep learning by applying Laplace approximations to sub structures within a larger network architecture. In terms of strengths, scalable approximate Bayesian inference methods for deep learning models are an important and timely topic. The paper includes an extensive set of experiments with promising results.   In terms of issues, the reviewers originally raised many concerns and the authors provided a large update to the paper. However, following that update and the discussion, several concerns remain. First, the reviewers noted that the originally submitted draft made claims about the optimality of the sub network selection procedure that were incorrect due to the use of a diagonal approximation. The authors subsequently retracted these claims and re focused on the idea that the subset selection approach is theoretically well motivated heuristic that performs well empirically. Following the discussion, the reviewers continued to express concerns about the heuristic nature of this procedure.   A second point has to do with scalability. The reviewers noted that the authors had only evaluated their approach on small data sets, leaving open the question of how scalable the method is. The authors responded by adding experiments on the same data sets using larger models, which does not squarely address the issue raised. Third, an additional point was raised regarding the lack of control of resource use in the experiments. The authors note that their approach can use more resources when available while many other methods can not. However, some methods including deep ensembles can also expand to use more resources, as can posterior ensembles produced using MCMC methods like SGLD and SGHMC. The authors need to consider quantifying space performance and time performance trade offs in the same units for different approaches to satisfactorily address this issue. While the authors added one set of experiments looking at deep ensembles in isolation, their conclusions that performance saturates for these models at low ensembles sizes seems to be hasty in some cases (e.g., deep ensembles show continued improvement for large corruptions in Figure 5(right) despite the claim by the authors that the models saturate after 15 epochs).   In summary, this appears to be a promising approach. While the authors made significant efforts to correct issues and address questions with the original draft, the majority view of the reviewers following discussion is that this paper requires additional work to more carefully expand on the revised results and to address the heuristic status of the sub network selection approach.
# Quality: The experimental evaluation is thorough and well designed.   # Clarity: After rebuttal, the paper is well written and clear in its contributions.  # Originality: The proposed approach builds over existing literature, as clearly indicated in the manuscript and acknowledged by the authors in the rebuttal, by mixing existing contributions in a novel fashion.  # Significance of this work:  This work deal with an important and timely topic. The experimental results are convincing and demonstrate strong performance for the proposed approach.  # Overall: This manuscript provides an incremental but solid contribution to the topic of model based reinforcement learning.  # Personal comments:   I disagree with Reviewer2, in that I feel that after the rebuttal the manuscript clearly and thoroughly states prior works and the novelty of the proposed approach. This is not a survey paper and should be hold to realistic standards.
The paper combines bi level optimization and reverse mode differentiation for flow estimation on networks. Most reviewers think the idea of incorporating physical constraints is interesting and novel, and the experiments convincing.
The paper provides some contribution to the field by exploiting in an unexplored  way background knowledge already covered by relevant literature. Presentation of the proposal is well structured and clear. The paper is also providing interesting theoretical and experimental results. The theoretical results, however, could be better explained.  Overall the proposed work seems to be incremental. Perhaps a deeper investigation into the relationships with diffusion augmentation would add more value to the contribution. 
The authors propose a learning approach based on mutual information maximization. By considering a view x, and two subviews, x’ and x’’, the authors provide a bound on MI by combining two InfoNCE like bounds on I(x’’; y) and I(x’; y | x’’). The authors show that optimising this (approximate) bound leads to improvements in several tasks covering NLP and vision.  This paper is aiming to address a significant problem for the ICLR community and provide a novel solution. The manuscript is well written and the main idea is clear. The reviewers appreciated the fact that the experimental setup covers both vision and NLP. On the negative side, the reviewers raised several major issues, both with the presented theory and the experimental setup. From the theoretical point of view, the approach hinges on a good  approximation for p(y|x ), which could arguably be as hard as the original problem. The author s response is definitely a step in the right direction, but the changes to the original manuscript are quite substantial and there is no time for a thorough validation of the updated claims. I will hence recommend rejection and strongly suggest that the authors incorporate the reviewers’ feedback and submit the manuscript to a future venue.
One reviewer is positive, but that review is not of high quality. The other reviewers agree that this paper is interesting, but has too many limitations to be accepted by a highly competitive venue such as ICLR.
This paper proposes to learn representations in an unsupervised manner using a generative model in which observations are generated by combining independent causal mechanisms (ICMs), in combination with a global mechanism. The authors introduce an unconventional mixture prior for the shared and independent components of the representation and train an encoder, discriminator and generator using a Wasserstein GAN with additional terms that enforce consistency in the data and latent space. Experiments consider variations of MNIST and Fashion MNIST and perform comparisons against a standard VAE, a β VAE, and the Ada GVAE.   Reviewers are broadly in agreement that this submission is not ready for publication in its current form. R4 in particular has left very detailed comments regarding clarity. The authors were able to in part address these comments, and R4 raised their score in response. That said, from a read of the manuscript in its latest form, the metareviewer (who is very familiar with literature on disentangled representations) is inclined to agree with the reviewers that this is work that has value, but is very difficult to follow in its current form. The metareviewer would like to suggest that the authors regroup, think carefully about how to improve clarity (in addition to addressing concrete points raised in reviews) and resubmit to a different venue. 
This paper develops an effective model free algorithm that achieves high sample efficiency. The empirical performance is appealing, which is comparable to model based policy optimization and significantly outperforms SAC. The paper is well written, and contains rigorous ablation studies. Weakness: the theoretical analysis is Section 3.1 is not thorough yet,  and it would be helpful to include more numerical comparisons with the Maxmin approach by Lan et al. 
This paper conducts a theoretical and empirical analysis of the Generative Adversarial Training method (GAT). Although many comments have been addressed in the rebuttal, the reviewers still have few (but important) concerns, including the memorization effects and the lack of comparisons. 
This paper explores losses and other training details to produce a model based agent for pixel input continuous control problems.  The authors present a rainbow like approach that combines various separate innovations into a single system.  They show an improvement over a previous baseline on this class of problem, and break down the contributions of the various components.  Though the paper was seen as clearly written, fundamentally, the reviewers did not feel they gained insight through the presentation of the experiments.  For example, one quirk brought up by multiple reviewers is that some combinations of methods show worse performance, but then adding yet another method makes things improved relative to baseline (the authors clarified that this was with the same hyperparameters).  Reviewers found this a bit confusing and insufficiently explored (i.e. was this just hyperparameter tuning or does just the right selection tricks actually need to be combined).  This confusion around method combinations is perhaps relatively minor by itself but indicative of how this paper did not build intuition for the reviewers.  Moreover, none of the reviewers were impressed by the magnitude of improvement over the baseline dreamer agent.  While it was acknowledged the the set of methods improved things, the reviewers felt that each innovation had already been independently validated as likely to improve sample efficiency, so the fact that they did so together was not especially insightful.      I d like to clarify for the authors that I believe this work was, in many respects, technically well executed.  Ultimately, based on the reviews and my own assessment, I don t think the scope was sufficiently ambitious considering the competitiveness of this conference.  While it is useful to occasionally produce summary works which pool a set of separate innovations, such papers must be insightful to readers, aggregate a sufficiently large number of innovations, and/or show striking performance gains.  The final reviewer scores are 4, 5, 6, 4.    
Dear authors,  as you have noticed this paper was not easy to review. I have hence invited 2 additional reviewers which I strongly respect and are very knowledgeable. After carefully reading the paper myself, I have to agree with one of the reviewers who said "... it [your paper] makes a good contribution to the literature ....". To be honest, we were working in my group on a very similar approach but did not manage to finish it (and I know how hard it is).  To conclude, when preparing to the final version, please try to go over the reviews, I am sure they can make your paper even stronger :)  
This paper presents a density ratio estimation approach to make the early decision for sequential data. The main contribution of this paper is the mathematical soundness of the proposed algorithm and all reviewers are unanimously positive about this paper with pretty good scores (7, 8, 6, 9, 7). However, despite the good scores, the verbal comments by the reviewers are not very strong except for one reviewer (R2); the reviewer with the highest score (9) did not provide detailed information about his/her rating. Also, the evaluation of this work is relatively weak because synthetic or simple datasets were employed for the experiment and the baseline methods are too straightforward. Also, it is not clear how the proposed algorithm can handle the data with sparse observations (data with idle times in the middle). Moreover, it does not provide rigorous stopping criteria although the authors proposed a simple method to determine the threshold, which is contradictory to the main objective of the proposed algorithm making early predictions on sequential data because the method requires "plotting the speed accuracy tradeoff curve on the test dataset." This response implies that it at least requires a withheld dataset. Although this issue can be regarded as a separate problem, the paper could have provided an ablation study with respect to the criteria.  Considering all these facts high scores but relatively low supports and confidences, and practical limitations, I would recommend accepting this paper as a spotlight presentation. 
Reviewers all agreed that this submission has an interesting new idea for learning object/keypoint representations: parts of a visual scene that are not easily predictable from their neighborhoods are good object candidates. Experimental gains on various Atari games are convincing. The main drawback at this point is that the evaluation is limited to visually rather simple settings, and it is unclear how the approach will scale to more realistic scenes. 
The revised paper is a solid improvement.  However, all reviewers and I find that there are still a number of issues that prevent the paper from being acceptable at the current stage.  For example, some important parts are still unclear, especially the definition of STI effect.  The observation of STI effect requires more theoretical or empirical investigation, in addition to a toy example.
It appears that this paper can benefit from additional detail and work before it becomes a stronger publication that is more convincing. The authors have done an impressive job responding to the reviewers and updating their paper, and multiple reviewers raised their score consequently. However, while multiple reviewers now recommend acceptance, there is no agreement on it. Even among the reviewers who recommended acceptance, there is a feeling on being on the fence specifically about the ability of the paper to make a convincing argument without considering a real life scenario and while only using toy settings. Indeed, this is a problematic aspect of the paper because the value of the paper lies in making that argument. Further, the paper would gain further from clarifying the writing further and connecting the paper more directly with the neuroscientific literature it aims to be connected to.
All reviewers agreed that the paper proposes some interesting and novel ideas on the use of OT for pooling. It also provides some nice insights and strong experimental results. As suggested by one of the reviewer, a discussion about the impact of the number of references may be of interest though.  
The reviewers found it hard to understand the motivation of using both oblivious sketching and maintaining feasibility throughout the course of the algorithm, given that the ultimate running times matched those of existing work. Because there wasn t a concrete improvement over prior work, the worry is what the impact of the paper would ultimately be. There was also a concern with novelty, similarity to the work of Cohen, Lee, and Song, and a reliance on fast matrix multiplication exponents. The paper could also benefit from an improved presentation. 
This paper compares “Graph Augmented MLPs” (GA MLP), which augment node features by a single aggregation of neighbors and then pass the resulting features through an MLP, to graph neural networks (GNNs). The paper establishes results on representational power of some GA MLP models being less powerful than GNNs. While practitioners may not change their behavior as a result, the work appears carefully done, is novel, and reviewers are mostly in agreement that the paper is a nice read and good contribution to the field.
This paper proposes a method for hierarchical decision making where the intermediate representations between levels of the hierarchy are interpretable. I personally really like this general direction, as did most of the reviewers. Unfortunately, it was felt that, even after discussion, this paper is not ready for publication. To summarize the general spirit of the objection to this paper, all reviewers found that the experimental section was faulty and did not match the claims of the paper. Specifically, criticism here surrounded first, the choice of experimental setting, which was not considered to be the best for testing interpretable hierarchical decision making approaches; and second, the choice of comparison/baselines, which did not give sufficient security that the results produced by the proposed approach were sufficiently impressive.  I am satisfied that the reviewers considered the paper fairly, gave constructive criticism, and took onboard the author feedback. As a result, I am recommending rejection. I nonetheless think that the high quality feedback provided here will enable the authors to prepare follow up experiments that may show their method in a more robust positive light, and encourage them to submit to a future conference, once armed with such results.
This paper provides a method of encoding inputs to a spiking neural network (SNN) using the discrete cosine transform (DCT). The goal is to create a more energy and time efficient means of doing inference with SNNs. The authors provide a description of the method, then show accuracy results on a variety of standard benchmarks. They also compare to a number of other methods for ANN and SNN based inference in the literature. Altogether, they show that their method allows for accurate inference using fewer spikes than other approaches, which can potentially reduce the energy used for inference.  The paper is fairly clearly written, and the results well articulated. The reviewers had a number of concerns, most notably related to questions of (1) clarity about the actual benefits of this approach, and (2) fair comparisons to other models. Altogether, the authors did try to address the reviewers comments, and at least one reviewer increased their score.   However, the actual scores for this paper remained very close to the acceptance threshold, and the first point was difficult to rebut without a lot more added to the paper. Ultimately, this paper is using a classic signal processing strategy to improve SNN run time, and the reviewers asked for some reason as to why that is desirable/novel. The author s answer was effectively that SNNs provide promise for low energy edge computing, and their method could make SNNs for edge computing even more efficient.  This is potentially of interest for edge computing, but the paper could do a lot more to demonstrate that. Specifically, some consideration of how this would actually operate on spiking chips or a more robust estimate of energy efficiency than that given at the end of section 4 would be required to make this paper a clear accept. Notably, the paper does not demonstrate that this technique could be used to significantly reduce the energy requirements for spiking chips, relative to other SNNs, just that this is more energy efficient than ANNs, which is already known for other spiking neural network approaches. Given this, and the scores relative to other papers at ICLR, a "reject" is recommended. However, the AC notes that this was a difficult decision, and this paper was right at the threshold.
This paper proposes a new method of learning ensembles of neural networks based on the Information Bottleneck theory, which increases the diversity in an ensemble by minimizing the mutual information between latent features of the different ensemble models. It shows promising results on classification, calibration and uncertainty estimation. The paper is well written and the comments were properly addressed.
The paper proposes to model uncertainty by combining quantile regression and Chebyshev polynomial approximation. The paper addresses the important problem of uncertainty quantification for black box models. However, some major concerns remain after the discussion among the reviewers. In particular, there has been some concerns around the clarity of the presentation. The proposal lacks a clear use case, e.g. where satisfying constrained black box uncertainty problem is a must have. 
The paper is well written, it is clear and concise. The idea of learning to generate text from off policy demonstrations is interesting. The results experimental results are good. The authors seem to address the concerns raised by the authors during the rebuttal.
All reviewers appreciated the main result in the paper, which gives  global optimality guarantee for constrained policy optimization for both tabular setting and NTK setting. However, there were a number of unclear parts of the paper reported by several reviewers (assumptions, hyperparameter tuning, complexity dependence on the number of neurons, experimental setups). On top of it, the AC also echoes with R1’s concern about the novelty of this work as it basically stacks existing results (TD by Dalal et al., Neural TD by Cai et al. (2019), NPG by Agarwal et al, CSA algorithm by Lan & Zhou).  These concerns made me reticent to recommend acceptance at this point. I strongly encourage the authors to continue their interesting work in considering the reviewer comments and strengthen the numerical experiments.  
This paper studies the problem of computing a similarity measure between two pieces of code. The main contributions are a configurable alternative (CASS) to abstract syntax trees (ASTs) for representing code and a model for embedding these structures within a Siamese net like architecture. While parts of the ICLR community that make use of ASTs would likely find interest in the options provided by CASS and the associated experiments, the contribution is mostly around feature engineering of AST like structures for one specific application, which is quite niche. The machine learning modeling appears fairly standard. Thus in total, I don’t see enough here to recommend acceptance.
This paper proposes a graph neural network architecture to learn representations for directed acyclic graphs. Specifically, the proposed method performs the aggregation of the representations from neighboring nodes in the topological order defined by the DAG, with a novel topological batching scheme, which allows to process the message passing operations in parallel. The authors propose theoretical analysis of the proposed methods, to show that it is invariant to node indexing and learns an injective mapping to discriminate between two different graphs. The proposed method is further experimentally validated on multiple tasks involving DAGs, and the results show that it outperforms existing GNNs, including existing methods that can capture DAGs such as D VAE (encoder).   The reviewers were unanimously positive about the paper. All reviewers find the performance improvements and time efficiency obtained with the proposed method to be satisfactory or promising, and one of the reviewers (R4) mentions that the tackled problem is important and the paper is well written. However, there were concerns regarding insufficient explanations, missing ablation studies, and missing details of some parts of the proposed method. Yet, most of the issues have been satisfactorily addressed during the interactive discussion period. I agree with the reviewers that the paper is tackling an important problem, find the paper well written, and the proposed DAGNN as practically useful. Thus I recommend an acceptance.   However, the contributions of the proposed work over D VAE, which also deals with DAGs, should be better described, as also noted by R2. The DAGNN uses attention, and can stack multiple layers as it is a more general GNN framework while D VAE is a generative model, but these seem like incremental differences over D VAE, and it is not clear which contributes to DAGNN’s superior performance over D VAE. Topological batching is a clear advantage of DAGNN over D VAE, but the experimental results showing the advantage of it over D VAE’s sequential training was missing in the original paper (while it was added later to the appendix). I suggest the authors to introduce D VAE in the introduction, acknowledge that it also tackles DAGs, and clearly describe how the proposed method differs from D VAE encoder in a separate section. Also, there needs to be an analysis on why the proposed DAGNN outperforms D VAE, as well as time efficiency comparison with the original D VAE in the main text.  
The authors suggest a VAE model for causal inference. The approach is motivated by CEVAE (Louizos et al., 2017) which uses a VAE to learn a latent representation of confounding between the treatment, target, and covariates. This paper goes beyond this approach and tries to design generative model architectures that encourage learning disentangled representations between different underlying factors of variation inspired by Hassanpour & Greiner (2019).   The reviewers agreed that the topic will be of interest to a large group of readers. While the first version of the papers raised questions about the experimental design, several questions on the architecture design were addressed during the rebuttal period (e.g., deeper architectures). Other improvements were suggested and not adopted (e.g., alternative methods to achieve better disentanglement). The ablation studies seem to suggest that some of the loss terms are not actually needed and that non probabilistic autoencoders (beta 0) also work well. We recommend aiming at improving the writing quality and coverage of more background material on the proposed architectures and causal factors.  
This paper investigates the problem of unsupervised domain adaptation and proposes a framework based on a specific type of disentangled representations learning. The paper is well written and the proposed method seems plausible. However, according to Reviewers #3 and #4, the proposed framework does not seems to be sufficiently different from existing ones, and the empirical results do not seem convincing enough.   Please also double check in C3, whether T and S should be marginally independent or conditionally independent conditioning on X.
The paper analyzes the behavior of random search based NAS and provided new insights (e.g., a low ranking correlation among top 20% candidate architectures in the search phase). An extensive set of experiments were also conducted. However, most reviewers found the incremental nature and similarity with previous works to be a concern. I would encourage the authors to better position their work and better explain the novel methodological aspects. 
This paper proposes a method for conditional inference with arbitrary conditioning by creating composed flows. The paper provides a hardness result for arbitrary conditional queries. Motivated by the fact that conditional inference is hard the paper therefore suggests a novel relaxation where the *conditioning* is relaxed.  There were various concerns from the reviewers regarding notation, comparison algorithms, and how the hardness result motivates the smoothing operation introduced. After careful study of the paper and all the comments I find that I am most concerned about the hardness result and how it motivates the smoothing operation that is done. Novel computational complexity results *as such* are not really in the scope of ICLR. There s nothing wrong with having such a result in a paper, of course, but a paper like this should be evaluated on the basis of the algorithm proposed.  Like R4, I do not follow how this hardness result is meant to motivate the smoothing that s applied. The paper is unambiguous that the goal is to do conditional inference. A hardness result is presented for conditional inference, and so a relaxed surrogate is presented. This has a minor problem that it s not clear the relaxed problem avoids the complexity boundary of the original one. There s a larger problem, though. The hardness result has not been sidestepped! The goal is still to solve conditional inference. The algorithm that s presented is still an approximate algorithm for conditional inference. R4 suggests that other approximation algorithms should be compared to. The authors responded to this point, but I am not able to understand the response. For the same reason, I think it is valid to ask for comparison to other approximate inference algorithms (e.g. without smoothing)  None of the above is to say that the smoothing approach is bad. It may very well be. However, I think that either the existing argument should be clarified or a different argument should be given.  Finally here are two minor points (These weren t raised by reviewers and aren t significant for acceptance of the paper. I m just bringing them up in case they are useful.)  Is Eq. 3 (proof in Appendix B.1) not just an example of the invariance of the KL divergence under diffeomorphisms?  Proof in appendix B.2 appears to just a special case of the standard chain rule of KL divergence (e.g. as covered in Cover and Thomas)
The paper studies the behavior of the intermediate ReLU like activations of trained neural networks and show empirically that the intermediate activation can be used as a hashing function for the examples with some key advantages, including almost no collisions and that there are desirable geometric properties (i.e. can use k means, k nn, logistic regression on these embeddings).  Pros:   The experimental analysis was solid and thorough investigating the effects of model size, training time, training set, regularization and label noise.   Cons:   An overall lack of novelty. It is already quite well known throughout the ML community that in many cases, using the intermediate embeddings serve as useful features to apply more classical methods such as kNN and clustering.  Overall, the reviewers appreciated the solid and thorough investigation into the hashing properties of neural network activation patterns, which convincingly confirms some intuitions about the behavior of activation patterns in neural networks. However, the reviewers also agreed that there was no significant new finding. There have already been many studies on clustering and kNN on the embeddings of a network. Thus, the core novelty of the paper appears to be the finding that almost every linear region has at most one datapoint after training (which does not seem too surprising given Hanin & Rolnick (ICML 2019)); however, without further novel implications of this finding, the impact of the paper is limited.
The paper uses free algebras for sequential data representation, and two of the reviewers and the AC find this highly innovative. There were numerous small issues brought up by reviewers (and reviewers disagreed some on the presentation), in particular R3 asking about the experiments, some of which were addressed in the rebuttal.  Overall, because the idea was unusual, it s a bit hard to place and judge this paper. In the end, in the opinion of this AC, the ideas are very creative and there is enough of a chance that this paper could become a very highly cited work, hence we recommend its acceptance.
This paper presents work on scene graph grounding under weak supervision.  The reviewers appreciated the consideration of this task and formulation of a solution for it.  However, concerns were raised over the importance of this weakly supervised grounding task, how it addresses challenges in previous methods, the empirical evaluation, insights obtained, motivation, and clarity of exposition.  After reading the authors  response, the subsequent discussion and reconsideration resulted in a sense that while the task is new, the overall contribution and remaining questions over empirical evaluation mean the paper is not yet ready for publication at ICLR.
I thank the authors for their submission and very active participation in the author response period. The paper is well written [R3,R4], tackles a hard problem [R4] in a novel way [R4] with interesting and convincing results [R2]. R3 noted that an empirical comparison to POET would be appropriate. However, in my view the authors addressed these concerns in a satisfactory manner. It seems that R3 has not updated their assessment nor confirmed their current score based on the author response. I am therefore discounting the only review voting for rejection and am siding with R1, R2 and R4. Thus, I recommend acceptance.
This paper addresses an interesting learning problem of a generative neural network on a simulated ensemble of protein structures obtained using molecular simulation to characterize the distinct structural fluctuations of a protein bound to various drug molecules. The main technical contribution is a geometric autoencoder architecture with separate latent spaces for representing intrinsic and extrinsic geometry. However, the reviewers think the benefit for modeling intrinsic and extrinsic geometry is not clearly explained and the experiments are not convincing at the moment. The paper can be potentially improved by addressing these two main issues. 
The approach explore the use of Conditional Risk Minimization (CRM) as a post hoc operation to amend a classifier decision by averaging a prior class hierarchy. The authors show that it is beneficial for ranking predictions without sacrifying top 1 accuracy.  The rebuttal period clarified some reviewers  concern on paper presentation and experiments, and all reviewers recommend acceptance after the discussion period.  Although the approach is simple and directly revisits the use of CRM for deep models, the AC considers that the contribution is meaningful, and that the proposed method provides predictions with good ranking and calibration properties. The paper also sheds light into interesting issues in state of the art methods integrating class hierarchies during training. The AC therefore recommends paper acceptance. 
The authors propose a new parameterization which (across multiple architectures) generalized hypercomplex multiplication and provides for small low dimensions strong performance at substantial parameter savings. All reviewers are happy with the theoretical contributions of the work, but would appreciate additional empirical evidence.
The paper introduces a linear projection method, inspired by ANOVA, for finding a supervised low dimensional embedding.  A positive aspect is that the method is straightforward, and it is even slightly surprising that in the family of linear models, there still was an uncovered "niche".  The paper was considered useful for the purpose studied in the paper, single cell RNA seq data analysis. But to claim broader usefulness, more evidence should be presented.  One particular detail which was brought up by all reviewers was the PCA preprocessing. For ICA it is a sensible choice, as linear ICA is essentially "just" a rotation of the PCA components. But the justification is not as good for a supervised method. PCA may be necessary in practice, but may lose important category relevant information.  The paper still needs a significant revision before publication.  Even though the method is straightforward method, a lot of time and discussion was required for expert reviewers to understand it. 
The idea of using multiple sparse matrices seems to be new, but the novelty of the idea alone isn t enough to convince the AC and reviewers (indeed, the idea might not be new, but has never been discussed in literature because of the drawbacks we discuss here). As the authors and reviewers/AC seem to agree, the actual benefits of sparse matrix multiplies are hard to realize, especially on embedded devices, so the contributions at this point are mainly hypothetical and only about the new idea. Each reviewer brought up issues (even the most positive reviewer) and mostly the reviewers were not persuaded by the rebuttal. In short, there wasn t evidence that this new idea could really contribute to the state of the art.  This is now a fairly crowded topic (e.g., all the  papers brought up by R3 in just that one class of methods), and new papers should beat state of the art and/or introduce new theory   an example would be a paper from last year s ICLR, https://openreview.net/forum?id HJfwJ2A5KX , "Data Dependent Coresets for Compressing Neural Networks with Applications to Generalization Bounds" (Baykal, Liebenwein, Gilitschenski, Feldman, Ru) which not only gives an efficient technique (not based on sparsity of weights) but also gives types of generalization guarantees.  As R1 said, the results are not state of the art, and we have to believe the authors that "an iterative like extension of our method could reach even better results". The rebuttal says that the paper s goal is to "pioneer a new approach to neural network compression". But if you can get better results with something better than Palm4MSA, then please do so, and demonstrate the evidence!  Right now, the paper assumes we could implement sparse multiplication efficiently on embedded devices, and assumes we could get better results: both these are quite hypothetical. The AC encourages a resubmission of this paper after these results have been addressed.
The paper proposes multiplicative filter networks (GaborNet and FourierNet) as functional approximations of deepnets. The proposed networks are a sequence of multiplications linear functions of sinusoidal or Gabor filters. The authors show that in some cases the performance of proposed networks outperforms the existing deepnets using ReLu activations. This representation is notably simpler as well. Moreover, compared to classical Fourier approach, the proposed method scales to higher dimensions in practice as well. The downside of the paper is that it is not clear how to empirically use exponentially many Fourier functions. Moreover, proposed methods have more parameters, and the additional parameters are linear in size of the hidden layer.  The paper is clearly written and the authors improved the quality of the paper and added additional experiments to support their claim through the review process and I appreciate that.
This paper is accepted, however, it could be much stronger by addressing the concerns below.  The theoretical analysis of the proposed methods is weak. * As far as I can tell, the proposition has more to do with the compatible feature assumption than their method. Furthermore the compatible feature assumption is very strong and not satisfied in any of their experiments. * Sec 4.2 does not provide strong support for their method. R2 points out issues with their statements about variance and the next subsection argues from an overly simplistic diagram.  The experimental results are promising, however, R3 brought up important issues in the private discussion: * Their implementation of SAC systematically produces results worse than reported in the original paper (they use a version of SAC with automatically tuned temperature https://arxiv.org/pdf/1812.05905.pdf); 1a) Their SAC gets average returns of 2.5k at 500k steps while the original implementation gets 3k at 500k steps; 1b) Their SAC on HalfCheetah 10k at 1M steps, original paper   11k at 1M steps; 1c) The same applies to Humanoid, there is no improvement with respect to the original SAC; * Their approach degrades performance on Hopper.  * They use non standard hyper parameters for SAC. 0.98 instead of 0.99 for the discount and 0.01 instead of 0.005 for the soft target updates. That might be the main reason why their SAC works worse than the original implementation.  * The authors use the hyper parameters suggested for HalfCheetahBulletEnv for all continuous control tasks. For HalfCheetah, however, the authors of the stable baselines repository (which this paper uses) suggest to use the hyper parameters from the original SAC paper (https://github.com/araffin/rl baselines zoo/blob/master/hyperparams/sac.yml#L48). Nonetheless, the results for the unmodified SAC reported in this work for HalfCheetah/Hopper/Walker/Ant are subpar to the original results, suggesting that the hyper parameters for HalfCheetahBulletEnv are suboptimal for these tasks.  Given the simplicity of the change and the promising experimental results (with some caveats), I believe the community will find this paper interesting and will lead to followup work that can patch the theoretical gaps.
The reviewers appreciated the paper s applied neural net approach to the problem of designing features for 2SLS regression for IV analysis as an alternative to sieve approaches. The paper would make a good contribution to ICLR. While the paper does not focus on theory   learning data driven features appears to be mostly heuristic   it should still be grounded in a sound approach to the IV problem, and the reviewers recommend various important technical clarifications regarding the foundations of IV models; the authors should implement these suggestions very carefully and correctly in future versions. For example, even if the structural models are well specified in that Eq. (5) holds for some parameters, since the dependence is non linear on parameters, it is not clear when we should expect this to be identifying of (theta_X,theta_Z) (these are in fact not identifiable in general) and when we should expect the proposed method to be consistent.
This paper proposes a self supervised learning algorithm to compute object centric representations for efficient RL in the context of robot manipulation tasks.  The key idea is to learn an object centric representation (using prior work on SCALOR) and use this to intrinsically generate goals for a SAC policy to achieve. The policy is a goal conditioned attention policy. The evaluation metric is a set of tasks to manipulate objects for a visual rearrangement task.   ${\bf Pros}: $ 1. The baselines are reasonable and consist of other unsupervised RL algorithms in recent literature.   2. Object oriented RL is a growing area of interest and this paper proposes a reasonably novel and validated set of ideas in this domain. I believe it will be of significant interest and potentially make an impact on research in robotics and deep reinforcement learning.  3. The goal conditioned attention policy can handle realistic scenarios, namely   multi object manipulation tasks  4. The attention mechanism also provides a reasonable solution to mitigate combinatorial hardness in multi object environments  ${\bf Cons}$:   1. Some of the reviewers felt that the experimental results from pixel inputs could have been pushed further. However, since the setup and algorithm is relatively novel, there are already many moving parts and this paper seems like a step in that direction  2. Experiments with larger set of objects would have been interesting to investigate and report.      
This paper proposes the use of federated learning to the application of steering wheel prediction for autonomous driving. While the application is new and interesting, the reviewers felt that the approach and results were mostly empirical. I suggest that the authors improve the conceptual/algorithmic contribution of the paper in a revised draft. Another suggestion is to include a better explanation of hyper parameter optimization used in the experiments. I hope that the reviewers  constructive comments will help the authors revise the draft adequately for submission to a future venue!
The paper seeks to answer the question on the necessity of the self attention matrix in Transformers and whether it is possible to synthesize it by alternate means other than pairwise attention.   The reviewers appreciated the main general idea and the wide range of experiments conducted.  However, there are some concerns on clarity and evidence supporting main claims. While authors tried to address certain concerns through revision and response, the results suggests that self attention matrix is still needed for strong performance and cannot be fully replaced by synthesizers. While authors also acknowledge this in discussions, If we do not consider the combined models (R+V or D+V), the empirical results do not look very convincing on the competitiveness of Synthesizers. They are only competitive on MT/Dialogue while Failing quite considerably on GLUE and Summarization. Overall, I felt very positive about the direction the paper pursues, but the empirical results doesn t seem to fully support the claims.   Quoting some points from reviewer discussions:  > `Comment: Moving towards an analytic framing would necessitate having the bare minimum set of experiments/comparisons before running additional analyses, but the bare minimum is still needed for this paper.  > Comment: I think the paper needs a round of revision and experiments need additional, carefully chosen baselines to adequately present synthetic attention in the context of existing solutions.  > Comment: this is not a reason that some random explored idea should be viewed as a great contribution, given that there are already several theory grounded papers appeared in ICML (linear attention..), NIPS (Linformer, follow up work from linear attention..), and ICLR (Random feature attention, Performer..) this year. Compared to those theoretically well motivated attention modification papers, this work is not that solid.
This paper studies efficient robust training. The key idea is to use backward smoothing as an advanced random initialization to improve a model s adversarial robustness. The approach is sound, well grounded, and quite logical. Results demonstrate the effectiveness.  However, there exists some limitations:  1) Andriushchenko & Flammarion, 2020 gives a better and more fundamental explanation on how to address fast adversarial training.  2) Backward smoothing does not generalize to standard adversarial training. In other words, it only works for KL divergence loss rather than cross entropy loss, and it seems that backward smoothing does not address the fundamental problem of fast adversarial training.  3) If we compare the performance of Fast TRADES and Backward Smoothing since Backward Smoothing intends to improve Fast TRADES, there is always a tradeoff between clean accuracy and adversarial robustness, e.g., Table 4 and Table 8.   4) Randomized smoothing is helpful for one step adversarial training and randomized smoothing in general seems to be orthogonal to the proposed method. Moreover, 2 step PGD training can perform similarly well to backward smoothing while being much simpler conceptually.  In the end, I think that this paper may not be ready for publication at ICLR, but the next version must be a strong paper if above limitations can be well addressed.
Rather than using backprop to train RNNs, this paper explores instead using GA s to train them along with an extra Minimal Description Length objective to search in the space of the simplest possible networks that can perform the task at hand. They demonstrate that the method can indeed find minimal RNNs that, when trained even on small corpus dataset of formal languages can generalize beyond the training data.  Most reviewers and myself agree that this work is really interesting, and also refreshing to see a new approach compared to the typical way of doing things. However, as we can see in the reviews, the work is not at the level of an ICLR conference submission at this point. R1, R3, and R4 s reviews breaks down the points of the papers into strengths and weaknesses, and I am inclined to believe that if the authors spend more time to try to address the weakness and improve the work, this can be a great paper in the future. Although R3 gave a score of clear reject (which is too low IMO), and the authors responded to their points, I m inclined to believe that this work warrants another revision.  Specifically, reviewers (and myself) believe that the baseline methods can indeed perform better than reported. And while, for a novel method, we don t expect the approach to scale to SOTA approaches for sequence modeling, it would improve the work vastly if there is evidence to show that it can scale to larger tasks, and give an impression that there can be a roadmap of attacking larger problems that standard methods can currently handle.  Currently, I would say the work is a great workshop paper, but would encourage the authors to continue to consider the feedback given here to work on a future revision.
Overall, the reviewers agree that there is definite value in the empirical evaluation you have provided. However, as you have acknowledged in your responses to the reviewers, the presentation could be significantly improved. A final point that was not touched upon by the reviewers where possible (e.g. certainly not ImageNet, but for some of the smaller datasets in Table 1) it would be helpful to have a comparison to fully Bayesian methods (you have linear regression and GPs, but I don t see the implementation details; my suggestion is to implement these within an MCMC framework, specifying reasonable priors over the (hyper)parameters).
The paper proposed a useful incremental extension to the monotonic GMM attention by incorporating source content. It has shown comparable performance for online and long form speech recognition, but falls behind on the machine translation task. For online ASR, it would be more convincing to include latency comparisons across different streaming models besides WERs.  The presentation of the paper can be further improved although it already got better based on reviewers  comments. As in the discussion, a more accurate description of the method would be "multi head Gaussian attention" instead of GMM attention.  The main factor for the decision is limited novelty and clarity can be further improved.
This paper proposes a contribution aiming at understanding the cause of errors in few shot learning. The motivation is interesting but the reviewers pointed out many aspects that require more precisions and polishing in addition to the fact that the upper bound provided it rather loose. The rebuttal provided addresses some concerns, but there are still some remarks that require some clarifications en work. Hence, I propose rejection.
The paper introduces a new method for encoding dynamics of temporal networks.  The approach, while not ground breaking, is interesting and the results are fairly convincing.  The submission raised a number of concerns from the reviewers. They questioned the complexity of the proposed approach (R3 and R4), the clarity/readability (R2 and R1), and appropriateness of the link sampling strategy (R2), as well as raised several more minor (from my perspective) issues. I believe that the authors adequately addressed most of these concerns in their rebuttal and the revision.  R2 has confirmed that they read the rebuttal and raised their score to strong accept. Unfortunately, the other reviewers have not engaged during the discussion period, and it is unclear if they are satisfied with the clarifications and changes. Nevertheless, after reading the authors  responses and skimming through the manuscript, I believe that most concerns have been addressed, and this is a good paper that deserves to be accepted. That being said, the issue of readability has been raised by the reviewers, and, while I do not think the paper is unreadable, I do agree that there is much room for improvement. I would encourage the authors to polish the manuscript for the camera ready version, as well as try to address the remaining concerns raised by the reviewers.   
Reviewers agreed that connecting neural networks with dynamical systems to create a new kind of optimizer is an interesting idea. After the authors  improvements, this is a strong submission of wide interest.
This is an excellent paper that provides analytical and empirical insights on the sample selection bias of pool based active learning. It provides two very practical methods of removing the bias.  Also, it shows that in over parameterized networks (like modern neural networks), the active learning bias could actually be useful.   I enjoyed reading the paper.  Reviewers are mostly very positive about the paper.  Experiments in the initial version were limited, and the authors have since added more experiments.   With the increasing interest on learning with limited data, this paper is very timely and useful.  I expect the paper to be of interest to many in the community.
On the positive side, this is a quite nice empirical exploration of the interaction between data parallelism and sparsity for training neural networks. The experiments are broad and detailed. On the negative side, the empirical results recapitulate what would be expected and what has already been seen in the literature, as the authors themselves point out ("We note that our observation is consistent with the results of regular network training presented in (Shallue et al., 2019; Zhang et al., 2019)."). And the theory presented, while it does explain the results nicely, is a trivial reformulation of the standard convergence result given in Equation 2. So while this is an interesting paper and the reviewers rated it positively on average, the sparsity exploration is _much_ more novel than the data parallelism exploration, and there are significant novelty weaknesses that need to be taken into consideration.
This paper is overall well written and clearly presented. The problem of ordered data clustering is relevant, and the proposed method is effective.  During the discussion, all reviewers agree with the strength of this paper and share the positive impression. The authors successfully addressed reviewers  concerns by the careful author response, which I also acknowledge. One of the reviewers raised the concern about the broader impacts, while it is also well addressed in the author response.  I therefore recommend acceptance of the paper.
In this paper, the authors provide a Riemannian version of gradient descent/ascent for min max problems on manifolds. Assuming a tractable retraction mapping for the descent/ascent step, the authors provide a complexity analysis for finding a (local) saddle point in the spirit of Lin et al. (2020).  The paper received three negative recommendations and one positive, with all reviewers indicating high to expert confidence. After my own reading of the paper, I concur with the majority view that the paper does not clear the (admittedly high) bar for ICLR. After discussing the paper with the reviewers, the concerns that led to this recommendation are as follows: 1. On the proposed examples: it is not clear what exactly is the motivation for the DNN training example with orthonormality constraints on the weights. In the paper, $x$ is the vector of DNN weights, so it lives in some real space $R^d$. The authors subsequently assume it is constrained to live on some Stiefel manifold, but which one? The Stiefel manifold is the set of all orthonormal $r$ frames on $R^d$, and the formulation in Section 6 doesn t clarify things. Moreover, the papers cited by the authors either concern the initialization of the DNN or a regularization by an orthonormality/orthogonality penalty. This is quite different since DNN training typically involves at least some neurons with very large weights (which is of course disallowed if the weights are constrained to live on some "norm 1" subspace). 2. The presentation is often lacking in mathematical rigor: in a Riemannian setting, it is crucial to distinguish between the Riemannian distance function and the Riemannian norm. However, the two were used interchangeably at several points, and the authors  revision wasn t satisfactory in this regard   even the basepoint for the norm is missing in cases where it is not made clear from the context on which point the norm is considered. 3. The cost of applying retraction based methods in DNNs is also unclear. On a Stiefel manifold, the only known retractions involve SVD, so they have a superlinear computational cost relative to the size of the input matrix. If the dimensionality of the input matrix is that of the parameter space of the DNN, the efficiency of the method seems somewhat limited.  The above concerns regarding DNN training are perhaps less important if we view this as a primarily theoretical paper. In that regard however, the novelty of this paper over that of Lin et al. (and other Riemannian minimization papers) is not clear, so I am forced to recommend rejection at this stage. That being said, I believe that a thoroughly revised version of this paper could ultimately be publishable at one of the top venues of the field, and I would strongly encourage the authors to pursue this.
The paper proposes  to effectively learn representation of 3D data (point clouds/meshes) using a spherical GNN architecture over concentric spherical maps. A method for converting point clouds to concentric spherical images is also proposed. Evaluation is done via 3D classification tasks on rotated data.  Strengths:   Interesting novel method for learning 3D representations   Technically sound   Performs similarly to spherical CNNs and other STOA on the Modelnet40 dataset  Weaknesses:   Presentation of the work needs to be further improved such that it is easier for others to reproduce   More in depth experiments are needed to justify how much Spherical GNN improves over other STOA, particular given how classification accuracy is very similar to STOA.
This paper overall received borderline negative scores. All the reviewers agree that the paper proposed an interesting approach to exploration for RNN based recommender systems. However, there are concerns around the experiments as well as the theoretical contribution. Specifically, a few reviewers pointed out that the experimental results are not convincing. Furthermore, some reviewers mentioned that the theoretical analysis is a relatively straightforward extension of LinUCB (to which the authors disagreed, but there wasn t any followup discussion from the reviewers). The bandit analysis is unfortunately outside my expertise.   I think this paper touches upon two different domains (RNN based recommender systems and bandit). If the authors consider that their contribution mainly lies in the latter, maybe ICLR is not the most suitable venue. 
Symmetries play an important role in physics, and more and more papers show that they also play an important role in statistical machine learning. In particular, employing symmetries might be the key to improve training and predictive performance of machine learning models.  In this context, the present paper shows how previous physical knowledge can be leveraged to improve neural network performance, in particular within Deep dynamic models. To this end, they show how to incorporate equivariance into resnets and u nets for dynamical systems. On a technical level, as pointed out by the reviews and also clearly mentioned by the authors, the basic building blocks are well known in the literature. However, dynamical systems also raises their own challenges resp. laws when it comes to modelling symmetries, as the authors argue in the paper and also clarified in the rebuttal. For instance, it pays off to adapt the techniques known from the literature deal better with scale, magnitude and uniform motion equivariance. This is a solid contributions and will help many other who want to apply DNNs to dynamic and physical models. 
The paper proposes an RL based approach for decision based attack. All the reviewers like the paper after the rebuttal phase, and we would like to encourage the authors to incorporate the new experiments in the camera ready version. Furthermore, some recent decision based attacks should also be included in the comparisons, such as  Li et al., QEBA: Query Efficient Boundary Based Blackbox Attack. (CVPR 2020)   Cheng et al., Sign OPT: A Query Efficient Hard label Adversarial Attack. (ICLR 2020)
All reviewers agree that the paper brings new knowledge in the field of locally supervised learning, and as such it should be accepted.  The authors should keep all reviewers  comments into account when preparing their camera ready version.
The range of the initial reviews was fairly high with overall scores ranging from 4 to 7.  The authors provided a good response that answered most of the reviewers  comments and questions. One of the reviewers even increased their score following the authors  response.   The focus of some of our discussions and what ultimately led to my suggestion was the related work of MIR [1]. The methodological differences between (the three versions of) MIR in [1] and GMED [this paper] appear less significant than what the current submission suggests. While there is some disagreement between the authors and Reviewer1 about the exact differences, I find that the current manuscript does not acknowledge the close relationship between these two contributions. Further, from the experimental standpoint and without further justifications the gains from GMED+MIR could be attributed to using more replay (from combining GMED and MIR).   In their response, the authors disputed the view of Reviewer1. I believe the source of the confusion between the author and the reviewer might be captured in this sentence from the author response to Reviewer1: The approach does not learn a generator that can “generate examples that are more forgettable for the classifier”; instead, feeding more forgettable examples in GEN MIR aims at reducing the forgetting of the generator.  Looking at Equations 2, 3 and Algorithm 1 from [1], in GEN MIR while two different procedures are used to obtain forgettable examples for the generator (B_G in Alg. 1) and the classifier (B_C in Alg. 1), the generator is used in both cases. In other words, the generator is used to generate examples for both itself and for the classifier. So, I think it s fair to say that the generator does indeed generate examples that are more forgettable for the classifier (Eq. 2).    I strongly encourage the authors to prepare another version of their work where the differences between MIR [1] and their contribution are clearly highlighted and the results show the advantages of GMED (including memory editing in data space).    [1] Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Laurent Charlin, and Tinne Tuytelaars. Online continual learning with maximally interfered retrieval. In NeurIPS 2019. https://arxiv.org/abs/1908.04742
This paper presents a method to combine graph convolutional neural networks (GCNs) with generative adversarial networks (GANs) for graph based semi supervised learning.  **Strengths:**   * It is a reasonable attempt to combine GCN with GAN for semi supervised node classification.   * The proposed method is general in that it can work with different graph neural networks.  **Weaknesses:**   * The novelty of this work is limited.   * The proposed method, GraphCGAN, has no significant performance improvement over state of the art methods.   * The writing has much room to improve in terms of both clarity and the linguistic quality.  Since both the novelty and the significance of this paper in its current form are limited, it is premature for publication. There is consensus among all the reviewers that this paper is not up to the acceptance standard of ICLR. 
Although the technical novelty is not very high, the finding that long run Langevin dynamics with convergently learned model provides comparable defense performance to adversarial training will give some impact to the community.  
Although the paper studies a relevant and important topic, which is about learning of hierarchy of concepts in an unsupervised manner, the reviewers raised several critical concerns. In particular, although the hierarchical structure of concepts is the key idea in this paper, the concept of hierarchy itself is not well explained. How to define the hierarchical level of concepts should be carefully and mathematically discussed. In addition, empirical evaluation is not thorough as reviewers pointed out. Although we acknowledge that the authors addressed concerns by the author response, newly added results are still confusing and more careful treatment is needed before publication. I will therefore reject the paper.   This work reminds me the the topic called "formal concept analysis" (e.g. see [1]), which mathematically defines concepts as closed sets and constructs a hierarchy of concepts in an unsupervised manner. This method can be viewed as co clustering and also has a close relationship to closed itemset mining. This approach is used in machine learning (e.g. [2]). I think it is beneficial for the authors to refer such existing and well established approaches to elaborate this work further.  [1] Davey, B.A., Priestley, H.A.: Introduction to Lattices and Order, Cambridge Univ. Press (2002)   [2] Yoneda, et al., Learning Graph Representation via Formal Concept Analysis, 	arXiv:1812.03395 
This is a well written paper, outlining a class of assistive algorithms. Being more or less a survey paper, it could do a better job of discussing  inverse reinforcement learning  and  collaborative inverse reinforcement learning . It could also be slightly more general: for example the human dewcision function need not be known if we model the interaction as a Bayesian game (then the human might have a latent type, which can be inferred together with the reward function). The active reward learning problem is sometimes referred to as  preference elicitation . In the end, it was not clear that the discussion in this paper had any actionable insights for future models or algorithms in this area. 
This paper has initially received mixed reviews, with two favorable and two unfavorable reviews. Several serious issues have been raised, in particular on experiments and validation; limited novelty; limited performance improvement; on the preliminary stage of the paper, in particular presentation and writing, and on the justification of key choices.  The authors provided responses to some of these issues, but in the discussion phase the reviewers (and the AC) judged the the responses did not sufficiently address the weaknesses of the paper, in particular:   The experimental setup does not assess the key innovation of the paper.    Several contributions claimed by the authors (in the paper and in the response) are judged not to be novel.   Concerns regarding the metric chosen to measure smoothness. and other issues.  The reviewers and AC agreed, that the paper has potential and merits, but that at at this point it is not yet ready for publication.
This paper proposes a method for regularizing image classifiers by encouraging their hidden activations to conform to a PDE.  This is a reasonable idea, and the authors clearly improved the paper a lot in response to the reviews.  However, the main tasks of MNIST and SVHN classification seem way too easy, and the baselines all need to be tuned to be as fast as possible for a given accuracy, if that s the relevant metric.  I agree with the reviewers that this line of work is promising but that the current paper is not sufficiently illuminating or well executed to meet ICLR standards.
Motivated by the importance of gameplay in the development of critical skills for humans and other biological species, this work aims to explore representation learning via gameplay in a realistic, high fidelity environment. Inspired by childhood psychology, they propose a variant of hide and seek game called "Cache" built on top of AI2 THOR, where one agent must place an object in a room such that another agent cannot find it, and demonstrate that the adversarial nature of the game helps the agents learn useful representations of the environment. They examine the difference in representations learned via such a dynamic, interactive adversarial gameplay approach, vs other more passive approaches involving static images.  The paper is well written and motivated, and easy to follow. All reviewers agree that the paper will be a great contribution to the ICLR community. I believe this is an important work, because not only does it challenge the traditional way of training many components of our systems passively (via static image recognition models), it synthesizes ideas from various disciplines (psychology, embodiment, ML) and provides an excellent framework for future research. For these reasons I m recommending we accept this work as an Oral presentation.
The paper presents a new regularizer based on singular value decomposition in embedding space to avoid model collapse. The reviewes liked the simplicity of the idea, but there were some remaining concerns regarding the experiments. Moreover, two reviewers mentionned some concerns with respect to the clarity of the paper. While some concerns have been addressed by the rebuttal, in particular regarding the clarity of the paper, the concerns regarding the experiments remained, and the reviewers agreed that the paper needs a revision before publication.   The main directions of improvement are to make the comparison with previous published results clearer, in particular comparing different methods with better hyperparameter tuning, and test on larger datasets. 
The paper proposes an approach to defining/tackling the question of separating "style" and "content" of images, and introduces a novel way to learn representation that disentangle these aspects of images. I think it offers some new ideas. The reviewers were split on the evaluation. Among the chief concerns with the initial submission were a problematic formulation of the objective, missing comparisons and analysis, and questions about novelty of the architecture (in particular w.r.t. AdaIn). I think the rebuttal/revision have addressed these fairly well. I do agree with R2 that some flaws remain, in particular the analysis could be more thorough/complete, and the paper could then be stronger. 
This paper proposes a simple yet effective approach for determining weight quantization bit lengths using RL. All the reviewers agree that the simplicity and performance improvements are a strong plus point. There are some concerns on applicability which have been sufficiently handled by rebuttal. AC recommends accepting the paper.
This paper proposes a new and general formulation for supernet, which encodes supernet with tensor network(TN). The idea is interesting and motivated.  However, the paper is well presented and the clarify needs to be further improved.  The effectiveness of algorithm is not well justified and experimental results are less convincing even after additional results provided in the revision. Most importantly, it is not clear that the  TENSORIZING  method can solve the current NAS s ineffectiveness problem.  It is confirmed that the reference to ICLR 2021 paper is not used for the decision of paper. 
Paper proposes and demonstrates a method to reconstruct 3d shape for a tree, from drone data.  While the reviewers all appreciated to work, all felt there were many shortcomings of the paper with respect to an ICLR audience: (a)  no machine learning novelty (b)  highly interactive data processing method (c) only one example processed tree shown (d) inadequate connections with relevant literature on 3d reconstruction, both general purpose, and examples applied to vegetation. (e) incomplete presentation of the method:   no ablation studies, no listing of the times required for individual steps of the processing.  In view of these concerns, we have decided to reject the paper.  But we hope you find the reviewers  comments helpful, and make use of them in a revision of the work. 
The paper proposes a method on multi agent options based policy transfer where agents help each other learn by exchanging policies.  The core idea behind the paper is novel, as it addresses a new and emerging topic of social learning, and of interest to ICLR community. The authors significantly improved the paper with additional experiments and theoretical analysis during the rebuttal process, resulting in a compelling case for the significance of the method.   Unfortunately, the paper requires addressing the clarity, and a careful proofreading pass, making it unsuitable to ICLR in its current form, 
This work presents a new theoretically motivated data augmentation technique. Reviewers agreed that the theory was interesting and has value, however raised concerns regarding the experimental evaluation which was limited to the Cifar datasets. There was some discussion over whether or not a comparison with AutoAugment would be fair, the proposed method is theoretically motivated whereas AutoAugment takes significant compute to train. I agree with the authors that if the method doesn t outperform AutoAugment on CIFAR, this would not necessarily invalidate their results. Nonetheless the work would be significantly strengthened if it included results. on additional datasets to stress test the theory. I recommend the authors add additional supporting evidence and resubmit.
The paper tackles program synthesis using a discrete latent code approach, enabling two level beam search decoding. The approach is well motivated, as program synthesis requires high level choices that affect long subsequences of the output, and discrete codes are amenable to heuristic search. Empirical results show improvements over methods with no latent variables and methods with continuous latent variables. However, the review process reveals that some of the claims made about the nature and necessity of the discrete latent codes is not sufficiently justified by the current analysis. With borderline assessments in a very competitive venue, *I cannot recommend acceptance*. I would like to encourage the authors to pursue this direction further, shedding more light on the nature of the latent representations learned as an interpretable planning mechanism.  The discussion was rich and surfaced a lot of concerns and issues with the paper, that I strongly encourage the authors to take into account.  After the author responses and internal discussion, many initial concerns were settled, but some remain. The two main concerns raised are: (1) that the paper makes overly ambitious claims about high level planning which is not backed by an analysis of the latent codes themselves, and (2) that the improvement may be due to increased generation diversity (which could be possible in continuous LV models too) rather than meaningful high level planning. I believe that after clarifying such remaining loose ends,  this work would be of great interest to both the field of program synthesis as well as the discrete representation learning community. 
While the reviewers found parts of the paper interesting, the main concern about this paper was lack of novelty and marginal improvements obtained by the proposed methods.
This paper proposes to incorporate additional prior knowledge into transformer architectures for machine translation tasks. The definition of problem is reasonable,  despite the fact that there is a long thread of work on adding knowledge of different types into neural architectures of NMT. The proposed model, however, needs to be better motivated, as to why the same thing cannot be done in a simpler way in the framework of transformers.  Judging from the exposition and the experiments, the proposed model is neither novel or empirically significant enough. The writing needs to be greatly improved to get rid of the grammatical errors and notational inconsistency.    I’d suggest to reject this paper  
This paper mostly received negative scores. A few reviewers pointed out that the idea of modeling user preference in the frequency domain seems novel and interesting. However, there are a few concerns around the clarity of the paper, the motivation of the proposed approach, as well as the experimental results being unconvincing (both in terms of execution as well as exploration of the results). The authors did not provide a response. Therefore, I recommend reject. 
Three referees support accept and one indicates reject. The issues pointed out by the reviewer who proposed rejection should be properly reflected in the final version.  First, regarding the synthetic experiment that illustrates the shortcomings of the existing GNN models, three reviewers, including myself, judged quite interesting. However, note the opinion of one reviewer that it is more appropriate to separate the influence of feature x and graph structure in the label generation method and each independently contribute to label generation. This part should be more justified in the final version.  In addition, it was pointed out that the expressive power of the model may be limited according to the parameterization type of the precision matrix, and there is a limitation that there may be a disadvantage in inference because it is a copula based probabilistic model. I think this characteristic is actually a fundamental limitation of the proposed method. However, three reviewers, including myself, thought that it was an interesting framework as a role that can complement the message passing architecture, and decided that the possibility of the proposed method was worth publishing. However, in order to reinforce this argument a little more, it would be better if the final version verifies it with more diverse GNN architectures and datasets.
This work improves deep generative models by applying Langevin dynamics to sample in the latent space. The authors test their method under different configurations (different loss functions) and various generative models (VAE, flow, besides GAN). Experimental results demonstrate the benefits of the proposed method in different generative tasks.   I tend to accept this solid work. I just have two suggestions: 1) the authors should discuss the connections and the differences between the proposed method and the energy based methods like (Arbel et al., 2020) in depth; 2) it may be more suitable to replace "Wasserstein gradient flow" with "Discriminator gradient flow" in the title.
This work presents an original analysis of using the weights of a neural network as a medium on which to hide information. Although the paper offers a novel perspective, its motivation and applicability remain unclear. As reviewer 3 points out, the proposed method does not seem very practical for any particular application, and the authors do not give a practical demonstration that shows the usefulness of the approach. It s not clear how the paper should be positioned with respect to previous work, and the proposed method is not directly compared with standard steganography schemes on metrics such as bandwidth, robustness etc, making it difficult to assess the value of the contribution. For these reasons I recommend rejecting the paper in its current form.
This paper describes an application of reinforcement learning to theorem proving in the connection tableau calculus.  The paper does a reasonable job in the application of RL techniques and the high level issues are important.  However, as the reviewers note, there is little connection to the notion of "analogy" outside of the very general idea that RL methods learn to generalize to novel situations.  I did not find the methods very original as it seems a somewhat mechanical application of RL methods.  That would be fine if the empirical results were convincing or surprising.  However, I found the Robinson arithmetic domains not very interesting as the problems were literally arithmetic, as in 2+5   7, rather than theorems such as the commutativity of addition.  The empirical results were not as convincing in the TPTP domains where MCTS seemed to dominate.  Also there are related papers in the area of deep learning applied to theorem proving that I believe dominate this paper ("learning to reason in large theories" and "an inequality benchmark".
For many problems such as ligand protein binding, quantitative structure activity prediction (QSAR), predicting protein function from structure, etc., the 3D geometry of the molecules is of great importance.  One way to represent this is simply to assign locations to all atoms in 3 dimensional space.  If using graph convolutional kernels or other relational representations such that aligning molecules is not necessary, these approaches with 3D geometry can be efficient and far more effective than 1D or 2D representations.  The contribution of the paper is to make this point and to produce a resource with this kind of 3D data.  Such a resource would be of high value.  Nevertheless, reviewers feel provision of such a resource is perhaps not a major contribution to the ICLR and ML communities.  There is a sense that more innovative and substantial contribution would come from addressing also the challenge that 3D geometry can changes and that there may be multiple low energy conformations of biomolecules that should be considered.  The authors contend that unlike ligands which are small and may have many low energy conformations, large biomolecules have a much more constrained conformational space.  This meta reviewer is sympathetic to the authors  point and appreciates the importance of the resource.  Nevertheless, even large biomolecules often have some portions of flexible conformation and high 3D structure variation that should be considered.  And indeed addressing the kind of multiple instance problem that arises by considering multiple conformations of large molecules or of ligands binding to large molecules would certainly require and likely yield bigger ICLR/ML innovations.  In the end the paper contributes a useful resource but does not excite the reviewers substantially enough, without those extensions or others, for a recommendation of acceptance at this time.
All the reviewers found interesting the use of Shapley values to provide feature attributions for fairness, however, the reviewers brought up a number of issues, particularly in terms of presentation and clarity. While the authors  responses did clarify some of these concerns, this was not enough for the reviewers to broadly support acceptance.
All reviewers agree that the writing is not precise. It does not help to find any novelty in the ideas, and the limited and too quickly described experiences are not convincing enough to forgive this problem. The authors chose not to oppose or comment on the detailed arguments provided by the reviewers. I agree with the reviewers in recommending the rejection of this paper. 
This paper offers an in depth analysis of attention in large scale language models including (AL)BERT and XLNet in the context protein representation learning, and obtains many interesting findings. This is not a typical paper with novel technologies proposed, instead, it studies the existing technologies in a specific (biology) context and explains what the learned representations and attention map really mean.  All the reviewers see the value in this paper and give positive feedback in general. At the same time, they also raised a few concerns, e.g., regarding the claim on “well calibrated" attention head, on some missing details of the algorithm description and the experiments,  on phenomenon vs. causality of the finding, etc. The authors really did a very good job in their rebuttal and paper revision, and most of these concerns were (at least partially) addressed, and a few reviewers raised their scores. With this, we are quite confident that this paper is above the bar of ICLR.  
This paper proposes to automatically determine when the SGD step size should be decreased, by running two "threads" of SGD for a bunch of iterations, divide those into windows, and then look at the average inner product of the gradients in the two threads in each window. If the inner product tends to be high, that indicates that there is still "signal" in the gradient and it should not be decreased. If it is low, that indicates that the gradient is mostly "noise". In the latter case, the learning rate is decreased by a factor of gamma and the length of the next phase is increased by gamma.  Theorem 3.1 essentially assumes smoothness, a bounded fourth moment for the stochastic gradient, and that the stochastic gradient error is not too far from isotropic. Then it shows that if the step size is set small enough, the standard deviation of the diagnostic (Q_i) can be upper bounded in terms of the expected value of Q_i. It follows that the probability of Q_i being negative cannot be too large (bounded in terms of the step size eta and the length of the windows l).  Theorem 3.2 adds the assumption of strong convexity and weakens the assumption on the gradient to a bounded second moment. Then it upper bounds the expected value of the diagnostic in terms of its standard deviation.  Proposition 3.4 gives a proof of convergence. As far as I can tell the proof is essentially that the learning rate decay can t be much worse than what would happen if the diagnostic *always* set to decrease. In particular: (1) It s impossible for the learning rate to decay too quickly, since the length of each phase is increased by gamma whenever the learning rate is decreased by gamma. (This is a "non adaptive result.) (2) The learning rate will eventually decay with probability 1.  Various concerns were brought up by the reviewers. Perhaps the most strongly voiced concern was that the proposed method is a heuristic rather than a method with a rigorous guarantee. For my part I am in agreement with the authors and other reviewers that heuristic methods for decreasing the learning rate are worthy of study given the large practical importance of this problem.  I concur with the concern raised by some reviewers that the theoretical component of the paper may not have little explanatory value for the results that are given. The assumption of strong convexity is not a major concern to me. (Though not true it can still give intuition.) More concerning is that theory essentially takes a fixed step size scheme (repeatedly decrease the step size by gamma and increasing the length of a phase by gamma) and then shows that the diagnostic can’t be too much worse. This isn’t in keeping with the motivation of being adaptive.  The reviewers were also concerned about the explanation of better results due to less overfitting. This may be true, but the theory makes no mention of overfitting.  There was a consensus that the experimental results were promising, though some minor issues were raised.  While the direction explored in the paper has value, there are enough open questions about the relationship of the theory to the experimental results to warrant another round of review.  Small thoughts, not significant to acceptance:  The current heuristic runs two separate threads and looks at the inner product of those gradients. An alternative to this would be to run a single thread along with a "ghost" thread that computes a different gradient at each iteration. It would be great to comment on the difference and why one might be superior to the other. A more radical alternative would be to run a single thread, but then compute the diagnostic on each half of the minibatch. A more radical alternative still would be to analytically do that splitting many times and average the results. This seems like it might simultaneously reduce the variance of the diagnostic and also reduce the computational cost.  2. The current heuristic runs two threads. Is there a tradeoff if you run more?  3. The statement of theorems could be more user friendly. To understand Thm 3.1, I needed to search o find the definitions of: eta, l, i, w, Q_i. With a small amount of effort this could be re written to remind the reader that w is the number of windows, l is the length, eta is the stepsize, etc. It is particularly unfortunate that sd() is never formally defined (only by reading the appendix did I discover that this was the standard deviation.)  4. The fact that the length of threads is always increased by a factor of gamma whenever the step size is reduced by gamma seems contrary to the spirit of the proposed diagnostic. After all, this "bakes in" a kind of "fastest possible" decay schedule. If the diagnostic were fully reliable, shouldn t this not be necessary? The decision to add this doe not get nearly enough discussion in the paper in my view.  5. I think it might be clearer to re state theorem 3.1 including the Chebyshev result after it.
This paper proposes a novel recurrent network called RIMs for improving generalization and robustness to localized changes. The network consists of largely independent recurrent modules that are sparsely activated and interact through soft attention. The experiments on a range of diverse tasks show that RIMs generalizes better than LSTMs.  The overall feedback from reviewers is positive: the paper is well written, the idea is interesting, and the experiments cover a wide range of diverse tasks.  The main concerns of most reviewers are the fairness of comparison, the limited novelty, and lacking details on how and why the system works. The authors pointed out that RIMs are not a straightforward combination of attention and RNN, and it has fewer parameters than LSTMs. They also conducted ablation study to demonstrate the benefits of RIMs and provided the missing details in the revised version.  In summary, this paper presents an important research direction for systematic generalization using modularized network. The paper is well written, the idea is novel and interesting, and the experiments cover a wide range of diverse tasks. Hence, it makes a worthwhile contribution to ICLR and I am recommending acceptance of this paper. 
This paper proposes a new algorithm that combines imitation learning and reinforcement learning, based on an extension of the free energy principal. The expert s demonstrations are encoded as a policy prior, and a posterior policy is inferred by maximizing expected rewards. While at a high level this is a promising direction, all the reviewers found the paper difficult to follow and verify its claims. This mostly due to a use of unusual and non conistent notations. The authors are advised to take into account the issues about clarity that the reviewers raised and improve the readability of their paper accordingly.
The paper presents a GCN based solution with a distance aware pooling method for diagnosis and prognosis of COVID 19 based on CT scan. It aims to address an important and timely problem. The proposed solution is reasonable.   The paper receives mixed ratings, and therefore we had extensive discussions. It is agreed by all of us that    (1) the novel contribution of the proposed method is relatively low compared with standard ICLR papers;   (2) the evaluation is interesting, but could be improved with state of art baselines on CT scan (not limited to GCN based method);   (3) the authors have improved the writing of the paper significantly, which convinces two reviewers to elevate their scores.   The paper addresses a timely topic, but there is still room for improvement in methodology and evaluation. We hope that the reviews can help the authors prepare a strong publication in the future.  
This paper presents work on semantic action conditioned video prediction.  The reviewers appreciated the interesting task and use of capsule networks to address it.  Concerns were raised over generalization ability of the proposed approach, points on clarity, scalability, and handling of uncertainty/diversity by the method.  After reading the authors  response, the reviewers engaged in discussion.  Over the course of this discussion, the reviewers converged on a reject rating, noting that the concerns raised above were not sufficiently addressed to warrant publication at this stage.
 This paper considers the problem of training neural networks to be robust to label shifts. To do so, it proposes to use a distributionally robust optimization (DRO): instead of minimizing the expected error with respect to the empirical data distribution, the worse case expected error is minimized over a KL divergence "ball" of distributions centered at the empirical distribution with a given radius. The main contribution of the paper is an efficient algorithm for achieving this optimization, that avoids the need to project onto the uncertainty set or to sample in non standard ways from the training set. The paper provides evidence on the ImageNet data set and the ResNet 50 architecture that the proposed AdvShift algorithm outperforms reasonable baselines.  Reviewers raised concerns about the novelty of the algorithm, and claims the paper makes about the infeasibility of the sampling required by one of the competing baselines, and the need for the Lagrangian parameter used in the algorithm to be well tuned. The rebuttals addresses the concerns suitably; in particular, the novelty of the algorithm lies it it being the first DRO based solution for the label shift problem and its efficiency obtained by using KL uncertainty set and the Lagrangian formulation of the problem, which allows a closed form solution.   Due to the strong empirical and theoretical performance of the proposed AdvShift algorithm, it is recommended that this paper be accepted.
The reviewers agree that the paper is addressing an interesting problem (cold start for representation learning on dynamic graphs). However, the proposed methods can be improved by proposing more novel ideas. At the moment, the proposed methods is a combination of GCN model for node classification and GAE model for link prediction. In this case, some analysis or theoretical justification may make the paper more interesting. Furthermore, the reviewers think the experiments can be improved. For instance, results on more datasets, more comparison methods and a different setup will strengthen the paper. 
This paper revisits the under explored "implicit" variant of Variational Intrinsic Control introduced by Gregor et al. They identify a flaw that biases the original formulation in stochastic environments and propose a fix.  Reviewers agree that there is a [at least a potential, R4] contribution here: "even the description of what implicit VIC is trying to do is a novel contribution of this work", in the words of R2, and "the derivation has theoretical value and is not a simple re derivation of VIC", in R4 s post rebuttal remarks. Several reviewers raised significant concerns around clarity, which were addressed in an updated manuscript, which also provided new visualizations and new experiments which reviewers found compelling. All reviewers agreed that the revised manuscript was considerably improved.  R4 s score stands at the 5, with the other reviewers all standing at 6. R4 s main concerns are around whether the missing term in the mutual information identified by the authors is a problem in practice on non toy tasks (echoing somewhat R3 s concerns re: high dimensional tasks). While this is a valid concern, the function of a conference paper needn t necessarily be to (even attempt) to provide the final word on a matter. Identifying subtle issues such as the one brought forth in this manuscript and re examining old ideas is a valuable service to the community, and this paper will serve as a beginning to a conversation rather than an end. The AC also considers themselves rather familiar with the original VIC paper, and found the results herein somewhat surprising and noteworthy.  I recommend acceptance, but encourage the authors to incorporate remaining feedback in the camera ready.
All four reviewers expressed very significant and consistent concerns on this submission during review. No reviewer is willing to support this submission during discussion. It is clear this submission does not make the bar of ICLR.
The authors empirically analyse the properties of datasets which lead to poor calibration. In particular, they show that high class imbalance, high degree of label noise, and small dataset size are all likely to lead to poor overall calibration or poor per class calibration. While there are some interesting insights in this work, the reviewers argued that the contribution is not substantial enough for ICLR. To improve the manuscript the authors should consider accuracy and calibration jointly and extend the results pertaining to label noise which were appreciated by the reviewers. For the former, the same conclusions hold for accuracy, instead of calibration, which raises the question of their relationship   is there a tradeoff? For the latter, the reviewers pointed to a concrete extension with structured label noise. Finally, the theoretical analysis is a step in the right direction, but the assumption on the width of the network required to fit the training set is too restrictive in practice. Therefore, I will recommend rejection.
The paper studies the problem of identifying what information to forget in attention mechanisms, with the goal of enabling attention mechanisms to deal with longer contexts. This is a simple yet intuitive extension:  self attention is augmented with an expiration value  prediction. Experiments were carried out on NLP and RL tasks. Overall, the paper has novelty in the proposed idea, however, there are concerns about the strength of the experiments; that the experiments fall short.
This paper received three borderline reviews (2+ / 1 ) and one positive review.  Having read through the reviews and author responses, the AC recommends the paper to be accepted.  The method, while simple, is proven experimentally to be effectively and will add to the body of work on key point localization.   The authors are requested to add their additional baselines in the response text to the revision of their paper if it has not already been done. 
Three reviewers are mildly positive, while one is negative. The substantive comments of the reviewers are consistent with each other; it is merely their evaluations that differ.   One contribution of the paper is that it shows how using temperature tuning can yield similar accuracy to using batch normalization; this is useful because batch normalization is not always possible. The revised paper shows improvements, and we appreciate the engagement of the authors with the reviewer comments. However, there are remaining weaknesses such as a weak argument based on the empirical.results.  This paper can be improved based on the comments made by the reviewers. We encourage the authors to resubmit to a future venue.
While many work in the literature (PlaNet (2018), Dreamer (2020), SimPLe (2019), etc.) learn world models to perform well on a particular task at hand, the motivation behind this work is that dynamics models benefit if they are task agnostic, hence would be able to perform a wider range of tasks, as opposed to just doing one task really well. In order to do this, they propose to learn a latent representation that models inverse dynamics of the system / environment rather than capturing information about the task specific rewards, and incorporate a planning for solving specific tasks in which they can measure performance.  To show broad applicability of their method, the authors tested their approach on Atari and DM Control Suite (from pixels), and also simple grid worlds to illustrate the concepts, and demonstrated strong performance over SOTA model free algorithms (even the ones that do not have open source implementations). Reviewers and myself agree that the paper is well written, easy to follow, and the approach is well motivated.  After the review period, the authors have done work to improve the draft, particularly including ablation studies with and without planning, addition comparisons, and improved visualizations, after taking in the comments and feedback from the reviewers after the initial reviews, which satisfied some of the reviewers. One reviewer asked for a real robotic task, but I feel that while it will help the paper, many existing works focus purely on DM control from pixels, and this work has performed experiments on both DM Control and Atari, two reasonably different domains, and IMO makes up for the lack of real world robotics experiment. That being said, a discussion on how the proposed method would work in a real robotic task, as suggested by R4 would be good to have.  I believe the work in its current state is ready for acceptance for ICLR 2021, and should be a fine contribution to the visual model based RL works. I m excited to see this work presented to the community, and I m going to recommend acceptance (Poster).
This paper was on the borderline. While there was some support for the ideas presented, concerns were raised about the experiments. The exposition would also need to better demonstrate the significance of the contribution.
The authors present a new set of trigger based backdoor attacks that use dynamic patterns that make detection harder. These attacks seem to be stronger with regards to state of the art attacks.   Some weaknesses is the need for full whitebox access of the model. Several key references are missing, and the comparison with other backdoor attacks in unclear.  Moreover, although the authors compare with trigger based backdoors, there are plenty of triggerless backdoors that can be viewed as dynamic, eg the attacks by  Bagdasaryan et al. http://proceedings.mlr.press/v108/bagdasaryan20a.html and works referencing this paper.  The paper indeed provides an interesting path towards dynamic attacks, but the lack of comparisons with state of the art literature, and also the need for whitebox access/high poisoning rate significantly limit the novelty of this work.   
This paper provides a privacy preserving method to boost the sample quality after training a GAN. The reviewers were unanimous that this paper should be presented at ICLR, with an important contribution to privacy preserving GANs.
This paper proposes an algorithm to address the disparate effect that DP has on the accuracy of minority/low frequency sub populations. Unfortunately the work does not actually guarantee or analyze the resulting privacy guarantees. In particular it may provide much worse privacy (or no privacy at all) to the minority subpopulation. The paper also calls their algorithm "fair" without using an accepted term or a careful discussion of what an algorithm needs to satisfy to be considered "fair". Using a more technical term such "reducing the accuracy disparity" would make much more sense.   
 This paper described a model that improves the performance of LM based pre trained sentence representation on semantic text similarity tasks (STS). The proposed approach is motivated by the observation that top layers in transformer based LMs are quite poor at this task per se. This paper proposes Contrastive Tension, a self supervised objective that drags representations of same sentence together, and pulls away representations of different sentences. The proposed method only relies on unlabelled data, and is relatively simple to implement. The paper demonstrates consistently strong results empirical results on the unsupervised semantic textual similarity (STS) task. Moreover, the paper provides reasonably good analysis.  On a negative side, the reviewers noted that the paper lacks a bit of analysis about the objective. The connection between the observation on layer wise performance of BERT on STS and the proposed contrastive training method is not clear. Second, while the result is interesting, its applicability is limited to STS.  Taking into account all the above, the reviews constitue a case for a solid weak accept. Therefore I recommend acceptance as a Poster contribution.
All the reviewers find the problem studied in the interesting. However all of them raise concerns about the assumptions made in the paper for the analysis. Reviewers find the assumptions very limiting and far from the practical training of GNNs. Improving the analysis by relaxing the assumptions further can significantly help the paper. 
Motivated by the fact that the benefit of overparameterization in unsupervised learning is not well understood than supervised learning, this paper analyzes normalized flow (NF) when the underlying neural network is one hidden layer overparameterized network and proves that for a certain class of NFs, one can efficiently learn any reasonable data distribution under minimal assumptions. The paper is very well motivated. However, the main concerns from the reviewers include (1) the writing quality and presentation are poor, even after revision during the author’s response; and (2) the analysis is limited in the neural tangent kernel (NTK) regime, which makes the results less significant. I agree with the reviewers’ evaluation and I think the first concern can be addressed by a careful revision, while the second concern needs additional nontrivial effort. Thus, I recommend rejection.
The reviewers are enthusiastic about this work, and the few comments that they had were appropriately addressed by the reviewers.
There is a substantial contribution in identifying novel questions/issues, as this paper certainly does. Neither I nor the reviewers have seen this issue of transient non stationary before, and the authors make a compelling case for it, especially in the supervised setting with the CIFAR experiments. It is less compelling through the RL experiments. As such, this paper is likely to inspire new work within the field. To me, Figure 1 is the most interesting aspect of the whole paper.  The initial approach by the authors is questionable in its effectiveness, and is likely to be improved by others in the future. Some of the results in Figure 3 are questionable, especially when you look at the individual curves in Figure 8. So overall, this means that the authors have identified a truly novel issue, and proposed an initial method that is just okay.  They ve done a nice job investigating this in a supervised setting, and need to push further in the RL setting.  The question is whether the novel contribution of the problem outweighs that the algorithm and its evaluation could use improvement.  The reviewers debated this in the discussion, with points on both sides, but the novelty of the question/issue (even if the investigation could use work) is likely to inspire further work in this direction.  Other notes: The authors could have evaluated the (impractical) version of their algorithm proposed in the first paragraph of Section 4.2. This would inform 1) whether their parallel training approximation is close to the optimal algorithm, and 2) whether the optimal (impractical) algorithm is capable of improving generalization significantly. If the latter is true, it would leave open a huge avenue of investigation to find better approximate solutions.
The paper proposes a constituent based transformer for aspect based sentiment analysis. The approach allows conducting aspect based sentiment analysis to leverage the syntactic information without pre specified dependency parse trees.   Overall, the idea is interesting. However, all the reviewers shared the following concerns:     Paper descriptions of methodology and experiments are not clear and require significant rewriting and reorganization.    The proposed approach is not well justified by the empirical study presented in the paper. Especially, a more detailed ablation study is required to justify the design.   We would suggest the authors addressing the feedback from the reviewers to improve the paper. 
The reviewers agreed that the paper presents interesting ideas but the presentation of the paper needs be improved. Also, the experiments and the related work section need be improved.
The paper goes over a long list of proposed clustering similarity indices and attempts to provide a taxonomy of those by their different approaches and the extent by which they satisfy a list of "desired properties" proposed by the authors. This is very much in the spirit of earleir work on clustering similaritie by [Meila 2007] and on clustering quality measures [Ackerman, Ben David 2008, 2009].  While there may be some interest in such a compendium, there is no much novelty in this paper and it relevance to practice is also unclear.
This paper extends the idea of hindsight experience replay (HER) to learn Q functions with relative goals by constructing a distribution over relative goals sampled from a replay buffer using a clustering algorithm. This approach is evaluated on three multi goal RL environments and is shown to learn faster than baselines.  ${\bf Pros}$: 1. Faster convergence as compared to baselines 2. Interesting use of clustering in the context of HER but this choice is made without strong justifications or formal arguments  ${\bf Cons}$: 1. Some of the key choices made in this paper are not justified or explained property, e.g.   the goal sampling strategy, choices made in the clustering algorithm and associated heuristics, implicit assumptions (e.g. R1 raised the question of using L2 distance in measuring metrics between two states)  2. There are several choices made without sufficient formal arguments, verification or guarantees.   The paper studies an interesting problem but could be made stronger by incorporating feedback received during the discussion period. 
The paper introduces a new loss, Maximum Categorical Cross Entropy, which combines the usual cross entropy loss with a maximum entropy regularisation term on the convolutional kernels, and is evaluated on image classification. The authors have trained a face classification algorithm on two datasets: UTKFace (https://susanqq.github.io/UTKFace/) and NIST colorFERET (https://www.nist.gov/itl/products and services/color feret database). The labels consisted in, respectively: White, Black, Indian, Asian, Others (over 18k images) and Asian, Asian Middle Easter, Black of African American, White, Hispanic (over 11k images) (see section 4.1 of the paper).  From the meta reviewer s perspective: As stated in the title, abstract and in paragraph 3 of section 1, the motivation of the paper is to reduce model overfitting and racial bias towards one category. However, there is no further discussion about any "ethical, societal and practical concerns when dealing with facial datasets, especially for the task of race or gender classification". It seems to me that a paper that implements a "race classification" algorithm should at least devote a substantially long part of the discussion on the validity of such a task and of such a labelling process, as well as question the motivations and potential misuses. Who labeled these faces and based on what visual characteristics? Were the subjects of the photographs consenting and did they self declare their ethnicities? Are the authors simply reproducing discredited phrenology assumptions about ethnicities and about "race", which is increasingly defined as a mere social construct? Given that there is nothing specific to face classification in the loss function, I wonder why did the authors decide to focus on ethnicity features? What exactly could a visual ethnicity classifier be used for? Given the sheer amount of questions raised by the paper, we have submitted it for review by the Ethics Board.  Summary of the reviews: Reviewers gave scores scores 3, 4, 5, 5 (without rebuttal from the authors), raising concerns about the novelty and contribution of the method (as it is simply combining maximum entropy with cross entropy), clarity of the explanation of the method, missing related work and baselines and evaluation metrics.  Based on the low scores, unfavourable reviews and an ongoing Ethics Board investigation, I recommend for this paper to be rejected.    While this paper is likely to be rejected (, I believe that these concerns should be raised and potentially reviewed by the Ethics Board (unless this is an obvious rejection). Thank you in advance for your time.
This paper considers a new model of input data specific for image classification problems. In particular, the high level idea is that each image contains certain patterns, and which patterns it contain decides its label. In this framework, under some stronger assumptions (e.g., patterns are orthogonal, one positive pattern and one negative pattern, PSI assumption, etc.) the authors showed that SGD on a 3 layer overparametrized convolutional network will be able to have a small sample complexity, while the VC dimension would be at least exponential in d. The paper also provided some empirical evidence on a modified MNIST dataset. While the idea seems to be an interesting first step, the reviewers find that the current version of theory still relies on fairly strong assumptions.
This work proposes new learning algorithms that fine tune ("tailor") a model at test time using unsupervised objectives. This formulation allows for introducing an inductive bias into the model that might improve generalization on unseen data. The proposed algorithm is demonstrated on two example tasks.  The reviewers like the topic and also find the proposed approach to be interesting. However, they are unconvinced by the current empirical evaluation of the method. Additional experimental evaluation could improve our understanding of the proposed method and help contrast it to previously proposed techniques. Given these reviews I recommend rejecting the paper at this time.
All reviewers expressed consistent enthusiasm on this submission during the review process. No reviewers expressed concerns and objections to accept this submission during discussion. It is quite clear this is a strong submission and deserves accept.
All reviewers gave, though not very strong,  positive scores for this work.  Although the technical contribution of the paper is somewhat incremental, the reviewers agree that it solidly addresses the known important issues in BERT, and the experiments are extensive enough to demonstrate the empirical effectiveness of the method.  The main concerns raised by the reviewers are regarding the novelty and the discussion with respect to related work as well as some unclear writings in the detail,  but I think the pros outweigh the cons and thus would like to recommend acceptance of the paper.  We do encourage authors to properly take in the reviewers  comments to further polish the paper in the final version.  
The paper solves a PDE using an additional penalty function between the derivatives of the function. On toy examples and two PDEs it is shown that these additional terms help.  Pros:   The motivation is to include derivatives in the computationa             Implementation and testing on several examples, including high dimensional ones             Timing is included in the latest version   Cons:  The loss is Sobolev norm of the residuals of the equation.               The usage of the norm of the residual is not 100% consistent with the smoothness properties of the corresponding equation. For example, for the Poisson equation, the problem is selected in such a way the solution is analytic. However, for example, if the zero boundary conditions are enforced, and right hand side is all ones, the solution will have singularities. Thus, the main challenge would be the case when solution does have the singularities (and it will have it in many practical cases). The L2 norm then is not the right functional for the solution to exist, not to say about the higher order derivatives. So, these functionals are not motivated by the theory of the solution of PDEs, but are rather focused on much smoother solution.       Convergence. There are quite a few papers on the convergence of DNN approximations to solution of PDEs. The presented methods might have converged to a local minimum. An important reference is the paper by Yarotsky D. Error bounds for approximations with deep ReLU networks. Neural Networks. 2017 Oct 1;94:103 14.   
This paper proposed a variant of the existing training method "progressive stacking", and showed good empirical results comparing with the normal training procedure for BERT models. It contains some interesting points on the technical side, e.g, freezing the bottom layers when training newly added layers, but there are several concerns:  (1) This proposed method is called progressive stacking 2.0 but there is no comparison against the original progressive stacking in experiments. We had to check the empirical results in the original paper of progressive stacking, and did not notice any performance improvement of this new method over the original one; (2) The proposed method even introduced one more hyperparameter to tune: the number of top layers to copy.  This hyperparameter seems hard to choose. Different choices of this hyperparameter may dramatically impact the performance of this new method.  An ablation study on this should be conducted, e.g.,  what will the results look like if we only copy the last layer?  (3) The original progressive stacking method does not provide any practical guidance on how to determine the time to stack when the training goes. This severely limits the practical value of progressive stacking. If one stacks layers too early or too late, the stacking method may have no advantage at all. Unfortunately, this method called 2.0 still leaves this most critical issue away. 
The paper addresses lifelong/continual learning (CL) by combining reusable components. The algorithm is based on, updating components, updating how they are combined for a given task and adding new components.   Reviewers had concerns about the learning workflow, how it could scale to harder CL streams and how it differs from existing LL/CL work.  They also asked for clarifications about compositionality. They highlighted the experiments as a point of strength.  After the rebuttal, all reviewers found the paper to be above the acceptance bar. 
The paper attempts to reduce computational cost of Transformer models. In this regard, authors generalizer PoWER BERT by proposing a variant of dropout that reduces training cost by randomly sampling a fraction of the length of a sequence to use at each layer. Further, a sandwich training method is used which trains a spectrum of randomly sampled model between the largest and the smallest size model. At test time, the best length configuration that balances the accuracy and latency tradeoff via evolutionary search is used. The reviewers found the general idea interesting, but raised a number of concerns. First, proper baselines should be used and related works be discussed. In particular, the method is built on top of Power BERT, yet it does not directly compare with it, and there was no good response when pointed out by a reviewer. Second, as the paper employs many tricks (some new some from prior work), but does not do any ablation studies to show how each of those contributes to the final accuracy gains. Finally, to showcase benefit compared to prior works in terms of computational cost a proper evaluation methodology and actual speedups for batch size 1 inference should be provided. Thus, an improved evaluation would benefit the paper a lot and paper in its current form is not ready for publication. 
While much of generative modeling is tasked with the goal of generating content within the data distribution, the motivation of this work is to examine whether ML techniques can generate creative content. This work has 2 core contributions:  1) Two new datasets of creative sketches: birds and creatures, that have part annotations (size ~ 10K samples for each set). The way the datasets are structured with the body part annotations will facilitate the creativity aspect of the approach later described in the paper.  2) This paper propose a GAN model that is part based, which they call DoodlerGAN. It is inspired partly by the human s creative process of sequential drawing. Here, the trained model determines the appropriate order of parts to generate, which makes the model well suited for human in the loop interactive interfaces in creative applications where it can make suggestions based on user drawn partial sketches.  They show that the proposed model, trained on the part annotated datasets, are able to generate unseen compositions of birds and creatures with novel body part configurations for creative sketches. They conduct human evaluation and also quantitative metrics to show the superiority of their approach (for human preference, and also FID score).  Many reviewers, including R1 and myself observe that the datasets provided, along with the parts based labeling and modeling approach are a clear advantage over existing datasets and methodology. With ever growing importance of generative models used in real world applications, including the creative industry, I believe this paper provides a much needed fresh take on creative ways of using our generative models besides making them larger, or achieving better log likelihood scores. Many reviewers, including R3, would think that this work is indeed a "Delightful, well written paper! I have concerns about its fit here." I strongly believe such works in fact definitely *do* belong at ICLR, and I think this work has the potential to get researchers in the generative modeling community to rethink what they are really optimizing for.  I believe this paper will be a great addition to ICLR2021, and I look forward to see their presentation to the community to spark more creativity in our research endeavors. For this reason, I m strongly recommending an acceptance (Poster).
The reviewers all  appreciated the insights drawn from this study as well at its thoroughness. I want to commend both authors for running additional experiments to strengthen the paper and reviewers for updating the scores accordingly.  Congratulations.
The paper provides empirical evidence that the sampling strategy used in point cloud GANs can drastically impact the generation quality of the network. Specifically, the authors show that discriminators that are not sensitive to sampling have clustering artifact errors, while those that are sensitive to sampling do not produce reasonable looking point clouds. They also provide a simple way (i.e. including AVG feature pooling) to improve generation quality for insensitive discriminator GAN setups. The reviewers agree that this is an interesting insight into the problem and this insight can help the community.  Based on the reviewers  comments and subsequent discussions, it becomes clear that the paper would be stronger and more compelling if the underlying hypothesis (i.e. the idea of sampling spectrum) is more rigorously defined (e.g. ideally with a theoretical grounding) and the claims/analyses are tied in with this definition. Such a grounded and precise setup would help in analyzing future generation discriminators that may not simply fall into the two discrete groups defined in the paper (i.e. sampling over sensitive and sampling insensitive). The results have promise, so the authors are encouraged to take into consideration the reviewer discussions to produce a stronger future submission. 
The goal of the paper is to learn policies that can solve a given task while adhering to certain constraints specified via natural language. The paper closely builds upon prior work on constrained RL and passes the representation of natural language constraints by pre training an interpreter. Experiments are done in a new proposed 2D grid world benchmark. Although reviewers liked the premise, the main issue raised is that the way natural language constraints are handled is no different from the way it is done in prior work on constrained RL. The authors provided the rebuttal and addressed some of the concerns regarding paper details. However, upon discussion post rebuttal, the reviewers and AC feel that the paper does not provide clear scientific insight because the natural language part is processed separately from the policy learning part. We also believe that the paper will immensely benefit with results in more complex environments beyond the 2D grid world. Please refer to the reviews for final feedback and suggestions to strengthen the submission.
The paper closes an important gap in our understanding of neural tangent kernels.  In addition, the used techniques are novel.  My low confidence is mainly based on the fact, that the review process at conference is not perfectly suited to deal with such papers, since their review would actually require both expert reviewers and substantially longer reviewing periods.
The reviewers agree that this paper has some strengths to it, and some commented that the revision improved the manuscript, but the paper remained borderline with no strong champions in its favor. The reviews are encouraging and suggest that the paper is a bit tightly packed for the conference format, and perhaps because it is dense it is hard to the strength and scope of contributions, while other relationships such as to the Bayesian context could be explored more fully. Multiple reviewers find that a longer improved version would "shine" in a better suited journal.   The decision to reject is independent of the fact that the authors seem to have violated the anonymity rules in the revised version.
This paper proposed a new variant of knowledge distillation. The basic idea is interesting although similar ideas have more or less appeared in the literature as pointed out by the reviewers. Our main concern on this work is that the real empirical improvements are too limited such that it is hard to conclude that the proposed method can really perform better than the baseline. In the meantime, the proposed method is much more computationally expensive. 
This paper is rejected.  The authors focus on offline RL for the sequential recommender system problem and propose an approach that: * builds multiple models based on splits of the offline data using domain knowledge * splits the policy into a context extraction system and context conditioned policy (similar to Rakelley et al.)  While R1 and R4 appreciate the changes, they both feel that the paper is not ready for publication at this time. R1 s main concerns is the generalizability of the proposed solution because it relies heavily on manually defined rules and domain expert knowledge. R4 was concerned with the definition and precision of robustness. How is robustness quantified? Finally, many of the baselines were not built for partially observed environments, so it is unsurprising that they perform poorly. Baselines with recurrent policies would strengthen the paper. 
The reviewers and I agree that the paper is well motivated and that there are good comparisons to prior work. However, the scope of the paper is rather limited, and there were some doubts about the overall conclusions and whether the current results fully support them. As such, I cannot recommend the paper for publication.
The paper studies the properties and advantages of temporal abstraction in hierarchical latent variable based video prediction approaches, producing interesting results on various simulated environments and the KTH action dataset.   The two key drawbacks of this paper are: limited visual complexity of datasets used for evaluation (a real video dataset like BAIR pushing would have really helped), and lack of comparison (conceptually or empirically) to relevant prior work including those raised by various reviewers, plus see more below.  Aside from this, does the paper claim to propose a new model, or perform an empirical study to evaluate an existing model class, or both? The answer to this question is not always clear from the paper and this confusion is reflected both in the reviews and reviewer discussions and also in the authors  own responses to these.  Some potentially relevant works that don t find mention in this paper (aside from those pointed out by reviewers):   NAOMI: Non Autoregressive Multiresolution Sequence Imputation https://arxiv.org/pdf/1901.10946.pdf   Long Horizon Visual Planning with Goal Conditioned Hierarchical Predictors https://arxiv.org/pdf/2006.13205.pdf   I do think these are all relatively easily fixed shortcomings, and I urge the authors to fix them in future versions.
This paper proposes a new method for pre training of language models in the e commerce domain. It introduces five objectives for pre training by incorporating domain knowledge into the model.  Pros • The paper is generally easy to follow. • Design of the pre training objectives is reasonable. • Experimental results are solid and convincing. • A useful method is proposed, and its effectiveness has been verified in the e commence domain.  Cons • Novelty of the work might not be enough. • Presentations can be improved. • It is not clear whether the proposed approach can be applied to other domains which may not have enough structured data.  The authors have made several things clearer in the rebuttal. They have also added new experimental results. However, the overall quality of the paper does not reach the level of ICLR from the viewpoint of novelty, significance, and clarity.  
The paper proposes a variant of recurrent neural networks based on Long Short Term Memory. Unlike the standard LSTM, the proposed mass conserving LSTM subtracts the output hidden state of the LSTM from its current cell state, thus preserving the "mass" stored in the cell states at each step. A left stochastic recurrent weight matrix is also used to conserve the "mass" across the time steps. Empirical experiments demonstrated the effectiveness of the proposed MC LSTM on a range of datasets such as addition & arithmetic tasks, traffic forecast, and rainfall modeling models.   Several issues were clarified during the rebuttal period in a way that satisfied the reviewers. However, some concerns still remain unanswered:  1) This is an empirical paper that proposes a modified LSTM that brings forward a few different ideas: L1 norm, stochastic transition matrices, and subtracting the output hidden states. An ablation study is a MUST in such an applied work. It has been pointed out by other reviewers that there are many prior references on LSTMs variants. It would greatly strengthen the paper by considering more diverse baselines. There is no experiment nor discussion on how much each modification helps wrt the final accuracy. Thus it remains unclear how the results can generalize to other problems.  2) Although the results seem convincing across various datasets that mass conservation seems to help, the datasets are non standard benchmarks in the machine learning conferences thus there is a lack of competitive prior baselines. As the proposed LSTM has a different number of parameters compared to the standard LSTM, is it fair to compare the different architectures under the same number of neurons? What happens if we compare the architectures with the same number of parameters? And how well does the model scale as we vary the hidden size? It would be helpful to keep the contributions into perspective by using standard RNN benchmark datasets such as Penn TreeBank or Wiki 8.  Overall, the basic idea seems interesting, but the lack of ablation studies significantly hurt the contribution and the positioning of the paper. Given the current submission, the paper needs further development, and non trivial modifications, to be broadly appreciated by the machine learning community.  
This paper extends Bootstrap DQN with multi step TD target. The initial submission had missing details, communication problems, and results lacking rigor. The authors made a clear effort to address the reviewers concerns.  This paper s contribution is supported primarily by the empirical results which need major work. The lack of statistical significance in the key results is a major problem. The new 5 run results (originally only 3 runs) shows no clear evidence of improving over the baseline. Additionally one must either justify the use of such few runs by investigating the distributions and using the proper statistical tools (Colas et al [2]) or simply do more runs. Regardless statistical significance in the precise sense is a requirement.  In addition other adjustments to the paper would strengthen it significantly: (1) The qualitative results like state visitations can be interpreted either in favour of the method or not, this could be improved with discussion or omitted see [1]; (2) the discussion of heterogeneity was informal; (3) discussion of the impact and sensitivity of hyper parameters should be included this includes addressing the concern that the performance of the baseline was as strong as it could be; (4) the current results do clearly separate if the improvement in performance (if it can be shown to be significant) is due to improvements in the rep via auxiliary task effect or the multi step return the reviewer has made a nice suggestion for an experiment here.  In summary, the reviewers did not find the text and examples in the paper convincing as to why the proposed method should be better than bootstrap q, and the results are not significant and need more work.   references that may be helpful: [1] https://openreview.net/forum?id rkl3m1BFDB&utm_campaign RL%20Weekly&utm_medium email&utm_source Revue%20newsletter [2] https://arxiv.org/abs/1806.08295
This paper proposes a decentralized attribution method to distinguish the generative models trained on the sample dataset. The key theoretic result is the derivation of the sufficient conditions for decentralized attribution and the design of keys following these conditions. Results are validated on two datasets with several generative models. The work is very interesting. R1: Overall, I am more positive. I am willing to raise my score to 6. However, the paper is still somewhat borderline. R3: Given the rebuttal, I am willing to raise my score to a 6 due to the added StyleGAN, PGAN, and other experiments and improved paper layout / clarity. The added experiments are welcome addition to the paper and demonstrate this technique. The lip and eyestaining are interesting results and I do hope this direction gets explored in the future. 
The paper addresses a pressing problem for applications involving clinical time series and introduce a pipeline that handle many of the issues pertaining to data preprocessing.  An important contribution is the software that makes the processing more seamless, which will, without a doubt, be useful to the community given the need for reproducibility.  The authors have responded suitably to reviewer comments with the main  leftover criticism  being that such a paper may not be the best fit for ICLR. This isn t a typical paper. However, something that introduces this level of automation and flexibility in handling time series has not been presented at this conference (or other ML conferences) to the best of my knowledge. It seems it could work in conjunction (as opposed to competing) with any new time series models/techniques that may be introduced.
The paper presents a multi agent RL algorithm where the rewards of the other agents are only known up to some accuracy. The setting is somewhat restrictive, in the sense that the transition is assumed to be known. It would perhaps have been more interesting for the paper to also consider unknown transitions, so as to bring it closer to work in single agent reinforcement learning. It also seems to not be making a very good job of linking the related work to the contribution of this paper (even after looking at the appendix).    Authors briefly say in the introduction  "Alternative frameworks improve robustness, e.g., to changes in environment dynamics, observation or action spaces (Pinto et al., 2017; Li et al., 2019; Tessler et al., 2019), but do not address reality gaps due to reward function mismatches, as they use inappropriate metrics on the space of adversarial perturbations"  Authors should try and better explain the differences with those papers. Do  they consider changes in dynamics rather than the reward? It appears that the former is more general than the latter. Couldn t the authors compare with them with an appropriate experiment?  It is also hard to see how this exactly connects with a reality gap. What is the  training  environment? What is the  testing  environment? This is simply a robust optimisation algorithm applied to multi agent games with partially unknown reward functions.   In addition the experiments themselves are not explained clearly.   On the plus side, I think the algorithmic details and experimental are interesting. If there was a better explanation and discussion/comparison with related work, then it would have been a good paper. Authors are encouraged to make a stronger effort to compare with other methods both in terms of the algorithm and experimentally.
This paper received two clear accept, one accept, one borderline accept and one reject review. R4 identified that the paper falls short in discussing recent works from CVPR and ECCV 2020 on the image inpainting and completion tasks which also tackle challenging scenarios in these tasks. The authors improve their related work section with these more recent works while pointing out that the task still remains unsolved and they propose an effective technique towards the solution. The meta reviewer recommends acceptance based on the following observations.   The submission proposes a GAN architecture for image inpainting using co modulation, which is similar to the weight modulation in StyleGAN2 but is conditioned on both the input image and the stochastic variable instead of only the stochastic variable. The main novelty of co modulation appears to be interesting as well as being generalisable to different tasks. The approach is shown to perform well in the image painting with large scale missing pixels and some image to image translation tasks. Furthermore a new metric P IDS/U IDS is proposed to evaluate the perceptual fidelity of inpainted images. 
This paper proposes a technique of decomposing the nonsymmetric kernel of determinantal point processes, which enables inference and learning in time and space linear with respect to the size of the ground set.  This substantially improves upon existing work.  The proposed method is well supported both with theory and experiments.  All of the reviewers find that the contributions are significant, and no major flaws are identified through reviews and discussion.  The determinantal point process might not be one of the most popular topics in the ICLR community today but certainly is relevant.
This paper provides a unified view of some known methods for monotone operator inclusion problems like Forward Backward Forward (FBF) and OGDA, and provides new convergence results for the stochastic version of a variant of FBF called FBFp. All reviewers initially recommended rejection. The rebuttal and the manuscript update addressed several concerns from the reviewers, though the general consensus after rebuttal was still that the paper lacked in significance for the ICLR community. The AC thinks that the paper could make an interesting overview paper in a more optimization / theoretically minded venue.
This paper addresses a meta learning method which works for cases where both the distribution and the number of features may vary across tasks. The method is referred to as  distribution embedding network (DEN)  which consists of three building block. While the method seems to be interesting and contains some new ideas, all of reviewers agree that the description for each module in the model is not clear and the architecture design needs further analysis. In addition, experiments are not sufficient to justify the method. Without positive feedback from any of reviewers, I do not have choice but to suggest rejection.  
Loosely, while IRM aims to find a feature mapping Phi s.t. response Y given Phi(X) is independent of the environment variables E, they suggest that when E is strongly correlated with Y, then it is possible for Phi obtained via IRM to involve environment variables. They motivate this by suggesting that if there exists a feature mapping Phi(X)   E, it would satisfy the IRM aim, but that this is undesirable.  They suggest instead requiring Phi(X)|Y being invariant to the environment.  The reviewers bring up a couple of concerns. The first is that it is not clear outside some simple examples when Y given Phi(X) being independent of E does not suffice. The second is that the authors also do not empirically validate their fix outside a single simple dataset. Moreover, what are the pitfalls of having Phi(X) given Y being independent of E?  Overall, this is an interesting kernel of an idea; it just needs to be fleshed out a bit more. 
The paper deals with adversarial attacks on graph neural networks, a new and promising field in graph representation learning. The paper analyzes a new extreme setting of attack for a single node, and presents important insights, albeit not new algorithms.  The reviewers were not particularly enthusiastic and complained about   limited novelty in light of Zuegner et al   missing baselines   doubts about the attack setting with a selected attacker node  The authors provided an elaborate rebuttal, including specific responses to the above questions, however, the final scores are not quite above the bar, especially having in mind the sheer number of submissions on graph deep learning this year. We, therefore, recommend rejection and encourage the author to publish the paper elsewhere.  
This paper proposes a method to quantify transference, which is a measure of information transfer across tasks, for multi task learning framework. Specifically, the transference is measured as the change in the loss for a specific task after performing a gradient update for another. The proposed transference measure is used to both understand the optimization dynamics of MTL and improve the MTL performance, either by grouping tasks or combining task gradients based on the transference. The method is validated on multiple datasets and is shown to bring in some performance gains over the base MTL model (PCGrad, UW MTL).   The majority of the reviewers were negative about this paper (4, 4, 5), while one reviewer gave it a positive rating (6). The reviewers in general agreed that the idea of measuring transference as the change in the loss with gradient updates is novel and intuitive. Yet, the reviewers had common concerns on the 1) weak performance improvements, and the 2) high cost of computing the transference. While computing the transference requires additional computations with linear time complexity, which may be problematic with a large number of tasks, the performance gains using it were rather marginal (less than 0.5% over the baselines).  Another common concern from the reviewers was its insufficient experimental validation, as a comparative study against existing works that perform task grouping is missing. Both the authors and reviewers actively participated in the interactive discussion. However, the reviewers found that the two critical limitations persist even after the authors’ feedback, and in a subsequent internal discussion, they reached a consensus that the paper is not yet ready for publication.   Thus, although the proposed method is novel and appears to be promising, it may need more developments to make it both more effective and efficient. Moreover, there should be more in depth analysis of its time efficiency, and other benefits (e.g. interpretability) that could be achieved with the proposed transference measure. Finally, while there exist many works on learning both hard or soft task grouping, the authors do not reference or compare against them. To name a few, [Kang et al. 11] propose how to learn the discrete task groupings, [Kumar and Daume III 12] propose to learn a soft grouping between tasks, [Lee et al. 16] propose to learn soft grouping based on asymmetric knowledge transfer direction across the tasks, and [Lee et al. 18] proposes the extension of [Lee et al. 16] to a deep learning framework. I suggest the authors to discuss and compare against the above mentioned works, and fortify the related work section by searching for more classical works on multi task learning.     [Kang et al. 11] Learning with Whom to Share in Multi task Feature Learning, ICML 2011   [Kumar and Daume III 12] Learning Task Grouping and Overlap in Multi task Learning, ICML 2012   [Lee et al. 16] Asymmetric Multi task Learning based on Task Relatedness and Confidence, ICML 2016     [Lee et al. 18] Deep Asymmetric Multi task Feature Learning, ICML 2018.
This paper is a bad fit for ICLR and the authors may consider submitting to more theoretical venues. This paper studies algebraic geometry (an area unfamiliar to most ICLR readers) of program synthesis, with the "hope that algebraic geometry can assist in developing the next generation of synthesis machines." Unfortunately, this paper does not get far enough down that path, and its implications cannot realistically be appreciated by an ICLR audience. The reviewers indicate that their low confidence is due to their lack of understanding of algebraic geometry and not due to their lack of understanding of program synthesis. The featured implication of the paper is that synthesized programs are singularities of analytic functions, which is not very meaningful to the ICLR audience. Even if external reviewers verified the correctness of the math, the ICLR audience would still not understand the implications. 
The authors propose a novel approach to a dialog based  automated medical diagnosis, and present promising empirical results. The focus of this work is on robustness and reliability besides just the accuracy of diagnosis, which appears to be an important aspect in medical applications. The paper is clearly written and well motivated. However, in there are still several concerns raised by the reviewers, and the paper may require a bit of extra work to be ready for publication.
This paper propose an approach to probabilistic meta learning for Bayesian optimization. The goal is to accelerate Bayesian optimization under the assumption that multiple related tasks require optimization. In terms of strengths, the paper addresses an important problem as it has applications to efficiently optimizing hyper parameters over multiple related data sets or models. In terms of weaknesses, the proposed approach is closely related to neural processes, but this connection was not made in the original paper. The authors were unfortunately not able to provide additional insights or results regarding this point during the limited discussion period and as a result, the novelty of the method at the core of this approach is in question. The reviewers also note that the experimental designs and comparisons performed are limited. For some smaller problems, more standard baseline methods like multitask GPs should be applied. The authors have also not compared to a number of other recent methods aimed at scalable transfer learning for hyper parameter optimization, as detailed in the comments of Reviewer 1. The reviewers agree that the paper is not yet ready for publication.
I think this is a very promising paper, but the work is not ready for publication.   The most significant concern shared by several reviewers is the insufficient evaluation. For example, the work is not compared with more traditional approaches to equivalence checking or any other baselines beyond ablations of the proposed method. Given that this is not the first paper to propose the use of deep learning to search for proofs, it seems important to compare to alternative methods. There is also a misalignment between the claims of novelty and the evaluation. For example, section 4 cites the novel approach to generating data as key to this approach, but the evaluation does not really address this claim. On the positive side, I was impressed with the ability to search for proofs of length 10 given the large branching factor, and I thought the results were promising.   The authors should also consider some of the concerns with presentation raised by the reviewers. 
The setting and the problem addressed by this paper has been considered as important and interesting to tackle with reinforcement learning. Yet, the reviewers expressed several concerns about this paper. Especially, the lack of comparison to state of the art methods and to the standard visualization methods was a shared concern. The empirical validation also appeared as not ambitious enough. Finally, the novelty in the field of machine learning was also questioned since the paper is mainly about applying existing algorithms to a known problem. 
This is a difficult borderline decision, with the reviewers evenly split in their final recommendation.  Overall, the authors provided good responses to the reviewer questions: this was much appreciated.  The reviewers requested additional ablations and explanations, which the authors provided.  A prevailing concern is that the experimental evaluation, restricted to a few standard MuJoCo environments, does not really demonstrated a distinctive advantage for the proposed approach.  In fact, one of the new ablations added raises concerns about the significance of the paper s main technical contributions: the \lambda_f 0 row added to Table 3 shows very strong reward results, which apparently obviates a key aspect of the proposed approach.   This work is interesting, and would like to see it published, but the current state of the evaluation does not support the significance of the main contribution.  I think the authors need to expand their empirical evaluation, as suggested, to better highlight the effectiveness of the proposed approach over the \lambda_f 0 baseline.  In the end, I think the authors would be better served by broadening the evaluation, isolating scenarios where the key proposal shows significant (rather than marginal) benefits, and publishing a more compelling version.
# Quality:  While the paper presents an interesting approach, Reviewer 2 raised relevant questions about the assumption of the theoretical justification that needs to be thoroughly addressed. Moreover, as noted by Reviewer4, the quality of the paper would also benefit from a more clear connection to existing model based reinforcement learning literature, besides [Pan et al.]. For example, how much of the proposed approach and results can be applied in other algorithms?  # Clarity:  While the paper is generally well written and only minor suggestions from the reviewers should be implemented.  # Originality: The proposed approach is a small but novel improvement over existing algorithms (to the best of the reviewers and my knowledge).  # Significance of this work:  The paper deal with a relevant and timely topic. However, it is currently very difficult to gauge the significance of this work, and it unclear if the results can be extended beyond toy benchmarks and to other RL algorithms. Several reviewers suggested additional experiments to strengthen the paper.  # Overall: This paper deal with an interesting topic and presents new interesting results. However, the current manuscript is just below the acceptance threshold. Extending the experimental evaluation and improving the clarity of the paper would crucially increase the quality of the paper. 
The paper proposes to improve image segmentation from referring expression  by integrating visual and language features using an UNet architecture and experimenting with top down, bottom up, and combined (dual) modulation.    Review Summary: The submission received divergent reviews with scores spanning from 2 (R2) to 5 (R3,R4) to 10 (R1).  The author response failed to address the reviewer concerns with some reviewer (R4) lowering their score tto 4 after the rebuttal.  It also became clear that some relevant work (Mei et al, 2018) was used for the baseline but not cited.  The author response also did not recognize the importance of significance tests.  As there is considerable work in the area of image segmentation from referring expression, and the proposed model is very similar to the LingUNet model of Misra 2018, the originality and significance of the work is fairly low.  The main contributions appears to be experimental comparisons of the three types of modulation (top down, bottom up, dual).  Pros:   Investigation of a important problem of grounding language to visual regions   Experimental study of whether dual modulation improves image segmentation from referring expression  Cons   Relatively minor novelty with limited analyses (R3,R2)   Missing citations (see R3 s comments).  Relevant work (Mei et al, 2018) which was the basis for the top down baseline model, was used but not cited or properly compared against   Relatively weak experimental results (R2,R4).  As R4 noted, while validation results are good, test results are weak compared to existing work, indicating potential overtuning.     No qualitative comparisons against baselines.     Cognitive claims not backed up and limited discussion/analysis (R2)  Recommendation: The AC concurs with R2, R3, and R4 that the work is limited in novelty and not ready for publication at ICLR.   Despite R1 s high score, referring expression for image segmentation is a well studied task, and it is unclear what are the key innovations of the proposed model over LingUNet.  Due to the limited novelty, relatively weak test results, as well as other flaws pointed out by the reviewers, the AC recommends rejection. 
Most reviewers are positive about this work, though they believe it is somewhat incremental, and its theoretical contributions are minor. None of the reviewers are very excited about this work. Overall, the PC believes this is a borderline paper.  Minor note: During the discussions, the paper by Xiao et al., "Characterizing Attacks on Deep Reinforcement Learning" (2019) was brought up. The authors claimed that they did not compare with that paper because the best attack there (obs fgsm wb) had already been studied. In a later stage of discussions, one of the reviewers stated that the method obs nn wb in that paper performed better in some domains. Even though this is not a major issue, it is advisable to the authors to make sure that this is indeed the case, and if it is, provide proper comparison with that paper.  We encourage the authors to consider the reviewers  comments to improve the paper and resubmit to a future venue.
The paper proposes a graph aligning approach generating rich and detailed labels given normal labels. Authors cast the problem in a domain adaptation setting, considering a source domain where "expensive" labels are available, and a target domain where only normal labels are available. The application scenario is the prediction of chemical compound graphs from 2D images, where a fully mediating layer is introduced to represent using a planar embedding of the chemical graph structure to be predicted.   The paper received ratings all below threshold. The main issue transversal to all reviewers relate to clarity of the presentation. Clear motivations for some of the adopted choices of the method and of the experimental procedure were also missing. In particular, missed to provide the clear usefulness of the main paper s contribution, i.e., to neatly show the importance of the mediating layer (ref. R4, R2).  The lack of important details in the method description and experimental results were also deemed a major shortcoming: cost of the optimization, model generalization not discussed, contradictory results on the different datasets considered, comparative analysis, partial ablation, are among the main quoted remarks.   Authors  rebuttal is carefully provided in general, but several issues are still remaining.  Hence, overall, given the above issues, I consider the paper not yet ready for publication in ICLR 2021. 
This paper revisits the design of positional embedding in the pre trained language models, and propose a new approach to handling the positional encoding.   Overall, the paper is well motivated. The authors have addressed most comments based on the review. The method proposed in the paper is simple and effective. Experiments are comprehensive and demonstrate the effectiveness of the proposed approaches.  
While the reviewer s noted a number of strengths of your paper, the approach that you took, and agreed that you had tackled an important problem, concerns remained about presentation and clarity. I agree. (Here are just a few miscellaneous comments: the very first paragraph of the Introduction needs to be rewritten for clarity, in my opinion. Later on page 1, you use the term "the dynamics of density" but you should not assume that the reader knows what that means. There are typos as well, e.g. "make all predictions base[d] on Equation (6)" on page 4. It would be helpful to know something about why you chose the experimental setups in Synthetic 1, 2, and 3. )  Regarding the similarities between this paper and a previously published article I believe that the authors have addressed these concerns; I hope they are careful to avoid this situation in the future.
Reviewers like the simplicity of the approach and the fact that it works well.  
This paper did experimental studies on how DPSGD and SSGD converge in different tasks. Some concerns were raised regarding the clarity, some unjustified claims, baselines and etc, and partially addressed after the rebuttal and discussions. However, some critical concerns remains. The reviewers agreed that the paper would be more appealing if these concerns can be well addressed.
All reviewers gave either borderline or negative scores; unfortunately, discussion was not lively, so scores remained the same. No reviewers voice strong support for acceptance, but acknowledge several merits of the work.
This paper received 4 reviews with mixed initial ratings: 7, 5, 4, 5. The main concerns of R1, R2 and R4, who gave unfavorable scores, included: lack of methodological novelty (analysis only paper), absence of experiments on real data (3 synthetic only benchmarks), missing baselines and an overall inconclusive discussion. At the same time R5 notes that the offered fair comparison between SOTA methods was indeed "much needed", and the paper can "serve an important role" in guiding future developments in the community. In response to that, the authors submitted a new revision and provided detailed answers to each of the reviews separately. R1, R2 and R4 did not participate in the discussion, and R5 stayed with the positive rating. AC agrees with R5 that the provided analysis is insightful, and the effort put into organizing the research community around a single set of benchmarks and metrics is indeed valuable. However, given a simplistic nature of the proposed datasets and lack of other methodological contributions, the submission is not meeting the acceptance bar for ICLR. After discussion with PCs, the final recommendation is to reject.
The paper proposes a novel method for representing persistence diagrams by embedding them to a Poincare ball.  The representation is learnable, and unifies essential and non essential features.   The experimental comparisons with existing representation methods show significant improvements in performance.    The flexible data driven embedding to a suitable geometric space is a novel idea, which will certainly advance the usefulness of TDA.  The  experimental resutls demonstrate well the advantage of the proposed repesentation.  The authors have also addressed the review comments appropriately, with some extra experiments.  This is also a good addition.   
This paper proposes a testing procedure to determine whether a policy is better than another policy with respect to long term treatment effects. The reviewers found the problem interesting and saw a lot of value in this work. One of the key concerns was the lack of clarity throughout the paper. The reviews helped the authors actively revise the paper, improving the paper s overall readability throughout the discussion phase. However, the reviewers did not change their ratings. While I agree that this work has merits, since there are many legibility issues, I cannot recommend its acceptance at this stage.   
This paper proposes a conditional language specific routing (CLSR)  mechanism for multilingual NMT, which also considers the trade off between language specificity and generality.  All of the reviewers think this paper is interesting for both idea and empirical findings. Therefore, it is a clear acceptance.
This paper aims to study the convergence of deep neural networks training via a control theoretic analysis. This is a very interesting approach to establish theoretical understanding of deep learning. However, there are several concerns raised by the reviewers:  1.	The contribution of this paper is limited. The results simply follow from standard optimal control. It is not clear what new insight the paper provides. 2.	There are already quite a few works on control theoretic analysis of deep learning. This paper did not do a good job on presenting its novelty and difference with existing works. 3.	The experimental part is weak. It only involves small data set and very simple networks.  Based on these, I am not able to recommend acceptance for the current manuscript. But the authors are encouraged to continue this research. 
This work addresses a novel and important real world setting for semi supervised learning – the open world problem where unlabeled data may contain novel classes that are not seen in labeled data.  The paper provides an approach by combining three loss functions: a supervised cross entropy loss, a pairwise cross entropy loss with adaptive uncertainty margin, and a regularization towards uniform distribution.    The authors were responsive to reviewers’ comments and have respectively improved their paper by adding experiments, including an ablation study of each component of the objective function, study of the effect regularization on unbalanced class distributions, reporting accuracy on pseudo labels.  While two reviewers have slightly increased their scores, some concerns still remain.  This is a borderline paper, and after some discussion and calibration, we decided that the work in its current form does not quite meet the bar for acceptance.
Taking all reviews and the work in consideration, unfortunately the work does not present the breadth it needs to sustain the claims it makes. In particular, there work requires to analyse more architectures/variations of datasets with different properties and to provide more careful ablation studies that shows the efficiency of the 3 different proposed methods. Potentially removing one of this methods in order to give more space to analyse the others that seem more promising. 
This paper relates the problem of influence maximization and adversarial attacks on GCNs.  The paper, and its formulation and assumptions stirred up quite a discussion among the reviewers and the authors. I do appreciate the thorough rebuttal that the authors provided, and the reviewers did take it into account (and revised their scores).  However, all in all, I am afraid that there are just a few too many concerns with this paper.  If the authors take the reviews to heart, they should be able to improve the manuscript and submit a stronger and improved version to the next conference. 
This paper presents a representation method for time series data in the sequential VAE, where the global feature z and local features  s are better disentangled. The intuition behind learning z is to maximize the mutual information between z and input x, while minimizing the mutual information between z and s. The second mutual information is estimated with a discriminator in the DRT framework. Overall, the methodology can be seen as reasonable applications of the disentanglement principle to sequential data. The authors have shown that z and s learned in this way is better disentangled as compared to beta VAE. In the end, the reviewers feel that while there is good intuitions/technical ingredients, the derivations in Section 3 are not very smooth, and several approximations/choices are not very carefully justified (e.g., choice of alpha, choice of DRT vs. other MI estimators), and perhaps stronger baselines than beta VAE can be used.  The reviewers rate this paper to be borderline.
The paper proposed a GNN model based on a weighted line graph (dual of the input graph), where information is simultaneously propagated on both graphs, coupling the two propagations at each step.   Overall, the reviewers were lukewarm about the paper, with some raised criticism including    limited novelty in light of Monti et al. 2018   limited theoretical justification   unconvincing and incomplete experiments, not offering significant improvement compared to other alternatives  While the presented approach is interesting, we believe the paper is below the bar and recommend Rejection.  
I thank the authors for their submission and participation in the author response period. The updated experiments are appreciated. However, after discussion all reviewers unanimously agree that the paper is not ready for publication and encourage resubmission to another venue. In particular, R2 and R3 have raised concerns regarding additional widely available baselines that need to be evaluated against and that the rebuttal has not addressed. I agree with this assessment, and thus recommend rejection.
This paper presents a knowledge distillation method for face recognition, by inheriting the teacher’s classifier as the student’s classifier and optimizing the student model with advanced loss functions. It received comments from three reviewers: 1 rated “Ok but not good enough   rejection”, 1 rated “Marginally below” and 1 rated “Marginally above”. The reviewers appreciate the simple yet clear methodology illustration and the well written paper. However, a number of major concerns are raised by the reviewers, including limited novelty, lack of comparison with more advanced knowledge distillation methods and their special case in face recognition. During the rebuttal, the authors made efforts to response to all reviewers’ comments. However, the rating were not changed. The ACs concur these major concerns and more comprehensive comparisons with the state of the art KD methods are necessary to better illustrate the contribution of this work. Therefore, this paper can not be accepted at its current state. 
The reviewers liked the overall idea presented in this paper. Although the idea as well as relevant tooling for incorporating constraints in the latent space has been studied a lot in the past, the authors differentiate their work by applying it in a new interesting problem. At the same time, some confusions about relation to prior work remain after rebuttal. Firstly, the theoretical additions to prior work (Srinivas et al. 2010) are still unclear in terms of significance   they feel more like observations made on top of an existing theorem rather than fresh significant insights. Furthermore, even if prior work has not considered exactly the same set up, it would still be needed to understand what the performance would be when considering prior models or prior datasets used in similar domains (e.g. suggestions by R2, R3). The latter would be desirable especially since the experimental set up used in this paper is deemed by the reviewers too simple (while the motivation of the paper is to solve an issue essentially manifesting in complex scenarios).
The paper addresses the problem of solving for the eigenpairs of a self adjoint differential operator. This problem, of course, is classical; the main innovations here are  a) the use a parametric form of the (pointwise) solution using a (shallow) neural network so as to avoid discretization, and  b) obtaining multiple eigenpairs simultaneously as outputs.  The techniques themselves (i.e., the architecture, the loss function, and the training procedure) are fairly standard.  There was some variance in the review scores. The reviewers appreciated the importance of the problem and the direction adopted by the authors. However, concerns were raised regarding: the limitations of the experimental evaluation; a lack of sufficient distinction from prior work; *very* limited comparisons with prior approaches; and the limited demonstration of generalizability. I agree with all these criticisms, and therefore vote to reject.  
The paper proposes three algorithms for the sparse PCA problem, where one imposes the additional constraint that the vectors have a small number of non zero entries. The proposed algorithms run in polynomial time and achieve provable approximation guarantees on the accuracy and sparsity. The reviewers identified the following strengths of the contributions: the algorithms are simple and have different strengths; the theoretical results are sound and perhaps even surprising; the presentation is clear. The reviewers identified the following weaknesses of the contributions: the running times of the proposed algorithms are high and they may not scale to large datasets, which significantly limits their application to machine learning datasets; the experimental evaluation is insufficient and it does not compare with some of the state of the art algorithms; the algorithmic novelty is limited. After weighing these strengths and weaknesses as well as evaluating the paper relative to other ICLR submissions, I recommend reject.
The paper introduces a new federated learning algorithm that ensures that the objective function optimized on each device is asymptotically consistent with the global loss function.    Both theoretical analysis and empirical results, evaluating communication efficiency, demonstrate the advantages of the proposed FedDyn method over the baselines.   All the reviewers recommend accepting the paper. To summarize the discussion:    R1 mentioned a very recent (NeurIPS 20) related paper and asks several questions. I believe that the authors nicely answered the questions and discussed the relation to the previous paper in detail.      R2 mentioned that the paper focuses solely on minimizing communication costs, ignoring costs of local computations. The authors argued that the local computation costs are comparable to those of the baselines, and, in general, communication costs are the main source of computation energy costs (pointing to previous work), and, thus, are a natural objective to optimize.  I believe that this adequately addressed this (and other) reviewer s concerns and the reviewer kept their score unchanged.    R3 had several concerns, which according to the reviewer were addressed in the rebuttal (they increased the score).    R4 points out several limitations of the method and theoretical analysis and believes that the rebuttal did not quite address the concerns. Nevertheless, remains positive about the paper, and believes that the shortcomings can be addressed in follow up work.  We share the reviewers  sentiment: it is a very nice and interesting paper, and should be accepted. 
This paper explores the role of hyperparameters in the separate phases of a classic pruning pipeline: mask identification and retraining. Key observations include a set of the hyperparameters to search relative to a standard regime as well as the identification that the layerwise pruning rates from mask finding are intertwined with these hyperparameters and are what chiefly affects the eventual performance of the pruned network.  The pros of this paper are that it works against the contemporary wisdom that the default hyperparameters for a model are the best for finding a mask for the model. Instead, there are improvements to be had by identifying a set of hyperparameters that lead to worse overall model accuracy, but better masks. Second, the work shows that the layerwise pruning rates are the key elements of these hyperparameters effect. The rates can in fact be transferred to more poorly performing network configurations and improve performance.  The cons of this paper, as noted by the reviewers, are the somewhat unclear implications of the technique. The added guidance on directions to improve hyperparameters is valuable but does not necessarily provide a cost effect strategy to find these. At its strongest, this guidance offers practitioners a recommendation to also consider hyperparameters for the initial model.   The stronger, forward looking implication is, instead, the connection to layerwise pruning rates. Specifically, while layerwise pruning rates have been demonstrated to be important in the literature (e.g., [1]), there has been a limited study into the exact nature of a good set of pruning rates versus a bad set of pruning rates.  Where this paper stops short of a clear result, is if were to connect excessive pruning of the earlier layers, or simply the layerwise rates themselves to another property of the network (e.g., gradient flow, or capacity) that indicates the improved eventual performance.  My Recommendation is to Reject. The paper s core experiments are well executed. However, this final detail, closing the gap between the portability of these layerwise rates and a conceptual understanding, is a key missing component.  Once done, that will make for a very strong paper.  [1] AMC: AutoML for Model Compression and Acceleration on Mobile Devices. Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li Jia Li, Song Han. EECV, 2018  
The authors propose a new dataset and compositional task based on the EPIC Kitchens dataset.  The goal is to test novel compositions and to build a transformer based network specifically for this inference (by analogy). Specifically, the analogy here references the use of nearest neighbors in the dataset.  There are a lot of concerns raised by reviewers which require a large number of changes to the presentation of the manuscript and they are not at present convinced by the current setup or experiments. Explicitly motivating which pretraining methods do or do not violate which aspects of composition and what role other factors like synonymy play in generalization is necessary. Several aspects of the claims made in the paper and in the discussion are big claims that require substantial discussion and analysis (e.g. the surprising weakness of pretrained models) which the reviewers do not feel can be so easily explained away (e.g. by domain shift).
This paper considers the reinforcement learning in rich observation setting. Concretely, the authors provide a provable sample efficient algorithm for the rich observation factored MDP. As the majority of the reviewers commented, although the techniques used in the proof share some similarities to the existing work, the analysis for the whole algorithm is still challenging. As a theoretical oriented paper, I think this paper should have a position in ICLR.   The major concern of the paper is the necessity of the assumptions (R2). The validation and justification of the Assumptions should be stated clearly in main text, even they are adapted from the prior work. 
The paper surveys existing differentially private data synthesis methods, and introduces an algorithm that learns both a generator and a classifier in a differentially private mode.  The problem is highly timely and important. Results are promising.  Main remaining concerns after discussion between the reviewers and the authors are:    reason why the proposed scheme can give better classification accuracy, should be clarified more    unclarity on conclusions that can be drawn from the experiments. The revised version has improved on this somewhat.  One explanation for the problems was suggested to be that the paper tries, at the same time, to both present a new method and be a survey. Is hard to do in a short paper, and as a result, the paper lacks focus. At the very least, more work is needed.  The authors are encouraged to continue their work on this important problem, and the review comments hopefully help in that. 
This paper presents a new algorithm for distributed multivariate mean estimation. This method performs significantly better than previous approaches when the input vectors have large norm but are relatively close to each other. The approach relies on lattices and randomized rounding. The approach is evaluated experimentally as well. Overall, there is consensus among the reviewers that this work solves a clean problem using non trivial ideas. I recommend accepting the paper.
The paper shows that show that methods for probabilstic lifted inference can also be used to "compress symmetries" in convolutional models over structured data. The resulting structured convolutional models are then shown to yield speed ups for learning graph neural networks, too. This is highly interesting since the existing literature rather considered how to make use of weisfeiler lehman for classification of graphs, both in neural and a kernel way. This paper, however, shows how to compress the computations. And it paves the way to connect equivariant learning lifted inference by exploiting the connection between lifted probabilistic inference / weisfeiler lehman and their algebraic formulations. 
I agree with the concerns raised by the reviewers. In particular, the issues of novelty and experimental evaluation (mentioned in the revision summary) remain the major weak points of the paper. My impression is that the changes made in the revision represent a significant experimental addition to the paper, one which might merit a full pass through peer review, and one which in any event did not alter the reviewers  scores. While I think this paper has something to contribute (and the empirical results suggest the method may outperform competitors), I think it would be improved by a rewrite (and possibly a restructure) that makes the part that is your contribution much more clear. For example, in the abstract, it s only in the sentence "We show both theoretically and empirically that potential vanishing/exploding gradients problems can be mitigated by enforcing orthogonality to the shared filter bases" that we actually get to the part that is really novel about this contribution (the "enforcing orthogonality"): that would ideally be much earlier in the abstract.
Authors extend the probabilistic PCA framework to multinomial distributed data. Scalable estimation of principal components in the model is achieved using a multinomial variational autoencoder in combination with an isometric log ratio (ILR) transform. The reviewers did not agree on the degree of novelty of the paper to PC estimation. The presentation of the paper can be improved. The reviewers criticise that large changes have been made to the paper during the rebuttal phase. Overall, the paper is borderline and due to the mentioned large changes I recommend a rejection (and re review at a different venue). 
This paper proposes a scalable optimization method for multi task learning in multilingual models.   Pros: 1) Addresses a problem which has not been explored much in the past 2) Presents very good analysis to show the limitations of existing methods.  3) Good results.  4) Well written  Cons: 1) Some missing details about various choices made in the experiments (mostly addressed in the rebuttal)  This is a very interesting and useful work and I recommend that it should be accepted. 
This paper introduces a few variants of neural ODE architectures to improve their expressivity.  The motivation and method make sense, but are fairly incremental.  The tasks are also fairly low dimensional and as one reviewer pointed out, reconstruction isn t a good benchmark task.  However, the paper seems well executed, and the rebuttals answered the expert rewiewers  concerns.
The reviewers raised several theoretical and empirical questions about the paper. During the rebuttals, the authors seem to  successfully address the experimental issues, in particular those raised by Reviewers 1 and 2. However, the theoretical concerns have mainly remained unanswered. Reviewer 2 has a major concern about the convergence of Algorithm 1, both Reviewers 3 and 4 (in particular Reviewer 3) have several questions about the theoretical analysis and see the technical contribution of the work relatively low. These are not minor issues and require a major revision to be properly addressed. So, I suggest that the authors take the reviewers comments into account, revise their work and properly address the issues raised by the reviewers, and prepare their work for an upcoming conference. 
This paper presents a new method of employing some existing techniques to improve robustness, which was verified through experiments. According to the reviewers’ comments and the authors’ responses to these comments, the reviewers generally appreciate the authors’ effort in properly improving and clarifying the proposed method. However, their major concerns still rely on the novelty of this paper, which is identified as a combination of some existing techniques. In addition, the proposed method at its current status still contains some un convincing points. Hence, the paper is recommended rejected.
This paper presents a self training idea for GCN models to help improve the node classification. The reviewers agreed that the technical contribution of the proposed approach is limited and the performance improvement seems marginal. 
Motivated by the possibility of Neural Architecture Search on domains beyond computer vision, this paper introduces a new search space and search method to improve neural operators. It applies the technique to problems in vision and text.  Reviewer 1 found the paper interesting and liked the motivation of considering different tasks in NAS. However, they found some aspects of the paper confusing and, like other reviewers, thought that the baselines were weak. The authors clarified some points, and R1 said that some, but not all, concerns were resolved. The reviewer improved their score by a point but still was not in favour of acceptance.  Reviewer 2 thought the paper was interesting but questioned its main claim: that it was proposing a search space over novel operators. They argued that what was discovered was similar to convolution and therefore not much had been gained. They questioned the significance of the ablation studies: there were a lot of them, but they focused on relatively simple tasks like MNIST and CIFAR 10. They also asked some clarifying questions which were answered by the authors. Pushing back on the point about the smaller scale of the experiments (in a general reply to all reviews), the authors said that the goal of their work was not advancing computer vision, but to push NAS beyond computer vision and simple search spaces to new application domains.  Reviewer 3 liked that the paper gave a good overview of the NAS problem and thought that it was ambitious. They also thought the approach was novel and promising. Like R2’s comment, R3 seemed disappointed that the search was over “reparameterized convolutions”. In fact, they thought that the paper was overselling its contribution. They pointed out that performance was still far from state of the art on the various benchmarks. The authors argued against this view of “reparameterizing convolutions” and claimed that the search space was, in fact, much larger than that of DARTS. R3 read and responded to the rebuttal, appreciating the response but ultimately thought that the search space wasn’t clear and comparisons fell short. Reviewer 4 shared similar pros & cons as have been pointed out by the other reviewers. They thought that operator search was limiting and that the paper should also consider topology. The authors responded to this, saying that they intentionally fixed the topology. Searching beyond operators was out of scope. R4 responded to the rebuttal though still had some remaining concerns both in terms of motivation and execution.  Multiple reviewers said that they would have considered the paper more favourably had an updated paper been submitted, addressing some of the original concerns. As it stands, all of the reviewers think that the paper has some merits but none believe after considering the author response, that the paper is ready for acceptance. I see no reason to overrule the consensus.
the paper undoubtedly tackles an interesting problem in the mainstream of learning with partial / unknown / weak / noisy / complementary labels. The authors have had a set of constructive suggestions and questions from the reviewers (and external comments), some positive, some negative. I find it a bit unsettling that to several major questions, the main feedback from the authors was a citation in the paper with no further action; (a) ablation tests of R2 end up in citing papers from a public comment, (b) R4 raised a key point in comment 2, with the links to partial labels learning. The authors’ answer is not satisfying as one would have hoped at least of a partial justification of the author’s approach in this context. The authors would have had time to develop at least elements of a formal comparison. Just citing the work is not enough;  
This paper addresses the question of RL in high dimensional spaces by learning lower dimensional representations for control purposes. The work contains both theoretical and empirical results that shows the promise of the proposed approach.  While the reviewers had initial concerns, including with a problem in a proof and questions around the contributions, after robust responses and discussions this paper is now in good shape.
This work tackles to address the sparse reward problem in RL. They augment actor critic algorithms by adding an adversarial policy. The adversary tries to mimic the actor while the actor itself tries to differentiate itself from the adversary in addition to learning to solve the task. This in a way provides diversity in exploration behavior. Reviewers liked the paper in general but had several clarification questions. The authors provided the rebuttal and addressed some of the concerns. Considering the reviews and rebuttal, AC and reviewers believe that the paper provides insights that are useful to share with the community. That being said, the paper will still immensely benefit with more extensive experimentation on standard benchmark environments like Atari, etc. Please refer to the reviews for other feedback and suggestions.
The paper proposes a learning framework for Hypergraphs. The proposed method can be viewed as generalisation of GraphSAGE to hyper graphs. Though the paper emphasises that there is significant differences between Hypergraphs and Graphs and hence new methods are required. However, the proposed methods are not significantly different than that used for Graphs. Thus the novelty seems to be limited and hence it is difficult to strongly argue for acceptance. 
This paper proposes an adversarial attack method based on feature contributive regions. In generating adversarial perturbations, Grad CAM heatmaps are used as constraints for the perturbation. The overall idea is interesting and straightforward. However, as several reviewers raised, similar methods have been proposed in prior literature (Yao et al. CVPR 2019 and Xu et al. ICLR 2019), therefore making the novelty questionable.   All three reviewers have indicated rejection, and there is no author response. The paper is therefore rejected. 
This work considers an apparent problem with current approaches to compositional generalisation (CG) in neural networks. The problem seems to be roughly: 1. prior work in CG aims to extract  compositional representations  from the training distribution 2. work on CG, the training set and the test set are drawn from different distributions therefore 3. we don t know whether these models can also extract compositional representations from the test distribution  All four expert reviewers were, to differing degrees, confused by this problem framing, largely because they consider the premise (1) to be false.   I am also aware of a large body of recent work on CG in neural networks (see those papers listed by R2) and, as far as i know, none of it involves extracting  compositional representations  from the training set. Rather, it involves learning something (from the training set) that enables strong performance on a test set that differs from the training set in a way that is informed by ideas of compositionaity.   As far as I know, there are very few  studies that try to identify compositionality by considering the internal representations of neural networks, so it feels incorrect to claim this is standard practice. Any work that goes down this route ought to have a very thorough treatement of the various thorny philosophical and theoretical treatments of compositionality in the literature. As pointed out by R4, the work in its current form does not do this.   In summary, this work attempts to solve a problem that none of the four expert reviewers consider to be in need of a solution. 
Most of the reviewers and AC found many claims of this submission unsubstantiated. 
I think there is a lot to commend in this paper: the general approach for training f_phi in this way is creative and interesting, the discussion of the amortization gap is thought provoking, and the general idea is not something that I have seen in the literature before. That said, the reviewers raise a number of important concerns about the approach, chief of which is that the paper s explanation for why the method works is questionable. The proposed method can be viewed as simply a more expressive policy architecture. In fact, I suspect strongly that the modest increase in performance is likely explained by this alone. The discussion with Reviewer 2 in particular makes this issue very clear. I don t think the authors offered a very compelling response to this. Therefore, I think there are just too many question marks about this approach to accept the paper for publication at this time.  I do however think that this line of research is very promising, and I would encourage the authors to continue this work and flesh out the evaluation to be more rigorous and complete, understand whether the increase in performance comes down simply to increased expressivity or if the discussion of amortization is closer to reality, and also address other concerns raised by R2 and the other reviewers.
This paper introduces a novel game theoretic view on PCA which yields an algorithm (EigenGame; Algorithm 2) that allows evaluation of singular vectors in a decentralized manner. The proposed algorithm is significant in its scalability, as demonstrated in the experiment on a large scale dataset (ResNet 200 activations). This paper is generally clearly written, and in particular Section 2 provides an easy to follow reasoning leading to the proposed game theoretic reformulation of PCA. I felt that the later sections are a bit condensed, including the figures. In the authors response major concerns raised by the reviewers have been appropriately addressed. I would thus recommend acceptance of this paper.  What I found particularly interesting in their game theoretic reformulation is that in the utility functions shown in (6) the orthogonality constraints $\hat{u}_j^\top\hat{u}_i 0$ have been removed and replaced with the soft constraints represented as the regularizer terms encouraging the orthogonality. Although several alternative forms for the regularizers would be possible, it is this particular form that allows an efficient gradient ascent algorithm which does not require explicit orthonormalization or matrix inversion is straightforwardly parallelizable.  Pros:   Provides a novel game theoretic reformulation of PCA.   Proposes a sequential algorithm and a decentralized algorithm for PCA on the basis of the game theoretic reformulation.   Provides theoretical guarantee for the global convergence of the sequential algorithm.   Demonstrates that the proposed decentralized algorithm is scalable to large scale problems.  Cons:   The latter statement of Theorem 4.1 requires conditions on the initialization, which are hard to satisfy in high dimensional settings.   Significance of the proposed game theoretic formulation in the context of game theory does not seem to be well explored. 
This paper improves MoCo based contrastive learning frameworks by enabling stronger views via an additional divergence loss to the standard (weaker) views. Three reviewers suggested acceptance, and one did rejection. Positive reviewers found the proposed method is novel and shows promising empirical results. However, as pointed out by the negative reviewer, the paper should have clarified about computational overheads of the method compared to the baseline (MoCo) in several aspects, e.g., their effective batch sizes or training costs, for the readers’ better understanding. As the concern was not fully resolved during the discussion phase, AC is a bit toward for reject. AC thinks the paper would be stronger if the authors include more ablations (and the respective discussions) regarding this point, e.g., CLSA multi (and  single) vs. MoCo v2 under the same training time, both at early epochs (~200; as reported in the author response) and longer epochs (after convergence; ~1000 and even more). 
This paper studies the training of multi branch networks, i.e., networks formed by linearly combining multiple disjoint branches of the same architecture.  The four reviewers seem to reach a consensus that the paper is not ready for publication for ICLR. 
This paper presents a framework for adversarial robustness by incorporating local and global structures of the data manifold. In particular, the authors use a discriminator classifier model, where the discriminator tries to differentiate between the original and adversarial spaces and the classifier aims to classify between them. The authors implement the proposed approach on several datasets and the experimental results demonstrate performance improvements. The idea of using the global data manifold into addressing robustness of the learning model is interesting. However, the technical contribution and novelty have not been explained very well.
The paper proposes inner ensemble method where output of inner layers are replaced by an ensemble average of them during inference time to reduce inference time and reduce variance. The authors include experiment results showing performance improvement of their method and use the theoretical analysis of the dropout and maxout to justify the goodness of the proposed method.   pros.  The authors consider a useful and interesting problem.   The proposed method is simple and easy to plugin.    The results show performance improvements in a number of cases.   cons.   However there are various concerns raised by the reviewers that I find are not well justified.   The use of inner ensembles are not well justified.   The baseline in the experiments miss the usual ensemble methods instead the authors use single model which does not have the same number of parameters.   novelty and significance  The authors do not clearly distinguish their method from earlier results and how it differs and improve over existing work. Given the current state of the paper, the novelty is not significant.   clarity of theoretical analysis is lacking.  Overall I suggest the authors improve the manuscript considering concerns and suggestions listed above and through reviewers and submit to an upcoming venue.    
This is a well written paper proposing a promising a series of zero cost proxies for NAS. Overall, the reviewers were convinced that the approach is sound and the results overall support the use of zero cost proxies (although they are a bit weak in some cases, e.g. rank correlations in A.3). Despite some concerns amongst the reviewers around the technical novelty of the method, mostly due to the use of estimators from the pruning at init literature, this is promising work at the intersection of different sub communities in ML.
This paper focuses on the problem of robust overfitting. The philosophy behind sounds quite interesting to me, namely, injecting  more learned smoothening during adversarial training. This philosophy leads to two simple yet effective methods: one leveraging knowledge distillation and self training to smooth the logits, and the other performing stochastic weight averaging to smooth the weights.  The clarity and novelty are above the bar of ICLR. While the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. Thus, all of us have agreed to accept this paper for publication! Please carefully address all  comments in the final version.
Given the reviewer s exchange with the authors, and my own examination of the paper, I don t think that it can be accepted in the present form.  First, since this paper aims at solving an optimization problem (for which existing methods exist, with theoretical guarantees) via a NN, it is important to compare appropriately to those methods, which is not done here.  Further, there are possible issues when applying these only to 2D data, and it is possible that it would not extend appropriately to other types of geometries, and costs in OT problems.
This work presents a straightforward and easy to understand method for using hypernetworks to adapt existing models to be able to increase their output space. The method itself is also interesting and is detailed enough for reproducibility. However, the experiments and results should be improved by expanding the demonstration of CHNs beyond the narrow P VAE application and comparing against relevant baselines in the recommendation system literature.  Pros   Clear writing.   Detailed hyperparameters to aid reproducibility.   Straightforward model.  Cons   Lack of sufficient comparison to related work, especially to existing recommendation systems that handle the cold start issue and to Vartak, 2017.   Limited results that only demonstrate application to P VAE meaning it s still unknown if CHNs work well with other models. The result on the synthetic dataset is also less persuasive.   Lack of sufficient ablations, i.e. training a SVM/linear regression model until convergence.
The paper focuses on the task of learning audiovisual representations through contrastive learning on unlabelled videos. This work is another addition to the ever growing literature on self supervised learning (SSL) with emphasis on video and multi modal data. The main contribution of this work is the manner in which it tackles a well known drawback of contrastive learning, namely the strategy used to sample negatives in the contrastive pipeline. The authors propose an active sampling strategy that adaptively chooses negative samples that are informative and diverse. This active selection technique is similar in spirit to many selector functions proposed in the active learning literature. It seems to be the first time it is used for contrastive SSL.   Based on all the reviews and the subsequent discussions, it seems that the reviewers  comments were addressed. The authors are commended on integrating the reviewers  suggestions and making the necessary edits to the paper in a timely manner. 
This paper investigates how to deploy adaptive learning rates in multi agent RL (MARL). In particular, the learning rates are adaptively chosen based on which directions maximally affect the Q function, and take into account the interplay and balance between the actors and the critics. The topic is certainly of great interest when designing fast convergent MARL algorithms. However, the reviewers point out the inadequacy and insufficiency of empirical gains in the reported experiments. Also, larger scale experimental settings are needed in order to provide more convincing evidence about the practical benefits of the proposed scheme.   
This paper describes a method for adapting an RL policy in a deployment environment that does not provide a reward signal.  This concern arises commonly when a task reward is available in a robot simulator but not on the physical robot where the policy is eventually deployed.  The proposed solution is to learn an inverse dynamics model as an auxiliary prediction task on an internal state embedding that is shared with the policy.  The policy is adapted during deployment by modifying the state embedding using this auxiliary task (with the assumption that the main objective remains unchanged).  The proposed method is tested with transfer between simulated domains and also on transfer from a simulator to a physical robot.  The experiments showed the method had consistently higher performance than alternatives.  The reviewers found many positive contributions in the presented paper. These include the problem s importance (R1, R2,R4), extensive experiments (R1, R2, R3), clear writing (R1,R4), simplicity and effectiveness in comparison to ablations (R3, R4).  The reviewers saw a weakness in the method s limitation to perceptual adaptation instead of dynamics adaptation (R1 4) and the lack of novelty (R4).  The author response addressed both concerns.  They stated that the method is novel for adapting to continuously changing environments in a self supervised fashion without rewards.  The authors modified the paper to clarify how the method demonstrates robustness to changes in the system dynamics.  The reviewers found the author response addressed their major concerns.  Four reviewers indicate accept for the contributions stated above and expressed no remaining concerns.  The paper is therefore accepted. 
The authors present a simple modification of existing byzantine resistant techniques for training in the presence of worst case failures/attacks. The paper studies two of the strongest attacks to date, that no other method, till now, has been able to address. The novelty is significant for the related byzantine ML literature. The authors further do a fantastic job in their experiments and sharing reproducible code. Some weak aspects of theory are in fact attributed to what the metrics and guarantees that the related literature studies. The novelty of this paper does not lie so much in the theory contribution, but more so on their experiments and presented intuition. I believe this will be a paper that people will build up on and the ideas presented here are of solid value and importane.
Four reviewers have reviewed and discussed this submission. After rebuttal, two reviewers felt the paper is below acceptance threshold. Firstly, Rev. 1 and Rev. 2 were somewhat disappointed in the lack of analysis regarding non linearities despite authors suggested this was resolved in the revised manuscript, e.g. Rev. 2 argued the paper without such an analysis is too similar to existing  linear  models, e.g. APPNP, SGC, and so on. While Rev. 3 was mildly positive about the paper, they also noted that combining several linear operators is somewhat trivial. Overall, all reviewers felt there is some novelty in the proposed regularization term but also felt that contributions of the paper could have been stronger. While AC sympathizes with this submission and hopes that authors can improve this work, in its current form it appears marginally below the acceptance threshold.
This paper initially received three negative reviews: 4,4,4. The main concerns of the reviewers included limited methodological novelty and an oversimplistic experimental setup. The authors did not submit their responses. As a result, the final recommendation is reject.
The paper is about the use of autoregressive dynamics models in the context of offline model based reinforcement learning. After reading the authors  responses and the other reviews, the reviewers agree that this paper has several strengths (well written, easy to follow, the approach is novel and simple to implement, the empirical evaluation is well executed and the results are reproducible) and it deserves acceptance. The authors need to update their manuscript by keeping into considerations all the suggestions provided by the reviewers (clarifications and additional empirical comparisons).
The paper proposes an approach to meta learning symmetries. While several approaches have recently emerged with similar goals, and sometimes greater convenience and empirical performance, the proposed approach has some interesting characteristics, such as changing properties of the architecture to extrapolate these symmetries. There was a quite a spread of opinions about the paper, the empirical results were not strong, and updates to the paper focused on helpful text additions, but did not substantively improve the evaluation or experiments. Notwithstanding, the paper is conceptually interesting, there are no major flaws, and there is sufficient support for it.
The paper proposes an efficient method to train generative models on multimodal data using a contrastive approach. Usually training such models requires significant training data to be able to learn patterns. The authors propose a variational autoencoder approach that enables multimodal learning of models using a data efficient approach, and shows the effectiveness of the approach on challenging datasets.  The authors have mostly addressed the feedback of the reviewers and done some of the necessary changes to the paper (e.g., adding more results and missing related work). They should make sure to address any lingering concerns about the paper, mentioned by the reviewers in their post rebuttal feedback.
This work provides additional insights into a class of generative models that is rapidly gaining traction, and extends it by potentially providing a faster sampling mechanism, as well as a way to meaningfully interpolate between samples (an ability which adversarial models, currently the most popular class of generative models, also have). The revised manuscript includes an extension to discrete data, which could potentially amplify the impact of this work. The authors have also run additional experiments in response to the reviewers  comments.  Reviewer 1 raised several concerns about the choice of language (i.e. referring to the proposed model as a diffusion model, and the precise meaning of  implicit  in the context of generative models). This is a fair point, as the authors introduce changes that affect the Markovian nature of the "diffusion" process, and a diffusion process is supposed to be Markovian by definition.  However, I think there is something to be said for the authors  argument of using the word  diffusion  to clearly link this work to the prior work on which it is based. Given that technically speaking, the original DDPM work already  abuses  the term to refer to a discrete time process, it is difficult to argue compellingly that  diffusion  should not feature in the name of the proposed model. Referring to  non Markovian diffusion processes  however seems more problematic, as this is a direct contradiction. If the authors wish to use this phrase, adding a few sentences to the introduction that justify this use would be helpful, and personally I feel this would be sufficient to address the issue (I noted that Section 4.1 already acknowledges that the forward process is no longer a diffusion). Plenty of work in our field abuses notation and this is justified simply with the phrase "with (slight) abuse of notation..."; I don t think this would be any different.  Reviewer 1 is technically correct that  stochastic  is an absolute adjective, i.e. something can only be stochastic or deterministic, there is nothing in between, and there are no degrees/levels of stochasticity or determinism. In practice however, it is quite often used in a comparative sense, and I believe I have in fact been guilty of this myself! I do not feel that it causes any ambiguity in this case. Indeed, the phrase  degree of stochasticity  seems to be in relatively common use in literature. While there may be more correct terms to use, I subscribe to the descriptivist view on language, and I do not think the comparative use of  stochastic  is a major issue here. The alternatives I can think of seem potentially more cumbersome (e.g. I wager that  more/less entropic  would be more poorly understood than  more/less stochastic ). Still, I recommend that the authors consider potential alternatives in the future, to avoid any confusion.  Overall, I think the reviewers  major concerns have been addressed in the revised manuscript. Given that all reviewers consider the idea worthwhile, I will join them in recommending acceptance.
This paper got 2 clear acceptance and 2 borderline recommendation. The main concerns lie in the clarity of the experiment results and settings (AR3). The authors address these questions in their response. AR2 has two important questions. One is whether the simplified assumption holds in the considered very complicated settings (i.e., the labels are noisy and long tailed). The other one is the lack of comparison with SOTA method for long tailed classification. The authors did good job in their response. They provide additional experiment results to address these questions. Overall, the quality of this submission meet the bar of ICLR acceptance, though AC has concerns on the complicated settings and the marginal performance improvement over the existing long tailed works.
This paper proposes a novel and powerful data augmentation strategy for few shot learning, producing convincing improvements over current approaches. The request by the reviewers to include additional ablations, more backbones, and an additional dataset have been satisfactorily  resolved, with the results remaining strong. The reviewers are all unanimous in their recommendation that the paper be accepted for publication. 
Compressing BERT is a practically important research direction. Our main concern on this submission is on its practical value. Comparing with MobileBERT in the literature, NAS BERT does not show advantages on any aspect: latency, prediction performance, or model size (less important), while being much more costly to build because of NAS. MobileBERT just simply narrowed the original BERT models (8x narrower than BERT large). So it is hard to convince the readers that adaptive size or NAS is interesting or matters. On the research side, this paper have some interesting points on designing the search space, but overall the novelty of this paper is limited, as all of the reviewers pointed out. It is also worth noticing that the claim of "task agonistic" in this paper does not fully hold: in the downstream tasks, the soft labels of the teacher model are required to train the compressed model. To be fully "task agonistic", the results on downstream tasks should be solely based on training with the ground truth labels, as in the MobileBERT paper. Once following the exact task agnostic experimental protocol, the reported performance in this paper may be significantly lower. 
This paper introduces a generative model termed generalized energy based model (GEBM).  The goal is modelling complex distributions supported on low dimensional manifolds, while offering more flexibility in refining the distribution of mass on those manifolds. The key idea is presented as parametrizing the base measure (called a generator in the paper) and the density with respect to this base measure separately. Figure 1 of the paper sketches the idea on a very clear toy example.  The pros: * Flexibility: Decomposing the full problem as learning the support and learning the density on this support  * Theoretical justification * Introducing the KALE objective * Comparative empirical results with GANs show the additional benefits. Empirically, the framework outperforms GAN with the same complexity. * Clear written paper  The lack of a comparison with GANs has been raised as a concern.  The authors have satisfactorily answered key questions and others raised during rebuttal and added several new references. They have also improved the narrative and included an additional experiment to contrast GEBM and GANs in response to AnonReviewer2, also provided more detail  on how the energy function (class) is chosen.  
This paper introduces an simple but potentially effective off policy TD algorithm.   Overall, the reviewers felt the work was incomplete and not yet ready for publication. The all recognized the authors made significant updates to the paper, but serious issues remain with the empirical work: studying the impact of the proposed extension on other algorithms, missing baselines (e.g., TDRC), scope of environments limited similar chain like domains, significant questions about how best parameter settings where chosen for comparison, etc.  This is clearly an interesting direction. If the authors can improve the experiments and better situate their method if the literature (connecting to the lit in off policy RL about accelerating and improving off policy TD methods this will become a solid contribution.
We thank the authors (and reviewers) for engaging in a detailed and constructive discussion, and providing a revised version of the paper after the initial round of reviews.  Regarding quality, the work is technically correct and the amount of experiments significant. However, as highlighted by reviewers 2 and 3, some important questions remain unanswered, in particular 1) more empirical evidence to support the claim that the UMAP loss is a relevant for neural networks, and 2) more comparison with existing approaches (beyond t SNE).  Regarding clarity, the paper is overall clear and pleasant to read. However, after the revision round, all details about the proposed methods have been moved to the annex. While the initial version was criticized for the opposite reason (all experiments were in a annex), the balance may not be found yet; e.g., the equation for the UMAP loss, which is at the core of the paper, would certainly find its place in the main part of the manuscript for an ICLR paper.  The originality is the weakest aspect of the paper (besides the lack of comparison with related work). As mentioned by several reviewers, plugging the UMAP loss to a differentiable model is nowadays an idea that lacks originality. What would be important to justify that such a "straightforward" idea makes it to ICLR would be to demonstrate convincingly that it outperforms existing alternative approaches.  Finally, regarding the significance of the work, it is limited by the lack of thorough comparison with existing method. On the other hand, if the method is implemented in a fast and easy to use package, it may find its public as illustrated by the positive evaluation of Reviewer 1 from a potential user point of view.
In general there is agreement under reviewers that the ideas/method presented are somewhat interesting/promising but also that the paper lacks a lot of clarity. Reviewers agree that the paper needs more work (on the method) and more extensive experiments to be convincing, and that in its current form it is not mature enough for publication at ICLR.
This paper proposes a Bayesian non parametric method for task incremental continual learning. It is more general than previous work in that it considers the network structure as a random variable and works for both supervised and unsupervised settings. Experimental results show that the proposed method outperforms prior work in the proposed tasks.  Pros:   It is well motivated.   It s theoretically sound.   It can do task inference.   It outperforms other methods in the proposed tasks.  Cons:   The experimental setup was not very challenging, because the dataset(MNIST) was simple and the network was shallow.   There was no ablation study to analyze the contributions of the algorithm to the performance.   There is not enough experiments to support the advantage of task inference.   The paper did not compare with the SOTA task incremental learning algorithms HAT and DEN.  The main concerns of reviewers are on the experimental section as listed in cons and the difference from previous work. The authors explained that their method has the advantages over previous work that it can do task inference. R3 agreed with the advantage and suggested more experiments in this direction should be performed. The authors conducted additional experiments suggested by the reviewers including comparison with HAT. They also uploaded a revised version to incorporate the comments from the reviewers.  I think the paper is well motivated and the idea of applying Bayesian non parametric for continuous learning is interesting. It could potentially motivate interesting future work on CL. However, the main advantages/contributions are not well presented and supported by the experiments. So at present time I believe there is much room for the authors to improve their paper before publication. I hope that the authors will be able to address the feedback they received to make this submission get where it should be.
The paper addresses the problem of improving generalization when few annotated data is available by leveraging available auxiliary information. The authors consider the respective merits of two alternatives: using auxiliary information as complementary inputs or as additional outputs in a multi task or transfer setting.  For linear regression, they show theoretically that the former can help improve in distribution error but may hurt OOD error, while the latter may help improve OOD error. They propose a framework for combining the two alternatives and show empirically that it does so on three different datasets.    All the reviewers agree on the novelty, interest and impact of the method. The rebuttal clarified the reviewers’ questions. I propose an accept. 
This submission analyses the VAE objective from the perspective of non linearly scaled isometric embeddings, with the aim of improving our information theoretic understanding of the variational objective.   Reviewers are in consensus that this submission in its current form is very difficult to read, even after revisions by the authors. The metareviewer, who is highly familiar with information theoretic and even information geometric interpretations of VAEs, similarly struggled to understand this paper. Many concepts (e.g. KLT transforms) are not introduced in a self contained manner, nor are related works like RaDOGAGA. Moreover the exposition introduces lots of notation (often somewhat implicitly) and requires more  high level plain English statements that signal the structure of the overall narrative to the reader. As a result, it is hard to understand the paper, even at the level of the contributions that are claimed by the authors. It appears that Section 3.4 should be read as culminating in a "correction" of the rate distortion view of VAEs proposed by Alemi et al. Unfortunately the metareviewer is not able to understand from the writing what fault the authors find with the proposed interpretation, and how their view informs a better perspective.  It is difficult to provide the authors with concrete addressable suggestions at this stage of revision of their manuscript. The metareviewer s advice would be to attempt to focus on defining a narrative structure that clearly explains what insights this perspective of VAEs contributes, what misconception it corrects, and how it corrects it –– and then focus on streamlining notation in a manner that makes it possible to follow along with the exposition more easily. 
The authors introduce an approach to train sparse RNNs with a fixed parameter count. During training, they allow RNN layers to have a non uniform redistribution across cell weights for a better regularization.They also introduce a variant of the averaged stochastic gradient optimizer, which improves the performance of all sparse training methods for RNNs. They achieve state of the art sparse training results on Penn Treebank and Wikitext 2.  The method achieves very good performance on sparse RNNs for challenging tasks. The paper is well written and provides solid analysis with new insights into sparse network models. Most reviewers believe it is a very solid paper.  However, the technical novelty of the paper is limited. It can be seen as some tweaks and improvements of existing techniques, which seem to work very well. Since the number of papers that can be accepted is very limited, and since technical novelty is an essential criterion for published papers at ICLR, I propose rejection.  
The authors have addressed the issues raised by the reviewers. All the reviewers think that the paper deserves to be published at ICLR 2021. The authors should implement all the reviewers’ suggestions into the final version, especially for clarity issues and clear explanations. The reviewer also encourages authors to investigate $m$’s effects on the convergence rate more to see whether there is a structural limitation in federated learning settings for future work. 
This paper provides a theoretically rigorous treatment of approximation properties and convergence analysis of LINEAR RNNs. The reviewers were divided in their evaluation. On the positive side, the presented relation between approximation error and required memory size is not obvious and interesting. On the less positive side, two of the reviewers raised the necessity of mathematical machinery that were invoked. Furthermore, its applicability is unclear in ML, since they aren t applicable to the usual nonlinear RNNs. However, given that the theoretical contributions are clear, the final decision was to accept.
This work presents a novel approach to improving text decoding. This is backed up by a solid analysis of cross entropy growth with top k vs top p and an interesting demonstration of repetition correlating with probability. The paper is well written and well organized. The authors  rebuttal was effective in convincing the reviewers. The human evaluation (added during the rebuttal phase) is a good demonstration of the effectiveness of the approach and so this paper s proposed decoding algorithm is likely to be impactful.  Pros:   Well written.   Solid theoretical analysis of cross entropy and its relation to top p and top k decoding. Good demonstration of how repetition is related to probability.   Interesting, novel and effective decoding algorithm.   Human evaluation of the algorithm s output.  Cons:   The approach has not been tested with a variety of language models.   Decoding quality still depends on a target perplexity which may need to be tuned.   Unnecessary dependence on Zipf s law in the basic decoding algorithm.
This paper studies the problem of multivariate mean estimation with a focus on the heavy tailed setting. The authors give an algorithm for this estimation task and then use it (in essentially a black box manner) to obtain heavy tailed estimators for various supervised learning tasks. As pointed out by one of the reviewers and my own reading, the theoretical contributions of the paper are weak and are subsumed by related work (some of which is not cited in the submission).  More generally, the extensive recent literature on the topic is not accurately represented in both the submission itself and the response to the reviewers comments. On the other hand, the experimental results of the paper hold some promise. However, at this stage, these experimental contributions by themselves are in my opinion insufficient to merit acceptance.  
The paper proposes a multi scale spatial temporal joint graph convolution for spatiotemporal forecastings. Many reviewers have concerns regarding novelty, baseline comparisons, and writing clarity of the draft.
The initial reviews were a bit split. R4 was slightly positive, R3 was slightly negative, and both R1 and R2 voted for rejection. The main issue was lack of proper comparisons with the SOTA methods and missing references. In the rebuttal, the authors added additional experiments as requested, but R1 and R2 were not convinced by the new results. In particular, R1 pointed out that even the unsupervised setup in [4] achieved 0.89 AUC, outperforming 0.86 as reported in the paper. The AC agrees with R1 and R2 that the paper cannot be published until more thorough comparisons are conducted. 
This paper proposes an algorithm to learn symbolic intrinsic rewards via a symbolic function generator. The policy optimizes this reward function and an evolutionary algorithm selects between a set of such policies. The core idea is that learning with such a symbolic reward function is useful in sparse reward environments and also enables better interpretability.  ${\bf Pros}$: 1. The learnt reward function has a relatively simple form and is therefore interpretable 2. The experimental section is quite extensive ranging from diverse tasks, control systems and agent systems. However there are some issues about showing clear need of the proposed method  ${\bf Cons}$: 1. There was a consensus among reviewers that the paper does not make a strong case for the symbolic reward generator. In the rebuttal the authors argued that as RL scales to real world problems, it will become necessary to use such a method. I can understand how it would be useful in the context of inverse RL or imitation learning. However, as R3 points out, in the cases considered in this paper, the rewards are fairly intuitive and explainable. The paper might become stronger by directly tackling problems with such constraints. 2. There is confusion about the details and scope in the current version of the paper. The paper would become stronger by incorporating all the feedback received during the review period. 
This paper was reviewed by 3 experts in the field. The reviewers raised their concerns on lack of novelty, unconvincing experiment, and the presentation of this paper, While the paper clearly has merit, the decision is not to recommend acceptance. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
The paper presents a new attack combining trojans (backdoor attacks) with adversarial examples. The new attack is triggered only if both a trojan and the respective adversarial perturbation are present. Experimental evaluation demonstrates that neither adversarial training (as a defense against adversarial examples) nor defenses against backdoors are effective against the new attack.  The proposed method is original albeit somewhat incremental (combination of two well known attack techniques). The main weakness of the paper, however, is its threat model. It is not clearly explained why the proposed attack would make sense for an attacker. Backdoor attacks are typically executed by model creators in order to force certain decisions on certain data. On the other hand, adversarial examples are generated by model users (or abusers) who have an interest in wrong model predictions (e.g., decisions made in their favor). The paper does not provide a convincing use case in which such combined attacks would be feasible.   Furthermore, paper s clarity can be improved. The introduction does not present a clear picture of poisoning attacks. It essentially treats poisoning attacks as equivalent to backdoor/trojan attacks. This is not true and a substantial body of research (starting from the seminal paper by Barreno et al. in 2006) has addressed indiscriminate poisoning attacks aimed at general deterioration of classifier performance. A distinction between a clean label and a poisoned label attacks is also not clearly presented. The notation of Section 3 is rather complex and confusing.   
The paper attempts to provide a theoretical explanation for benefit of language model pretraining on downstream classification task. In this regard, the authors provide a mathematical framework which seems to indicate that the distribution of the next word, conditional on the context, can provide a strong discriminative signal for the downstream task. The reviewers found the formulation insightful, interesting, and novel. Also reviewers enjoyed reading the well written paper and appreciated its cautious in its tone. As correctly pointed out by reviewers, the proposed framework might not directly align with techniques used in practice. Applicability of the framework to other pre training approaches is limited.  Also, there are some unresolved concerns about $O(\sqrt{\epsilon})$ assumption still. Nevertheless, reviewers reached a consensus that the framework would be beneficial for the community and attract follow up works. Thus, I recommend an acceptance to ICLR. Following reviewer suggestion, it is strongly recommended that extensions section be expanded in the revised version using the extra page.
Although the reviewers like the general idea of the paper, there are concerns regarding the clarity of the statements, especially in stating the main assumptions, referring to related work, and how well the experiments support the results of the paper. Although the authors  long response addressed some of the issues/comments raised by the reviewers, not all of them are convinced that the paper carries enough novelty and is ready for publication. I would suggest that the authors take the reviewers  comments into account, revise the paper, and make it ready to be submitted to an upcoming conference. 
Four knowledgeable referees have indicated reject. I agree with the most critical reviewer R4 that the model design lacks a clear and transparent motivation and that the experimental setup is not convincing, and so must reject.
The paper proposes a reinforcement learning algorithm that combines trust region policy optimization and entropy maximization. The starting point is the Lagrangian of a constrained optimization problem that upper bounds the change in the policy and lower bounds the entropy of the policy. The paper proves that the algorithm converges, and evaluates it experimentally in MuJoCo domains.  The main issues raised by the reviewers were related to the proofs (see especially R1) and experimental evaluation (R4). The authors did a great job improving the paper during the discussion phase, but some of the issues remain unresolved, and thus reviewers find the paper not to be ready for publication. Thus, I m recommending rejection.
While all reviewers see a lot of value in the paper, it cannot be accepted in its current form: too many issues with clarity. A more focused paper, with clear task and contributions is recommended. The revisions and answers to reviewer questions are greatly appreciated and go a long way towards addressing these concerns for a future submission.
This paper received divergent scores (one strong negative and three positives). The positive reviews praise the clear intuition/motivation and strong empirical performance, while the negative review considers the proposed approach ad hoc with limited novelty. I read the paper myself and found myself leaning more towards the negative score. In more details:   I think the paper proposed a cleverly engineered solution to employ two separate RNNs to model two different subsets of the users (active v.s. inactive). To combat overfitting, the authors proposed some tricks: 1) use MF to learn a better initialization and 2) tie some of the parameters together. The ablation study shows that both are quite useful, and not too surprisingly when you have two powerful RNNs and work hard to make sure they don t overfit, they perform better than a single RNN. As I mentioned earlier, this whole approach is quite cleverly engineered and executed. But it s not clear to me if this is something that the ICLR community can benefit from (maybe except a relatively small proportion). I believe this paper can find a bigger audience in venues like KDD whose deadline is coming up.   Furthermore, the authors presented some theoretical analysis to justify their intuition, which to me feels a bit forced. To start, a big assumption is that the optimal user embeddings are very concentrated, which is a rather strong assumption and I hardly believe it will hold in practice (what can even be considered as "concentrated" in high dimensional space?). Following that, the theorem implies that the initial embedding for the inactive users should be the expected optimal embedding, but then later in the paper this point seemingly got completely ignored and instead the authors just proposed to learn a common initialization. Additionally, the theorem suggests that there is an optimal threshold, but later in the paper again this point got completely ignored. I know the theory makes assumptions and builds on simple cases. But my point is that you don t need theorem to show me that two RNNs can perform better than one when trained properly (which, I admit, is non trivial and is the main contribution of the paper). There has been a lot of discussion around the ML community about the trend of making your paper look "mathy" and I don t think this is a good thing.   Minor comment: in MF, the objective wrt each embedding is convex, but the whole optimization is not jointly convex and it is not likely that you can get to the global optimum. It is relatively insensitive to initialization though (comparing with neural nets). 
This paper proposes a new selection paradigm for selecting the optimal architecture in neural architecture search (NAS), in particular for methods that involve a one shot model and that deploy gradient based methods for the search. Basically, the paper focuses on examining the max selection very closely and found the magnitude of architecture weights are misleading. Instead, the paper proposes much more intuitive finalization step, pick the operator that has the largest drop in validation if the edge is removed. All reviewers agreed that the idea is interesting, the paper is well written, and the results found in the paper are interesting. In addition, author response satisfactorily addressed most of the points raised by the reviewers, and most of them increased their original score. Therefore, I recommend acceptance.
This paper studies the problem of Pareto fairness without having pre defined protected groups. The reviewers agree that the problem studied here is interesting and relevant. During the initial review period, reviewers identified a major correctness issue. The authors have then substantially changed the algorithm and experiments in the rebuttal period in order to address the issues. Now the convergence result in the paper follows more directly from the prior work of Chen et al. Overall, the technical novelty of the paper appears to be limited. Even though the authors have also strengthened the related work discussion, they should also consider discussing the comparison between their work with that of Lahoti et al., as suggested by one of the reviewers.  
This paper proposes to use high dimensional representation for labels to strengthen the adversarial robustness of deep neural networks. Experimental results demonstrate that the proposed method improve adversarial robustness. All reviewer agree that the authors propose an interesting idea and this direction deserves further exploration. On the other hand, the reviewers also raise a serious question: There is a lack of explanation of why high dimensional representation of labels improve adversarial robustness. Therefore, it is not clear if the proposed method can defend refined attacks tailored to such dimensional label representation. The authors are highly encouraged to conduct deeper analysis, especially on the robustness against finer attacks.
The paper proposed an interesting method to improve the robustness of DARTS and hence to alleviate the mode collapse. The idea consists of adding an auxiliary skip connection branch that complements the output of the cell function together with a depth analysis about the effect of the auxiliary branch. The proposed approach is validated on a few benchmarks showing the effectiveness of the proposed approach. All reviewers agreed that the idea is simple, efficient and interesting. author response satisfactorily addressed most of the points raised by the reviewers, and most of them increased their original score accepting the paper. Therefore, I recommend acceptance.
There was a predominantly positive feedback from the reviewers so I recommend acceptance of the paper. It is well written and well motivated tackling an important problem: That in self supervised learning one might encode different invariances by default, even if some of these invariances are useful for downstream tasks (e.g. being rotation invariant may be detrimental to predicting if an image has the correct rotation on a phone). For this, they propose a simple, yet elegant approach and validate it on many downstream tasks. Given the recent interest in self supervised learning, this appears to be a relevant and interesting paper for the ICLR community.
This paper introduced a log barrier based regularization method to reduce the dynamic range of data types in neural networks. As pointed out by the reviewers, there are many technical issues. The authors agree with the reviewers in the rebuttal, though claimed that they are fixed in the revised version of the paper.  Experimental results are not convincing. It is not clear how the proposed method is evaluated. Accuracy of MobileNet using the proposed method is quite significantly lower compared to previous works. The work needs additional results/comparisons with other highly relevant papers on fixed point training.  There are also many clarity issues that need to be fixed.
The paper tries to find better semi positive definite (SPD) manifold networks using neural architecture search. However, as pointed out by the reviewers, the paper has a few weaknesses: (a) it lacks in novelty, (b) it lacks in experiments that are mentioned in the SOTA papers, (c) the experiments should be performed with the same model complexity for fairness.  
This paper makes a thorough investigation on the idea of decoupling the input and output word embeddings for pre trained language models.  The research shows that the decoupling can improve the performance of pre trained LMs by reallocating the input word embedding parameters to the Transformer layers, while further improvements can be obtained by increasing the output embedding size.  Experiments were conducted on the XTREME benchmark over a strong mBERT.  R1&R2&R3 gave rather positive comments while R4 raised concerns on the model size.  The authors gave detailed response on these concerns but R4 still thought the paper is overclaimed because the experiments were only conducted in a multilingual scenario.  
## Description   The paper proposes an improvement to binary neural networks with real valued skip connections between pre activations, by introducing more flexible learnable non linearities on the real valued connections. The parametric non linearity is actually linear at initialization, which makes the training easier at the beginning. Due to learnable parameters it eventually adjusts to a more complex one, able to refine the accuracy. I think this idea is a good finding.  ## Review Process and Decision The reviewers initially gave low ratings to the paper, indicating that the contribution is incremental and not fully clearly presented.  There was no detailed discussion with the authors, since the author s response and the rebuttal revision came in the very end of the discussion period. In the subsequent discussion phase the reviewer board has not indicated any major changes to the initial reviews/ranking. The AC checked the paper and supports rejection.  ## Details  The authors are encouraged to improve the paper carefully addressing points proposed by reviewers.  I think the argumentation of the paper should be improved. Some explanations are intuitive, but operating with fuzzy notions and may in fact be incorrect or irrelevant. The paper should be made more precise, based on verifiable arguments.  I think the following is crucial and not made clear in the paper: The non linearities inserted before the sign function *do not affect the result of sign*. They indeed affect only the residual connections. Furthermore, the structure of residual connections should be fully clarified to reveal that there are complete real valued paths all the way from the input to the network to its output, made of the residual connections with their own learnable parameters (and 1x1 convolutions) and (learnable) non linearities and an intake from binary convolutions on the way. The learnable non linearities can in principle improve performance just because the real valued paths can learn better.  I paste below feedback by reviewers to author s response (I believe they would agree to share it with authors but did not find a suitable way of doing it):  ## Response by R1: I acknowledge that I read and appreciated the authors  answers to my questions. I think the idea of analyzing the role of non linearities is nice and I tend to confirm my score. But I also agree with other reviewers that, as it is, the paper has some unclear parts and would not complain if it is rejected.  ## Response by R3: Thanks for your responses to answer my questions for the paper. I agree with the results of the proposed FBTN for improved Binary Neural Networks (BNNs). However, my concern about the novelty of using group convolution modules in BNNs has not been addressed. I think the paper is not sufficient enough to publish at the conference. So, I do not change my rating of the paper.  ## Response by R4: I maintained my rating when combining other reviews and responses to them, despite of their well response. It is still questionable whether FPReLU, one of the main contributions they claimed, actually improves the performance of BNN remarkably. In particular, this is supported by the fact that the performance of BNN on ResNet 34 which the techniques in this paper were applied does not show much difference from  Real to Bin  model.
This paper introduces a self training strategy for semi supervised learning for few shot sequence learning.   It builds on ideas from an existing work on robust deep learning that adaptively reweights examples for learning to reduce impact of noisy examples, here the noisy examples are introduced to the student network training by the teacher network.  Two main novel points, one is on  selectively constructing the validation set used for adaptive reweighting.  Another idea is to move from the sentence level reweighting to token level reweighting.   The paper shows strong results suggesting the proposed method can effectively learn under few shot learning.  A primary concern from the reviewers is that the paper has limited novelty given that it primarily applies existing ideas to a slightly different problem.  Another concern is that the system consists of many components, each of the choices could have other viable options.  The ablation studies indicate these components are useful compared to when removed, but fail to explore possible alternative choices. One of the questions is whether token level reweighting is necessary. It would have been nice to see an ablation study comparing against a baseline using sentence level reweighting.   
While reviewers believe that the motivation of the paper is strong and the idea is interesting the ultimate execution of the paper is not up to the standards of ICLR. I believe the biggest concern is the precise privacy guarantee of the method. As pointed out, it is an extremely strong assumption that the model structure of the adversary is known (or even approximately known). Standard privacy guarantees are either information theoretic or based in computational hardness. This work does not provide such guarantees. While there has been recent work on using adversarial learning to learn models that are robust to such adversaries, they have been heavily criticized within privacy and security communities due to the lack of such guarantees. I was not convinced by the authors response to such questions: there are plenty of cryptographic/privacy preserving schemes that work in the honest but curious setting, and techniques that use the standard guarantee of differential privacy do not suffer from large slow downs.  Thus, I would urge the authors to modify this work so that it can leverage the guarantees of well known cryptographic/privacy preserving schemes. If done so, these arguments about privacy will go away and the paper will have a much better shot at acceptance.
This paper consider a classical multi armed bandit problem (then a more general RL setting) and prove some upper and lower bounds, in cases that were not explicitly studied in the literature.  However, those results are very incremental and do not justify (maybe yet, going beyond the sub Gaussian case could be interesting) yet acceptance.
The paper presents a personalized federated learning approach using a mixture of global and local models. Four reviewers evaluated this paper; one of the reviewers is luke warm (6) while the rest of the reviewers pretty negative to this work (3, 3, 3). The reviewers pointed out many weaknesses, especially about novelty, motivation, contribution, presentation, etc. Most importantly, although the idea of a "mixture of experts" makes sense, it is not clear what the real technical contribution of this paper is in terms of federated learning.  Considering all the comments by the reviewers, I believe that this paper is not ready yet for publication. The authors need to improve the novelty and technical soundness of the proposed direction to convince the readers including reviewers. 
The paper received mixed reviews. While AnonReviewer1 and AnonReviewer2 liked the idea of jointly learning global local representations, the other reviewers were concerned about the technical novelty. Reviewers also raised various questions about the experiments and ablation studies. AC found that the rebuttal well addressed the reviewers  questions about the experiments, but it failed to elaborate on the "why" of combining global and local self supervised representations. AC agreed with AnonReviewer3 and AnonReviewer4 s concerns on technical novelty. Considering the reviews, we regret that the paper cannot be recommended for acceptance at this time.  The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
The paper proposes an optimization framework that automatically adapts the learning rates at different levels of a neural network  based on hypergradient descent.  The AC and reviewers all found the approach interesting and promising and appreciate the author feedback.  We strongly encourage the authors to incorporate the points and additional results provided in their response to the reviewers.  Additionally, some concerns remain to be addressed regarding initialization of hyper parameters of combination weights. Specifically it would be important to further investigate the impact of such initialization on the optimization performance. Furthermore, additional experiments with other network optimizers would be valuable as pointed out in the reviews.
The authors present a matrix factorization for the social behavior of honey bees in a hive. All the reviewers appreciated the interesting application.  However, substantial concerns were raised about the model motivation and the interpretation of the learned factors. To quote one reviewer, "Some of these bells and whistles may not even be needed, so simplifying the model and streamlining the text would go a long way for me." Another said, "The paper requires more principled motivation for the choices the authors made as well as cleaning up the notation."  The authors did address some of these concerns in discussion, but there are too many lingering concerns to recommend acceptance.  Given the unique application of this paper, the authors might also consider a journal that specializes in computational biology instead.  
This paper presents a method for attacking few shot learners with poisoning a subset of support set. I believe this might be the first work to address adversarial examples for meta learners (or few shot learners), which is a timely issue. A common concern raised by most of reviewers is in the novelty of this work, in the sense that the method builds on a basic attack strategy (such as PGD) in the standard adversarial example setting. Authors responded to this, summarizing what s new in this paper. Episodic training for few shot learners requires consuming support set (instead of single training data point). It is a nature of most meta learning methods. Thus, it is easily expected that the adversarial attack for few shot learners is naturally extended to poisoning a support set (or its subset) instead of a single data point. Certainly such extension may entail a new strategy. However, during the discussion period with reviewers, concerns on the novelty of such extension still remains. In particular, the few shot learning algorithms do not allow big changes in the original model. The algorithms analyzed are prototypical networks that do not utilize fine tuning, and MAML that fine tunes for a small number of pre fixed steps. So the transfer of adversarial samples may not be counted as a major contribution.  
Reviewers agree that this is a very promising paper, with an excellent overview of existing techniques for semi supervised and neuro symbolic learning. However, reviewers also agree that the paper is not ready. With one more revision for clarity, some limited empirical validation and illustration of the theory, and focus on the essential message, this could become a seminal paper for our understanding of semi supervised learning. Luckily the reviews provided ample feedback, and the authors should be able to submit a very competitive paper next time around.
The paper focuses on individual fair ranking and proposes an approach for that based on optimal transport. The reviewers are in general positive about the paper, however, there are a a couple of concerns that I believe should be addressed before publication.  First, I find the treatment of the term "counterfactual" misleading in the paper.  Counterfactual fairness has been proposed in the literature as a causal notion of individual fairness. However, as far as I can see in the paper, there is not such a causal treatment of counterfactuals in the paper. Thus, I suggest the authors to reconsider their treatment of counterfactuals in the paper, as it may trigger confusion. Second, I also agree with R1 that  it is unfair as SenSTIR is the only algorithm to use the same kind of "counterfactual" data than the one used for the evaluation.   
 Description:  The paper presents a generative model, SketchEmbedNet, for class agnostic generation of sketch drawings from images. They leverage sequential data in hand drawn sketches. Results shows this outperforms STOA on few shot classification tasks, and the model can generate sketches from new classes after one shot.  Strengths:   Detailed, technically sound, presentation    Shows that enforcing the decoder to output sequential data leads to a more informative internal representation, and thus generate better quality sketches    Improves over unsupervised STOA methods  Weaknesses:   Experiments are done against methods that do not use the sequential aspect of sketches. Because ground truth in this case contains much more data, comparison is not quite fair.   Will have been useful to see results against a baseline that uses it.   Quality of sketches generated from natural images is low
I thank the authors and reviewers for the discussions. Reviewers raised major concerns regarding the significance of the results and experiments. Given all, I think the paper needs more work before being accepted. I encourage authors to address comments raised by the reviewers to improve their paper.    AC
I join all five reviewers in recommending acceptance.  There was some discussion about a comparison with WaveGrad (Chen et al., 2020), a contemporaneous work that explores a similar modelling approach for speech generation. While I agree that such a comparison is a useful addition to the manuscript, I do not think it is reasonable to request anything beyond an acknowledgement and citation of the work from the authors as a condition for acceptance. Further discussion and comparison experiments could be valuable, but I believe that should not factor into the final decision. My position is most similar to Reviewer 4 s in this sense. The current version of the manuscript briefly discusses the differences between WaveGrad and DiffWave, which I think is more than sufficient. (As an aside, another difference potentially worth discussing is that the "noise schedule" for WaveGrad can be adapted at inference time, enabling a trade off between inference speed and sample quality, which I believe is not possible for DiffWave in its current form.)  There was some debate about the weakly conditioned generation results; I believe they are a nice addition to the paper, although it would have been suitable for publication without them. They certainly do not detract from it, and might inspire further work in weakly conditioned audio generation (e.g. music). There were also concerns about the clarity of writing, which I believe the authors have addressed in the current version of the manuscript.  This work stands out because it applies a relatively fresh idea in generative modelling to a domain of great practical importance, which has long been dominated by traditional likelihood based models, with compelling results. While this implies a limited degree of technical novelty, I do not think that is grounds for rejection, and in fact I would argue that making new ideas work well for practical problems is just as important.
This paper proposes to improve the robustness of computer vision models through a new augmentation strategy. There are two primary contributions of the work, first the use of a bottleneck autoencoder to generate discretized variants of the clean image, and second a slight variant of the task loss, where the task loss is evaluated on the augmented image vs the clean image as is done in prior work. Reviewers argued that the method did not meaningfully improve upon prior work, the method alone underperforms AugMix on existing benchmarks, and when combined with some additional augmentations from AugMix the gains were marginal. Additionally, when there were gains in robustness it was unclear as to the source. The work would be improved with additional experimental evidence that the claimed benefit of the information bottleneck is substantial for improving robustness (for example, when DJMix+RA outperforms AugMix, is this due to the use the of autoencoder or is it primarily due to the new task loss?). I recommend the authors incorporate additional reviewer feedback and resubmit.
All reviewers appreciated the main idea in the paper for solving the nonconvex nonconcave minimax problems, which is deemed an extremely hard open problem. However, as R1 also pointed out, neither the theoretical nor the experimental results seem particularly strong, given that many variations of GDA and theoretical understanding of different notions of optimality have been recently developed. The paper fails to draw proper comparisons to these existing work.  Unfortunately, the paper is slightly below borderline and cannot be accepted this time.  
Quantization is an important practical problem to address.  The proposed method which quantizes a different random subset of weights during each forward is simple and interesting. The empirical results on RoBERTa and EfficientNet B3 are good, in particular, for int4 quantization.  During the rebuttal, the authors further included quantization results on ResNet which were suggested by the reviewers. This additional experiment is important for comparing  this proposed approach with the existing methods which do not have quantization results on the models in this paper. 
The paper addresses the problem of batch normalization (BN) in federated learning, which is of great interest to the community including practitioners.  The proposed method here simply excludes the BN parameters from the aggregation, and evolves them locally.  As a main contribution, reviewers particularly liked the solid justification of the proposed scheme, both with substantial theory and extensive experiments. Presentation style can be slightly improved, the usage at test time can be clarified more, and some references mentioned by R3 should be added, but this overall does not affect the strong level of contributions present in the work, and the discussion phase with the authors was already constructive.
The paper gives a learning augmented algorithm for estimating the support size of a discrete distribution. The proposed algorithm is evaluated experimentally, showing significant improvements in the estimation accuracy. The reviewers unanimously agreed that the contributions are strong and relevant. I recommend accept.
The reviewers unanimously agreed that this is an interesting paper that belongs at ICLR. The use of optimal transport in neural topic models is novel and the paper is well written.  A common theme among the reviewers was that they would like to see more intuition and justification. I suggest you bear this in mind while editing the final version of the paper. I also believe that R3 brings up valid points about evaluating perplexity   I don t think the lack of perplexity results are a reason to reject the paper, but I believe they can be calculated here (see eg the reference R3 provided) and they would give a clearer view of the model s performance. 
Reviewers concurred that this is an interesting paper with contributions worthy of publication. The authors also provided many details in the rebuttal which makes the paper even more strong.
This work applies collocation, a well known trajectory optimization technique, to the problem of planning in learned visual latent spaces. Evaluations show that collocation based optimization outperforms shooting via CEM (PlaNet) and  shooting via gradient descent.  Pros:   I agree with the reviewers that this idea makes sense, and will very likely be built on in future work   the authors have very actively addressed most comments of all reviewers that engaged in discussion  cons:   I agree with the reviewers that this is a very simple and straightforward application of collocation methods to the visual latent space domain. Furthermore, the chosen tasks are fairly simplistic, meta world has a variety of tasks, most of which are more complex than the reaching and pushing task that were chosen for this manuscript.     Even with all the updates, the evaluation is still very shallow. I agree with the reviewers that obtaining results for both settings: a) visual MPC with pre trained (or even ground truth) dynamics model b) in the model based RL setting, for which the model is being learned, is important. While the authors have added some of these experiments, a detailed discussion of how the results change from a) to b) is missing.  Furthermore, when using collocation in this MBRL setting, how should dynamics constraints be enforced (should they even be enforced when the model is still really bad?). How does the comparison between collocation and shooting fare when you use dense/shaped rewards for the sawyer tasks? Many questions come to mind, some of which that have been raised by the reviewers, and my main point is that simple idea + in depth analysis of some of these questions would have created a stronger contribution.   Alternatively, real system experiments would have increased the significance of this work.    I don t see any direct references of gradient based visual latent space planning (shooting), but related work on this does exist.   In my opinion, a simple straightforward idea is no reason to reject a paper. However, currently, the reader does not learn when collocation should be considered over other trajectory optimization methods, when attempting to plan in a learned visual latent space. And what some of the main remaining challenges are. Because of this I lean towards recommending reject, and would encourage the authors to deepen their analysis of collocation in visual latent space.
This paper proposes an early stopping strategy based on the disparity of gradients between two batches from the *training set*. Such a criterion would make the held out validation set unnecessary. The idea is motivated by theoretical arguments and benchmarked on experiments, but some issues still need to be worked on.   Regarding theory, the theorems here assume implicitly the independence of the gradients computed on two different batches of training data, but the conditions where gradients on independent examples computed *on trained weights* are independent (or close to being independent) should be discussed. Regarding experiments, the protocol is unusual in that the proposed stopping criterion is compared to a stopping criterion relying on k fold cross validation, instead of the usual stopping criterion on held out validation. What is the motivation for this protocol? Why should we expect a small variability in the optimal number of iterations over the k runs?  In addition, the experiments consider a normalized definition of gradient disparity for which no theory is provided. Although there is an interesting correlation between this normalized gradient disparity and generalization error, this link does not seem strong enough to pick the right number of iterations.   Detail:  Still regarding independence, reporting the empirical standard errors on k fold cross validation is debatable since it is not related to the theoretical standard error (see e.g. [Bengio et al.: No Unbiased Estimator of the Variance of K Fold Cross Validation. J. Mach. Learn. Res. 5: 1089 1105 (2004)]) 
The paper considers the problem of learning interpretable, low dimensional representations from high dimensional multimodal input via weak supervision in a learning from demonstration (LfD) context. To mitigate the disparity between the abstractions that humans reason over and the robot s low level action and observation spaces, the paper argues for learning a low dimensional embedding that captures the underlying concepts. The primary contribution of the paper is the ability to learn disentangled low dimensional representations that are interpretable from weak supervision using conditional latent variable models.  The paper was reviewed by three knowledgeable referees, who read the author response and discussed the paper. The paper considers a challenging problem in learning from demonstration, namely dealing with the disparity that exists between the ways in which humans and robots model and observe the world, a problem that is exacerbated when reasoning over high dimensional multimodal observations. As the reviewers note, the use of variational inference to learn low dimensional interpretable representations from weak supervision is compelling. The primary concerns are that the contributions need to be more clearly scoped and that the experimental evaluation is a bit narrow. The authors make an effort to resolve some of these issues, in part through the inclusion of an additional experiment that considers pouring tasks. However, the extent to which this second task mitigates concerns about the narrow evaluation is not fully clear. The paper would be strengthened by the inclusion of experiments in a less contrived setting (and one for which the concepts are not necessarily disjoint) as well as a clearer discussion of the primary contributions.
Protein molecule structure analysis is an important problem in biology that has recently become of increasing interest in the ML field. The paper proposes a new architecture using a new type of convolution and pooling both on Euclidean as well as intrinsic representations of the proteins, and applies it to several standard tasks in the field.   Overall the reviews were strong, with the reviewers commending the authors for an important result on the intersection of biology and ML. The reviewers raised the points of:   weak baselines (The authors responded with adding suggested comparison, which were not completely satisfactory)   focus mostly on recent protein literature   the reliance of the method on the 3D structure. The AC however does not find this as a weakness, as there are multiple problems that rely on 3D structure, which with recent methods can be predicted computationally rather than experimentally.   We believe this to be an important paper and thus our recommendation is Accept. As the AC happens to have expertise in both 3D geometric ML and structural biology, he/she would strongly encourage the authors to better do their homework as there have been multiple recent works on convolutional operators on point clouds, as well as intrinsic representation based ML methods for proteins. 
Reviewers acknowledged that the problem addressed in this paper is interesting and is not solved by the existing literature. They appreciated that the setup was well defined and the paper was clearly written. Yet they kept several concerns after the rebuttal. Especially, they expected the comparison to be done with algorithms using both demonstrations and rewards and the current empirical evaluation was not judged as fair. Also, the simple baseline consisting of adding an LSTM to BC to integrate past observations has not been considered either. This baseline is still missing to assess the quality of the proposed method. 
The paper studies the problem of stochastic optimization where the gradient noise process is non stationary. While this is an important problem in the community, the reviewers find that the assumptions are poorly justified. While the authors provided extensive feedback, the reviewers did not change their initial assessment. This paper can therefore not be accepted in its current form. I think the reviewers provided some very critical and useful feedback and I therefore strongly encourage the authors to take advantage of this feedback to resubmit their paper to another venue. 
This paper was a difficult decision. Overall it seems to be a quality paper, well written and with many experiments, in particular evaluating learned representations across various tasks and datasets. The authors were also quite courteous in their replies which is appreciated. I really like the point the paper makes about video as a natural augmentation and I find that novel amid the recent NCE surge, where most papers rely critically on augmentation. R4 was also very positive about the paper overall concept.  In terms of paper weaknesses two of the reviewers voted for rejection because the paper ignores existing work on contrastive learning from videos. The authors rebuttal is that they are the first evaluating on images, not on videos. All reviewers also point out limited technical novelty, which the authors acknowledge. Finally, R1 is not very confident about the experiments.  Overall, and after calibration, the appropriate recommendation seems to be rejection.  
This is a borderline case (quite comparable to the other borderline case in my batch). The paper has received careful reviews and based on my weighting of the different arguments I arrive at an average score between 5.75 and 6.. The authors present some worthwhile ideas related to disentanglement that deserves more attention and that could spark more research in this direction. At the same time, the level of novelty and significance of this work remains a bit limited. Taken together the paper is likely not compelling enough to be among the top papers to be selected for publication at ICLR. 
The paper builds upon a recent paper BiQGEMM, providing a binary coding based post training quantization technique. The authors show how to combine magnitude based importance metrics to these techniques and achieve superior performance. The use of importance metrics for quantization and pruning is not new, and magnitude based metrics are among the more common metrics. With that in mind, the novelty of the paper is in the integration of importance metrics to the techniques of BiQGEMM. The provided methods lead to several hyper parameters and the task of tuning these can be non trivial and time consuming. Due to this the authors devote a detailed section showing how to properly tune these hyper parameters. This is appreciated and indeed alleviates the problem coming with new hyper parameters.  The paper received mixed opinions by the reviewers related to its overall novelty, but the resulting conclusion is that although the combination of binary coding based quantization with importance scores is not trivial, the challenges faced relate more to correct implementation as opposed to scientific novelty. Combined with other issues raised by the reviewers such as a need for further comparison with existing work, this lead me to recommend rejection for this paper. 
This paper presents two new representation learning tasks (losses) based on contrastive learning that when combined with a language modeling loss result in a better multilingual model. Experiments on machine translation and XTREME demonstrate the benefit of the proposed method compared to strong baselines.  I think this is an interesting paper that advances multilingual representation learning. The authors have incorporated many suggestions from the reviewers to improve the paper during the rebuttal period. I recommend to accept the paper, but also strongly suggest the authors to make an official submission to XTREME to validate their results.
The paper describes a new technique to train an adversarial MDP to perturb the observations provided by the environment.  This adversarial MDP is then used to train an RL agent to be more robust.  Since the adversarial agent essentially defines an observation distribution for the environment, the RL agent needs to optimize a POMDP.  This is nice work that was unanimously praised by the reviewers.  It produces stronger adversaries and more robust RL agents than previous work.  This represents an important contribution to the state of the art of robust RL.  
The paper proposes a very interesting decomposition of the neural tangent kernel, which promises to decouple effects of the parameters and data. The authors illustrate the effects of this decomposition by considering pruning strategies for initialization. While the approach looks promising, the current paper is somewhat premature: The only "hard"  theoretical result, Theorem 1, is a direct consequence of the decomposition.  Its consequences for  training discussed in the subsequent paragraph involve quite a few approximations, yet the effects  of these approximations remain unclear. This general, high level tone is kept when discussing the  initializations.   Finally, the N(0,1) response to Reviewer 3 worries me.
No discussion or answers to concerns are offered by the authors. Given this, the current consensus remains the same as the initial review status, and AC s meta review cannot provide any additional information.  This leads to rejection
The paper proposes an approach to selectively update the weights of neural networks in federated learning. This is an interesting and important problem. As several reviewers pointed out, this is highly related to pruning although with a different objective.  It is an interesting paper but is a marginal case in the end due to the weakness on presentation and evaluation.   
R4 of this submission was slightly positive on this submission while all other reviewers expressed quite significant concerns in their reviews. R4 also agreed that the originality and experimental results as presented in this submission are not sufficient during discussion, although he/she pointed out the incorporation of long range structural information is novel. Given the above recommendations and discussions, a reject is recommended.  
This work proposes the Federated Matching algorithm as a novel method to tackle the problems in federated learning. The paper is well written and original, and it contributes to the state of the art. 
This paper deals with unsupervised image to image translation and proposed a geometric constrains for better structural similarity between the source and the target. Experiments are done using multiple GAN frameworks and demonstrate reduction in distortions in the generated images.  The reviewers appreciated the contributions, but were overall not very enthusiastic about the paper, with two rejection recommendations. In particular, the criticism regarded   limited applicability; shape similarity does not always translate into a good visual result; scenes with multiple similar objects might be severely distorted   some results show a strange mixture of styles   missing implementation details   only small quantitative improvement   similarity to prior works on perceptual loss   lack of clarity about the use of mutual information for geometry preservation, and implementation details   unconvincing baselines  The authors provided an extensive rebuttal addressing some of the above comments. However, many of the doubts remained because of which we believe the paper cannot be accepted. 
This paper focuses on  once for all (OFA) network training towards developing accurate models for different hardware platforms and varying latency constraints. The paper proposes an approach to significantly reducing model search space and thus training costs without losing in predictive performance. The paper is well written, and the authors provide a thorough and convincing response to the concerns raised by some of the reviewers.
The paper considers a variant of the point goal navigation problem in which the agent additionally receives an audio signal emitted from the goal. The proposed framework incorporates a form of acoustic memory to build a map of acoustic signals over time. This memory is used in combination with an egocentric depth map to choose waypoints that serve as intermediate subgoals for planning. The method is shown to outperform state of the art baselines in two navigation domains.  The reviewers all agree that the paper is very well written and that the evaluations are thorough, showing that the proposed framework offers clear performance gains. The idea of combining acoustic memory as a form of map with an occupancy grid representation as a means of choosing intermediate goals is interesting. However, the significance of the contributions and their relevance are limited by the narrow scope of the audio video navigation task, which seems a bit contrived. The paper also overstates the novelty of the work at times (e.g., being the first use of end to end learned subgoals for navigation). The author response resolves some of these concerns, but others remain.
This work makes the observation that gradients in neural network training are approximately distributed according to a log normal distribution. This observation is then used to compress and sparsify the gradients, which can be useful in distributed optimization of neural nets. The reviewers indicate that this contribution is novel and useful and they do not find any major issues with the presented work. I recommend accepting the paper for a poster presentation.
This paper analyzes a version of optimistic value iteration with generalized linear function approximation.  Under an optimistic closure assumption,  the algorithm is shown to enjoy sublinear regret.  The paper also studies error propagation through backups that do not require closed form characterization of dynamics and reward functions.  Overall, this is a solid contribution and the consensus is to accept.
The paper identifies a subtle gradient problem in adversarial robustness  imbalanced gradients, which can cause create a false sense of adversarial robustness. The paper provides insights into this problem and proposes a margin decomposition based solution for the PGD attack.  Pros:   Novel insights into why some adversarial defenses may make some versions of PGD overestimate robustness.   Proposes a method that is motivated by such findings of imbalanced gradients.   The proposed attacks are shown effective across a wide range of defenses.  Cons:   The proposed gradient imbalance ratio could be better motivated: i.e. how is it connected to the scheme of margin decomposition?   Limited novelty in the attacks: i.e. variant of the existing PGD and MT attacks with some proposed changes.   Various concerns with experiments (i.e. stepsize tuning, choice of hyperparameters).  Overall, the reviewers felt that there were some interesting ideas and directions presented in the paper; however, the reviewers also felt that the contribution was of marginal significance and more confidence in the various components (i.e. how the proposed metrics measure the imbalanced gradient effect and various concerns in the experiments) would have made the paper more convincing.
The paper addresses learning with noisy labels, by detecting and correcting samples with noisy labels. Reviewers had concerns about the empirical evaluations, specifically about comparing to additional methods, about hyperparameter tuning, and about the improvements being vey small. There was also a concern that the analysis of the objective does not take into account explicitly the L2 regularization induced by weight decay. Based on these concerns the paper is not ready yet for publication.   
The paper received 3 reviews with positive ratings: 7,6,7. The reviewers appreciated overall quality of the manuscript, thoroughness of the evaluation, and practical importance of this work (mentioning though that the technical novelty is still not high). They also acknowledged impressive empirical performance. The authors provided detailed responses to each of the reviews separately, which seemed to have resolved the remaining concerns. As a result, the final recommendation is to accept this work for presentation at ICLR as a poster.
The paper received reviews from experts in representation of invariant functions. They all have expressed concerns regarding the novelty of the technical contributions, and the lack of appropriate comparisons to existing results. This applies in particular to representation of symmetric functions using neural networks which was largely covered by previous works, as acknowledged by the authors. The authors are encouraged to consider the valuable inputs by the reviewers and revise accordingly. 
This paper studies how to improve the worst case subgroup error in overparameterized models using two simple post hoc processing techniques. All reviewers were positive about the paper, though R5 questioned the novelty of the paper which built heavily on a few previous papers (in particular, it builds heavily on Sagawa et al. 2020a,b). The AC is satisfied with the authors`  response clarifying the novelty. Given that this topic is quite timely and of interest to the ICLR community, and that this paper presented a clean investigation on it, the AC recommends acceptance.
All reviewers are positive or very positive about this work. The authors successfully addressed all questions. I believe this paper should be accepted.
The paper proposes to lear molecular descriptors that account for the 3D structure of molecules. This is done by using first a "Hamiltonian Engine" that runs a brief simulation, predicting the structure of the small molecule by minimizing a learned potential energy, and second, a message passing algorithm that uses the predicted structure as input. The reported experimental results show state of the art performance.  Strengths:  1   Relevant contribution through the Hamiltonian Engine.  2   Strong empirical results.  Weaknesses:  3   Some reviewers mentioned that the readability of the paper could be improved.  I recommend the authors to also take into account the concerns of AnonReviewer1 to improve the paper.
From the positive side the problem addressed by the paper could be of potential interest in the case there is noise in the features associated to each node of the graph. The paper is mostly well written and clear. The proposed approach is based on solid mathematical grounds.  On the other hand there are concerns about:  i) motivation: it is not clear how significant the proposed approach is since the authors were not able to clearly highlight the advantages with respect to the standard approach where already the weight matrix (via learning) can play the role of a low pass filter for node’s features. Maybe the main advantage is given by the fact that the network does not have to learn a low pass filter, however this needs a better clarification;  ii) suggested approach: the authors are using an approach that seems to be more complex with respect to simpler ones already proposed in literature and not mentioned in the paper. In addition to that, the simpler approaches have convergence guarantees that have not been proved for the proposed approach;  iii) significance of the experimental results: the obtained experimental results are obtained by using a model with more parameters with respect to the baselines. Comparisons versus baselines with a similar number of parameters are necessary to have a fair assessment of the merits of the proposed approach. 
The paper proposes speeding up iterative simulations of complex dynamics systems based on connected rods. Traditionally, these systems alternate between forward integration of the dynamics and constraint projections. Instead of replacing this entirely with end to end trained ML, here ML is only added a single point in the method to speed up the iterative solver itself, more precisely by providing initial estimates for the constraint projection step. This is done with graph networks.  At initial evaluation, the paper had two slightly favorable reviews (6) and two unfavorable reviews (4) and was therefore on the fence leaning towards rejection.  Reviewers appreciated a well motivated method and in an interesting problem.  However, on the downside, issues raised where lukewarm performance; novelty (a direct application of graph networks); lack of generality of the approach; similarity to graph networks applied on mesh based physics simulations, and similarity to NN applied for speeding up elasticity simulations; application on the finest level only; memorization/lack of generalization; simplicity of baselines; simplicity of tasks (including the added more complex tree task).  There seemed to be some confusion on whether one of the reviewers had read the initial NeurIPS submission only (which he also had reviewed) or also the ICLR submission; the authors seemed to be upset up this possibility and made it clear in their responses, but the AC can assure them that the proper version has been read, reviewed and discussed; the author s responses in that respect were not helpful.  The authors provided responses to most of the raised issues, and several reviewers acknowledged that the paper had been improved, in particular by adding comparisons (e.g NN search).  However, in spite of these improvements, the discussion among reviewers and AC revealed that the paper still has serious issues, in particular minor novelty, lukewarm improvements, and some issues re: comparisons to baselines. While the reviewers acknowledged merits in the idea, the weaknesses hindered them to champion the paper for acceptance at this point, and the AC concurs, recommending rejection.
This paper addresses the problem of how best to sample hard negatives during contrastive learning, a topic of importance for the recently resurgent field of metric learning / contrastive loss based unsupervised representation learning. Backed by theoretical results for a new low variance version of the NCE, the paper proposes an easy to implement "Ring" method for selecting negatives that are at just the right level of difficulty, neither too hard nor too easy.  Happily, this is a paper that has improved significantly through the interactive peer review of a dedicated set of reviewers combined with prompt responses from the authors. Perhaps the result that tipped this paper over the line in my assessment: the new experimental results now show significant gains from applying the "Ring" approach for hard negative sampling to near state of the art implementations of the MoCo v2 approach, which is among the leading unsupervised visual feature learning approaches. 
I have serious concerns about how experiments are reported in this paper. Most methods tried to compare at an iteration complexity of roughly 100 epochs because it is known more computation improves performance very significantly but the computational resources are limited for many researchers, especially in academia. While this convention may not be the ideal way to compare different methods, for fairness, this practice has been followed in most of previous papers.   Unfortunately this paper disregarded this practice, and on Imagenet the reported results from previous works were mixed at 100 epochs (e.g. STR) and at 500 epochs (rigL — which was explicitly marked to be 5x in the original paper) without any clarification, and the only other method in the table showing comparable performance to the proposed method, LRR,  also requires many more than 100 epochs. Moreover, the authors did not explicitly disclose the equivalent epochs of their algorithms in the Imagenet experiments, and this is not acceptable. Based on the information inferred from the current writing, it is extremely likely that significant unfair advantages were given to the proposed algorithms.   Since the authors did not report experiments appropriately,  this paper cannot be accepted in its current form regardless of other potential merits of the proposed methods. I hope the authors view this outcome positively, and proactively fix the problem. If in revised versions, the experiments are reported according to the common practice, I am sure the work would become publishable. 
The paper introduces a simple and interesting method that adaptively smoothes the labels of augmented data based on a distance to the “clean” training data. The reviewers have raised concerns about limited novelty, minor improvement over baselines, and insufficient experiments. The author’s response was not sufficient to eliminate these concerns. The AC agrees with the reviewers that the paper does not pass the acceptance bar of ICLR.
Reviewers found the new framework interesting. However, reviewers are unsatisfied with empirical evaluations. More experiments and discussion are needed.
The paper presents a new dataset for multimodal QA that is deemed interesting, relevant and well executed by all reviewers. Multimodality in NLP (QA included) is an increasingly important topic and this paper provides a potentially impactful benchmark for research in it. All reviewers acknowledge that.  We hence recommend to accept this paper as a poster. We recommend the authors to further improve the draft before camera ready by using the recommendations made by the reviewers with a particular focus on an extended discussion wrt prior work on VQA and other. The paper should also add more precisions on the license(s) related to the images used in the dataset. 
 The paper attempts at controllable summarization in two dimensions: Length, and content. Authors try to achieve this through training data generation approach, where they provide a standard BART model with additional keywords (extracted using a BERT model) in training.  The paper s main motivation on controllable summarization is important and interesting, and despite simplicity, the results are generally positive on multiple datasets. However, despite positive results, reviewers raised several critical concerns, some of which remained unresolved after reviewer/author discussion period. Examples include concerns regarding lack of methodological novelty over prior work (R1, R2, R4), unfair/incomplete comparisons with prior work (R2, R4, R5), and not evaluating on a real user controlled setting instead of automatic keywords (R1, R4). Although the authors tried addressing human evaluation in their revision, some reviewers remained unconvinced.  Some quotes from reviewer discussions:  > I m not convinced the human eval was done properly.  > My concerns are not completely addressed and the score remains unchanged. For human evaluation, I agreed with Reviewer X. 
This paper was referred to the ICLR 2021 Ethics Review Committee based on concerns about a potential violation of the ICLR 2021 Code of Ethics (https://iclr.cc/public/CodeOfEthics) raised by reviewers. The paper was carefully reviewed by two committee members, who provided a binding decision. The decision is "Significant concerns (Do not publish)". Details are provided in the Ethics Meta Review. As a result, the paper is rejected based Ethics Review Committee s decision .  The technical review and meta reviewing process moved proceeded independently of the ethics review. The result is as follows:  This paper considers sparse (L0) attacks against binary images analysis systems, in particular OCR.  The major concern of the reviewers seems to be similarity to other methods in the literature, but reviewers did not specify any specific methods to compare to.  Because it was not possible for reviewers to address such vague concerns, and because I believe the authors did a good job differentiating their work in the rebuttal, I think the paper is of good merit.  
There seems to be some disagreement between Reviewers, with some borderline scores and some very good scores. After careful consideration of both reviews and answers, and after reading the updated version of the paper with some detail, I believe the approach is valuable. The use of scores for detecting out of distribution data is very novel and presents a number of opportunities for further research, both theoretically and empirically. Overall, my recommendation is to ACCEPT the paper. As a brief summary, I highlight below some pros and cons that arose during the review and meta review processes.  Pros:   Straightforward method. "Trivial application".   Novel application to medical images.   Robustness of default hyper parameters.   Future open sourcing of the code and model checkpoints.   Topic highly relevant to the ICLR community.   Well written paper + relatively good visualizations.  Cons:   Lack of comparison with other existing approaches.   Intuition/explanation/motivation on why the method works could be improved.   Effect of hyper parameters could be further discussed/analyzed.   Concerns about applicability of the approach. 
This paper introduces a method to estimate dynamics parameters in recurrent structured models during the learning process. All three reviewers agreed that the idea is interesting and the proposed method could be potentially useful. However, two of the three reviewers have a serious concern about the lack of comparison with other approaches. I agree with these two reviewers; due to the lack of discussion and comparison with existing studies, I cannot recommend accepting this submission in its current form. 
This paper proposes training Gaussian mixture models using SGD, creating an algorithm appropriate for streaming data. However, we feel that the current manuscript does not sufficiently support the proposed method, and lacks insight into its workings. The reviewers believed the method lacked justification (while the authors claim to have added theoretical justification to the revised manuscript, I did not see any such new theory), and were not convinced that the method offered a significant improvement on existing methods. 
This paper presents an extensive evaluation of two language models: GPT 3 and UnifiedQA on 57 tasks. The results demonstrate that these models are still far from expert level accuracy and do not know when they are wrong.  I think this is an interesting paper that provides useful insights into the capability of large scale language models. The authors also plan to release their dataset and have addressed some of the concerns from the reviewers to improve the paper during the rebuttal period.
Although there was some initial disagreement on this paper, the majority of reviewers agree that this work is not ready for publication and can be improved in various manners. After the discussion phase there is also serious concern that the experiments need more work (statistically), to verify if they hold up. More comparisons with baselines are required as well. The paper could also be better put in context with the SOTA and related work. The paper does contain interesting ideas and the authors are encouraged to deepen the work and resubmit to another major ML venue.
The paper claims to draw a connection between pruning and differential privacy. There seem to be conceptual issues with the paper, highlighted by all reviewers (see particularly Reviewer 3 s review), which the authors had no response to. For example, a function approximating another does not imply any transfer of differential privacy. This is a fundamental issue that the authors would need to address.  Some papers (suggested by the area chair and reviewers) related to differential privacy and pruning that the authors may wish to be aware of include https://arxiv.org/abs/1503.02031 and https://arxiv.org/abs/2008.13578 
# Quality: The technical contribution of the paper seems reasonable and there were only minor points being highlighted by the reviewers.  # Clarity: The paper would benefit from being more polished. During the rebuttal, the authors suggested that several reviewers misunderstood the paper. This alone should encourage the authors to improve clarity.  # Originality: Several reviewers presented concerns about the claims of the authors and the existence of connections to existing literature. Nonetheless, the proposed approach seems novel to the best of the reviewers and my knowledge.  # Significance of this work:  The topic of the manuscript is relevant and impactful. However, several reviewers suggested to include additional baselines in the experiments to validate the goodness of the proposed approach.  # Overall: The paper presents an interesting idea, with a high potential impact. Despite the interesting topic and some interesting insights, all the reviewers agree that the manuscript is not ready for publication just yet. I want to encourage the authors to keep improve it and resubmit it at the next conference.
This paper presents work on zero shot learning.  The reviewers appreciated the simplicity of the method and its clear exposition.  However, concerns were raised over novelty, motivation, and empirical validation.  After reading the authors  response, the reviewers remained of the opinion that these concerns have not yet been addressed sufficiently.  Based on these points, the paper is not yet ready for publication.
The paper recieved three consistently positive reviews. While I agree with most of them, I have two major concerns regarding the novelty of the paper, which the authors are strongly recommended to address in the final version.  1. Taking derivative with respect to the parameters of transformation isn t novel. The standard tangent prop algorithm has been around for over a decade:  P. Simard, Y. LeCun, J. S. Denker, and B. Victorri. Transformation invariance in pattern recognition tangent distance and tangent propagation. In Neural Networks: Tricks of the Trade. 1996. (see Eq 26 in https://halshs.archives ouvertes.fr/halshs 00009505/document)  Salah Rifai, Yann N. Dauphin, Pascal Vincent, Yoshua Bengio, Xavier Muller. The Manifold Tangent Classifier. NIPS 2011. (see Eq 6 therein)  I understand there is some normalization in Eq 5, sampling of \alpha, \alpha , and the direction, and using the expectation to approximate the norm of the gradient. But such novelty is really incremental, or at least some empirical comparison will be necessary. It will also be necessary to cite the tangent distance/prop literature.  2. The new gradient based regularizer in Eq 11 and 12 appears completely decoupled with contrastive learning. It can be applied to any representation learning where f_\theta is an encoder. It does not use any substantial element from contrastive learning, although it might be "inspired" by contrastive learning. One may argue that such generality is an advantage, but 1) there is really no need to take such a big detour into contrastive learning just in order to derive the invariance regularizer in Eq 12, and 2) writing in this way can be quite confusing and/or misleading.
We thank the authors for detailing their answers to the reviewers and uploading a new version of the paper with more details and experiments. While the experimental section has improved in the revision, the fact that the method proposed is an ad hoc sequence of 6 heuristic steps, not supported by theoretical justification, and that the paper is hard to follow and not rigorous in its statements, remain.  For example, the authors explain in their response that "[we] developed a new method to solve a fundamental mathematical problem". If this is the case, then one would expect the mathematical problem to be rigorously formulated, and the fact that the method solves it supported by theoretical justifications.  Regarding the problem statement (section 2.2), the authors write three equations, which however are ill defined or ambiguous. For example, equation 2.1 has an expectation. Does that mean that X is a random matrix? This is nowhere stated, and there is no expectation in the following equations. Equations 2.2 and 2.3 are about a "truncated SVD" operator, which is also not rigorously defined. Literally, the authors state in Section 2.1 that for a matrix Z, tSVD¨*(Z) USV, where S is the "diagonal matrix Z s singular values, where all except for the top r singular values are forced to be zero". But what is r? Is it a parameter of tSVD*? Or is it, as suggested in the last sentence, "the numerical rank of Z" that suggests that it depends on Z, but in that case the "numerical rank" should be defined if it is different from the rank. I take these examples to highlight that the authors should consider writing rigorous and correct equations to define the problem.  Regarding the fact that the method proposed solves the problem, the authors add in the new versions some lemmas to support the claim. Alas, these lemmas also lack rigor (and therefore correctness) in their statement and proofs. For example, in Lemma one, the statement mentions a mysterious "if the other r_{MS,i} are large enough" (what is "large enough"), mention "by expectation at least one matrix slice.." (what does the "by expectation at least" mean?), and claim as main result "at least one matrix slice [...] will enrich the matrix" (what does "enrich" mean?). Looking at the proof of Lemma 1, it is just based on a probabilistic argument that if you randomly pick enough rows or columns in a matrix, they will hit a given subset with some probability. However, the authors seem to forget that randomly selecting rows and columns is only one step in their algorithm, and that the output of "Matrix_Slicing" is obtained by subsequent steps (maximizing inner products etc...). In conclusion, the statement of Lemma 1 is not rigorous, and its proof is also vague and not correct. More generally, this lemma and the following ones are far from providing evidence that the method proposed is likely to solve the problem.  While the method may be a heuristic approach with some empirical merit, we therefore believe that the paper is currently not ready for publication.  
The paper proposes adaptive optimization algorithms for federated learning that are federated versions of existing adaptive algorithms such as Adam, Adagrad, and Yogi. The paper establishes convergence guarantees for the proposed algorithms and performs an extensive experimental evaluation. Following the discussion, the reviewers were positive about the paper and felt that the author responses addressed their concerns. I recommend accept.
While all reviewers agree the problem of TEEs for model training is well motivated, the reviewers remain divided on whether the concept of randomly selecting computations to verify has sufficient novelty, and whether the proposed gradient clipping method is well motivated. 
This paper experimentally observes the negative transfer in Multi task Graph Representation Learning and proposes to solve the negative transfer with a novel Meta Learning based training procedure. However, the proposed methods seems not technically sound. There are some concerns about this paper：1. The technique contribution of this paper is limited. The method proposed in this paper is just an application of MAML in Graph Representation Learning with a little variation. 2. This paper only compares SAME with the vanilla MTL method, which adopts the uniform weights. However, the vanilla MTL method commonly performs poorly. The state of the art MTL methods should be taken into comparison, for example MGDA [1]. 3. The traditional Meta Learning framework introduced in Algorithm 4 is misleading.  4. The experimental analysis of this paper is not sufficient. For example, the paper has not analyzed whether the improvement comes from the meta updating or comes from the singularly training strategy.  [1]. Sener, Ozan, and Vladlen Koltun. "Multi task learning as multi objective optimization." NIPS 2018.  
Thanks for your submission to ICLR!  This paper considers a novel unsupervised image clustering framework based on a mixture of contrastive experts framework.  Most of the reviewers were overall positive about the paper.  On the positive side, they noted that the paper had an interesting idea, was well motivated, written well, and had solid results.  Also, the authors provided detailed and useful responses to the reviews, which further strengthened the case for accepting the paper.  On the negative side, one reviewer felt that the paper seemed a bit preliminary and its presentation could improve.  Also, there was some concern about missing comparisons / discussion to previous work (including from a public comment) or data sets (e.g. ImageNet 10).  Again, the authors responded well to these concerns.  Given that the overall response was quite positive with the paper, I m happy to recommend accepting it.  
The paper introduces a new formulation for learning low dimensional manifold representations via autoencoder mappings that are (locally) isometric by design. The key technical ingredient is the use of a particular (theoretically motivated) weight tied architecture coupled with isometry promoting loss terms that can be approximated via Monte Carlo sampling. Representative results on simple manifold learning experiments are shown in support of the proposed formulation.  The paper was generally well received; all reviewers appreciated the theoretical elements as well as the presentation of the ideas.  However, there were a few criticisms. First, the fact that the approach requires Monte Carlo sampling in very high dimensions automatically limits its scope. Second, the experiments seemed somewhat limited to simple (by ICLR standards) datasets. Third and most crucially, the approach lacks a compelling enough use case. It is not entirely clear what local isometry enables, beyond nice qualitative visualizations (and moreover, what the isometric autoencoder provides over other isometry preserving manifold learning procedures such as ISOMAP). Some rudimentary results are shown on k NN classification and linear SVMs, but the gains seem to be in the margins.   The authors are encouraged to consider the above concerns (and in particular, identifying a unique use case for isometric autoencoders) while preparing a future revision.
This paper considers convex optimization problems whose solutions involve the solution of linear systems defined in terms of the Hessian. It presents algorithms that reduce the runtime of standard iterative approaches to solving these problems by iteratively sketching the Hessian; the novelty lies in the fact that the authors use the idea of learned sketches which have been used prior for problems in data mining. In particular, the authors use the approach to learning sketches of Liu et al., 2020 to learn the entries in sparse sketching matrices for the Hessian, and propose using the Iterative Hessian Sketch algorithms of Pilanci and Wainright, 2016 to iteratively solve the concerned optimization problem. The advantages of learned sketches are that they may allow using smaller sketch sizes while making progress on the problem, as they are learned to work well on the distribution of Hessians from which the problem instance is drawn.  The consensus of the reviews is that the idea of using learned sketches for convex optimization seems to be novel, and this paper is an interesting attempt, but falls short of the level of contribution required for publication in ICLR. The main concern is that the theory provided for the use of the learned sketches is incremental: the analysis does not reflect the fact that the sketches are learned; instead, the algorithm builds in a safeguard by using both a random sketch and a learned sketch, and the analysis uses the properties of the random sketch to proceed. The empirical results are suggestive, but the convergence rates of the learned and random sketches do not vary much, so the benefits seem marginal for most of the problems considered (with the exception of a standard least squares problem, for which we know learned sketching performs well).   The paper is recommended to be rejected, as the theory is weak, and the empirical results are borderline. 
The paper presents a computational model for transformer encoders in the form of a programming language (called RASP), shows how to use this language to "program" tasks solvable by transformers, and describes how to use this model to explain known facts about transformer models.  While the reviewers appreciated the novelty of the main idea, the evaluation and the exposition were found to be below the ICLR bar. As a result, the paper cannot be accepted this time around. I urge the authors to prepare a better new version using the feedback from the reviews and discussion.  In particular, the paper would be much stronger with a discussion of how the ideas here can help with improving the transformer model, and whether these ideas generalize to models other than transformers.
This paper  * adheres to the Bayesian interpretation of MC dropout and applies it to Transformer based NMT, thus approximately sampling from the NMT model s posterior predictive distribution $Y_*|x_*, \mathcal D$ * as the NMT predictive distribution is over a discrete sample space, the authors compute variance of pairwise comparisons between the translation and other candidate outputs in a beam of likely translations (the authors call this BLEUVar).  Whereas the work is potentially interesting it does not seem ripe for publication. Here are some of the issues I d like to highlight:  1. OOD detection. Detection in input space seems like a natural baseline. The authors argue that OOD detection in output space takes the downstream task into consideration, but going through the conditional also makes the task considerably more difficult and computationally challenging. Though we appreciate the author s point, we don t see it as a good enough reason to discard OOD detection in input space as a serious alternative.   2. Why BDL? The motivation for Bayesian methods is clear, but BDL can at best *approximate* Bayesian reasoning, thus the question does deserve an answer. The reviewers asked for experiments that demonstrate empirically the relevance of the Bayesian formulation, for example, one reviewer suggested to compute BLEUVar in the frequentist case, and that makes perfect sense. Consider this: $q(\theta)$ likely under estimates posterior uncertainty, so let s say that $\operatorname{Var}(\theta|\mathcal D)$ is rather small, then BLEUVar as presented is in fact not capturing posterior predictive uncertainty (due to entropy of $Y_*|x_*, \mathcal D$), but rather sampling uncertainty (due to entropy of $Y_*|x_*, \theta$).   3. Unrealistic experiments: we all agreed that the experiments are weak. For example, we do not share the authors  excitement for the results around a foreign language as an example of OOD data point, we see it as an artificially simple case. We also expected more interesting cases of mixed domain data sets (for ideas, check tasks within WMT and IWSLT, as well as resources such as Opus and low resource language pairs as those in FLORES) and more generally different levels of noise (e.g., synthetic data produced by other translation engines, round trip/back translations are very typical in low resource settings).   Additional remarks/suggestions: * in my personal view, BLEUVar should *not* be based on biased statistics (beam search introduces all sorts of unknown biases); the pairwise comparison mechanism behind BLEUVar is similar to what MT researchers call minimum Bayes risk decoding (a frequentist criterion for making decisions under uncertainty). * we do believe the setting explored in this paper *is* related to confidence estimation, and even though I agree with the authors that a direct comparison is not per se needed, CE datasets could still prove useful for evaluation;  Though the paper has been appreciated for it dispenses with quality annotation, for it attempts to quantify estimation uncertainty (or epistemic, if the authors prefer) rather than sampling uncertainty (or aleatoric), and for other technical contributions (such as BLEUVar), we think this paper needs more than subtle/careful positioning, it really needs to acknowledge the relevance of certain alternatives and evaluate against them (BDL need not win every comparison, that s not so much the issue, the issue is that the current picture is too incomplete).   A final (personal) remark. I noticed the exchange regarding the suitability of the paper to an ML (vs NLP) venue. I personally do not think your submission is more or less appropriate to one or the other on the grounds of its technical content. The expert reviews attached suggest enough ideas for improvements, and I would imagine an improved version of the paper having a good chance at any major ML (or NLP) venue. 
All of the reviewers thought that this paper addresses an interesting and important problem.  Several of the reviewers thought that the paper gave a creative approach for training bloom filters and this would be of interest to the community. 
The work studies the transferability of perturbations/adversarial attacks on DRL agents. As a way to mitigate the cost of generating individual perturbation for each state in each episode, the authors proposed several variants to use same perturbation across different states across different episodes. While reviewers recognize the potential of the direction, they are not comfortable accepting the paper at its current state. The experiment results in its current form does not provide enough support to the claim. In particular, it is unclear how much the shared perturbation changes the original perception in comparison to the individual comparison, and how should the impact number differences be interpreted. Reviewers brought up concerns regarding all the experiments being evaluated on a DDQN agent, and not enough clarities has been provided on the different design choices. If perceptual similarity is not the indicator of environment transferability, do the authors have intuition on what does? 
The paper proposes to learn layout representations for graphic design using transformers with a masking approaching inspired by BERT.  The proposed model is pretrained on a large collection of ~1M slides (the script for crawling the slides will be open sourced) and evaluated in several downstream tasks.   Review Summary: The submission received slightly negative with scores of 4 (R3) and 5 (R2,R4).   Reviewers found the paper to be well written and clear, and the problem of layout embedding to be interesting.  Reviewers agree that the use of transformers for layout embedding has not been explored in prior work.  However, the paper did not have proper citation and comparisons against prior work for layout embedding, and lacked systematic evaluation.  Reviewers also would like to know more details about the dataset that was used for pre training.    Pros:   Novel use of transformers for layout embedding (not yet explored in prior work)   Use of large dataset of slides  Cons:   Lacked proper citation and comparisons against prior work for layout embedding   Lacked systematic evaluation   Missing details about the dataset  Reviewer Discussion: During the author response period, the authors responded to the reviews indicating that they will improve the draft based on the feedback, but did not submit a revised draft. As there was no revision to the submission, there was limited discussion with the reviewers keeping with their original scores.  All reviewers agrees that the direction is interesting but the current submission should not be accepted.  Recommendation: The AC agrees with the reviewers that the current version is not ready for acceptance at ICLR, and it would be exciting to see the improved version.  We hope the authors will continue to improve their work based on the reviewer feedback and that they will submit an improved version to an appropriate venue.  
The paper studies an interesting problem motivated by VLSI design. The reviewers agree that there are interesting aspects of the RC algorithm. Nevertheless, the paper could be improved by a clearer characterization/apples to apples comparison to baselines, particularly regarding computation cost, use of parallelism, as well as a more thorough contrast to state of the art in general. Given the contribution is experimental, and this is a well studied problem, it is important to establish whether the solution is indeed best in class; cost due to training should be taken into account, and minimized to the extend possible. Going beyond the baselines considered here, as well as reviewing possible theoretical connections to other problems and guarantees, would also strengthen the paper.
This paper proposes a new method for learning a model for spatio temporal data described by an (unknown) spatio temporal PDE. The model learns a continuous time PDE using the adjunct method and uses graph networks to perform message passing between different discrete time steps on a grid obtained with Delaunay triangulation.  The method initially 3 favorable and 1 unfavorable ratings, but convincing responses to some of the raised issues led to unanimous recommendations for acceptance (not all reviewer feedback after the rebuttal has been made public, but feedback has been made to the privately AC on these issues by different reviewers).   The reviewers appreciated novelty of the method and numerous ablations.  Initially perceived weaknesses were some key experiments on generalization over different grid discretizations; the simplicity of some experiments, and links to different prior art   many of these points have been dealt with by authors in their response.  The AC concurs and proposes acceptance.
This paper introduces a dataset and a trained evaluation metric for evaluating discourse phenomena for MT. Several context aware MT models are compared against a sentence level baseline. The paper develops metrics which evaluate the models according to their performance on four discourse phenomena: anaphora, lexical consistency, coherence and readability, and discourse connective translation. Data is released for three language pairs (all using English as the target language).   First, I’d like to point out that creating datasets and benchmarks for analyzing/evaluating discourse level errors in machine translation is an extremely valuable contribution. This paper is addressing a very relevant problem and even though there is no new model/method/algorithm being proposed, this work *fits* this conference   it is my opinion that the community should welcome and value more than it currently does the efforts spent in creating high quality datasets that can help make progress in the field.  There was substantial discussion among reviewers about this paper.   The main weaknesses raised by the reviewers were:   Limited information about the process to create the anaphora test, which was a contribution of prior work (Jwalapuram et al. (2019)   this was addressed in the updated version; but the anaphora challenge sets seem to be only a minor update over previous work.   All language pairs use English as the target language, and it is not simple to extend this approach beyond English target languages.   Lack of detail on how BLEU scores were computed (tokenised? true cased? My recommendation is to use sacrebleu)   this was clarified in the rebuttal.   The evaluated NMT models all date from 2018 or earlier.   Two of the 4 benchmarks (anaphora and coherence) are evaluated by neural models trained on WMT outputs, which makes the interpretation of scores is opaque, and their validity is unclear.  While the creation of a benchmark for discourse evaluation of MT is a laudable effort as mentioned above, it is my opinion that due to some of the weaknesses above the current version of this work is not yet ready for publication. However, I strongly encourage the authors to improve upon these points and resubmit their work to another venue. I list some suggestions below to improve this paper.  My biggest concern with the current version is the last weakness above. As pointed out by a reviewer, the framework of Jwalapuram et al. (2019) provides empirical support for the model s sensitivity (if there is a pronoun error, does the metric pick it up?). But they don’t necessarily capture model *specificity* (if the metric ranks one output higher, can we be confident that this is because of a pronoun translation error?). For the coherence metric, authors make an argument that their metric is sensitive to coherence issue, but concerns remain about whether it is sufficiently specific to these issues. In the rebuttal, authors argue that BLEURT is sentence level, but they could easily aggregate sentence level judgments and report correlation between BLEURT and human coherence  judgments to show that their metric correlates better with human coherence judgments than BLEURT or even just BLEU. Besides BLEURT, I would add there are other recently proposed metrics that may capture discourse phenomena (neural metrics trained against MQM annotations or sentence level human assessments with document context) and should be compared against: check COMET [1] or PRISM [2] (the latter is sentence based but could be adapted for paragraphs or documents).  There is also prior work comparing various context aware machine translation approaches against a sentence level baseline, some with negative findings [3,4,5]. I suggest the authors look at this related work in future iterations of their paper.  [1] https://arxiv.org/pdf/2009.09025.pdf  [2] https://arxiv.org/pdf/2004.14564.pdf   [3] https://www.aclweb.org/anthology/2020.eamt 1.24.pdf  [4] https://arxiv.org/pdf/1910.00294.pdf   [5] https://www.aclweb.org/anthology/2020.emnlp main.81.pdf  
Given two data measures in R^d, this paper proposes to use a NN to augment the representation of each data point found in these measures with additional coordinates. The measures are then compared using the sliced Wasserstein distance on these augmented representations. Because this augmentation is injective by design (the original vectors are part of the new representation) simple metric properties are kept. The authors propose to learn in a robust/adversarial way these augmentations. They propose simple experiments to illustrate that idea.  Although I found the idea interesting, I think it falls short of acceptance at ICLR. I agree with the sentiment of other reviewers 1 and 2 that defining another variant of robust/NN inspired variant of the W distance is interesting, but at this point the readership of the conference expects more than simple experiments on toy data and hard to interpret GAN results. I think there is value in the draft as it stands now, but that more efforts are needed to convince this variant is scalable / useful for other downstream tasks (e.g. W barycenters, or other easier to interpret W problems in lower dimensions).  minor comments   as it stands, equation 2 is wrong if you do not add more conditions on the cost function d(.,.).    " the idea of SWD by projecting distributions onto hypersurfaces rather than hyperplane"  > this is wrong, the projection is done onto lines or curves, not hyperplanes or hypersurfaces. 
This paper presents novel results on linear identifiability in discriminative models, with three of the four reviewers arguing for acceptance. The paper went through an extensive round of edits, which incorporated detailed responses to issues raised by the reviewers.   While this paper would be a nice contribution to the conference, some reviewer concerns remain unresolved, so we encourage the authors to revise and resubmit to a future venue.
The authors use Empowerment for morphology optimisation, a quite novel idea. After initial unclarities and various improvements on the submission, the reviewers unanimously voted for acceptance of the paper.  
The average review ratings for this paper is somewhat borderline. The paper provides mathematical characterizations on when ReLU neural networks are injective. The paper has very nice ideas, but the reviewers also pointed out several key concerns:   1. “Given that the DSS condition takes exponential time to check, how do you check for injectivity of a given network?” 2. “But after you train a network using some training dataset, these matrices are no longer random or generic. How do you ensure that the network is still injective?” 3. “With leaky ReLU or flow model, global injectivity is automatically satisfied for non degenerate weight matrices, and in most applications, we don t see much difference”  I think points 2&3 are particularly important here. It seems that for practitioners, if injectivity is a key concern, then one can just use leaky relu with well conditioned weight matrices that guarantee injectivity. Note that well conditioning is easy to check and relatively easy to enforce. It’s unclear to the AC why one has to stick to particularly the relu activation and the recipe provided by corollary 2 and the paragraphs below Corollary 2. (It also seems to the AC that Corollary 2’s construction is fundamentally similar to the using leaky relu, but the AC is not quite sure.) Given that a much easier workaround (using leaky relu and full rank matrices) is available and is widely used in prior works (when it’s necessary), the AC, unfortunately, does not see that the paper could have a strong impact on the ML community and does not think the experimental results are sufficient to justify that this is a better idea than using leaky relu. In the AC’s opinion, the paper might be more compelling in a math venue.   
This paper proposes a graph information bottleneck (GIB) framework for subgraph recognition, including the proposal of a MI objective as well as a bi level optimization scheme for minimizing said objective. The paper receive mixed reviews, with two reviewers in favor of acceptance and two reviewers in favor of rejection.   One negative reviewer was too short to judge and had low confidence. I think most of the concerns arise from lack of understanding of the work and the authors adequately address this on the rebuttal. The authors are encouraged to make minor modifications for clarity. In particular, classical IB considers random variables x, z, y, and learns latent representation z that is maximally informative about output y and sufficiently informative about input x. Therefore, it is natural to expect that the input to GIB is a random graph.   The other negative reviewer finds the paper lacks novelty and points to multiple references. The positive reviewers also ask about the connection with additional references. Im my opinion, the authors do an excellent job at clarifying the differences with all prior work mentioned by the reviewers, including the closest one, a GIB paper in NerurIPS 2020. In my view, the present submission contains sufficient novelty relative to prior work, specifically as it focuses on a different problem (sub graph) and proposes a different optimization method. That being said, I think it is absolutely essential that the author responses be added to the paper. In other words, the final version must add citations to the relevant work mentioned by the reviewers and clarify the differences.  All other comments from the two remaining reviewers are very positive: the reviewers find the paper contributes with "quite interesting information theoretic objective functions that actually work on multiple graph learning tasks" and "makes a clear theoretical contribution to the field as well as provides sufficient empirical evaluation." I share the views of the positive reviewers and recommend acceptance, subject to the authors incorporating their responses to the reviewers  comments.
This paper introduces modifications that allow to make the training of contrastive learning based models practical. The goal of the paper is very interesting, and the motivation clear. This paper tackles a very important issue with recent unsupervised feature learning methods. However, while the goal is great, the present submission does not provide time improvements on par with the ambitions of this work. As noted by R2, many other hacks could be used in conjunction with the current work to scale this goal to the extreme, yielding time improvements which would be of a more impressive magnitude. In its current form, this paper unfortunately doesn’t meet the bar of acceptance. Given the interesting scope of this work, I strongly encourage the authors to take the feedback from reviews and discussions into account and submit to another venue.
This paper introduces a bag of techniques to improve contrastive divergence training of energy based models (EBMs), particularly a KL divergence term, data augmentation, multi scale energy functions, and reservoir sampling. The overall paper is well written and clearly presented.   In response to the major concerns from reviewers, the AC recognizes the authors  effort in expanding related work and adding ablation on the effects of the KL loss. However, reviewers remain unconvinced by the significance of the current results. In particular, the quality improvement by adding the KL terms is subtle compared to using reservoir sampling (as evidenced in the contrast of the last two rows in Table 2). Moreover, the authors are also encouraged to compare additionally with recent development in EBM, as pointed out by R2 & R4.  The AC does find the results on downstream tasks such as out of distribution quite promising and interesting. Perhaps it s worth expanding the discussion with formal reasoning on why KL loss helps in this case.   All four knowledgeable reviewers are leaning towards rejection, the AC respects and agrees with the decision.  
The paper studies the problem of evaluating representations and proposes two new metrics: surplus description length and epsilon sample complexity.  Pros:   A good overview of existing methods and their corresponding weaknesses (i.e. sensitivity to dataset size and insensitivity to representation quality and computational complexity).   The proposed procedures seem to be well supported conceptually.   Has an efficient implementation.  Cons:   While the theoretical results in the Appendix are appreciated and do provide some insight into the procedures, they more or less seem straightforward and don t answer some important questions (i.e. what is the sample size necessary in terms of epsilon? is it exponential in some dimension?).    More insight could have been provided into where the noisy measurements come from in these metrics as there appear to be many components in the calculations that could be contributing to the noise (i.e. dataset distribution, dataset size, bootstrap samples, probe initializations, etc).   The methods make an assumption that the performance is monotonic in the dataset size, which is often not the case (i.e. there is a subfield regarding removing noisy label examples to improve performance; moreover there are investigations in the active learning literature that suggest sometimes performance degrades with more training data).   It appears that the proposed metrics are based on data efficiency (i.e. least number of samples to get obtain a desired performance). However, such may have more of a dependence on the distribution of the data and how the examples are chosen (i.e. they can be actively chosen) moreso than the actual representation. This may or may not be an issue but may deserve at least some discussion.  Overall, the reviewers appreciated the new methods proposed and how they relate and improve upon previous methods; however, as currently presented, most were unconvinced about its significance which was a key reason for rejection.  
The paper proposes a unification of three popular baseline regularizers in continual learning. The unification is realized through a claim that they all regularize (surprisingly) related objectives.  The key strengths of the paper highlighted by the reviewers were: 1. The established connection is valuable and interesting, even if weaker than suggested originally 2. Good motivation (unifying different regularization methods is useful for the community) 3. Clear writing  The key weakness of the paper is a weak empirical validation of the claim that these three regularizers work *because* they regularize the norm of the gradient (as mentioned in the discussion by R3). Rather, the key claims are correlational. The authors correctly say that (1) the three regularizers all regularize related objects (namely different norms of the gradient) and (2) they reduce forgetting. However, it is not sufficiently well demonstrated that (1)  > (2). Relatedly, given that the paper does not have a very clear theoretical contribution, it would be really helpful to demonstrate utility. It would be useful to extend experiments that apply these insights to developing novel regularizers or improving/simplifying hyperparameter tuning.  Additionally, in the review process, the link was discovered to be weaker than originally suggested. The paper casts the relation in terms of the Fisher Information Matrix, which suggests it is theoretical and sound. After the discussion, it seems that viewing this relationship in terms of the Fisher Information Matrix is somewhat misleading. The three different regularization methods all regularize different norms of the gradient (L1 or L2), which are empirically, and under some assumption theoretically, related. More precisely, EWC regularizes the trace of the *Empirical* Fisher, which is equivalent to the L2 norm of the gradient of the loss function. SI regularizes a term similar to the L1 norm of the gradient. These effects were seen by the reviewers to be somewhat loosely related to the Fisher Information Matrix.  Based on the above, I have to recommend rejection. I would like to thank the Authors for submitting the work for consideration to ICLR. I hope the feedback will be useful for improving the work.
The reviewers unanimously agree that this paper is a strong accept; it makes important progress in developing our ability to query relational embedding  models.
The paper proposes a variant of the hierarchical VAE architectures. All reviewers felt that the paper s clarity was lacking. While the authors made very significant improvements during the feedback phase, which were recognized by reviewers, the paper could use a revision that takes clarity into account from the ground up. I also think that the ablation studies should be expanded (if SOTA is not the goal, then science should be), e.g., compare to the setting in which q does not share the bijective layers with p.
Overview: This paper introduces a maximum mutual information method for helping to coordinate RL agents without communication.  Discussion: Some reviewers leaned towards accept, but I found the two reviewers recommending rejecting to be more convincing.  Recommendation: This is an important research topic and I m glad this paper is focusing on the problem. Hopefully the reviews will help improve a future version of this paper. I agree that this is a new way of using mutual information, but it seems more like a small improvement rather than a very significant step forward.   In addition, I think the setting needs to be better motivated. This is a centralized training with decentralized execution (CTDE) setting, and this paper helps the agents coordinate. In CTDE, the agents work in the environment and then pool their information to train before deploying on the next episode. I don t understand why, e.g., in multiwalker, agents would not be able to communicate while walking, can communicate after they succeed or drop the object (the episode ends), and then cannot communicate once the next episode starts.
The paper shows that it is possible to reconstruct private images from CPU cache line and OS page table accesses side channels, using a generative model on top of side channel traces. The reviewers agree that the problem is interesting and the experimental evaluation makes a convincing case that such an attack is possible. The author rebuttal was useful in clarifying some aspects of the paper, and the discussion on possible mitigation strategies is a nice addition to the paper.
The paper studied multi objective reinforcement learning (MORL), and provided a Bayesian optimization approach for challenging MORL scenarios in several simulation environments. The reviewers generally find it interesting to account for robustness in a MORL setup, and all appreciate the algorithmic contributions. However, there were shared critical concerns among \ reviewers in the technical clarity and positioning of the work.   The paper has gone through substantial changes during the rebuttal period, which addressed some concerns regarding the experimental details; however, the major revision raised further issues that affects the clarity of the work. The reviewers are hence unconvinced that the paper is ready for publication. In addition to addressing the existing comments on clarifying the experimental details and properly positioning the work against prior art, a reorganization and optimization of the main content would be beneficial for future submission. 
The paper proposes a method for inference in models with GP priors and neural network likelihoods for multi output modelling, dealing with the problem of scalability and missing data. The paper builds upon previous work on inducing variables for scalability on GP models and inference networks for amortization (reducing the number of parameters to estimate) and dealing with missing data.  There are several concerns about the paper in terms of generality/flexibility of the approach, as the proposed model shares the NN parameters across tasks and the results on the small datasets do not show improvements wrt baseline such as GPAR. The authors’ comments provide somewhat satisfactory replies to these issues. Nonetheless, the major drawback of this paper is its novelty as the ideas on the paper have been explored extensively in the GP literature. Although the authors do make a case for scalability when using inference networks, there are other previous works that perhaps the authors are unaware of, for example, https://arxiv.org/abs/1905.10969   and even more sophisticated inference algorithms than can serve as truly state of the art competing approaches (for example based on stochastic gradient Hamiltonian Monte Carlo, https://arxiv.org/abs/1806.05490). 
All four reviewers were against accepting the paper. A major point shared by everyone was lack of clarity: this included its overall writing, its discussion toward prior work, and imprecise math to explain the ideas. The paper did improve quite a bit over its revisions. Whether this clarified all of the reviewers  understanding of the paper remains unclear. The work may ultimately need another cycle of reviews to assess its quality.  Another shared point are a number of recommended ablations in the experiments, as well as going through more comprehensively in the set of studied datasets (R3), effect of AE choices (R2), and alternatives to the geodesic (R1, R2).
 This paper addresses a crucial problem with graph convolutions on meshes.  The authors identify the issues related to existing networks and devise a sensible approach. The work presents a novel message passing GNN operator for meshes that is equivariant under gauge transformations. The reviewers unanimously agree on the both the importance of the problem and the impact the proposed work could have.   Suggestions for next version:   The paper is unreadable without the appendix and somehow it would be better to make it self contained   Additional references as suggested in the reviews.   Expanded experiments as suggested by R4, will also improve reader s confidence in the method.   I would recommend acceptance. I would request the authors to release a sufficiently documented and easy to use implementation. This not only allows readers to build on this work but also increase the overall impact of this method.
The paper proposes to explain the representation for layer aware neural sequence encoders with multi order graph (MoG). Based on the MoG explanation, it further proposes Graph Transformer as a graph based self attention network empowered Transformer. As commented by the authors, a main purpose of Graph Transformer is to show an example application of the MoG explanation.  During the discussion period, after reading the paper and checking the code, the AC had raised a serious concern: There is a big gap between the MoG motivation and the actual implementation. The AC had urged the referees to take a careful look at the implementation details, in particular, Lines 524 561 in the attached code: "supplement/fairseq 0.6.2_halfdim_gate⁩ ▸ ⁨fairseq⁩ ▸ ⁨models⁩ ▸transformer.py". The AC had made the following comments to the referees: "Whether the performance gain of Graph Transformer over Transformer is due to the MoG explanation is highly unclear. There is no direct evidence, such as appropriate visualization, to support that. In a high level description, instead of using a usual skip connection that would combine beforex and x, the actual implementation is to 1) define increamental_x   x   beforex, 2) let increamental_x attend on beforex to produce x1, let beforex attend on increamental_x to produce x2, and let increamental_x attend on increamental_x to produce x3, 3) combine beforex, x1, x2, x3 in a certain way to produce the layer output."  Reviewer 2 responded to the AC s concern: "After examining the transformer.py and Section 2 & 3, we cannot understand why the output of self attentions could be regarded as MoG subgraphs? The authors did not explain the connection. In their code, the graph transformer seems to just utilize 3 multi head attentions (line 539 541) in their encoder. Using MoG to interpret the outputs of three attentions (line 539 541) is not very convincing. The link is weak. We agree with your comments."  To summarize, the link between the actual implementation in the code and all the MoG explanations is quite weak, and the technical novelty of the actual implementation is not strong enough for an ICLR publication. Therefore, the AC recommends Reject.     
This paper introduces Signatory, a library for computing functionality related to the signature and logsignature transforms. Although a large body of the initial literature on the signature in ML focuses on using it as a feature extractor, more recent works have incorporated within modern deep learning architectures and therefore, the importance of having GPU capable libraries (with automatic differentiation) that implement these transforms. Several algorithmic improvements are incorporated into the library. Some of the computational benefits of this library wrt to previous ones are demonstrated empirically.   There were some concerns from the reviewers about accepting library papers at ICLR. Library papers clearly fall into the ICLR CFP and, therefore, library, frameworks and platform papers that can be relevant and impactful are welcome contributions to the community. Additionally, more signature related papers are appearing are mainstream ML venues, hence, despite the poor scalability wrt input dimensions, this paper is definitely relevant.   Perhaps one the drawbacks of this paper is the lack of a more rigorous empirical evaluation. The authors have added a deep learning benchmark, which is welcome but only on a toy dataset. There are still some concerns about the wide applicability of the signature (and its relatives) given its exponential scaling. That’s why applications on more realistic problems will be welcome. At the very least, It will be good if the authors incorporate a separate section discussing the limitations of the signature transform (and the library), especially in terms of computations and scalability.  
The paper presents a self supervised model based on a contrastive autoencoder that can make use of a small training set for upstream multi label/class tasks. Reviewers have several concerns, including the lack of comparisons and justification for the setting, as well as the potentially narrow setting. Overall, I found the paper to be borderline, the cons slightly greater than the pros, so I recommend to reject it.
The paper proposed a method for adversarial robustness by considering information from the edge map of the images. Two reviewers point out the similarities of the paper with previous work ([1]) and it is unclear whether the benefits come from binarization of the input or from shape information. As such, the paper is not suggested for publication at this time
Shapley values are an important approach in extracting meaning from trained deep neural networks, and the paper proposes an innovative approach to address inefficiencies in post processing to compute Shapley values, by instead incorporating their computation into training.  There was a robust discussion of this paper, and the authors  comments and changes substantially strengthened the paper and the reviewers  view of it, to the point that all reviewers now recommend acceptance.  Some lingering concerns remain that the authors should continue to work to address.  Is the method of computing Shapley values used as the baseline in the paper really state of the art, or artificially weak?  The empirical results were methodologically sound but not as strong as one might expect or hope.  These concerns detract somewhat from enthusiasm, but nevertheless the paper yields an innovation to a widely used approach to one of the most pressing current research problems.  The reviewers had a number of smaller suggestions that should also be incorporated including more significance testing and reporting of resulting p values.
This paper combines recently emerging NTK theory and kernels with DEQ models. In particular the authors use the root finding capability of DEQ models to compute the corresponding NTK of DEQ models for fully connected and convolutional variants. The reviewers raised various concerns including lack of experimental details, incremental theoretical results(which the authors agree with but postulate that this is a practical paper), lack of proper literature review, explaining how it applies in practical scenarios and grammatical mistakes. Some of these concerns were addressed during the response period but none of the reviewers were fully satisfied with the author s response. While I think there are interesting ideas in this paper I agree with the reviewers that a substantial revision is required and therefore recommend rejection.
Description: The paper presents a method for encoding a compressed version of an implicit 3D scene, from given images from arbitrary view points. This is achieved via a function, learning with a NeRF model, that maps spatial coordinates to a radiance vector field and is optimized for high compressibility and low reconstruction error. Results shows better compression, higher reconstruction quality and lower bitrates compared to other STOA.  Strengths:   Method for significantly compressing NerF models, which is very useful since such models are often trained for every new scene   Retain reconstruction quality after compression by an order of magnitude   Weaknesses:   The need for decompressing the model before rendering can be done means reduced rendering speed. This also requires longer training times.   Experiments against other scene compression + neural rendering technique will have further strengthened the papers’s claims    The techniques used are well established, and thus there is not as much technical novelty. 
This paper consider an important problem CounterFactualRegret (CFR) minimization, and proposes a new algorithm to solve this problem.  Reviewers raised many questions and concerns that the authors chose not to answer.  We can only recommend rejection
This paper has been evaluated  by three expert reviewers, two of whom recommended rejection and one acceptance. Two of the three reviews are particularly detailed and thorough. Both point out a few points of conceptual issues that leave the reader confused. These key issues have not been addressed sufficiently in the rebuttal to result in changing the reviewers  assessments. One major concern is lacking novelty of the work as presented, which limits its current utility to the ICLR audience. I recommend a rejection.
There was quite a bit of internal discussion on this paper. To summarize:   The idea is very neat and interesting and likely to work   The paper is likely to inspire future work   There are still serious doubts  about the experimental evaluation that is not entirely up to par with current standards     The reviewers were not convinced 100% by the arguments about the  custom  environments     The reviewers were not convinced 100% that the baselines were given their best shot  While the paper has potential to provide valuable input for the community, it needs a bit more work before being presentable at a highly competitive venue like ICLR. 
Four expert reviewers (after much discussion, in which the authors seemed to do a pretty good job addressing a lot of the initial complaints) unanimously voted to accept this paper.   Everyone seemed to agree that the idea was interesting, and it is indeed interesting. There were generally complaints about benchmarking; there always are for papers about program synthesis.   One complaint I have, but that I didn t really see mentioned, is that the system as described is pretty baroque.  I have a hard time imaging how you d scale something like this up to more complicated contexts,  and honestly I m not sure even in some of the contexts where it was tested if it would really outperform a well engineered top down synthesizer. Maybe this is just an aesthetic preference that only I have, and maybe ideas need to start out overly complicated before the most useful bits can be extracted from them and refined.   At any rate, I do think that this paper gives a cool new research contribution and that people will want to read it, so I am recommending acceptance. 
This paper performs an empirical comparison of similarity based attribution methods, which aim to "explain" model predictions via training samples. To this end, the authors propose a handful of metrics intended to measure the acceptability of such methods. While one reviewer took issue with the proposed criteria, the general consensus amongst reviewers is that this provides at least a start for measuring and comparing instance attribution methods.   In sum, this is a worthwhile contribution to the interpretability literature that provides measures for comparing and contrasting explanation by training example methods. 
This paper proposes a new quantum machine learning framework which is evaluated on the MNIST dataset. While the paper was relatively well written, reviewers noted that most of the ideas are already well established and used in quantum machine learning community. Thus it was not clear what novelty is provided relative to related work.
This paper studies the following broad question: How can we predict model performance when the data comes from different sources? The reviewers agreed that the direction studied is very interesting. While the results presented in this work are promising, several reviewers pointed out some weaknesses in the paper, including a confusion between absolute loss and excess loss, and the limited scope of the experiments. Overall, this paper does not appear to be ready for publication in its current form. In my personal opinion, if the concerns raised by the reviewers are appropriately addressed, this work could be publishable in a high quality venue.
While the authors appreciated the proposed contrastive training scheme and the strong related work summary, all authors agreed that the approach was severeley limited by being a pure selection based method. Without the help of another model that proposes molecules, the approach can only select reactants from an existing set. As target molecules become more complicated, the modeller must make a choice: (a) use a much larger initial candidate set which hopefully encompases all molecules necessary to make the target molecule, or (b) use another model to propose new intermediate molecules. The authors went with (b) which harmed their novelty claim: a big reason why retrosynthesis is hard is because of the need to generate unseen molecules, and if this is left to an already proposed model, the current approach is not adding much methodological novelty. While their approach does improve upon existing work in the multi step setting, there s even more recent work that has not been compared against (e.g., https://arxiv.org/pdf/2006.07038.pdf) so the improved performance may be outperformed.  The fix is straightforward: modify the methodology to also propose intermediate molecules. This will fix the novelty complaint and strengthen the practicality argument: practitioners could directly use this approach to discover synthesis routes. The authors could slightly update the related work, add comparisons against recent methods, and take into account the other feedback given by the authors. The paper is very nicely written, the proposed changes are purely methodological, and not insurmountable in my opinion. I would urge the authors to make these changes which I believe will result in a very nice paper.
This paper tackles the problem of classifying a set of points given the knowledge that all points should have the same class. There seems to be a consensus among the reviews that under the assumptions made, the authors provide thorough experiments convincing that their method is useful. However, the paper has two weaknesses that are too strong to ignore. First, the clarity of exposition seems to be lacking, specifically a clear motivation for this new setup as well as its connection to the abstract problem being solved. Second, the assumptions made seem to be too strong, and the solution seems to rely on these strong assumptions too much.  
This paper investigate the interesting problem of policy seeking in reinforcement learning via constrained optimization. Conditioned on reviewers  judgements, this is a good submission but hasn t reached the bar of ICLR.
This paper received 3 reviews with mixed initial ratings: 9,4,5. The main concerns of R1 and R3, who gave unfavorable scores, included lack of novelty and hence limited value of this work for the ML community. At the same time, R5 strongly advocates for acceptance and mentions meaningful contributions in the context of the specific application, including the new dataset. In response to that, the authors submitted a new revision and provided detailed responses to each of the reviews separately, which did not change the position of the reviewers. The AC agrees with R1 and R3 that, even though the biometrics related contributions are relevant, the scope of this work is too narrow and application driven for presentation in the main track of ICLR. As a result, the final recommendation is reject.
This paper has generated a lot of great discussion and it presents a very different way of doing image recognition at scale compared to current state of the art practices.  All reviewers rated this paper as an accept. This work is interesting enough that in my view it really deservers further exposure and discussion and an oral presentation at ICLR would be a good way to achieve that.
This paper studies the effect of anomaly detection using supervised learning with non representative abnormal examples on the TPR of the anomaly detection model. Experiments demonstrate that when the abnormal examples presented in the training set are not representative of the abnormal examples in the target distribution, this can lead to bias in estimating the TPR.   Pros:    This paper considers an important issue of tuning anomaly detection models in the absence of representative anomalies.   The paper is well written and easy to read. Cons:   The analysis and experiments provided in the paper are not surprising, and repeat, although perhaps in more detail, known effects of learning with a non representative training sample. 
This paper explores the effect on decoding accuracy (predicting hidden representations from fMRI datasets) from fine tuning models by injecting structural bias.  This paper specifically focuses the attention of BERT on syntactic features of the text, which (for one dataset) appears to improve the decoding performance.  The paper s motivation is strong, and complex concepts are communicated clearly.  The review period was very productive.  There were some questions about analyses, and the validity of the statistical tests, but through some very thorough back and forth with the reviewers, this seems to have been resolved.  There is a good amount of analysis done on the resulting language models to try and determine the impact of finetuning or attention on the models. However, the results on the fMRI two datasets appear to be very different, and it s unclear why (and isn t clearly related back to the extensive language model analyses).  We would have liked to have seen a more thorough analysis of the stark difference in performance, and some convincing explanations for the difference based on the analyses.     P.s. A minor point, but the Wehbe paper uses Chapter 9 of Harry potter, not chapter 2. 
The authors develop novel adaptive adversarial attacks for 3D Point Cloud Classification tasks. They show that many existing defenses are broken by develop a novel pooling operation, DeepSym, and demonstrate that using this they can achieve significant improvements in adversarial robustness of 3D Point Cloud Classification.   All reviewers agreed that the paper makes interesting contributions and that the setting of 3D point cloud classification is interesting from a security perspective. The shared concern of the reviewers was around novelty. This was addressed in the rebuttal to some extent, but there remained some lingering questions that made this paper borderline and unfortunately, the program committee had to decide to reject it.  I would urge the authors to revise their manuscript to clarify the novelty relative to prior work (particular those that use similar pooling operations) and resubmit to a future venue. 
The paper presents a hierarchical version of NMF for the CP decomposition of tensors.  The idea is similar to Chinocki etal 2007 and extends Gao etal 2019, and in Chinocki was presented for the standard linear formulation with regularisation terms.  The extension here doesn t use the standard ALS algorithm but rather presents a neural network analogue, though the functions are still linear, its just that back prop etc. are used for the computation.  The authors point out their formulation is a more flexible representation and optimisation (in response to AnonReviewer5), and thus represents an improvement.  While this is an interesting implementation, in NNs, the model is still fairly simple.    Moreover the experimental results are restricted to a few data sets.  There are literally hundreds of NMF variants in publication and many different evaluations are done.  The experimental work here, while showcasing the work, is not extensive.  For instance, more empirical comparisons should have been made against prior hierarchical NMF on a battery of data.  So this is good, publishable work, and the authors have repaired many of the issues raised by the reviewers.  The work, however, is borderline in empirical work and the contribution is not strong.
Most of the reviewers agree that this paper presents interesting ideas for an important problem. The paper could be further improved by having a thorough discussion of related works (e.g. Placeto) and construct proxy baselines that reflect these approaches.   The meta reviewer decided to accept the paper given the positive aspects, and encourages the author to further improve the paper per review comments.   Thank you for submitting the paper to ICLR.  
This paper explores the connection between diversity of gradients and discriminativeness of representations. Based on the observations, authors propose Discriminative Representation Loss (DRL).  This paper resulted in a lot of discussions and specifically, R5 s detailed comments helped the authors improve their paper. Authors did a good job in making significant improvements to the paper based on the reviews, including better connecting the theory and experiments. However, after much discussion it was felt that experiments and analysis still needed improvement, leading to a decision to reject. The authors are encouraged to use the reviewers  post discussion updates to further improve and submit to a future venue.
Summary: The authors observe that a range of Laplacian type operators used in graph neural networks can be embedded in a parametric family, so that the precise form of the Laplacian used can be determined by the learning process. Empirical evaluation and some (limited) theoretical analysis are provided.   Discussion: The authors have provided detailed replies and also additional experiments. That has addressed major concerns, and most reviewers now agree the paper is good. One reviewer is more skeptical, mostly regarding presentation. I agree with some of the points raised in this regard, but see them as less of an issue   I would consider the presentation improvable, but acceptable.  One weakness I should mention is that the two theorems provided are frankly trivial. I appreciate this is  only  a conference submission, but I would nonetheless call the fact that symmetric matrices have real eigenvalues (theorem 1) an observation, not a result.  That similarly holds for any direct consequence of Gershgorin s theorem (theorem 2). The entire page used to state this could perhaps be put to better use for additional empirical results.   Recommendation: The program committee (the AC and program chairs) were hesitating about this paper but decided to recommend acceptance. The idea is neat and simple, presentation and empirical evaluation are fine, if improvable (we strongly recommend the authors to invest time). What is phrased as theory is trivial, but also admittedly not the main focus of the paper.    
This paper presents a deep reinforcement learning method that aims at ensuring resilience to observational interference. During training labels that indicate presence or absence of interference are available to the algorithm. The training objective is augmented to learn the prediction of interference that is used at test time to infer the interference label. The experimental results show superior performance in comparison to other baseline RL methods.  The main objection raised by the reviewers was on the confusing and possibly unsound causal formulation. The authors  clarifications during the discussion did not eliminate bur rather exacerbated the reviewers  doubts. I read the paper in full myself to understand whether the reviewers  confusion was justified, and whether it could be easily resolved by an improved explanation, or it is a more serious issue.  I did not succeed in clearly understanding the causal formulation nor its relevance, and also have  soundness concerns. Figure 2a does not seem to be a correct explanation of the causal mechanism. It is also not clear from this figure why z is called confounder. More generally, I was not able to reach a coherent and sound causal formulation from the authors  explanation. My conclusion is that the framing of the paper as causal inference based is not well justified. 
This paper empirically studies the impact of different types of negatives used in recent contrastive self supervised learning methods. Results were initially shown on Mocov2, though after rebuttal simCLR was also added, and several interesting findings were found including that only hardest 5% of the negatives are necessary and sufficient. While the reviewers saw the benefit of rigorously studying this aspect of recent advances in self supervised learning, a number of issues were raised including: 1) The limited scope of the conclusions, given that only two (after rebuttal) algorithms were used on one datasets, 2) Limited connections drawn to existing works on hard negative mining (which is very common across machine learning including metric learning and object detection), and 3) Limited discussion of some of the methodological issues such as use of measures that are intrinsically tied to the model s weights (hence being less reliable early in the training) and WordNet as a measure for semantic similarity. Though the authors provided lengthy rebuttals, the reviewers still felt some of these issues were not addressed. As a result, I recommend rejection in this cycle, and that the authors bolster some of these aspects for a submission to future venues.   I would like to emphasize that this type of work, which provides rigorous empirical investigation of various phenomena in machine learning, is indeed important and worth doing. Hence, the lack of a new method (e.g. to address the selection of negatives) was not the basis of the decision. While the paper clearly does a thorough job at investigating these issues for a limited scope (e.g. in terms of datasets), a larger contribution is expected for empirical papers such that 1) we can ensure the generality of the conclusions (across methods and datasets), 2) we have a conceptual framework for understanding the empirical results especially with respect to what is already known in adjacent areas (e.g. metric learning and object detection), and 3) we understand some of the methodological choices that were made and why they are sufficiently justified. 
The paper proposes to apply Mixup to Federated Learning (FL) for addressing the challenge of non iid data. The idea is very simple, but seems to work well in empirical evaluation. Some concerns were raised regarding the communication costs and privacy. The authors rebuttal and revised draft provide reasonable answers to these concerns.   For the final version, it is suggested that the authors can address the following issues:  1) Improve the writing   especially the formulation of the proposed method  2) Provide more discussions and experiments on the communication costs. 
This paper makes an innovative change to the adjacency matrix definition in graph convolutional neural networks (GCNs) (Kipf & Welling, 2017).  The change results in computationally efficient isometric transformation invariance.   There were a number of concerns raised by reviewers, and the author responses and revisions, and the subsequent discussion, resulted in most of these concerns being satisfactorily addressed.  On reviewer continued to feel the paper was entirely theoretical and therefore not appropriate to ICLR, but that opinion was not shared more broadly and is not held by the area chair.
The paper proposes a novel method for greed layer wise training by considering the learning signal from either backprop or from the additional auxiliary losses. SEarching for DecOupled Neural Architecture learns to identify the decoupled blocks by learning the gating parameters similar to gradient based architecture search algorithms, such as DARTs.  The empirical experiments demonstrated the effectiveness of SEDONA on CIFAR and TinyImageNet using various ResNet architectures. Several issues of clarity and the correctness of the main theoretical result were addressed during the rebuttal period in the way that satisfied the reviewers. The ideas in this paper are interesting and are broadly applicable. Additional experiments / discussions on the tradeoff between initial search cost and accuracy should be included in the final version. 
Thank you for your submission to ICLR.  As noted, several of the reviewers had fairly low confidence in evaluating this submission.  However, based upon the reviewers and commenters who were familiar with this line of work, as well as my own evaluation of the paper, I believe it is clearly worth publishing at ICLR.  The proposed method pushes the boundary in methods for exact branch and bound based verification of neural networks, using clever tricks from existing relaxations.  And while the method is still likely to be relegated to relatively small networks for the time being, pushing forward the state of the art in exact verification is still a worthy goal suitable for publication at ICLR.  I thus think that the paper is quite clearly above the bar, and should be accepted for publication.
This paper addresses a method that incorporates the task similarity (via task gradients) into the meta learning. The inner loop update is done by kernel regression with the similarity between gradients of tasks considered, and the outer loop is the gradient update with a particular regularization. Without any doubt, it is a timely and important topic to develop a meta learning method in the presence of outlier tasks. All reviewers criticized the experiments were done on only simple datasets without any ablation study. Authors revised their manuscript, to include more experiments, and tried to clarify the relation of their method to MetaSGD. Unfortunately, however, even after the author response, reviewers were not convinced that their concerns were resolved. In particular, it was claimed that the revised version still lacks comparisons to previous relevant work.  
This nice paper gives a better understanding of how Curriculum Learning (CL) affects image classification. In particular, it gives insight into cases such as noisy training data and limited training time. It shows that examples can be rated by difficulty to some extent, in that the order in which examples are learned seems to be consistent across runs. The paper is thorough and well written.
The paper proposes to use a feature extractor (encoder) $C(x)$, pre trained with label supervision or contrastive learning on a large image dataset, to both regularize the discriminator s last feature layer $D_f(x)$ and encode the data $x$ itself as the conditional input of the generator $G(z|G_{emb}(C(x)))$. The main purpose is to help the training of GANs when there is a limited number of images in the target domain. A clear concern of this approach is that to generate a fake image, one will need to first sample a true image, making the model unattractive if the training dataset size is large (need to store the whole training dataset even after training). To mitigate this issue, the authors propose to fit up to 200k randomly sampled $G_{emb}(C(x))$ with a GMM with 1k components. To validate the practice of requiring a GMM (a shallow generative model) to help a GAN (a deep generative model) to generate, the authors have done a rich set of experiments under state of the art GAN architectures or training methods (SNGAN, BigGAN, StyleGAN2, DiffAugment) to illustrate the efficacy of the proposed data instance prior and its compatibility with the state of the art methods in a variety of settings. In the AC s opinion, the paper is missing references to 1) related work that combines VAE (or some other type of auto encoder) and GAN, which often helps stabilize the GAN training [1,2,3], 2) VAE with a VampPrior [4], and 3) more broadly speaking, empirical Bayes related methods where the prior model is learned from the observed data (see [5] and the references therein). The potential advantages of using a VAE rather than a GMM to help a GAN to generate include: 1) there is no need to store 1k GMM components, which may require a large amount of memory; 2) there is no need to subsample the training set; and 3) the VAE and GAN can be jointly trained. The AC recommend the authors to discuss the connections to these related work in their future submission.  [1] Larsen, Anders Boesen Lindbo, et al. "Autoencoding beyond pixels using a learned similarity metric." International conference on machine learning. PMLR, 2016.  [2] Zhang, Hao, et al. "Variational Hetero Encoder Randomized GANs for Joint Image Text Modeling." International Conference on Learning Representations. 2019.  [3] Tran, Ngoc Trung, Tuan Anh Bui, and Ngai Man Cheung. "Dist gan: An improved gan using distance constraints." Proceedings of the European Conference on Computer Vision (ECCV). 2018.  [4] Tomczak, Jakub, and Max Welling. "VAE with a VampPrior." International Conference on Artificial Intelligence and Statistics. PMLR, 2018.  [5] Pang, Bo, Tian Han, Erik Nijkamp, Song Chun Zhu, and Ying Nian Wu. "Learning Latent Space Energy Based Prior Model." Advances in Neural Information Processing Systems 33 (2020). 
Although the reviewers found the paper well written that analyzes a relatively popular algorithm (TD(0) version of A3C), there are concerns regarding the novelty of the convergence results given those for A2C, the comparison of the results with those for A2C, and the sufficiency of the experiments. Although the authors addressed some of these issues/comments during the rebuttals, it seems none of the reviewers is excited about the paper and there still exist concerns regarding the novelty of the results and how they are compared with those in the literature. I would suggest that the authors take the reviewers  comments into account, have a more comprehensive discussion about the relation of their results with those in the literature (two time scale algorithms), and prepare their work for future conferences. 
This paper proposes a very interesting approach using Wasserstein distance between graph embedded by GNN to perform prediction. The paper is well written and the experiments suggest that the method works  well in practice.  Several reviewers had some concerns about  computational complexity and model parameters that were very well answered during the discussion and with the new version of the paper but it was not enough to convince them to change their overall opinion on the paper.   Note that a lengthy discussion with the reviewers stemming from an unclear Figure 3  was done about the need for a  contrastive regularization that raised important questions that should be addressed. In short, despite the claim by the authors that the contrastive regularization is important the experiments show very little difference in performances with or without the regularization which asks the question of its usefulness. As a matter of fact looking at Figure 3 the regularization will spread the samples in the distributions, making the prototype more similar . The argument about the sample collapse is not good enough because if the sample collapse completely during the optimization the method converges toward prototype L2 which is not the case since the proposed approach is much better than L2 even without regularization. So if there is some collapse it actually serves the discrimination and leads to a better solution. Also the Wasserstsein can reverse the collapsing (the samples  are never exactly at the same position and the gradient can be very different) if it helps for the optimization problem as is well known from the Wasserstein GAN literature.   Due to the remaining concerns of the majority of reviewers, the AC recommends a reject but encourages the authors to resubmit the paper after taking into account the reviewers comments and the questions about the regularization .
One referee recommends acceptance, while three referees recommend rejection. All referees agree that augmenting GAT with structural information is an interesting direction to explore; however, they raised concerns about the empirical validation of the method, the related work covered, as well as the discussion of insights such as the method s limitations. The rebuttal addresses R2 s concerns by better positioning the work w.r.t the literature and by discussing the method s potential limitations. The rebuttal also covers R1 and R3 s comments on scalability and complexity of the proposed approach. However, the rebuttal only partially addresses the evaluation concerns of the reviewers. After discussion, all reviewers agree that further work should be devoted to remedy this. I agree with their assessment and hence must reject. In particular, I would strongly recommend following the referees  suggestions and consider incorporating experiments on additional OGB datasets, including a GAT based comparison to those (and on OGB arxiv), and eventually toning down their claims.
The paper proposes a rather complex algorithm for unsupervised doamin adaptation. While the paper provides detailed explanation, some motivation and some experimental resulst, it does not provide any theoretical guarantees for its performance. More concerning, since domain adaptation can only succeed when there is a close relationship between the source and target tasks, and only with algorithms  that take that relationship into account, any scientific proposal for domain adaptation should include a clear discussion of the assumptions driving the proposed algorithms and of the circumstances under which the proposed approach  may or may not work. This is missing in the current submission.   More specifically, a similar ocncern was voiced by Reviewer 3  Namely ".The generalization error (both theoretically and empirically) of the gradient approximation is unclear. It is necessary to analyze how effective and under what conditions the proposed approximation can work for the expected target loss optimization." Thsi point was not addressed in teh authors  rebuttal.  Anotehr key concerning point that was also brought up by reviewer 3 read: "It needs elaboration why the density ratios can be directly replaced as discriminator predictions, which seems not straight forward and is the main difference to the conventional DRL." In response the authors cite the paper by Bickel et al 2007 but it falls short of addressing the well know fact that density ratio cannot be reliably estimated from samples of bounded size. The authors should have explained specific assumptions that can make this step of their algorithm og through.
While the submission has promising components, the reviewers were not able to reach a consensus to recommend acceptance. The main concerns is that (1) theorem statements and assumptions are not clearly explained, and (2) the novelty of the approach is not made clear, and (3) there remain concerns on whether the experimental results are due to hyperparameter search or improvements due to the model.  
This paper attempts to explain why popular UNMT training objective components (back translation and denoising autoencoding) are effective. The paper provides experimental analysis and draws connections with ELBO and mutual information. Reviewers generally agree that the paper s goal is worthy: trying to form a better theoretical understanding of successful approaches to UNMT.  However, most reviewers raised serious concerns about the current draft and suggested another round of revision and resubmission. Specifically, reviewers were concerned that some of the analogies used to explain UNMT are underdeveloped. Further, reviewers pointed to issues with clarity that made some of the arguments hard to follow. Finally, one reviewer argued that many of the results are expected and agree with common understanding of UNMT in the literature, thus undermining their value to some extent. I tend to agree with reviewers that this paper is not ready for publication in its current form. Thus I recommend rejection. 
There is clear consensus on this submission. Reviewers cite a lack of comparison with recent state of the art methods and experiments on more realistic datasets. Though the reviewers find aspects of the approach interesting, the decision is to reject. 
The paper studies offline RL, which is an important topic in high risk domains. Compared with the existing works, this paper gives a tractable method to explicitly learn the model representation w.r.t the stationary distributions of two policies. This method is pretty general and could be paired with other pessimistic model based RL methods.  The experiments are limited to simpler domains, and could be extended to include harder tasks from other continuous control domains. Some examples could be domains such as in Robosuite (http://robosuite.ai/) or Robogym (https://github.com/openai/robogym). These environments have higher dimensional systems with clearer implications of representation learning.   There are concerns on writing style and comprehension.    The work is on the one hand very specialized, on the other hand just an incremental modification of existing methods.    The presentation is very dense and quite hard to grasp, even with the Appendix.   The formalism, while important, can be very loose in terms of bounds. While that does open questions in RL theory, it would be useful for authors to be more candid about this fact in the paper.  I would recommend including the response to R1 in the paper.  Other relevant and concurrent papers to potentially take note of:   Fine Tuning Offline Reinforcement Learning with Model Based Policy Optimization (https://openreview.net/forum?id wiSgdeJ29ee)    Robust Offline Reinforcement Learning from Low Quality Data (https://openreview.net/forum?id uOjm_xqKEoX)  Given the overall positive reviews, I would recommend acceptance. However, the method would benefit from additional pass on re writing to make the manuscript more accessible, which in turn to increase impact of this work. 
The paper studies the effectiveness of few shot learning techniques in settings where the training labels are imbalanced. While addressing an interesting practical problem, reviewers raised concerns about the paper s technical depth, insufficient distinction to existing techniques for coping with label imbalance, and limited qualitative conclusions from the results. The authors incorporated some of these comments in their revision, but a more comprehensive update on the latter two points appears appropriate.
This paper proposes a new approach to learning deep generative models with induced structure in the latent representation. All four reviewers gave the same score of 6 to this paper, showing a consensus that the paper is above the bar for acceptance. The authors did a commendable job of detailed replies to reviewer comments, which as R1, R3, and R4 all note has improved the clarity and quality of the paper, addressing their concerns.
The paper introduces group equivariant self attention networks constructed by defining positional encoding that are invariant to the group action considered. This is related to equivariance in set networks . The work is sound and the idea of infusing the inductive bias via the positional encoding is  interesting and leads to improvement  results when comparing transformers with equivariance and without it, nevertheless more work needs to be done to bridge the gap in terms of performance with CNNs as pointed by the reviewers.  Authors made an admirable efforts in the rebuttal and in the revision of the paper clarifying most of the reviewers questions and concerns.   Accept   
Two reviewers suggested to reject and the other reviewer also thought it below the threshold.
The paper presents an extensive empirical evaluation of several loss functions and regularization techniques used in deep networks. The authors conclude that the classical softmax is significantly outperformed by the other approaches, but there is no clear winner among them. Moreover, the authors have noticed two interesting facts, (1) the choice of loss function affects only upper layers of neural networks with the lower layers being very similar to each other, (2) losses that result in greater class separation also result in higher accuracy, but their features are less transferable to other tasks.  I agree with the authors that the comments of Reviewer 2 are shallow and not informative. Therefore, they were not taken into account in making the final decision and, as AC, I read the paper very carefully. Regarding Reviewer 4, however, I found his comments to be valid. There is a message that the authors want to communicate, and the reader that needs to decode this message using a noisy channel. Therefore, I encourage the authors to accordingly revise the paper to make the message much clearer.   The experimental papers that compare a wide spectrum of methods are always hard to judge and this judgement is often very subjective. There are several seminal papers of this type, but not so many for several reasons. I agree with the authors that such studies are very valuable and give evaluation being not biased by authors of a given method. They are also very time  and resource consuming. But there should be a general consensus how such an experiment should be performed. The authors of the particular methods should also be able to give right feedback to make their methods to be run appropriately. Therefore, there exist several websites and initiatives that try to fulfill these requirements. As said above, any paper of this type will be judged very subjectively.    The discoveries made by authors should also be presented in a different way. One should start with a hypothesis that, for example, the lower layers are not affected by the loss function and then perform appropriate theoretical and empirical studies to verify the hypothesis. The same applies to the other discovery. In that way the message of the paper would be much clearer. I suppose that analysis of each of the discoveries deserves its own paper.   
The paper proposed a novel RL based solution to the optimal partial of DNNs which is interesting to readers.  However, the paper is not well presented and hard to follow. The lack of comparisons agains existing solutions and inconsistencies in the writing as pointed out by the reviewers largely weakens the submission.  There s also no updates to the paper based on reviewers  comments.   The main reason for the decision is lack of clarity and significance justifications.
The paper introduces LEAD, a decentralized optimizer with communication compression that can achieve linear convergence rate in the strongly convex setting. In terms of novelty, the authors should still add a discussion of `Magnusson et al., 2019, On Maintaining Linear Convergence of Distributed Learning and Optimization under Limited Communication`,  which is a related linear convergence result in the deterministic (full gradient) case, and relates to the analysis here which is stochastic but also exploits the deterministic case. Nevertheless, reviewers reached consensus *with communication compression in the given time* that the paper in its current form is well written and the results are presented clearly in both experiments and theory (which builds up on the earlier NIDS algorithm). The presentation of the algorithm can be slightly improved. We hope the authors will incorporate the remaining smaller open points such as mentioned by R1, such as making the constants in the convergence bounds explicit when comparing with other methods.
This paper gives a new algorithm for the CCA problem. The main idea of the new algorithm is to reformulate the matrices in the CCA problem as a product of three matrices: one orthonormal matrix, one rotation and one upper diagonal matrix. The algorithm then performs remannian gradient descent to these components. The per iteration complexity of the algorithm is O(d^2k) while the (local) convergence rate is O(1/t). Overall the reformulation is interesting and the algorithm seems effective in practice. On the other hand the convergence rate proof relies on local strong convexity and it s not clear why the algorithm converges globally (or even what is the radius of convergence locally).
This paper introduces ATC, which is a contrastive learning on observations separated in time, to learn representations that do not need to take rewards into consideration. These learned representations allow, for the first time, a real disentanglement between representation learning and control, as the agent can simply load such a representation, “freeze it”, and still recover performance of end to end deep reinforcement learning agents.  Overall, all reviewers agree this is a promising direction. Nevertheless, there has been extensive discussion (with the authors and privately) about the significance of the reported results due to the small number of seeds. On one hand, there’s the argument that there is a wide range of experiments and that should compensate for a small number of seeds in individual experiments. On the other hand, there are experiments with as little as two seeds (e.g., DMControl multi env) and this can be seen at most as anecdotal evidence. There’s also the argument that we, as a community, should be striving for more reliable and meaningful experiments in reinforcement learning. Moreover, there have been concerns about how “variance” is being reported (max and min performance) and, although the authors replied to that, an alternative plotting was never shown.  Importantly, at this point it is not clear how many seeds were used in each experiment (Figures 6, 7, 9, 11, 12, 13 do not report the number of seeds used). It is said that each curve represents a minimum of 3 random seeds, but that is very informal and not that useful. Exactly stating the number of seeds would be the right thing to do, not to mention that in the rebuttal it is said that 8 game pretraining for Atari multi env uses 2 seeds, contradicting the original claim. Also, sometimes, different methods, in the same experiment, are  “averaged” across different numbers of seeds (“DMLab offline   ATC is 4 seeds, PC and CPC are 2 seeds each”). This is particularly problematic because of the small number of seeds and potentially high variance. Reporting the max over 4 numbers drawn from a Gaussian distribution is very likely to lead to a larger number than when reporting the max over 2 numbers drawn from the same Gaussian distribution.   I do acknowledge the effort to increase the number of seeds during the rebuttal phase, but it is hard to accept a paper with unknown results. We have very little evidence to believe that going from 2 seeds to 5 seeds is not going to change the results. The reviewers couldn’t agree on the variance of this process as well. Some say the variance of PPO is low between runs when using the same hyper parameters while others mention papers (e.g., Deep RL that matters) that show how much variance one can have across these methods. Thus, I cannot accept this paper conditioned on more seeds being added to the final version because we don’t know what the results will look like. Since this paper is mostly an empirical study, it should have thorough experiments and a careful analysis of the results, but the small number of seeds prevents that in my opinion. Thus, as difficult as it is given the promising direction of the paper, I’m recommending its rejection. I strongly encourage the authors to increase the number of runs in their experiments and to use a more standard measure of variability (e.g., standard error, standard deviation) when reporting their results. This will then be a very strong submission for a future conference. 
The authors did a good job responding to reviewer concerns.   While the reviewers still consider the method described in the paper to not be especially novel, at least one is impressed by the practicality.  imo the authors  attention detailed ablations and analysis post review makes the paper worth including in the conference.
The paper proposes an approach to learn sparse embeddings for documents/labels which can be trained by using multiple GPUs in parallel, and are more amenable to nearest neighbor search.   The paper certainly seemed to have botched  comparison to SNRM and requires to fix the claims in  section 5.1.  But, the impressive performance on extreme classification tasks is quite convincing. Also, reviewers in general are quite enthusiastic about the paper. So we would recommend the paper for acceptance, but authors certainly need to take comments of reviewers into account (especially around baselines and comparison to SNRM).
This paper aims to do efficient epistemic uncertainty quantification for model based learning for control. It does so by augmenting the dataset with synthetic data around the true data points, and trying to classify whether a point is close to the training set or not. I agree with many of the criticisms that R3 and R5 brought fourth. Namely, it s not clear why a kernel density estimate couldn t be used instead (runtime complexity is cited as the reason, but could be addressed through approximations, inducing points etc). It is not clear how to set the sampling distribution for X_epi. Also, since efficiency is a motivation for the work, I suggest that the authors look at and cite:  https://arxiv.org/abs/2002.06715   I think at the moment the paper is not ready for publication, but the idea is interesting. Aside from comparing with the work above, what would improve this paper is an automatic way to select the distribution, or at least the covariance, of X_epi.  
This paper presents a method to improve the calibration of neural networks on out of distribution (OOD) data.  The authors show that their method can be applied post hoc to existing methods and that it improves calibration under distribution shift using the benchmark in Ovadia et al. 2019.  However, reviewers felt that the theoretical justification for why this works is unclear (see detailed comments by R1 and R4), and some of the choices are not well justified. Revising the paper to address these concerns with additional theoretical and/or empirical justifications should improve the clarity and strengthen the paper.  I encourage the authors to revise and resubmit to a different venue. 
The paper presents an extension of recent implicit representations for view synthesis, such as NeRF. The presented formulation accepts an image set as input at test time, and can thus in principle be applied to new scenes. The idea is sound, but reviewers had concerns with the presentation and the experimental results. The work is primarily evaluated on the simplistic ShapeNet domain, which a number of reviewers found unconvincing. Concerns remain even after the authors  responses, and the AC agrees that the work can benefit from further investment before it is published.
The paper analyzes MDPs with execution delays. Interesting theoretical results and experiments are provided, which show the benefits of the proposed algorithms. However, some issues are highlighted in the reviews, such as the lack of theoretical analysis of the proposed delayed Q learning method, and the simplicity of the experiments. The latter is at least partially addressed by the authors in the rebuttal, and the new experiments should be incorporated in the final paper.  
The paper provides a new distance preserving embedding based on a recent result called sigma delta quantization. The authors notice that in many realistic scenarios, the input vectors are well spread and under assumptions regarding the spreadness provide a fast technique to convert the input vectors into binary vectors, possibly of lower dimension. For completeness, the authors analyze the setting where the vectors are not spread and show that by using a randomized Walsh Hadamard transform, their results still apply. The authors do not provide a completely novel approach, to quote R2 “On a technical level the results in this paper are hardly too surprising for the JL community, but it is nice to see this analysis worked out in detail”. That being said, they show that a natural idea indeed works out by providing both a theoretical analysis and experimental results. The experiments can be more thorough but do convey the point that the result indeed works and moreover is somewhat robust in that it works well even when the formal requirements do not entirely hold. There are a few issues mentioned by the reviewers that should be addressed: A clearer exposition of the guarantees and assumptions, some comparison with previous papers. However given the responses and discussions these seem minor and fixable towards a camera ready version. I recommend accepting the paper 
The paper introduces an approach to self train a source domain classifier on unlabeled data from the target domain, considering the few shot learning setting when there is significant discrepancy between the source and target domains. While the reviewers pointed out a few weaknesses, such as somewhat limited methodological novelty  and lack of comparisons with other methods, they all recommend acceptance as final decision. The paper is beautifully written. The proposed method is very simple, but yields excellent results in a very practical problem, which should be of wide interest to the ICLR community. The experimental evaluation is rigorous and the ablation studies are convincing. The AC agrees with the decision made by the reviewers and recommends acceptance.
The review phase was very constructive, where reviewers raised several opportunities for improvements. The authors did a very good job in their rebuttal, which led some reviewers to change their opinion in a positive direction. Overall, reviewers agree that this is the borderline paper with remaining concerns about the weak experimentation.  The paper was again discussed by the Area Chair and Program chairs.  Due to the competitive nature of the conference and the high bar of experimental evaluations expected by empirical papers, the paper was finally rejected.  We hope authors will use the feedback from the reviews and make a stronger submission in near future. 
The paper studies a hierarchical or multi level version of local SGD, extending earlier work by (Wang & Joshi, 2018), (Lin et al, 2018) and  (Jiang et al. 2019) among others. It gives novel convergence rates in relevant settings, such as by allowing different workers to take different numbers of local steps within a given time interval. The current analysis is restricted to the IID data case, but still insightful, and might serve as a useful building block for follow up research in the future. Smaller concerns remained that the presented multi level results cannot exactly recover local SGD as a special case. Nevertheless the consensus remained that the overall contributions and relevance of the paper remain above the bar. In the discussion phase, several concerns were clarified and additional deep learning experiments have been added to the paper, which is appreciated.
This papers considers the problem of accuracy disparity in regression for the case of binary sensitive attributes. It provides bounds for accuracy disparity and introduces two methods to enforce this criterion based on representation learning.   The reviews are in agreement that the paper is generally clear and well written, but have different opinions regarding the significance of the contribution and the experimental section. I did read the paper with care myself and overall I do share the concerns raised by Reviewers 3 and 4 that the paper does not place itself accurately wrt to the current literature, both in the discussion and the experimental section. The response to the reviewers about methods that can achieve accuracy disparity for classification is not satisfactory, also considered that two of the analysed datasets are about binary classification tasks. Regarding the results on these datasets, it would be useful to report classification accuracy rather than (in addition to) R^2.  The proposed methods do not seem to show a significant advantage versus the methods considered for comparison.  Minor comments: The proposed methods are inspired by Theorem 3.2. However, is not enforcing accuracy disparity by minimising some distance between conditional distributions what the literature does? I cannot think of other meaningful ways to achieve this criterion. In fact, referring to this theorem, the authors says  However, it is nearly impossible to collect noiseless data with group invariant input distribution. Moreover, there is no guarantee that the upper bound will be lower if we learn the group invariant representation that minimizes dTV(D0(X), D1(X)) alone, since the learned representation could potentially increase the variance. In this regard, we prove a novel upper bound which is free from the above noise term to motivate aligning conditional distributions to mitigate the error disparity . But minimizing dTV(D0(X), D1(X)) might not be desirable if the dependence between the sensitive attribute A and the data is considered legitimate. The point of using conditional distributions is to allow that dependence to be retained.      
This work proposes a framework to search for the topology of an artificial neural network jointly with the network training, via a genetic algorithm that can decide structural actions, such as addition or removal of neurons and layers. An extra heuristic based on Bayesian information criterion helps the optimization process decide on its decisions about the topology. They demonstrate improvements over baseline fully connected networks on SVHN and (augmented) CIFAR 10.  Reviewers and myself agree that this is an interesting idea, and that the paper is easy to follow. While I may not agree that we need to achieve SOTA on these datasets, or see large scale ImageNet type experiments for novel ideas, I agree with the reviewers, esp R1 s point that the current experiments are not satisfactory to meet the bar for acceptance at ICLR.  CIFAR 10 and SVHN are well established tasks, and showing baseline accuracy of 75%/48% on them respectively doesn t seem to do them justice, especially when most methods (even with low compute requirements) can get > 95% on both, for the past few years. For this work to be of interest to the broader community, it needs to be improved to incorporate at least respectable baselines on these small datasets, and perhaps be improved to work beyond fully connected networks.  At this stage, we need to see a revision of the method and see improvements before an acceptance decision can be made.
The paper describes an autoencoder based approach to anomaly detection.  The main weakness—not untypical for papers in this application area—is the experimental section.  The problem itself may be not well defined, and of course that makes practical comparison difficult. Perhaps different measures—e.g., remaining life—may be better to compare on, and give better data sets.
This work proposes to improve Mixup by using soft labels, removing the need for input mixup. The reviewers found the paper was clear and found the experiments promising. The reviewers raised concerns about the lack of experiments comparing this approach to Mixup+Label smoothing, which were addressed during the rebuttal by the authors. However, the reviewers did not find the empirical evidence strong enough given that this is mostly an empirical contribution. The authors do not necessarily need to train on the full Imagenet, but it would be beneficial to evaluate on more standard settings on the dataset considered to facilitate comparison to previous work.
The paper is concerned with modeling multi relational data with joint hierarchical structure. For this purpose, the authors extend box embeddings to multi relational settings, supporting the modeling of cross hierarchy edges and generalizing from a subset of the transitive reduction. The reviewers highlight that the paper is, overall, well written and organized, relevant to the ICLR community, and that the proposed method offers promising experimental results. Furthermore, the author s rebuttal clarified some concerns of the initial reviews (e.g., relation to GumbelBox, comparison to additional baselines etc.) and improved the manuscript.  However, after rebuttal there exist still concerns regarding the current version. Reviewers raised concerns regarding novelty, clarity, and the empirical evaluation (importantly modeling more than 2 relations; it would also be good to understand more clearly why some of the newly added multi relational hyperbolic baselines perform worse than uni relational Poincare embeddings). While the paper and the proposed method clearly have promise, I agree with reviewers that the manuscript would require an additional revision to clarify these points. Given the positive aspects of the paper, I d strongly encourage the authors to revise and resubmit their work given this feedback.
This paper addresses a central problem in inference in implicit models  classical approaches on such problems ( ABC ) rely on computation of summary statistics, and multiple methods for automatically finding summary statistics have been proposed. This paper provides a fresh take on this classical problem, by providing a methods for finding information maximising summary stats. The work is original, likely impactful, and carried out rigorously and carefully. The reviewers flagged some issues with empirical comparisons, as well as discussion or relevant work  those issues mainly seem to have been resolved in the review process. Moreover, given the originality of the approach, and provided that the description of empirical comparisons and relationship with other work are carefully and conservatively worded, I believe this will be worth publishing even if it is not always the  best  method on all problems. 
The paper aims at understanding why self supervised/contrastive learning methods  transfer well when used as pretraining for fine tuning downstream tasks  (compared to e.g., supervised pretraining based on the cross entropy loss). Three reviewers recommend acceptance, whereas one reviewer recommends borderline rejection, arguing the take home message of the paper is not very clear. While this is a legitimate concern, the AC agrees with the majority that the paper does shed light on the differences between supervised and self supervised pretraining (based on interesting empirical findings) and recommends acceptance.
Many papers have been written on calibrating neural networks recently.  This paper presents a definition of calibration that is more robust than the popular ECE measure while also being more discerning than the Brier score.  Then it proposes a practical spline based method of post editing the output softmax scores to make them more calibrated.  The method is shown to be better than existing methods both on their measure and established measure (thanks to reviewer s questions on that.). The paper should be of much interest to the community.
The paper introduces "Concept Embeddings"  to  Prototypical Network, which are part based representations and are learnt by a set of independent networks (which can share weights).  The method first computes the concept embeddings of an input, and then takes the summation of the distances between those concept embeddings and their corresponding concept prototypes in each class to estimate the class probability. The experiments validates the proposed methods on 4 benchmarks in three different domains, including vision, language and biology. For the biology task, the authors also develop a new benchmark on cross organ cell type classification.  The key novel idea of transferable concepts results in significantly improved generalization ability over the existing few shot learning methods.  Although some reviewers raised concerns about not using other few shot image classification datasets such as MiniImageNet these are not appropriate benchmarks, as the method requires the “part based concepts” to reasonably span the space of all images which is a characteristic of fine grained image classification problem.   Although this does limit the scope of the method, the fact that it is applicable for multiple tasks is a strong counteragument to the claim that it is too limited, so overall I disagree with the assessment of one reviewer that the choice of benchmarks is insufficient.  
Four knowledgeable referees lean towards rejection because of the missing detailed complexity analysis [R1,R2,R3], the choice of rather small datasets which hinders the rigorous evaluation of GNN models [R3,R4], missing state of the art comparisons [R2] and ablations [R4]. The rebuttal addressed some of the concerns raised by the reviewers, in particular, clarifications request by R2, smoothness of the weights questions of R4, and the difference in performance of the baseline methods of R1. However, after discussion, the reviewers are still concerned with the missing ablations, comparisons, and complexity analysis. I agree with their assessment and therefore must reject. However, I agree with the reviewers that this is an interesting approach and encourage the authors to consider the reviewer s suggestions for future iterations of their work. 
The paper addresses a very important issue in GNN, the definition of a well defined pooling function for node aggregation. The proposed Graph Multiset Transformer, although not entirely new, seems to be useful in practice. Issues related to experimental results, as well as problems with presentation, have been solved by the authors s rebuttal, that presented solid experimental results and analysis. Concerns about the real expressivity of the proposed approach when compared to Weisfeiler Lehman graph isomorphism test do not affect the contribution delivered by the paper, that seems, at this point, significant. 
Two knowledgeable reviewers and one fairly confident reviewer were positive (7) about this submission. The authors  response clarified a few questions and comments from the initial reviews. The paper provides exact bounds that close the gap between lower and upper bounds, and that helps us understand these networks better. With the unanimously positive feedback, I am recommending the paper to be accepted. 
This paper is about training a discrete policy that maps an image representation through a differentiable time dependent path planning module. The method is based on [1] and the reviewers are concerned about lack of novelty with respect to this work, and also with [2], however the latter only appeared a few weeks before the ICLR deadline, so I am not factoring it in my recommendation. Unfortunately, in light of [1], 3/4 reviewers do not recommend acceptance, and I agree with them.  [1] Vlastelica, Paulus, Musil, Martius, Rolinek. Differentiation of Blackbox Combinatorial Solvers (2020).  [2] Yonetani, Taniai, Barekatain, Nishimura, Kanezaki. Path Planning using Neural A* Search (2020).
This paper suggests an extension of previous implicit bias results on linear networks to a tensor formulation and arguably weakens some of the assumptions of previous works (e.g. loss going to zero is replaced with initialization assumptions). The reviewers were all positive about this work, saying it is clearly written and an original significant contribution. There were a few issues raised (e.g. the novelty of the proof techniques) and the authors responded. The reviewers did not clarify if this response satisfied these concerns, but did not change their positive scores. I will take this to indicate they still recommend acceptance.
The paper shows that replacing fully connected layers by dense layers in the networks used by actors and critiques in RL can improve the results significantly.  The improvements for several RL techniques across several benchmarks are very nice.  That being said, replacing fully connected layers by dense layers is not particularly novel and it is not clear why dense layers instead of resnet layers works better.  The reviewers appreciate the addition of experiments confirming that dense layers work better than resnet layers.  This addresses an important concern of the reviewers.  However, at this point in deep learning, it is well known that fully connected layers do not work well in general and therefore engineers are expected to use resnet, dense or highway style connections to improve performance when increasing the depth.  The fact that published baselines in OpenAI, TensorFlow and PyTorch do not use those improved networks is one thing, but this does not justify the publication of a paper.  The paper suggests that an RL specific architecture will be proposed, but at the end of the day what is being proposed is not specifically for RL, but rather the addition of new connections to the inputs similar to the well known dense architecture to augment fully connected layers in RL.  It is not clear why this works better than resnet connections.  Another alternative that was not considered are highway networks.  To strengthen the contribution of the paper, the authors are encouraged to provide an analysis of the possible approaches and to provide some insights.  
This paper studies a timely problem and consider an interesting approach, but overall there were many concerns about technical details and the validity of the framework. The positive reviewer also mentioned concerns about the experiments, which others also found to be an insular comparison with weak baselines. Following the response period, in discussion there are additional concerns arising related to the lack of details, for instance related to possible unidentifiability of the model. As one reviewer discusses,  the authors are attempting to use RNNs to impute missing infection status labels when the missingness mechanism is assumed to be (i) not at random, (ii) playing out over time (as it is unclear whether Y^t is assumed (conditionally) independent of Y^t  with t  << t), and (iii) subject to interference (whether someone is tested is the  treatment  here since it s a missingess problem and one person s propensity to be tested could causally affect another person s downstream infection status since apparently no Markov independence is assumed. There is also consensus that the writing quality can be greatly improved. Overall this work contains some ideas with potential in a thorough revision 
This paper presents a method for improving the learning of neural controlled differential equation (CDE) models. Neural CDE models provide a number of advantages over neural ODE models in terms of their ability to incorporate continuous time observations. The primary strength of this paper is that it proposes a mathematically rigorous approach to enable neural CDE models to be learned more efficiently from long time series by converting the CDE to an ODE via the log ODE method. The results are promising in that the method is able to simultaneously improve accuracy, reduce running time and reduce memory required during learning.   The paper has two main weaknesses. First, the authors claim that due to the problems they are solving (time series with up to 17,000 steps), there are no viable baselines outside of the family of methods that they are proposing. As was noted in the reviews, it would be advisable to consider even very basic baselines for these experiments in addition to current benchmark results. For example, the EigenWorms data set was used in the time series classification benchmark described in Bagnall et al. and there are benchmark results available that appear to outperform those shown in Table 2 (see mean test accuracy results reported here: http://www.timeseriesclassification.com/results/AllAccuracies.zip). The authors are also encouraged to consider even coarse RNN approximations such as partitioning the time series into tractable blocks for learning. It is not clear that the data sets actually have long range dependencies despite being long.   The second weakness is that the representation that underlies the log ODE method (the log signature transform) has been used in previous work in conjunction with discrete time RNNs. It can be viewed as a preprocessing method in a sense, as was noted by a reviewer. However, it is much more fundamentally integrated with methods for solving CDE s than its prior application to RNNs indicates.   Overall,  support for the paper did not rise to the bar required for acceptance, but we encourage the authors to revise and re submit the work to a future venue.  
This paper proposes a new method to perform knowledge distillation (KD) for transformer compression, where two types of contextual knowledge, namely, word relations and layer transforming relations, are considered for KD. Both pair wise and triple wise relations are modeled.   This paper receives two weak reject and two weak accept recommendations. On one hand, the reviewers appreciate that the authors have added more results into the paper to solve their concerns. On the other hand, several concerns still exist. (i) With regards to the compute performance trade off, the gains of the method does not seem too great. One reviewer feels that the authors tried to downplay the cost of their method too much. Though we care more about the inference time, the development time in practice should also not be underestimated. (ii) Compared with TinyBERT, the performance gain looks marginal on the GLUE benchmark (Table 1). (iii) It will make the paper more convincing if pre training experiments can be performed.   Overall, after reading the paper, the AC thinks that the novelty of the proposed method is somewhat limited. The AC is also hesitant about whether modeling word relations and layer transforming relations simultaneously are needed. The choices for ablation study are also not totally clear.   For example, in Figure 2, it is not clear why the authors choose SST 2 to plot the figure; in Table 5, it is unclear why SST 2, MRPC and QNLI are selected, but not others. When looking at Table 5, it is not totally convincing it is needed to model both WR and LTR, or it is needed to introduce both pair wise and triple wise relations. More careful ablation studies are needed. It also remains unclear what kind of word relations or layer transforming relations are learned.   In summary, this is a borderline paper, and the rebuttal unfortunately did not fully address the reviewers  main concerns. On balance, the AC regrets that the paper cannot be recommended for acceptance at this time. The authors are encouraged to consider the reviewers  comments when revising the paper for submission elsewhere.
This paper proposes a new method for conditional text generation that uses contrastive learning to mitigate the exposure bias problem in order to improve the performance. Specifically, negative examples are generated by adding small perturbations to the input sequence to minimize its conditional likelihood, while positive examples are generated by adding large perturbations while enforcing it to have a high conditional likelihood.   This paper receives 2 reject and 2 accept recommendations, which is a borderline case. The reviewers have raised many useful questions during the review process, while the authors has also done a good job during the rebuttal to address the concerns. After checking the paper and all the discussions, the AC feels that all the major concerns have been solved, such as more clarification in the paper, more results on non pretrained models, and small scale human evaluation.    On one hand, reviewers found that the proposed method is interesting and novel to a certain extent, the paper is also well written. On the other hand, even after adding all the additional results, the reviewers still feel it is not super clear that results would extend to better models, as most of the experiments are conducted on T5 small, and the final reported numbers in the paper are far from SOTA.   As shown in Table 1 & 2, the AC agrees that the final results are far from SOTA, and the authors should probably also study the incorporation of CLAPS into stronger backbones. On the other hand, the AC also thinks that T5 is already a relatively strong baseline to start with (though it is T5 small), and it may not be necessary to chase SOTA. Under a fair comparison, the AC thinks that the authors have done a good job at demonstrating its improvements over T5 MLE baselines.   As a summary, the AC thinks that the authors have done a good job during the rebuttal. On balance, the AC is happy to recommend acceptance of the paper. The authors should add more careful discussions to reflect the reviewers  comments when preparing the camera ready. 
The paper seeks to empirically study and highlight how disentanglement of latent representations relates to combinatorial generalization. In particular, the main argument is to show that models fail to perform combinatorial generalization or extrapolation while succeeding in other ways. This is a borderline paper. For empirical studies it is also less agreed upon in general where one should draw the line about sufficient coverage of experiments, i.e., the burden of proof for primarily empirically derived insights. The initial submission clearly did not meet the necessary standard as the analysis was based on a single dataset and studied only two methods (VAE and beta VAE). The revised version of the manuscript now includes additional experiments (an additional dataset and two new methods), still offering largely consistent pattern of observations, raising the paper to its current borderline status. Some questions remain about the new results (esp the decoder). 
This work presents an improved lower bound on the communciation complexity of distributed optimization in some settings. While reviewers agree that the paper is addressing a challenging and important question, all reviewers questioned the significance of the contributions of this work. In particular, two reviewers felt that the novelty of this work is limited. Unfortunately, the author response was unable to adequately address these concerns.
The paper proposes to deep neural network models with elements of the weight from algebras, and considers a wide range of algebras and large scale promising experiments. The paper raised a heated discussion.  Pros:     Using algebras, one can hope for more efficient architectures     Numerical experiments on a wide range of problems  Cons:     The theoretical grounding provided in the current version of the paper is not sufficient. The study is empirical (nothing wrong about it), but there is no clear understanding/explanation of why particular choice is better than another, and also why it works in the particular setup.     The title does not reflect the content of the paper. It is too broad, and also in some sense “provocative”. The reader expects something much more significant from it.    Experiment setup: the resulting flops/accuracy figure (main result, Figure 1) does not contain error bars.  I.e., the accuracies should be averaged over several random seeds in order to guarantee the resulting metrics. Also, this figure does not show a clear advantage over the ResNet 50 baseline.
This paper proposed a new optimization framework for pruning CNNs considering coupling between channels in the neighboring layers. Two reviewers suggested acceptance and two did rejection. The main concerns of the negative reviewers are (a) limited novelty, (b) limited performance metrics and (c) limited baselines. The authors  response did not fully clarify the reviewers  concerns during the discussion phase, and AC also agrees that they should be resolved to meet the high standard of ICLR. Hence, AC recommend rejection.  Here is additional thought from AC. The authors propose ours c and ours cs. The latter is reported to outperform the former in terms of FLOPs, but AC thinks the former may have merits in other more important performance metrics, e.g., the actual latency and/or memory consumption on a target device. More discussions and results for this would strengthen the paper.
The paper proposes a novel framework to develop useful auxiliary tasks and combine auxiliary tasks into a single coherent loss. The idea is good and the experiments are sufficient to verify the arguments. All the reviewers agree to accept the paper.
The paper investigates interference in reinforcement learning and introduces a novel measure that can be used in value based methods. Although the reviewers acknowledge that the paper has merits (the topic is relevant and the paper is well written), they feel that the contribution is not sufficiently supported by either a theoretical or empirical analysis. The authors  responses have solved some of the reviewers  concerns, but they agree that this paper is not ready for publication in its current form. I encourage the authors to update their paper following the reviewers  suggestions, in particular by improving the empirical analysis where comparisons with alternative methods (e.g., AVI/API methods that introduce regularization) need to be added.
After the rebuttal stage, all reviewers lean positive (in final scores and/or in comments during the discussion phase). The AC found no reason to disagree. The benefit of the proposed method is demonstrated in many diverse settings, and the authors argue novelty in that no prior work addresses both fg/bg imbalance and relation distillation. 
This paper considers a new setting of robustness, where multiple predictions are simultaneously made based on a single input. Different from existing robustness certificates which independently consider perturbation of each prediction, the authors propose collective robustness certificate that computes the number of predictions which are simultaneously guaranteed to remain stable under perturbation. This yields more optimistic results. Most reviewers think this is a very interesting work and the authors present an effective method to combine individual certificate. The experimental results are convincing. I recommend accept.
Although this paper proposes an intriguing method for using neuron importance based regularization to reduce catastrophic forgetting in continual learning, the method is substantially based upon Jung et al (2020), reducing its novelty. Additionally, the experimental evaluation was unconvincing that the proposed method is an improvement over current methods and precisely how the proposed method differs from Jung et al.  The authors are encouraged to revise the paper to incorporate the reviewers suggestions and many of the points the authors raised in their rebuttals, which the reviewers felt were not adequately addressed in the current version of the paper (as mentioned in private discussions among the reviewers).
Clarity: The paper is well written with illustrative figures.  Originality: The originality of the paper is relatively restricted, mainly due to the resemblance with the work [1]. However, there are important differences, that the authors nicely pointed out, and we encourage them to include these in the final version of the paper.  Significance: The paper points out a relevant issue in using normalization techniques such as batch normalization together with momentum based optimization algorithms in training deep neural networks. While the paper could be considered "another algorithms for training NNs", the papers illustrates nicely the main arguments, and is backed up with more than sufficient experimental results.  Main pros:   In the main pros, AC and reviewers admit the phenomenal job in responding to reviewers  questions and requests   The paper provides experimental results on various tasks and datasets to demonstrate the advantage of the proposed method.   After the reviews, The authors also reinforced their empirical investigation by reporting standard deviation of the results, which allows to better appreciate the performances of SGDP and AdamP. Finally, they also added the experiments with higher weight decay, showing that indeed 1e 4 was the best value.  Main cons:   One reviewer requires more explanation why the proposed update in equation (12) yields smaller norms ||w_{t+1}|| than the momentum based update in equation (8).
The authors propose the Bures metric (a distance between covariance matrices of the last feature layer of a discriminator) as an extra loss to mitigate mode collapse. The metrics bears some similarity to the covariance term in FID, and builds upon a number of GAN papers that augment GAN losses with differences in covariances between real and generated data. As the reviewers noted, the authors did an admirable job of performing an apples to apples comparison with other GAN alternatives, and use a number of metrics to demonstrate their results. Unfortunately, the most extensive comparisons usedthe DCGAN architecture, which is now 2 3 years too old for a potential reader to ascertain how well the proposed method would work on her problem. Moreover, the reviewers identified discrepancies in the baselines of those the experiments, as the numbers reported in this paper seemed to indicate poorer performance and the numbers reported in the original papers.  During the rebuttal phase, the authors demonstrated that these methods also perform well with using ResNet architectures on CIFAR 10 and STL 10, and the method is competitive with more recent models. As noted by the reviewers, however, these new comparisons are not as extensive and controlled as those that used DCGAN. Furthermore, results on more difficult datasets, such as ImageNet, are missing.  Had the extensive experiments used ResNets instead of DCGAN, or if the authors demonstrated promising results on ImageNet, I would recommend acceptance. Unfortunately, I think the audience for this paper in 2020 2021 would be relatively limited, so I have to recommend rejection.
**Overview**:  The paper tries to answer which mutual information (MI) objective is sufficient  for representation learning (repL) in reinforcement learning (RL). Three common objectives are considered: forward, state, and inverse. The paper shows that only the forward objective is sufficient for learning, i.e., sufficient for learning of optimal policy/value function. The authors also demonstrate this phenomena using empirical experiments.  **Quality, Clarity, Originality and Significance**:  All the reviewers believe this paper is novel in terms of methodology, i.e., evaluate the sufficiency of the repL in terms of down stream tasks. However, there is a lack of clarity in the experiment sections. The authors have provided more details in the rebuttal phase. The reviewers also have concerns that this paper may be too far from typical experimental settings to have a real impact on the field. An unofficial review pointed out there is a mistake in the proof of the paper. The authors later also confirmed the flaw and claimed it is fixed.  **Recommendation**: The paper is indeed interesting and novel. However, the impact to the practice community might not be significant. That being said, the paper should warrant publication eventually. However, the authors changed large amount of text about the proofs before and after rebuttal, which also introduced some additional typos, confusions, and also technique sloppiness or flaws. The reviewers are concerned about this. Overall I believe that the paper is not in a state to be published yet. 
 The paper offers a more systematic treatment of various symmetry related results in the current literature. Concretely, the invariance properties exhibited by loss functions associated with neural networks give rise to various dynamical invariants of gradient flows. The authors address these dynamical invariants in a unified manner and study them wrt different variants of gradient flows aimed at reflecting different algorithmic aspects of real training processes.   The simplicity and the generality of dynamical invariants are both the strength and the weakness of the approach. On one hand, they provide a simple way of obtaining non trivial generalities for the dynamics of learning processes. On the other hand, they abstracts away the very structure of neural networks from which they derive, and hence only allow relatively generic statements. Perhaps the approach should be positioned more as a conceptual method for studying invariant loss functions.   Overall, although the technical contributions in the paper are rather incremental, the conceptual contribution of using dynamical invariants to unify and somewhat simplify existing analyses in a clear and clean symmetry based approach is appreciated by the reviews and warrant a recommendation for borderline acceptance.  
I thank the authors for their submission and very active participation in the author response period. The reviewers and I acknowledge the importance of designing toy environments that allow the community to systematically investigate strengths and weaknesses of RL approaches. That said, the reviewers have criticized that it is unclear whether experiments on the proposed toy MDPs would transfer to more complex standard RL benchmarks (such as Atari) [R1 & R2], and that the proposed metrics and axes of variation seem not well motivated or systematic [R1,R2,R3 & R4], thus casting doubts regarding what insights the community will be able to gain from experiments on MDP Playground. In particular, I agree with the reviewers R1 s and R4 s assessment that proposing many dimensions of variation, even if they are orthogonal, without a well formulated motivation and grounding in actual tasks the community cares about is not particularly helpful for advancing our understanding of current challenges in RL. Post rebuttal, R2 and R4 stand by their strong stance against acceptance; and R1 has increased their score as a result of the improvements of the updated paper, but they still lean towards rejection. Thus, I recommend rejection.
This paper arose a number of questions and concerns among Reviewers that made it get below average scores (unfortunately, Reviewers did not provide further feedback on the rebuttal). After discussion between the Program Chairs, calibrating decisions across all submissions and, given the drawbacks mentioned below, it is decided that this paper does not meet the bar for this year s ICLR. Therefore, the final decision is to REJECT the paper. As a brief summary, I highlight below some pros and cons that arose during the review and meta review processes.  Pros:   Further developing on a simplification of previous approaches (learning diffusion sigmas).   Proposal of a new noise schedule.   Improving the log likelihood of diffusion based generative models.   Improving generation time.  Cons:   Similar FIDs as non improved approaches (in some cases).   Focus on log likelihood may not be of paramount importance for a generative task.   Dichotomy between better FID and better NLL could be further discussed.   More comparison with other approaches and further data sets could be done.   A bit ad hoc noise schedule. 
This paper describes a procedure for estimating the number of clusters in assortative sparse networks generated from a stochastic blockmodel, where the average degree scales sublogarithmically with the number of nodes. The approach proposed by the authors is based on the spectra of the Bethe Hessian matrix.  The article is well written. The reviewers raised a number of questions regarding the ad hoc procedure for estimating the parameter $\zeta$ needed to estimate $K$, and the limited experiments. While the authors provided some additional experiments in the revised version, including on a real world dataset, the overall article stills appears to be too borderline for ICLR. Adding experiments on other real world datasets would strengthen the paper.   I recommend rejection.
The paper presents a fair filter network to mitigating bias in sentence encoders by constructive learning. The approach reduces the bias in the embedding while preserves the semantic information of the original sentences.   Overall, all the reviewers agree that the paper is interesting and the experiment is convincing. Especially the proposed approach is conceptually simple and effective.   One suggestion is that the model only considers fairness metric based on the similarity between sentence embedding; however, it would be better to investigate how the "debiased embedding" helps to reduce the bias in more advanced downstream NLP applications such as coreference resolution, in which researchers demonstrate that the bias in underlying representation causing bias in the downstream model predictions. 
The paper considers a problem of weak mean estimation under a differential privacy like constraint. Specifically, estimating the signs of a (sparse) mean, and not the actual values.   The reviewers brought up a number of concerns, including the weak privacy guarantee (a type of average case privacy). Other lesser concerns include inaccuracies in comparisons with the literature and lack of interest in the algorithm/method itself.  As there was no response from the authors, there was little further discussion afterwards, and the reviewers remained in their opinion to reject the paper.
This paper presents an approach for training GCNs by learning to select subgraphs to train on to improve efficiency when transferring the model to larger graphs. In the proposed method, a meta model and a light weight GCN are trained iteratively in turns. Results are presented on medium to large graph datasets such as Reddit, Flickr, Yelp, PPI, and OGB Product.  The reviewers agree that the method and the results are interesting, and that the topic addressed in the paper is important, but that the paper generally needs more work in terms of presentation, motivation, experimental evaluation, and theoretical analysis to meet the bar for acceptance, which the authors also acknowledge during their rebuttal. 
In this paper, the authors aim to develop a new method for credit assignment, where certain types of future information is conditioned on.  The authors are well aware that naive conditioning on future information introduces bias due to Berkson s paradox (explaining away), and introduce a number of corrections (described in section 2.4 and 2.5).  The authors illustrate their approach via a number of simulation studies and constructed problems.  I think it would be nice if the authors found a way of connecting their notion of counterfactual to one used in causal inference (for instance, I think there is a connection via e.g. importance correction terms).  Reviewers were worried about the contribution being incremental given existing work (from 2019), and relative simplicity of the evaluation of the approach, compared to existing similar work. 
The paper studies the effect of importance weighting schemes on the implicit bias of gradient descent in deep learning models. It provides several theoretical results which give important insights on the effect of the importance weighting scheme on the limit of the convergence, as well as convergence rates. Results are presented for linear separators and deep learning models. A covariate shift setting is also studied. The theoretical results are supported with empirical demonstrations, and also lead to useful insights regarding which weighting schemes are expected to be more helpful. They also explain some previously observed empirical phenomena.    Pros:   New theoretical results which provide important insights on an important topic   Empirical demonstrations which support the theoretical results   Cons:   No significant issues.  
The paper formalizes domain adaptation by taking the causal (generative) direction of dependencies p(image | class, domain).  They evaluate an ELBO surrogate loss by fitting a reverse q function that is new for this setup, and add a term to the loss that induces independence between class and domain. The paper also  proves identifiability conditions. The approach is then evaluated on two semi synthetic and small datasets, showing some improvement.   Reviewers were concerned about presentation and the experimental validation. The authors addresses some of the concerns in their rebuttal, but several reviewers found that the experimental evidence was still lacking, and that the authors should evaluate their approach in more standard and realistic benchmark datasets.  As a result, the paper cannot be accepted in its current form       
The reviewers appreciate the idea of hyperparameter planning and the thorough experimentation. Some concerns remain regarding the comparison between this method and SlowFast that require to be addressed. Also, the scope of the paper that targets hyperparameter optimization networks for action recognition specifically, may be too narrow for an ICLR audience. 
The paper studies efficient strategies for selection of pre trained models for a downstream task. The main concerns consistently raised by the reviewers were limited methodological novelty, insufficient experimental analysis, unclear findings, and positioning of the paper with respect to related work that was ignored in the initial version. After the author response, R4 raised the score to borderline accept (still indicating the paper is weak without proper comparisons with other methods), whereas all other reviewers remained negative. The paper does have merits, as the methods are simple, and the problem is very practical (and somewhat understudied). However, the AC agrees with the majority that the paper is not ready for ICLR. The novelty is limited and the paper would benefit from more experiments, such as comparisons with simple baselines like early stopping as indicated by R1 and R3, and other methods such as Task2vec which address the same problem. The authors are encouraged to revise the paper according to the reviewers comments and submit it to another top conference.
This paper presents work on temporal logic representations in neural networks.  The paper builds on work on Neural Logic Machines (Dong et al.), adding temporal quantification.  The main positives to the method are this contribution of the temporal reasoning layers (e.g. iii in Fig. 2).  This layer provides an interesting extension of existing literature in the area.  The main concerns raised by the reviewers were the following:   Contribution of the paper over previous NLM techniques   Relation to graph neural networks and other message passing techniques   Use of hand crafted initial features and resultant comparisons to baselines   Empirical validation  After reading the authors  responses the reviewers reconsidered their positions and engaged in discussion.  While the reviewers appreciate the addition of temporal reasoning layers as an interesting contribution, there is still concern over the magnitude of this contribution and its effectiveness.  Additional points raised in the discussion include   Lack of evaluation on standard, challenging datasets and comparisons to state of the art   Ablation study in response (Fig. 6) does not consider absolute removal of temporal layers (main contribution).  Overall, while the paper does contribute an interesting inductive bias for learning with temporal data, the current evaluation is limited in terms of its effectiveness at the classification tasks in the experiments.  Based on the concerns raised in the initial reviews and subsequent discussion, it was determined that the paper is not ready for publication in ICLR.  
This paper is truly borderline. On one hand, the theoretical contribution seems novel and interesting, however, there appears to be somewhat of a gap between theory and practice.   There is unfortunately another problem. According to the authors, the main contribution of this publication is arguably the introduction of the nearest neighbor as the positive example in the triplet loss. However, the authors seem to be unaware of the history of the triplet loss. It was originally introduced by Schultz & Joachims 2004 as a loss over all triplets.  Weinberger et al. 2005 changed it and use the nearest neighbor as "target neighbor", which is called "easy positives" here, as the objective of LMNN. In 2009 Chechik et al. subsequently relaxed this positive neighbor formulation to any similarly labeled sample (going back to the Schultz & Joachims formulation) but sampling triplets. The re introduction of the nearest neighbor as "easy positive" was then covered by Xuan et al. 2020.    Unfortunately all of this diminishes the novelty significantly and it is clear that the paper in its current form does not have a strong enough contribution. I do encourage the authors to take a close look at the original LMNN publication and Xuan et al and write an improved re submission for the next conference that maybe focuses more on the theoretical contribution.  Good luck,  AC
The authors investigate theoretical properties of meta learning. In particular, the train validation split to tackle the linear centroid meta learning problem is investigated in asymptotically regimes (a non asymptotic analysis of the train train estimator has been added later on). It is shown that the train validation method has statistical consistency, while the train train method has a statistical bias to the centroid. Yet, in the noise free setting both methods have statistical consistency. Furthermore, the train train method is superior to the train validation method in the sense of the asymptotic MSE. Based on the asymptotic analysis the optimal ratio of the data splitting for the train validation method is derived. The theoretical findings are corroborated by some numerical experiments. The main theoretical result suggests that the train train method + optimally tuned regularization is a strong alternative to the train validation split, hence the authors  recommendation that for meta learning train train method should be preferred.  The reviewers and the area chair appreciated the theoretical findings and the subsequent effort to improve the paper the authors put in place during the discussion period. Unfortunately, the tight competition among papers in this year s edition of the conference makes this specific submission not compelling enough for publication.  I would like to encourage the authors to sumbit to another ML venue in the near future, while considering improvements in their experimental validation (as also suggested by some reviewers). 
The paper proposes a margin based adversarial training procedure. The paper is lacking in terms of proper dicussion of related literature e.g. similarity and differences to MMA, the "theoretical" discussion on page 5 is incomplete as there is no way how one can estimate the perturbed samples to do the analysis (the authors seem to implicitly already assume that the adversarial samples lie on the decision boundary) and the underlying assumptions are not clearly stated, the reported robust accuracies  (see https://github.com/fra31/auto attack for a leaderboard of adversarial defenses) on MNIST and CIFAR10 are worse than that of MMA which are in turn worse than SOTA. Thus this paper is below the bar for ICLR.
The contributions of this paper lie in two areas: a new benchmarking dataset and a new way to generate benchmarking datasets. Overall, the reviewers are split in their assessment based on which particular area they are focusing on. Reviewers who focus more on evaluating this work as a new benchmarking dataset, correctly point out that the variation within the search space has been shown to be limited and that the evaluation focuses on an overly studied and toy (by today’s standards) dataset such a CIFAR 10. In terms of choice of dataset, this work is indeed a step backwards from nasbench201, which includes more datasets, although it is a step forward in terms of size of the search space. As one reviewer correctly points out “This paper doesn’t present a benchmark. It provides a model that represents computationally efficient means of getting network accuracies from the DARTS search space”. The authors argue that the combination of the DARTS search space and its evaluation on CIFAR 10 is the de facto evaluation standard in the NAS community, which is also true. Ultimately, benchmark datasets somewhat direct the attention of the community and this attention would be better directed elsewhere, not on DARTS+CIFAR 10, as pointed out by some of the reviewers.  On the other hand, this work is as much about a new benchmark as it is about a *protocol* to generate new benchmarks. Specifically, a big part of the appeal and novelty here lie in the idea of training a predictive model from a small subset of architecture evaluations. From this perspective, the authors showed evidence that their approach is sound, economical (in terms of computational cost) and robust to sources of bias in the selection of the architectures to evaluate. The limitation here is that this was only shown in search spaces that where either small or lacking diversity and thus it’s unclear how general its findings are.  Overall, this is very much a paper that could have gone either way in terms of acceptance. It’s a step in the right direction in terms of methodology that can be used to generate reproducible benchmarks in a computationally efficient way. It’s a step backwards in terms renewing focus on measures of performance that (arguably) we have all overfit to.  
Reviewers generally appreciate the contributions of the paper, namely the horocycle neuron, Poisson neuron, and the universal approximation properties. However, there are concerns, especially by R4 and R5, that the presentation is confusing, lacks clarity, and should be substantially improved.  Note: Theorem 1.7 in (Helgason, 1970) is proved explicitly for the case n 2, not for general n as claimed in (9).  Thus the Laplacian eigenspace motivation needs to be re written/re examined.
This paper proposed Q value weighted regression approach for improving the sample efficiency of DRL. It is related to recent papers on advantage weighted regression methods for RL. The approach is interesting, intuitive, and bears merits. Developing a simple yet sample efficient algorithm using weighted regression would be a critical contribution to the field. The work has the potential to make an impact, if it has all the necessary ingredients of a strong paper.  However, reviewers raised a few issues that have to be addressed before the paper can be accepted. As some reviewers pointed out, there seem to be unaddressed major issues from previous submissions. Novelty appears limited, especially because the proposed approach is very similar to recent works (e.g., AWR). The experiment section lacks comparison to recent similar algorithms, and the available comparisons appear to be not strong enough to justify merits of the proposed algorithm. Theorem 1 requires an unrealistic state determines action assumption for the replay buffer. Although the authors made an effort to justify this assumption, it remains very problematic and rules out most randomized/exploration algorithms.
The paper establishes an interesting relationship between poisoning and online learning. Instead of framing the poisoning problem as a bi level optimization problem as what is done conventionally, the paper proposes reducing the poisoning attack design to an online learning problem in which the adversary decides on a poisoning data point in a sequential manner.  The data point that leads to the largest difference of loss between the current model and the target model is selected as the next point to inject into the training data.  Pros + If the loss function is convex, the proposed algorithm is guaranteed to converge to the target, and the paper provides a lower bound on the number of samples needed to reach the target model from the current model.   + Experiments on SVMs show the advantageous performance of the proposed attack algorithm.    Cons   The reason why the proposed online learning method could outperform the KKT based attack is not well justified theoretically. Experimentally, the paper would be stronger if evaluations are done on more diverse settings and data.      Reviewers have expressed concerns on how practical the proposed algorithm is, since the guarantees are established on convex loss functions. The paper would be stronger if  the authors can further compare the attack with deep learning poisoning algorithms on larger datasets.   I truly believe that the paper’s exploration of convex models is an important step towards understanding the poisoning attacks and is a step towards understanding attacks for non convex models. However, I do think that the paper could make a more profound contribution and impact with stronger experimental evaluations.   Therefore, I would classify this paper as borderline, toward weak reject compared with other papers.  
This paper proposes a method for collaborative multi agent learning and ad hoc teamwork. The paper includes extensive empirical results across multiple environments (including one of known outstanding high difficulty) and repeatedly performs favourably in comparison to a suitable set of state of the art methods. The proposed method is motivated by theoretical analysis, which was considered interesting but its connection to the method in the initial paper was weak.   Overall, there are remaining concerns which have not been fully addressed in the discussion phase. The authors  responses and discussion with the reviewers should be utilised to improve the material s presentation and to clarify the theory empirical connection in future revisions of the paper.
While the authors provided extensive responses to the reviewers and most of the reviewers did a good job of accounting for the author responses the final ratings for this paper was unanimously 5s   all marginally below acceptance. The paper s positioning, writing were identified as key issues that remained to be addressed. The AC recommends rejection.
This work proposes a novel, interesting and simple technique to improve the model robustness to distribution shift. The proposed method is called Adversarial Batch Normalization (AdvBN) which is based on adversarial perturbation of BN statistics. Authors provide extensive experiments to show the effectiveness of AdvBN. All reviewers agree that the proposed method is interesting and novel. The main concern of reviewers is about the some of the details of the empirical evaluation of the proposed methods which makes its effectiveness less clear. In particular, the following concerns are shared among the reviewers:  1  Authors give different treatments to Stylized ImageNet compare to other tasks by using auxiliary BN at inference time instead of standard BN and further results provided by authors show that the improvement over previous methods disappear if they use standard BN for inference on Stylized ImageNet. I think authors could mitigate this issue by further investigation or providing a better explanation on why they have a different treatment for Stylized ImageNet (other than the fact that auxiliary BN has a better performance on that task). The other potential remedy is to come up with an automatic way to decide which one to use at the inference time using a batch of "unlabeled" validation data.  2  The improvement of AdvBN over AugMix and AdvProp (which was added during the rebuttal) is not clear. In particular, both methods improve over AdvBN on ImageNet C. If standard BN is used for AdvBN on Stylized ImageNet, then both AugMix and AdvProp improve over AdvBN. That only leaves ImageNet Instagram as an ImageNet variant where AdvBN shows a clear improvement over AdvBN and AugMix. A potential solution is to try combining AugMix and AdvBN (not sure if AdvProp could be combined effectively) to see if there is a way to get maximum benefit out of these methods.  3  The empirical section could be improved by doing experiments in a systematic way. That is for any choices made in the experiment design, there should be a reason that is explained clearly. For example: 1) applying the same type of data augmentation on all methods (or reporting all methods with and without data augmentation). 2) compare to all baselines on ResNet 50 and then pick the top 2 baselines (say AugMix and AdvProp) and then compare them on DenseNet and EfficientNet. 3) comparing with the same baselines as (2) on the segmentation task.  Finally, I want to thank authors for engaging with reviewers, running many experiments during the rebuttal period and updating the paper accordingly. I also want to reassure authors that my final evaluation of the paper is based on: 1) reading all reviews and responses 2) weighing the reviews based on their substance, quality and engagement of reviewers 3) looking at the initial and final revision of the paper. In particular, even though the average score of this paper is low, in my opinion it is a borderline paper. After taking all of the above into account, my decision is to recommend rejection. Even though the proposed method is very interesting, there are three clear valid concerns all of which can be addressed as I suggested above. Without addressing those concerns, the empirical advantage of the proposed method is not demonstrated properly. I think after addressing those concerns this paper will be in a much better shape, more useful for ML community and hence receives the attention it deserves. I sympathize with authors that their efforts during the rebuttal period did not result in improving reviewers  scores but I want to emphasize that I did take all those updates into account when making my recommendation for this paper. 
This paper sits at the borderline: the reviewers agree it is a well written and interesting paper, but have concerns about efficiency as well as a comparison with the neural process (the authors did include a revision with this comparison, though the numbers they report are worse than in the original neural processes paper on the same experiment). Ultimately, this paper probably requires another round of reviews before it is ready for publication.
The paper proposes a variational family of distributions for posterior estimation in sequential latent variable models. The paper does so by extending variational recurrent neural networks so as to use a variational mixture posterior and capture more realistic multi modalities in the data.   During the review process, it was suggested to improve the clarity of the paper, provide results on an additional dataset and a visualization of the latent distributions. I commend the authors for addressing these issues satisfactorily.  Overall, the paper is well motivated and well written. However, when considering the novelty of the paper, although none of the reviewers raised this issue, I believe the paper heavily relies on previously proposed ideas and therefore, its contribution can be seen as incremental. Additionally, something important to highlight is in section 2, with regards to deep state space models (SSMs). The authors make a rather strong claim with regards to assumptions on the variational distribution. However, one can find out of the box implementations where this is not the case, see e.g. https://pyro.ai/examples/dmm.html that implements deep Markov models with posteriors based on inverse autoregressive flows. A comparison with such approaches may be also required.  
The paper studies the features extracted by the pre trained language model and how fine tuning makes use of these features. The paper is well motivated by two lines of research in the NLP area   1) probing approaches for understanding the features extracted in the pre training model, 2) model behavior analysis that shows models take shortcuts for making predictions. The paper provides a comprehensive study to bridge the gaps between these two lines of discussion.   All the reviewers agree the paper has strong merits and concerns have been addressed.   
The paper introduces an approach for learning the dynamics of PDEs. It makes use of bi directional LSTMs trained to regress future values from past observations, up to a given horizon. Experiments are performed on data generated from numerical solvers on two examples, inviscid Burgers and a Navier Stokes system. While the topic is fine, the solution is nothing more than regression with sequence models and only shows that RNNs could learn to predict the data generated by these PDEs. The reviewers also highlight that the comparison with the baselines is not appropriate.
The paper proposed Twin L2O (learning to optimize) for extending L2O from minimization to minimax problems. The authors honestly discussed the limitation of Twin L2O and proposed two improvements upon it with better generalization/transferability. While some reviewer had some concerns on the motivation of applying L2O to solve minimax problems and the motivation of the loss function design (why objective based one is chosen but not gradient based one), the authors have done a particularly good job in the rebuttal. Even though this is more a proof of concept paper, it indeed has novel and solid contributions, and should be accept for publication.
This paper proposes a method of decentralized mechanism design to reduce the price of anarchy. Based on the detailed responses of the authors, all reviewers were satisfied by the technical contribution after the rebuttal period.   There was, however, a heavily engaged and lengthy discussion between most reviewers regarding the applicability of the method and how it links to the motivation given in the paper. The paper could be improved by (1) highlighting an exemplar real world use case in the paper motivation (there are a couple mentioned briefly in the introduction but one of these could be emphasized more); and (2) connecting the choices made in the design of the approach to the opening motivation sections and exemplar use case.  The level of engagement from most reviewers demonstrates a good level of interest from a representative sample of the ICLR community but demonstrated that their remained work outstanding to clarify the core message and significance of the contribution.
The paper considers ensambling of smooth classifiers to improve certified robustness. Theoretical results are provided showing that taking ensambles of a large number of models is useful, while experiments show that combining only a small number of models improves performance. On the negative side, the experiments are somewhat inconclusive, as the base models are not state of the art, and the combined results do not achieve state of the art performance. In  this respect, further studies would be necessary to explore the effectiveness of the proposed technique.  In summary, while the topic of the paper is interesting and timely, the proposed ensambling technique is not especially exciting (as it is what one would naturally expect). On the other hand, the problem is reasonably well investigated (e.g., details are worked out well, both theoretical and experimental results are presented), although further experiments are needed (as recommended by the reviewers) to properly assess the potential and limitations of the approach. Accordingly, all reviewers agreed in the discussion that this is a borderline paper. Therefore, unfortunately, it cannot be accepted this time due to the heavy competition at the conference. The authors are encouraged to resubmit a revised version to the next venue, taking into consideration the reviewers  recommendations.
This work explores an auto regressive density estimator based on transformer networks. The model is trained via MLE with an additional MMD regularization term.  Various experiments are performed on small benchmarks and show good results on density estimation. It is great to see that such a simple model is indeed very effective for density estimation on various small benchmarks (such as 2D density estimation and MNIST).   The ablation experiments are informative and justify some of the model choices (such as the use of RNN to encode "positions"). Experiments are nicely chosen and paint a broad picture of the behaviour of the studied model.  The paper and author responses, however, excessively exaggerate the extent to which these results are relevant to the bigger picture in comparison to existing literature (e.g. flows and existing auto regressive models).  As it has been extensively discussed with the reviewers, the proposed model is a straightforward application of a transformer network to auto regressive modelling, this is specially so in light of existing work on auto regressive models with transformers [e.g 1, 6, 8], self attention [e.g 2]. BERT [7] itself can be used for auto regressive modelling almost out of the box (with the appropriate choice of masks during training).  At various points in the paper and author responses, it refers to flow models as "complicated/expensive" counterparts. These arguments are unfounded: auto regressive models are particular cases of flows [3], and there are no obstructions to using transformer networks inside flows (in fact they have been already used, to achieve permutation equivariance and long range correlations [e.g. 4]).  The paper leaves comparisons to spline flows out, arguing they are "hard to implement". This is quite conspicuous, as not only spline flows are straightforward to implement, they produce results entirely on par with the presented model (as an example, look at Fig 2 from [9] in comparison to Fig 1 from this paper). Finally, the paper also misses an important discussion about the computational complexity of the proposed method. Auto regressive models are considerably slower to sample from in relation to other types of directed models. Even more so with transformer networks as conditioners. For instance, flows [3, 5] allow for substantially faster sampling of large dimensional data relative to auto regressive models (by exploiting parallel sampling).   Extra comments:  The paper says "... Self attention also enables permutation equivariance and naturally enables TraDE to be agnostic to the ordering of the features ... " This is true only for a *single* conditional $p(x_i | \text{Transformer}(x_{0 \ldots (i 1)}))$, not for the *joint* density. It is actually not straightforward to build auto regressive models that are permutation invariant or that incorporate other forms of domain knowledge in general. As an example, see [4] for how transformers and spline flows can be used to produce exact permutation invariant densities.   [1] Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D. and Sutskever, I., 2020, November. Generative pretraining from pixels. In International Conference on Machine Learning (pp. 1691 1703). PMLR.  [2] Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, Ł., Shazeer, N., Ku, A. and Tran, D., 2018. Image transformer. arXiv preprint arXiv:1802.05751.  [3] Papamakarios, G., Nalisnick, E., Rezende, D.J., Mohamed, S. and Lakshminarayanan, B., 2019. Normalizing flows for probabilistic modeling and inference. arXiv preprint arXiv:1912.02762.  [4] Wirnsberger, P., Ballard, A.J., Papamakarios, G., Abercrombie, S., Racanière, S., Pritzel, A., Rezende, D.J. and Blundell, C., 2020. Targeted free energy estimation via learned mappings. arXiv preprint arXiv:2002.04913.  [5] Huang, C.W., Krueger, D., Lacoste, A. and Courville, A., 2018. Neural autoregressive flows. arXiv preprint arXiv:1804.00779.  [6] Sun, C., Myers, A., Vondrick, C., Murphy, K. and Schmid, C., 2019. Videobert: A joint model for video and language representation learning. In Proceedings of the IEEE International Conference on Computer Vision (pp. 7464 7473).  [7] BERT: Pre training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming Wei Chang, Kenton Lee, Kristina Toutanova; ACL 2019.  [8] Child, R., Gray, S., Radford, A. and Sutskever, I., 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509.  [9] Durkan, C., Bekasov, A., Murray, I. and Papamakarios, G., 2019. Neural spline flows. In Advances in Neural Information Processing Systems (pp. 7511 7522).
This paper falls in the borderline area and there are still some concerns (for instance by AnonReviewer5 and AnonReviewer2) that deserve further treatment. Given that most ideas can only be validated in experiments (as the results are not theoretical), some points that remain are the comparison with other approaches (there are reasonable comparisons, but there are very famous contenders missing such as xgboost, ok that LightGBM is, but why not the other?), details about the tuning, the significance of results (practical and statistical is not complete/detailed enough), and the reasoning about situations with many rules and interpretability seems to be worth exploring/discussing further.
Three of the reviewers are significantly concerned about this submission while R3 was positive during review. During discussion, R3 also agreed that there are concerns not only on experimental designs and results but also the proposed model. Thus a reject is recommended.
All of the reviewers are impressed by this paper s empirical results and they agree that this is a good paper and should be accepted. Some questions about the theoretical justification of the proposed method and its potential practical impact remain open, but the empirical results are impressive and can result in more research in understanding Cyclic Precision Training (CPT) and improving quantized training of neural nets. I suggest acceptance as a spotlight presentation.
The paper initially received a mixed rating, with two reviewers rate the paper below the bar and two above the bar. The raised concerns include the need for an autoregressive model for upsampling and the effect of batch sizes. These concerns were well addressed in the rebuttal. Both of the reviewers that originally rated the paper below the bar raise the scores. After consulting the paper, the reviews, and the rebuttal, the AC agrees that the paper has its merits and is happy to accept the paper.   
This paper proposes a new hard attention model for the image classification as a way to achieve explainability. Two of the reviewers do not find the output of the system interpretable, which is a fatal weakness for a paper on XAI. R1: The visualization in Fig.5 shows only that the region selected in each timestep indeed has the maximum EIG. But how to interpret the explainability from the glimpse sequence is still confusing. I can hardly perceive the sequence using my knowledge. R2: However the output of the system is not so appealing either in performance or explainability. R3: Post discussion note: For me, it s a bit hard to say the proposed methodology is novel. Authors needs to explain why the proposed model is different from pre existing methodologies regarding attention mechanism.   R4: Due to the above, the recommendation is Reject   but the authors are strongly encouraged to do experiments on more challenging data and compare to a newer baseline. 
Summary:  This paper introduces a different, interesting definition of safety in RL. The paper does a nice job of showing success with empirical results and providing bounds. I think it provides a nice contribution to the field.  Discussion: The reviewers agree this paper should be accepted. The initial points brought up against the paper have been successfully addressed or mitigated. 
The paper focuses on NeuralODE and shows that for the implementation popular among ML community, one of the equations is not an ODE and can be replaced by an integral. This is implemented using "seminorm" (just assigning zero weight to the last equation).  Pros:    Well written   Useful to replace the "standard" implementation    Consistent benchmarking  Cons:    Contribution is too limited   Used in several "prior" codes without explicit ICLR submission.    (My personal) The title is not good: more on the "hype side" of the story, rather than progressing the field. I don t think we need to put every single minor fact into a ICLR submission.  For example, one can just compute the integral as an alternative by any suitable quadrature rule. That would add 10 15 function evaluations at most, since most of the functions in NeuralODEs are quite smooth.
This paper proposes to address the class imbalance problem by defining an over sampling strategy based on oversampling. It brings potentially interesting ideas. The reviewers agree on the fact that the experiments are limited, some methodological aspects require some clarifications and the writing needs to be improved.  The authors did not provide any rebuttal. Hence I recommend rejection. 
This paper analyses the signal propagation through residual architectures; then suggests a scaling method which, together with weight standardization, allows to train such networks to high accuracy with batch norm; it demonstrates that the method performs better than previous methods (Fixup, SkipInit), and can be used on more advanced architectures.   The reviewers initially had several concerns, but after the author s revision, these concerns were addressed and most reviewers recommended acceptance. One reviewer did not respond, but I think these concerns were addressed. I think it will help to further convince the readers on the usefulness of the method readers if the authors would check the sensitivity to the learning rate with the current method and compare with other methods (SkipInit, Fixup, BN). The reason I m suggesting this is that I think one of the main reasons BN is still in popular use is that it commonly tends to make training more robust to changes in hyper parameters, such as the learning rate (while other methods, like SkipInit and Fixup, require more hyper parameter tuning).  Overall the analysis and the suggested method seem useful, especially at a small batch size and the writing is mostly clear, so I recommend acceptance.   
The paper proposes several simple alternatives to generate adversarial examples for deep reinforcement learning algorithms based on image distortions such as lighting change, blurring and rotation, and show the performance of DRL agents degenerate as a result. Most reviewers appreciate the simplicity and computational efficiency of the proposed attacks. The results revealed by the work is however rather unsurprising, given similar attacks have been evaluated for DNNs. The authors did not offer much more insight on the presented results beyond that, for example, robustness of different DRL algorithms with regards to these attacks as mentioned by reviewer 2, sensitivities of the parameters for each attack proposed, effectiveness of different attacks on different environment and possible combination of attacks. 
 The paper proposes to generate human like question for a image by using additional information (hints) such as the textual answer and visual regions of interest (ROIs).  The visual regions are used to guide the question generation so that the model can generate relevant and informative question.  The question generation problem is formulated as a graph to sequence problem, starting from an object graph, and using GCNs with attention to align text and visual regions and to generate an appropriate question.  Review Summary: The submission initially mixed reviews (scores from 3 to 6).  While reviewers find the problem interesting and work to be mostly solid, reviewers felt that the novelty of the work was limited (R1, R3) and that some of the presentation was unclear (R1, R3) with some missing details (R2).  R2 was not sure if the VQG was a useful task, and R4 felt that the initial submission was missing a key experiment on whether the generated questions can be useful as data augmentation for VQA. The reviewers were impressed by the extra experiments performed by the authors in the rebuttal and the revised draft, and indicated that most of their concerns were addressed, In particular,  the generated questions were shown to be useful as data augmentation.  Many reviewers increased their scores, ending with 3 scores of 6 (R1,R2,R4) and 1 score of 5 (R3).    Pros:   The use of generated questions as data augmentation for VQA is an interesting direction   Strong empirical results with thorough experiments (and user study)  Cons:   The technical novelty of the work is still rather limited (R1)   The paper was difficult to follow (R1,R3) with a lot of moving parts, making it potentially difficult reproduce (R1)   The authors indicated that they will open source the code.   The grammar and wording of the writing is poor even after revision and should be improved  Example of poor writing (a full proofreading pass is recommended):   Section 2.1: "Mora et al. (2016) firstly makes an attempt to adapt"  > "Mora et al. (2016) adapted", "abstract and general results"  > "imprecise and generic questions"   Section 3.2: "The most important point of our first issue located in how to effectively find..." (it s unclear what this means)   Section 4.1: "standfordcoreNLP"  > "Stanford CoreNLP"   Section 4.5: "shown in 3"  > "shown in Figure 3"  Recommendation: The AC agrees that the work is addresses an interesting area of generating questions as data augmentation for VQA.  Despite the improved reviewer scores, the AC agrees with the initial assessment that the work has limited technical novelty.  The AC also found the writing of the paper to be poor and difficult to follow at places even after revision.  Due the limited novelty, the issues with exposition, and the many changes to paper, the AC believe that the work would benefit from another round of review and should not be accepted at ICLR in its current form.  Given the positive response, the authors are encourage to improve their work and writing and resubmit to an appropriate venue (the AC believes the work would be more appreciated in a vision or language venue).
This paper provides a global convergence guarantee for feedforward three layer networks trained with SGD in the MF regime. By introducing the novel concept of neuronal embedding of a random initialization procedure, SGD trajectories of large width networks  are shown to be well approximated by the MF limit, a continuous time infinite width limit (Theorem 3). Furthermore, under some additional assumptions the MF limit is shown to converge to the global optimum when the loss is convex (Theorem 8, case 1) and for a generic loss when $y y(x)$ is a deterministic function of input $x$ (Theorem 8, case 2). The global convergence guarantee presented in this paper is based on less restrictive assumptions compared with existing studies. All the reviewers rated this paper quite positively, with less confidence however, seemingly because of mathematical thickness of the proofs. Although the reviewers did not manage to check every detail of the proofs, they agreed that the reasoning seems mathematically sound as far as they can tell. The authors response adequately addressed minor concerns raised by the reviewers. I am thus glad to recommend acceptance of this paper.  Pros:   Introduces the idea of a neuronal embedding, which allows establishing relation between SGD on large width three layer networks and its MF limit in a quantitative way with a less restrictive setting.   Provides a global convergence guarantee under the iid initialization, in the sense that if the MF limit converges it attains the global optimum.   Shows that the global convergence guarantee does not require convexity of the loss when a deterministic function is to be learned.  In particular, the uniform approximation property, rather than the convexity of the loss, plays a crucial role in proving the  global convergence guarantee (it allows translation of the vanishing gradient in expectation at convergence into the almost sure vanishing gradient), which is a quite original contribution of this paper.
The paper introduces new tighter non asymptotic confidence intervals for off policy evaluation, and all reviewers generally liked the results. I recommend acceptance of this paper. Some concerns of Reviewer2 and Reviewer3 are not fully addressed in your rebuttal. Please make sure to address all remaining issues.
This paper presents a method to formulate learning of causally disentangled representation as a part of the encode decoder framework. Although the reviewers agree that the paper presents some interesting ideas, they feel the paper is not ready for publication yet.  In particular, I encourage the authors to take the feedback of reviewer R2 into account, which is quite detailed and provides substantive ways of improving the work. After all, I recommend rejection.  
The paper provides an interesting set of theoretical ideas to improve the estimation of normalizing flows on datasets that fail to be fully dimensional. Although the method is appealing, I believe the paper falls a bit short of acceptance at the conference. Too many practical issues are left out, as discussed by reviewers, and the method seems promising but not fully connected to the rest of the literature on estimating low dimensional distributions living in high dimensional spaces. We encourage the authors to use the feedback contained in this round of reviews to improve their work.
This paper introduces a novel convolution like operator called "optimal separable convolution" which is based on minimizing number of operations given a fixed receptive field.  Authors provide further empirical results to show the effectiveness of their proposed operator.  Overall, this is a very interesting work. There is a consensus among reviewers that this work is well motivated, novel and principled. However, reviewers have pointed to several issues that makes this a borderline paper and consequently none of the reviewers were willing to argue for the acceptance. After reading the paper, reviewers  comments and authors  response, I would summarize the main areas of improvements as follows:  1  The "optimal separable convolution" is derived theoretically using "volumetric receptive field condition". However, this condition is not discussed and motivated enough in the paper. For example, different parametrization with the same volumetric receptive field could impose very different expressive power or implicit bias. Why is this not important? Adding discussions/experiments to motivate this condition would improve the paper.  2  The derivations in Sections 2.3 and 2.4 are not well presented and are hard to follow. I suggest authors to use the convention of having a formal Theorem statement followed by the proof. This is important since one of the main contributions of the paper is a principled derivation.  3  All reviewers were concerned with the wall clock time. Authors responded that theoretical #FLOPs is more important because wall clock time is hardware dependent. However, authors reported the wall clock time using CPUs. I understand that wall clock time is hardware dependent but that only means algorithms that can have better wall clock time on the current hardware are more likely to be useful because there is no guarantee that the hardware would be adjusted based on one algorithm especially if the promised improvement is not large enough. Therefore, I think reporting Wall clock time on GPUs is important which was not done here.  4  Even though authors mention several operators in Table 1, they only compare against depth separable conv in the experiments. Even based on FLOPs, the current empirical results are not very promising. For example:   a) The gap between o ResNet (the proposed method) and d ResNet is not significant in Fig 3. In particular when #FLOPs is low, d ResNet and o ResNet have similar performance.   b) In Tables 2 and 4, o ResNet shows small improvements but uses more FLOPs. Even if authors can t exactly match #FLOPs, they should make sure that the proposed method uses less FLOPs than others not the other way around.  c) In Table 3, authors only compare to ResNet and d ResNet is removed.  Considering the above issues, I think the paper is marginally below acceptance threshold. Given the novelty of the work, I want to encourage the authors to improve the paper by taking Reviewers  comments into account and resubmit their work.  
There is a broad consensus that this paper explores an interesting and novel problem space. Nonetheless, in their initial assessment, the reviewers pointed to a few limitations of the paper including lack of strong baselines, lack of an ablation study, and weaker results according to the HIT@10 metric.   The authors provided an improved version of their paper as a response. The new paper added two baselines, is better written, and justifies some of the HIT@10 results (basically, the metric is biased for this task).  After discussion, the reviewers find that the contribution of the current manuscript falls short of the acceptance threshold.  In particular, the reviewers find that: 1) this contribution is for a specific domain of recommender systems, an area of interest, but perhaps only relevant to a subset of the ICLR community; 2) while more recent baselines helped, there has been lots of more recent work on collaborative filtering models for recommender systems over the last few years (the Wide&Deep baseline is from 2016); 3) since some of the usual recommender systems  metric does not seem appropriate here, why not suggest new ones (or propose a slightly different evaluation protocol); 4) the proposed model is useful, but somewhat incremental given prior work. All in all, while any of these limitations on their own might not have been sufficient to warrant rejection, I find that their combination does.   Given the interest in this new task, I do strongly encourage the authors to pursue their work. I also find that the qualitative study propsoed by Reviewer 4 could add another interesting angle to this paper (I also imagine that it might not be that easy to carry out). 
This paper looks at a natural application of robust learning for vehicle routing. The paper introduces some new ideas for this RL problem; although the problem has been considered before.  The paper gives a nice algorithm with extensive experimental contributions.    The paper has some shortcomings. The reviewers found there to be a lack of clarity in the mathematical definitions.  Moreover, there were modeling choices that the reviewers felt needed more thorough explanation.   For these reasons, this paper falls below the bar.  The authors are encouraged to revise the manuscript taking these concerns into consideration.  
The paper addresses counterfactual fairness learning using generative approach. While acknowledging the importance and potential usefulness of generative approach, the reviewers and AC raised several important concerns that place this paper below the acceptance bar:   (1) low degree of novelty – see multiple concerns and suggestions by R2, R3, R4;  (2) the model is not justified by a causal mechanism (R4), and it remains unclear under which condition the proposed GAN approach is ensured to obtain unbiased counterfactual samples (R2);    (3) lack of technical rigor when presenting the model – see R4’s request to relate to the DAG models, see R1 multiple questions regarding the reinforced data sampler;   (4) lack of empirical evidence (R3) and evaluation details, e.g. on cross validation and more recent methods (see R4’s recommendations);    (5) related work is not discussed in sufficient details – see R4’s elaborate comment.  Among these, (4,5) did not have a substantial impact on the decision but would be helpful to address in a subsequent revision. However, (1), (2) and (3) make it very difficult to assess the benefits of the proposed approach and were viewed by AC as critical issues. In the rebuttal it is stated that ‘Our counterfactual examples are generated using a powerful generator rather than a fixed synthesizer in Kusner et al. (2017)’ – more rigorous comparison has to be provided to support such statement. AC would urge the authors to contrast and compare their synthetic counterfactual examples with Kusner et al on the datasets where causal graph has been built. [Razieh Nabi and Ilya Shpitser. Fair inference on outcomes., AAAI2018], Figure 2 postulates causal graphs for the Compas and Adult Income datasets evaluated in this paper.  A general consensus among reviewers and AC suggests, in its current state the manuscript is not ready for a publication. We hope the detailed reviews and encouragements are useful for revising the paper. 
The authors propose the 2 Wasserstein barycenter problem between measures. The authors propose a novel formulation that leverages a condition (congruence) that the optimal transport (Monge) maps, here parameterized as potentials, must obey at optimality. The introduce various regularizers to encourage that property. The idea is demonstrated on convincing synthetic experiments and on a simple color transfer problem. Although experiments are a bit limited, I do believe, and follow here the opinion of all reviewers, that there is novelty in this approach, and that this paper is a worthy addition to the recent line of work trying to leverage ICNNs/Brenier s theorem to solve OT problems.
This paper proposes a model for disentangling content and dynamics, but unlike the majority of previous work, the dynamics are modeled using ODEs rather than their discrete approximations   RNNs. The reviewers agree that the paper is well written, and the results look good, especially for longer trajectories. Hence, I am happy to recommend this paper for acceptance.
This submission got 3 rejection and 1 marginally below the threshold. In the original reviews, most of the concerns lie in the limited novelty, the inferior performance to some existing similar works and the limited scalability of the proposed method. Though authors provide some additional experiments, the reviewers still feel the experiments are not convincing and keep their ratings. AC agrees with the reviewers comments on this paper. Though achieving SOTA performance is not necessary for every submission, NAS alike method is purely pursuing better performance (either higher accuracy or better efficiency). Thus, the performance is also important for evaluating a NAS paper. From the reviewers, the proposed method does not show better performance than some existing works, like BiFPN. This makes the value of the paper is not clear, in particular considering the method novelty is limited. The authors could consider to improve the submission in the experiments to better justify the proposed method, either achieving better performance or higher efficiency than existing works. At its current status, AC cannot make accept recommendation. 
The paper proposes a robust formulation of Deep Subspace Clustering (DSC) based on the correntropy induced metric (CIM) of the error. All three reviewers recommend rejection. Their major critiques are limited novelty, insufficient experiments and similar performance to non deep methods. The rebuttal highlights that the novelty is not DSC or CIM, but rather that the formulation does not require knowing the labels. I agree with the reviewers that the paper s novelty is very limited and didn t find the author s response compelling enough to overturn the reviewer s opinions. 
This work appears to be a promising start to a research direction. However, as the reviewers noted, the work does not compare to alternative approaches and the presentation of the work overall is incomplete.
This paper introduces a form of cubic smoothing for use with ODE RNNs, to remove the jump when new observations occur.  I think this paper s motivation is based on a misunderstanding of what the hidden state of an RNN represents.  Specifically, an RNN hidden state is a belief state, not the estimated state of the system.  I think R2 is right that it s correct for a filter to jump when seeing new data.   It s not a matter of whether the phenomenon being modeled is slow changing or not.  The filtering output is a belief state, which can change instantaneously even if the true state does not.  The important distinction to make is filtering (conditioning only on previous in time data) vs smoothing (conditioning on all data).  The smoothing posterior should generally be smooth if the true state changes slowly.  As R4 notes, all of the tasks are based on interpolation, which is not what the ODE RNN is trying to do, and the proposed method would make the same predictions as a standard ODE RNN.  Finally, as R4 notes, "The authors do not provide any experimentation on real world irregularly sampled time series".
This paper studies the implicit acceleration of gradient flow in over parameterized two layer linear models. The authors show that the amount of acceleration depends on the spectrum of the data without assuming small, balanced, or spectral initialization for the weights, and establish interesting connections between matrix factorization and Riccati differential equations.  While this paper provides some interesting results regarding implicit acceleration in training linear neural networks, the reviewers raised quite a few questions and concerns about some claims made in the paper, as well as an inadequate comparison with previous work. Even after the author s response and reviewer discussion,  the reviewers  doubts are still not completely cleared away. I feel the current form of the paper is slightly below the bar of acceptance, and encourage the authors to carefully address reviewers  comments in the revision.
This paper proposes a unified way of data augmentation using a latent embedding space   it learns a continuous latent space for transformation, and finds effective directions to traverse in this space for data augmentation. The proposed approach combines existing approaches for data augmentation, e.g., adversarial training, triplet loss, and joint training.  The paper also identifies input examples where the model had low performance and creates harder examples that help the model improve its performance. It is evaluated on multiple corresponding to text, table, time series and image modalities and outperforms SOTA except on image data.  The paper has responded to the reviewers  feedback to provide more detailed experiments with stronger baselines and also ablation studies to show the effectiveness of different components of the approach. The results can be further improved by thorough empirical comparison to other SOTA methods, and by using other loss functions (e.g.,center loss, large margin loss and other contrastive losses) as alternatives to the triplet loss proposed in the paper.  Some reviewers have pointed out that the paper is somewhat limited in it s novelty, since it combines existing off the shelf modules/losses and similar methods have been tried in the past   the novel contributions of the paper should be clearly highlighted in the revised submission.
Dear Authors,  Thank you very much for submitting this very interesting paper.  This work analyzes the effect of gradient descent training on the compositionality of the learned model. Their main argument is that GD tries to use the redundant information in the data and, as a result, it doesn t generalize well. The paper then tries to show that theoretically and empirically with some simple experiments.  There is a general consensus among all the reviewers that this paper is not suitable for publication at ICLR. The authors do not entirely address most of the concerns raised by the reviewers during the rebuttal.   If the authors improve the clarity of the paper, making some of the propositions and theories more concrete and grounded in experiments as well, I would recommend them to resubmit this paper to a different venue since the premise of the paper is important and interesting.  Some of the reasons:    The paper claims that the gradient descent can not ignore the redundant information without providing sufficient empirical results. Though the part that is not clear to me whether if it is a credit assignment or an optimization problem. I agree with R1 that it is not clear what type of new insights from the proofs.    As R1 mentions, this paper s claim seems too strong and not supported by experiments.    R2 finds part of the paper unclear and thinks that some of the paper s propositions and theories are either trivial or wrong. The rebuttal doesn t seem to be doing a good job in terms of addressing those concerns.    R4 also is confused with the paper thinks that some of the theories are incorrect.   
This paper proposes an approach for active learning in CNNs. The method computes the expected reduction in the predictive variance across a representative set of points and selects the next data point to be queried from the same set.   Pros:   The method is rather simple and seems practical.    The paper is generally well written.  Cons:   The novelty of the paper is limited, as it essentially applies a known approach to CNNs.   The performance gains presented in experiments seem rather mild, and may not justify using this method.
It is important to develop efficient training methods for BERT like models since they have been widely used in real world natural language processing tasks. The proposed approach is interesting. It speeds up BERT training via identifying lottery tickets in the early stage of training. We agree with the authors s rebuttal that autoML is not that related to the work here. Our main concern on this work  is its worse than BERT performance showed in Table 2. The performance gap is significant. Sufficiently more training steps would fill the performance gap but the proposed method may have no advantage any more over the normal training procedures. To make this work more convincing, we would like to suggest to include experiments on comparing different methods under similar prediction performance.  In addition, since the main claim of this work is for training efficiency,  it will be helpful to show the advantage of this method by directly presenting the training curves/ results of different methods.   Overall this paper is pretty much on the boundary. We encourage the authors to resubmit this work once these issues are well resolved. 
The paper explores how to effectively conduct negative sampling in learning for text retrieval. The paper shows that negative examples sampled locally are not informative, and proposes ANCE, a new learning mechanism that samples hard negative examples globally, using an asynchronously updated ANN index.  Pros • The problem studied is important. • Paper is generally clearly written. • Solid experimental results. • There is theoretical analysis.  Cons • The idea might not be so new. The contribution is mainly from its empirical part.  During rebuttal, the authors have addressed the clarity issues pointed out by the reviewers. They have also added additional experimental results.
The paper presents a framework for incorporating physics knowledge (through, potentially incomplete, differential equations) into the deep kernel learning approach of Wilson et al. The reviewers found the paper addresses an important problem and presents good results.  However, one of the main issues raised by R1 is that, although the proposed method can be applied to broader settings such as that of incomplete differential equations, there are still regimes where the comparison is not only possible but perhaps insightful. An example baseline is the work of Lorenzi and Filippone, “Constraining the Dynamics of Deep Probabilistic Models” (ICML, 2018). Another critical issue, raised by R4, is the insufficient clarity in the presentation. Many of the concerns raised by this reviewer were clarified in the discussion and I thank the authors for their engagement. However, the AC believes some of the points raised by R4 in this regard were left unaddressed in the paper and the manuscript does indeed require at least one more iteration.  The format violation concerns raised during the reviewing process did not affect the decision on this paper, as the PCs confirmed that they did not meet the bar for desk rejection and recommended to assess the paper on its technical merits.
This paper investigates various pathologies that occur when training VAE models. There was quite a bit of discussion (including "private" discussion between the reviewers) about the theory presented. Particular concerns included: For Theorem 1, while the required conditions formalise the setting in which the learned likelihoods are poor, it s unclear whether these particular conditions they are useful in practice or provided deep insight; for Theorem 2, its relevance and importance was not necessarily clear. In general the results in these two theorems are closely related to known challenges (e.g. that using the ELBO to optimise parameters may lead to bias), without necessarily providing as much new insight as one might hope.  I would note that all the reviewers included positive feedback as to the quality of the experiments, showing the impact of these pathologies on downstream tasks. However, as written much of the paper focuses on the theory — too many of the (very interesting!) figures and experimental results are relegated to the appendix.
This paper explores a foundational problem in AI around learning abstractions that allow for easier planning.  The work proposes a specific procedure for learning temporally abstract, discrete representations in which it becomes tractable to perform graph based search.  Evaluation is performed on two 2D tasks where a goal is specified visually and the system must produce the actions to achieve this target state/observation.   The reviewers were in a uncommonly tight consensus as to their evaluation of the paper (all 4).  All reviewers essentially expressed that the motivation of the paper was solid but that the domains considered were too simplistic for validation of this class of approach, especially in light of substantial previous work in the area that was not adequately captured in the baseline comparisons.  The authors did not respond to the reviewers.  My decision is to reject the paper.
This submission generated significant discussion between the reviewers; three of them ended up on the "accept" side, but one remained firmly in the "reject" camp.  The main strength of the paper is that it tackles a very hard problem: learning an unsupervised generative model (and accompanying inference model) of scene graph structures given only image data. As one reviewer mentioned, it is remarkable that the authors were able to get their system to work at all, given the seeming intractability of this problem. The work builds upon a clear line of prior work in this area, and the type of data on which it is evaluated ("toy" synthetic datasets a la CLEVR) is consistent with prior art.  Multiple reviewers brought up the "toy" nature of the dataset as a drawback to the paper, but most agreed that this is not reason to reject the paper. Rather, the paper demonstrates a convincing proof of concept that this kind of model can be built, and improvements in the elements out of which the model is composed (generative and inference networks) should improve its applicability to real world data.  Another question mark raised by multiple reviewers: could a simpler, handcrafted inference procedure work just as well or better? The authors included a new experiment against a hand coded heuristic in their rebuttal, and their method outperforms it. One reviewer noted that more careful tuning might make a heuristic perform as well as the proposed method, but it is still clear that it is not trivial to get a hand coded solution to perform well (even for this "toy" data). Another reviewer pointed out that this is one of the main attractions of variational inference methods: the ability to specific knowledge as simple generative priors rather than complex bottom up inference procedures.  One reviewer, R1, remains negative about the paper. His (it is a he; I know this reviewer) main concern is that the scene graphs used are shallow and have a simple structure, and thus (a) it s not clear what value they add, (b) a simple postprocess could reconstruct them, assuming the individual object parts could be detected, and (c) it s not clear whether the method would generalize to deeper/more complex hierarchies. He believes this calls into question the validity of the entire method.  I am sympathetic to this argument, but I think setting the bar this high may prevent progress in this field. For point (a), the authors included an image manipulation application in their rebuttal again, a proof of concept, not a directly useful tool. For point (b), the authors did compare against a hand coded inference baseline and achieved better results, so while this may be possible, it is probably not as easy as the reviewer suggests. (c) remains an open question, to me. But even if this method as presented cannot generalize to more complex scene graphs, it likely paves the way for future work that can.
The paper provides a method to train boosted decision trees to satisfy individual fairness. All of the reviews suggest that this paper is well written and gives novel techniques for solving an interesting problem. The authors have addressed most of the concerns raised by the reviewers during their response. However, the authors should follow a suggestion in the reviews and include the running time in the empirical evaluation.
The paper proposes to introduce ideas from singular theory to deep learning. All reviewers agree that the work is not yet ready for publication. The key issue seems to boil down to the fact that the paper does not propose nor verify any clearly motivated scientific hypothesis. Relatedly, the work includes many too broad or unscientific claims such as "To understand why classical measures of capacity fail to say anything meaningful about DNNs". Such statements should be given more precisely and with a proper citation.    Based on this I have to recommend rejecting the paper. At the same time, I would like to thank the Authors for submitting the work for consideration to ICLR. I hope the feedback will be useful for improving the work.
This work extends the lottery ticket hypothesis to lifelong learning and, in particular, it tackles the problem of class incremental learning. This is an important and difficult problem, and of great interest to the community. The authors considered top down and bottom up pruning strategies. The proposed approaches were validated on existing benchmarks (CIFAR10,CIFAR100, and Tiny ImageNet), reaching state of the art results, and showing that catastrophic forgetting could be alleviated. While some questions remain in terms of practical relevance, they authors showed the existence of winning tickets in the continual setting. There were concerns regarding clarity and requests for additional experiments, but all were convincingly addressed and the clarifications provided by the authors in their rebuttal further strengthened the paper.
In this paper, a data mapping method to a latent space designed for outlier detection is proposed. Outlier detection by latent space mapping has been extensively studied in the literature. Unfortunately, this paper does not fully discuss the relation of the proposed method with a large amount of existing literature and lacks novelty. 
The paper proposes a variant derivative free optimization algorithm, that belongs to the family of Evolution Strategies (ES) and zero order optimization algorithms, to train deep neural networks. The proposed Random Search Optimization (RSO) perturbs the weights via additive Gaussian noise and updates the weights only when the perturbations improve the training objective function. Unlike the existing ES and black box optimization algorithms that perturb all the weights at once, RSO perturbs and updates the weights in a coordinate descent fashion. RSO adds noise to only a subset of the weights sequentially, layer by layer, and neuron by neuron. The empirical experiments demonstrated RSO can achieve comparable performance when training small convolutional neural networks on MNIST and CIFAR 10.   The paper contains some interesting ideas. However, there are some major concerns in the current submission:  1) Novelty: there is a wealth of literature in optimization neural networks via derivative free methods. The proposed algorithm belongs to Evolution Strategies and other zero order methods, (Rechenberg & Eigen, (1973); Schmidhuber et al., (2007); Salimans et al., (2017). Unforunately, among all the rich prior works on related algorithms, only Salimans et al. (2017) is merely mentioned in the related works. Furthermore, the experiments only compared against SGD rather than any other zero order optimization algorithms.   Many ideas in Algorithm 1 was proposed in the prior ES literature:    Evaluate the weights using a pair of noise,  \deltaW and +\deltaW in Alg1 Line13 14 is known as antithetic sampling Geweke (1988), also known as mirrored sampling Brockhoff et al. (2010) in the ES literature.    Update the weights by considering whether the objective function has improved or not was proposed in Wierstra et al. (2014) that is known as fitness shaping.  Given the current submission, it is difficult to discern the contribution of the proposed method when compared to the prior works. In addition, the convergence analysis of the zero order optimization was studied in Duchi et. al. (2015) that includes the special coordinate descent version closely related to the proposed algorithm.  2) Experiments:     Although the experiments showcase the performance of sequential RSO, the x axis in Figure 4 only reported the iterations after updating the entire network. The true computational cost of the proposed RSO is the #forwardpass x #parameters, that is much more costly than the paper currently acknowledges. Also, RSO requires drawing 5000 random samples and perform forward passes on all 5000 samples for every single weight update. It will be a great addition to include the #multiplications and computation complexity of RSO and the baseline algorithms.     More importantly, the paper only compared RSO with SGD in all the experiments. It will significantly strengthen the current paper by including some of the existing ES algorithms.    In summary, the basic idea is interesting, but the current paper is not for publication and will need further development and non trivial modification.   
This paper analyses a recurrent neural network model trained to perform a simple maze task, and reports that the network exhibits multiple hallmarks of neural selectivity reported in neurophysiological recordings from the hippocampus— in particular, they find place cells which also are tuned to task relevant locations, cells which anticipate possible future paths, and a high proportion of neurons tuned to task variables.    The reviewers appreciated the interesting empirical analysis, and the demonstration that multiple such features could arise in the same neural network— to the best of my knowledge, this had not been demonstrated explicitly before. However, there were also multiple concerns, which lead to this paper beeing discussed extensively and controversially. In particular, it is not clear which features arise from which learning objective, for example, for place cells to arise, do we  just need sensory prediction, or do we need q learning? In addition, there were some points in which the tightness of the analogy between model and biology is questionable— in particular, this refers to the comprising between hippocampal recordings and the evaluation of the network.  Finally, it is also clear that  some of these observations reported in the paper are, indeed, empirical observations rather than explanations. Because of these shortcomings, there was no consensus and strong support from the reviewers for acceptance of the paper.  After extensive discussion between both the reviewers, the AC and the program chair, the final decision was to not accept the paper. We do hope that the reviews will help you in improving the study and its presentation. It clearly has potential to be a valuable contribution to the literature.  
All the reviewers shared the concerns about the novelty and the quality of the results. Comparisons with some SOTA results are missing, and the inclusion of deblurring/denosing tasks is not convincing. The authors carefully addressed these issues in the rebuttal but the reviewers didn’t change their mind afterwards. After carefully examining the results in the paper, the AC agrees with the reviewers that the improvement on image quality, if any, seems to be too small to warrant a publication. 
This paper suggests extending pre trained contextual language models to use both fine grained and coarse grained tokenizations of the sentence. A sentence is tokenized twice and then each is passed through a transformer block, with shared parameters except the embeddings. Having 2 granularities shows gains.  Pros    Easy to read paper, straightforward method   Gets experimental gains from using word/phrase combo   Evaluates on a range of tasks  Cons    Novelty is limited, since other models like SpanBERT and ZEN already explore different tokenization granularities   Improvements may come as much from the ensemble of two models as the two tokenization granularities   Number of parameters or amount of computation are increased by method, though authors do significantly address this in their revised paper.   Some over claiming of results when there are modest incremental gains on small models (the abstract sentence "outperforms the existing best performing models in almost all cases" suggests that we are going to get results of a new model outperforming the state of the art models on tasks, but really we get improvements over baseline models at the BERT base size. I believe this is fine for experiments to show the scientific value of ideas but it should not be described as it presently is in this abstract.   Gains are more for Chinese than English  On the better results for Chinese: Isn t the reason that the results are more impressive for Chinese because in Chinese the fine grained version is just single characters, but this is more fine grained than standard BERT word pieces in English, where the word pieces are already commonly words, most of which would be two or more character sequence in Chinse (whether for common words like, say "fishing" or "vault" or place names like "Mississippi"), so the fine grained Chinese here is more fine grained than the standard English wordpieces, and so not too surprisingly there are bigger gains from using the Chinese word segmenter granularity. But really this is sort of equivalent to how the original BERT authors showed that you could get gains by masking whole words not individual word pieces. And at any rate, the value of word segmentation for Chinese was already shown by Yiming Cui et al. s paper on Chinese BERT, no?  Overall the strong majority of reviewers were unconvinced that this paper was suitable for ICLR 2021. They mainly emphasized concerns of novelty, missed or unfair comparisons, concerns of extra parameters or computation, and the fact the paper is somewhat incremental. I would add to that that to the extent that this paper is primarily an examination of the value of different granularities, that feels much more like a linguistic question for an NLP conference than an ML question well suited in particular to a conference on learning representations like ICLR. That is, the choice of granularities is hand specified, and/or the grouping is done by simple n gram statistics, not by learning representations. As such, I do not think the paper should be accepted to ICLR at this time, and in general think that an NLP venue may be more appropriate for it. 
The focus of the submission is to define divergences on discrete probability measures. Particularly, the authors propose a common generalization of the well known concept of maximum mean discrepancy and kernel Stein discrepancy.  As summarized by the reviewers the submission is in a rather preliminary form: 1)The work lacks motivation. 2)Literature review (there are 4 references in total) and numerical illustrations are missing. 3)The submission lacks proper mathematical formulation/rigor. I highly recommend the authors to not submit similar draft manuscripts in the future.
The paper study to what extent languages are hard to model by a conditional language model based on information theoretic measurements.   Overall, the reviewers value the systematic and extensive controlled experiments present in the paper. However, the presentation of the paper makes it very hard to follow and reviewers all still complain that it is hard to understand the take home message of the paper.    Despite the reviewers also appreciate the authors  effort in improving the paper, submitting the revision, responding to the feedback, they still conclude that significant reorganizing and revising of the paper is needed before it can be published.   In particular, the paper may be able to improve by backing up the empirical study with some linguistic phenomena or by a more careful rewriting in explaining and discussing the empirical results.   Some other strong arguments such as "Our application of statistical comparisons as a fairness measure also serves as a novel rigorous method for the intrinsic evaluation of languages, resolving a decades long debate on language complexity." may need to be carefully revised. In this particular example, it is unclear how this paper "resolve" the debate on language complexity by demonstrating a few experiments.  Several sentences like this one should be revised.    
This paper proposes a GAN for video generation based on stagewise training over different resolutions, addressing scalability issues with previous approaches. Reviewers noted that the paper is clearly written, proposes a method that improves upon the DVD GAN architecture by reducing training time and memory consumption, and has competitive quantitative results.  On the other hand, the more negative reviewers are concerned that the empirical improvements demonstrated are somewhat incremental, and that there is not much novelty as the proposed approach is similar to other methods that decompose the generation process into multiple stages at different temporal window lengths and/or spatial resolutions. The authors argue that these criticisms are subjective and non actionable. I sympathize with their frustration, but an acceptance decision for a competitive conference like ICLR does involve some subjective judgment as to whether the method and/or results meet a high bar beyond mere correctness. For this submission that s a close call, but between the novelty/incrementality concerns and the other more minor issues raised by reviewers (e.g., missing frame conditional evaluation) I believe this paper could benefit from another round of revisions and improvements and recommend rejection.  I hope the authors will consider improving the submission based on the reviewers  feedback and resubmitting to a future venue, as the paper certainly has merit. To this end I have a few concrete recommendations for the authors which could have flipped my recommendation to an accept if implemented:  * Report results in the frame conditional setting for comparison with DVD GAN and other methods that operate in this setting. * Proofread the paper more thoroughly. I noticed several typos while skimming the paper, e.g. in the theory section, the second term of eq. 6 confusingly uses $\rho$ instead of $\log$. (Relatedly, given that appendix B.1 reports that the hinge loss is used, I m not sure whether $\log$ is correct in the first place   this probably deserves further explanation or correction.) * Demonstrate/argue more convincingly (in one way or another) that SSW GAN s improved efficiency really expands the frontier of what was possible before. It is true that the 128x128/100 video samples contain 2x as many total pixels as DVD GAN s 256x256/12 samples, but this isn t a *strict* improvement as the spatial resolution is smaller, and a 2x difference leaves space for reviewers to reasonably wonder whether previous methods really couldn t have matched this if pushed. Some possible examples of this: show that SSW GAN can generate longer 256x256 videos (a strict improvement over what was possible with DVD GAN), or orders of magnitude longer (e.g., 1 minute) but still temporally coherent videos at 128x128, or videos with substantially improved subjective sample quality at the same (or higher) resolution. * The paper notes that "DVD GAN models do not unroll well and tend to produce samples that become motionless past its training horizon". If this were quantified, e.g. by additionally reporting IS/FID/FVD separately for different timestep ranges, it could make a more compelling argument in favor of SSW GAN.